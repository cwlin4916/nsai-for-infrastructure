# file to implent a simple game we can test alpha zero on at a very basic level


import gymnasium as gym
from gymnasium import error, spaces, utils
from gymnasium.utils import seeding
import numpy as np



class NetGameEnv(gym.Env):
    metadata = {'render.modes': ['human']}

    def __init__(self, nsites=10):
        """
        Every environment should be derived from gym.Env and at least contain the variables observation_space and action_space 
        specifying the type of possible observations and actions using spaces.Box or spaces.Discrete.

        Example:
        >>> EnvTest = FooEnv()
        >>> EnvTest.observation_space=spaces.Box(low=-1, high=1, shape=(3,4))
        >>> EnvTest.action_space=spaces.Discrete(2)
        """
        self.bitflipmode = True  # "setting" a 1 flips it  back to 0
        self.sparsemode = False  # score is only given at end of (fixed length?) episode

        self.nsites = nsites
        self.nones = 2
        self.maxsteps = 2 * nsites if not self.sparsemode else nsites - self.nones
        self.observation_space = spaces.MultiBinary([self.nsites]) #, seed=42)
        self.action_space = spaces.Discrete(self.nsites)
        self.reset()

    def step(self, action):
        """
        This method is the primary interface between environment and agent.

        Paramters: 
            action: int
                    the index of the respective action (if action space is discrete)

        Returns:
            output: (array, float, bool, dict)
                    information provided by the environment about its current state:
                    (observation, reward, done, truncated, info)
        """
#        print ("prestep state <a, s, s(a)>", action, self.state, self.state[action])
        self.step_count += 1
        done = self.step_count == self.maxsteps
        r = -1 - self.bitflipmode  # why bitflipmode affect this ?
        r = -1 

        if action == -1:  # this is being used as a flag for a bad action (e.g., generated by garbage rule)
            return self.state, r, done, False, {}

        if self.state[action] == 0:
            r = 1
        if self.bitflipmode:
            self.state[action] = 1 - self.state[action]
        else:
            self.state[action] = 1
        done = done or sum(self.state) == self.nsites

        normalizer = self.nsites # playing around with scale of pi vs value loss
        if self.sparsemode:
            r = sum(self.state)/normalizer if done else 0
        # if done:
        #     print ("Net Episode done <s, r, t, steps>", self.state, r, done, self.step_count)
        return self.state, r, done, done, {}

    def reset(self, seed=None, options=None):
        """
        This method resets the environment to its initial values.

        Returns:
            observation:    array
                            the initial state of the environment
        """
        ones = np.random.choice(range(self.nsites), self.nones, replace=False)
        self.state = np.zeros(self.nsites, dtype=np.float32)
        self.state[ones] = 1
        self.step_count = 0
        return self.state, {}

    def render(self, mode='human', close=False):
        """
        This methods provides the option to render the environment's behavior to a window 
        which should be readable to the human eye if mode is set to 'human'.
        """
        pass

    def close(self):
        """
        This method provides the user with the option to perform any necessary cleanup.
        """
        pass
    

def make_netgame(nsites=10):
    env = NetGameEnv(nsites)
    return env




