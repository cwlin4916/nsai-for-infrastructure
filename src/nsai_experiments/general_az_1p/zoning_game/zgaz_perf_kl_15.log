[gkonars@kl2 nsai_experiments] CPU $ cat zgaz.log
Neural network training params are {'epochs': 10, 'batch_size': 2048, 'learning_rate': 0.0003, 'l1_lambda': 0, 'weight_decay': 1e-05, 'value_weight': 50.0, 'policy_weight': 1.0, 'persist_optimizer': True}
Neural network training will occur on device 'cuda'
Agent config: n_games_per_train=3000, n_games_per_eval=300, n_past_iterations_to_train=10, threshold_to_keep=0.5, reward_discount=1.0, mcts_params={'n_simulations': 100, 'c_exploration': 0.5}, n_procs=None, external_policy=None, external_policy_creators_to_pit={'random': <function create_policy_random at 0x7fa5a0a23240>, 'individual greedy': <function create_policy_indiv_greedy at 0x7fa5a0a234c0>, 'total greedy': <function create_policy_total_greedy at 0x7fa5a0a236a0>}
RNG seeds are fully specified

Training iteration 1 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 28.03 seconds
Training examples lengths: [64795]
Total value: 18670.01
Training on 64795 examples
Initializing optimizer
Training with 32 batches of size 2048
Epoch 1/10, Train Loss: 6.1523 (value: 0.0514, weighted value: 2.5697, policy: 3.5827, weighted policy: 3.5827), Train Mean Max: 0.0298
Epoch 2/10, Train Loss: 4.7697 (value: 0.0238, weighted value: 1.1894, policy: 3.5803, weighted policy: 3.5803), Train Mean Max: 0.0300
Epoch 3/10, Train Loss: 4.1761 (value: 0.0120, weighted value: 0.5983, policy: 3.5778, weighted policy: 3.5778), Train Mean Max: 0.0298
Epoch 4/10, Train Loss: 4.0959 (value: 0.0104, weighted value: 0.5208, policy: 3.5751, weighted policy: 3.5751), Train Mean Max: 0.0303
Epoch 5/10, Train Loss: 4.0360 (value: 0.0093, weighted value: 0.4643, policy: 3.5718, weighted policy: 3.5718), Train Mean Max: 0.0304
Epoch 6/10, Train Loss: 3.9889 (value: 0.0084, weighted value: 0.4212, policy: 3.5677, weighted policy: 3.5677), Train Mean Max: 0.0303
Epoch 7/10, Train Loss: 3.9255 (value: 0.0073, weighted value: 0.3629, policy: 3.5626, weighted policy: 3.5626), Train Mean Max: 0.0306
Epoch 8/10, Train Loss: 3.8835 (value: 0.0065, weighted value: 0.3273, policy: 3.5562, weighted policy: 3.5562), Train Mean Max: 0.0309
Epoch 9/10, Train Loss: 3.8333 (value: 0.0057, weighted value: 0.2852, policy: 3.5481, weighted policy: 3.5481), Train Mean Max: 0.0312
Epoch 10/10, Train Loss: 3.7849 (value: 0.0049, weighted value: 0.2463, policy: 3.5386, weighted policy: 3.5386), Train Mean Max: 0.0320
..training done in 7.46 seconds
..evaluation done in 8.85 seconds
Old network+MCTS average reward: 0.30, min: -0.20, max: 1.06, stdev: 0.21
New network+MCTS average reward: 0.38, min: -0.23, max: 1.06, stdev: 0.23
Old bare network average reward: 0.26, min: -0.34, max: 0.98, stdev: 0.22
New bare network average reward: 0.26, min: -0.34, max: 0.98, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.24, max: 0.94, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: 0.05, max: 1.43, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.22, max: 1.51, stdev: 0.22
New network won 202 and tied 12 out of 300 games (69.33% wins where ties are half wins)
Keeping the new network

Training iteration 2 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 34.04 seconds
Training examples lengths: [64795, 65039]
Total value: 42723.21
Training on 129834 examples
Training with 64 batches of size 2048
Epoch 1/10, Train Loss: 4.0570 (value: 0.0108, weighted value: 0.5387, policy: 3.5183, weighted policy: 3.5183), Train Mean Max: 0.0326
Epoch 2/10, Train Loss: 3.8903 (value: 0.0083, weighted value: 0.4175, policy: 3.4728, weighted policy: 3.4728), Train Mean Max: 0.0360
Epoch 3/10, Train Loss: 3.7405 (value: 0.0068, weighted value: 0.3406, policy: 3.3999, weighted policy: 3.3999), Train Mean Max: 0.0427
Epoch 4/10, Train Loss: 3.5896 (value: 0.0060, weighted value: 0.2998, policy: 3.2898, weighted policy: 3.2898), Train Mean Max: 0.0556
Epoch 5/10, Train Loss: 3.4289 (value: 0.0055, weighted value: 0.2741, policy: 3.1548, weighted policy: 3.1548), Train Mean Max: 0.0758
Epoch 6/10, Train Loss: 3.2548 (value: 0.0048, weighted value: 0.2419, policy: 3.0129, weighted policy: 3.0129), Train Mean Max: 0.1031
Epoch 7/10, Train Loss: 3.1047 (value: 0.0044, weighted value: 0.2222, policy: 2.8826, weighted policy: 2.8826), Train Mean Max: 0.1316
Epoch 8/10, Train Loss: 3.0146 (value: 0.0049, weighted value: 0.2431, policy: 2.7716, weighted policy: 2.7716), Train Mean Max: 0.1549
Epoch 9/10, Train Loss: 2.8470 (value: 0.0036, weighted value: 0.1783, policy: 2.6687, weighted policy: 2.6687), Train Mean Max: 0.1802
Epoch 10/10, Train Loss: 2.7719 (value: 0.0038, weighted value: 0.1916, policy: 2.5803, weighted policy: 2.5803), Train Mean Max: 0.1988
..training done in 15.23 seconds
..evaluation done in 10.29 seconds
Old network+MCTS average reward: 0.37, min: -0.15, max: 0.98, stdev: 0.22
New network+MCTS average reward: 0.42, min: -0.19, max: 1.01, stdev: 0.23
Old bare network average reward: 0.27, min: -0.24, max: 0.99, stdev: 0.22
New bare network average reward: 0.27, min: -0.30, max: 0.88, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.20, max: 1.09, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: 0.09, max: 1.21, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.12, max: 1.38, stdev: 0.22
New network won 173 and tied 6 out of 300 games (58.67% wins where ties are half wins)
Keeping the new network

Training iteration 3 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 37.14 seconds
Training examples lengths: [64795, 65039, 64860]
Total value: 69055.14
Training on 194694 examples
Training with 96 batches of size 2048
Epoch 1/10, Train Loss: 2.7378 (value: 0.0077, weighted value: 0.3858, policy: 2.3520, weighted policy: 2.3520), Train Mean Max: 0.2195
Epoch 2/10, Train Loss: 2.5799 (value: 0.0065, weighted value: 0.3274, policy: 2.2525, weighted policy: 2.2525), Train Mean Max: 0.2563
Epoch 3/10, Train Loss: 2.4775 (value: 0.0056, weighted value: 0.2818, policy: 2.1957, weighted policy: 2.1957), Train Mean Max: 0.2722
Epoch 4/10, Train Loss: 2.4004 (value: 0.0049, weighted value: 0.2443, policy: 2.1562, weighted policy: 2.1562), Train Mean Max: 0.2800
Epoch 5/10, Train Loss: 2.3625 (value: 0.0047, weighted value: 0.2346, policy: 2.1279, weighted policy: 2.1279), Train Mean Max: 0.2822
Epoch 6/10, Train Loss: 2.3269 (value: 0.0044, weighted value: 0.2177, policy: 2.1092, weighted policy: 2.1092), Train Mean Max: 0.2827
Epoch 7/10, Train Loss: 2.2924 (value: 0.0040, weighted value: 0.1975, policy: 2.0949, weighted policy: 2.0949), Train Mean Max: 0.2838
Epoch 8/10, Train Loss: 2.2737 (value: 0.0038, weighted value: 0.1880, policy: 2.0857, weighted policy: 2.0857), Train Mean Max: 0.2841
Epoch 9/10, Train Loss: 2.2543 (value: 0.0035, weighted value: 0.1760, policy: 2.0783, weighted policy: 2.0783), Train Mean Max: 0.2852
Epoch 10/10, Train Loss: 2.2410 (value: 0.0033, weighted value: 0.1672, policy: 2.0739, weighted policy: 2.0739), Train Mean Max: 0.2856
..training done in 17.07 seconds
..evaluation done in 11.18 seconds
Old network+MCTS average reward: 0.39, min: -0.31, max: 1.06, stdev: 0.22
New network+MCTS average reward: 0.43, min: -0.24, max: 1.13, stdev: 0.23
Old bare network average reward: 0.26, min: -0.42, max: 1.07, stdev: 0.22
New bare network average reward: 0.27, min: -0.42, max: 0.91, stdev: 0.22
External policy "random" average reward: 0.24, min: -0.32, max: 0.88, stdev: 0.22
External policy "individual greedy" average reward: 0.52, min: -0.02, max: 1.31, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: 0.14, max: 1.31, stdev: 0.21
New network won 185 and tied 6 out of 300 games (62.67% wins where ties are half wins)
Keeping the new network

Training iteration 4 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 36.98 seconds
Training examples lengths: [64795, 65039, 64860, 64704]
Total value: 98394.36
Training on 259398 examples
Training with 127 batches of size 2048
Epoch 1/10, Train Loss: 2.3053 (value: 0.0066, weighted value: 0.3319, policy: 1.9734, weighted policy: 1.9734), Train Mean Max: 0.3004
Epoch 2/10, Train Loss: 2.2333 (value: 0.0054, weighted value: 0.2685, policy: 1.9648, weighted policy: 1.9648), Train Mean Max: 0.3128
Epoch 3/10, Train Loss: 2.2037 (value: 0.0049, weighted value: 0.2436, policy: 1.9601, weighted policy: 1.9601), Train Mean Max: 0.3179
Epoch 4/10, Train Loss: 2.1887 (value: 0.0046, weighted value: 0.2318, policy: 1.9570, weighted policy: 1.9570), Train Mean Max: 0.3202
Epoch 5/10, Train Loss: 2.1630 (value: 0.0042, weighted value: 0.2089, policy: 1.9541, weighted policy: 1.9541), Train Mean Max: 0.3225
Epoch 6/10, Train Loss: 2.1466 (value: 0.0039, weighted value: 0.1951, policy: 1.9515, weighted policy: 1.9515), Train Mean Max: 0.3241
Epoch 7/10, Train Loss: 2.1426 (value: 0.0039, weighted value: 0.1929, policy: 1.9496, weighted policy: 1.9496), Train Mean Max: 0.3252
Epoch 8/10, Train Loss: 2.1155 (value: 0.0034, weighted value: 0.1690, policy: 1.9465, weighted policy: 1.9465), Train Mean Max: 0.3265
Epoch 9/10, Train Loss: 2.1055 (value: 0.0032, weighted value: 0.1614, policy: 1.9441, weighted policy: 1.9441), Train Mean Max: 0.3275
Epoch 10/10, Train Loss: 2.0982 (value: 0.0031, weighted value: 0.1556, policy: 1.9426, weighted policy: 1.9426), Train Mean Max: 0.3280
..training done in 28.87 seconds
..evaluation done in 10.93 seconds
Old network+MCTS average reward: 0.43, min: -0.28, max: 1.26, stdev: 0.25
New network+MCTS average reward: 0.44, min: -0.24, max: 1.13, stdev: 0.25
Old bare network average reward: 0.27, min: -0.39, max: 1.04, stdev: 0.23
New bare network average reward: 0.28, min: -0.53, max: 1.14, stdev: 0.25
External policy "random" average reward: 0.26, min: -0.68, max: 1.12, stdev: 0.25
External policy "individual greedy" average reward: 0.54, min: -0.26, max: 1.32, stdev: 0.24
External policy "total greedy" average reward: 0.64, min: 0.06, max: 1.38, stdev: 0.22
New network won 168 and tied 3 out of 300 games (56.50% wins where ties are half wins)
Keeping the new network

Training iteration 5 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 38.49 seconds
Training examples lengths: [64795, 65039, 64860, 64704, 64923]
Total value: 128489.36
Training on 324321 examples
Training with 159 batches of size 2048
Epoch 1/10, Train Loss: 2.1515 (value: 0.0058, weighted value: 0.2876, policy: 1.8639, weighted policy: 1.8639), Train Mean Max: 0.3420
Epoch 2/10, Train Loss: 2.1006 (value: 0.0048, weighted value: 0.2411, policy: 1.8595, weighted policy: 1.8595), Train Mean Max: 0.3503
Epoch 3/10, Train Loss: 2.0725 (value: 0.0043, weighted value: 0.2157, policy: 1.8568, weighted policy: 1.8568), Train Mean Max: 0.3538
Epoch 4/10, Train Loss: 2.0598 (value: 0.0041, weighted value: 0.2054, policy: 1.8544, weighted policy: 1.8544), Train Mean Max: 0.3552
Epoch 5/10, Train Loss: 2.0389 (value: 0.0037, weighted value: 0.1862, policy: 1.8527, weighted policy: 1.8527), Train Mean Max: 0.3571
Epoch 6/10, Train Loss: 2.0278 (value: 0.0035, weighted value: 0.1774, policy: 1.8504, weighted policy: 1.8504), Train Mean Max: 0.3576
Epoch 7/10, Train Loss: 2.0236 (value: 0.0035, weighted value: 0.1743, policy: 1.8494, weighted policy: 1.8494), Train Mean Max: 0.3584
Epoch 8/10, Train Loss: 2.0050 (value: 0.0032, weighted value: 0.1576, policy: 1.8473, weighted policy: 1.8473), Train Mean Max: 0.3596
Epoch 9/10, Train Loss: 2.0040 (value: 0.0032, weighted value: 0.1579, policy: 1.8462, weighted policy: 1.8462), Train Mean Max: 0.3600
Epoch 10/10, Train Loss: 1.9854 (value: 0.0028, weighted value: 0.1412, policy: 1.8442, weighted policy: 1.8442), Train Mean Max: 0.3614
..training done in 28.43 seconds
..evaluation done in 11.50 seconds
Old network+MCTS average reward: 0.49, min: -0.19, max: 1.36, stdev: 0.25
New network+MCTS average reward: 0.49, min: -0.12, max: 1.38, stdev: 0.26
Old bare network average reward: 0.31, min: -0.45, max: 1.08, stdev: 0.25
New bare network average reward: 0.31, min: -0.30, max: 1.01, stdev: 0.24
External policy "random" average reward: 0.29, min: -0.19, max: 1.04, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: -0.18, max: 1.28, stdev: 0.24
External policy "total greedy" average reward: 0.67, min: 0.09, max: 1.47, stdev: 0.23
New network won 145 and tied 12 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 6 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 38.19 seconds
Training examples lengths: [64795, 65039, 64860, 64704, 64923, 64703]
Total value: 158627.66
Training on 389024 examples
Training with 190 batches of size 2048
Epoch 1/10, Train Loss: 2.0236 (value: 0.0049, weighted value: 0.2466, policy: 1.7769, weighted policy: 1.7769), Train Mean Max: 0.3736
Epoch 2/10, Train Loss: 1.9783 (value: 0.0041, weighted value: 0.2045, policy: 1.7738, weighted policy: 1.7738), Train Mean Max: 0.3809
Epoch 3/10, Train Loss: 1.9707 (value: 0.0040, weighted value: 0.1986, policy: 1.7721, weighted policy: 1.7721), Train Mean Max: 0.3823
Epoch 4/10, Train Loss: 1.9473 (value: 0.0035, weighted value: 0.1774, policy: 1.7699, weighted policy: 1.7699), Train Mean Max: 0.3845
Epoch 5/10, Train Loss: 1.9372 (value: 0.0034, weighted value: 0.1688, policy: 1.7684, weighted policy: 1.7684), Train Mean Max: 0.3852
Epoch 6/10, Train Loss: 1.9255 (value: 0.0032, weighted value: 0.1582, policy: 1.7672, weighted policy: 1.7672), Train Mean Max: 0.3865
Epoch 7/10, Train Loss: 1.9240 (value: 0.0032, weighted value: 0.1582, policy: 1.7658, weighted policy: 1.7658), Train Mean Max: 0.3868
Epoch 8/10, Train Loss: 1.9068 (value: 0.0028, weighted value: 0.1425, policy: 1.7644, weighted policy: 1.7644), Train Mean Max: 0.3881
Epoch 9/10, Train Loss: 1.9071 (value: 0.0029, weighted value: 0.1443, policy: 1.7628, weighted policy: 1.7628), Train Mean Max: 0.3885
Epoch 10/10, Train Loss: 1.8987 (value: 0.0027, weighted value: 0.1366, policy: 1.7622, weighted policy: 1.7622), Train Mean Max: 0.3890
..training done in 35.66 seconds
..evaluation done in 11.62 seconds
Old network+MCTS average reward: 0.47, min: -0.21, max: 1.10, stdev: 0.26
New network+MCTS average reward: 0.51, min: -0.10, max: 1.20, stdev: 0.25
Old bare network average reward: 0.31, min: -0.35, max: 0.90, stdev: 0.24
New bare network average reward: 0.31, min: -0.28, max: 0.95, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.44, max: 1.16, stdev: 0.24
External policy "individual greedy" average reward: 0.56, min: -0.09, max: 1.43, stdev: 0.25
External policy "total greedy" average reward: 0.68, min: -0.02, max: 1.40, stdev: 0.23
New network won 171 and tied 6 out of 300 games (58.00% wins where ties are half wins)
Keeping the new network

Training iteration 7 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 38.79 seconds
Training examples lengths: [64795, 65039, 64860, 64704, 64923, 64703, 64600]
Total value: 189710.84
Training on 453624 examples
Training with 222 batches of size 2048
Epoch 1/10, Train Loss: 1.9192 (value: 0.0043, weighted value: 0.2131, policy: 1.7061, weighted policy: 1.7061), Train Mean Max: 0.4008
Epoch 2/10, Train Loss: 1.8939 (value: 0.0038, weighted value: 0.1897, policy: 1.7042, weighted policy: 1.7042), Train Mean Max: 0.4053
Epoch 3/10, Train Loss: 1.8812 (value: 0.0036, weighted value: 0.1788, policy: 1.7024, weighted policy: 1.7024), Train Mean Max: 0.4070
Epoch 4/10, Train Loss: 1.8625 (value: 0.0032, weighted value: 0.1611, policy: 1.7014, weighted policy: 1.7014), Train Mean Max: 0.4088
Epoch 5/10, Train Loss: 1.8626 (value: 0.0032, weighted value: 0.1623, policy: 1.7003, weighted policy: 1.7003), Train Mean Max: 0.4088
Epoch 6/10, Train Loss: 1.8515 (value: 0.0030, weighted value: 0.1524, policy: 1.6992, weighted policy: 1.6992), Train Mean Max: 0.4099
Epoch 7/10, Train Loss: 1.8385 (value: 0.0028, weighted value: 0.1411, policy: 1.6974, weighted policy: 1.6974), Train Mean Max: 0.4106
Epoch 8/10, Train Loss: 1.8347 (value: 0.0028, weighted value: 0.1382, policy: 1.6965, weighted policy: 1.6965), Train Mean Max: 0.4111
Epoch 9/10, Train Loss: 1.8250 (value: 0.0026, weighted value: 0.1293, policy: 1.6957, weighted policy: 1.6957), Train Mean Max: 0.4121
Epoch 10/10, Train Loss: 1.8236 (value: 0.0026, weighted value: 0.1291, policy: 1.6945, weighted policy: 1.6945), Train Mean Max: 0.4123
..training done in 43.45 seconds
..evaluation done in 12.50 seconds
Old network+MCTS average reward: 0.46, min: -0.21, max: 1.35, stdev: 0.25
New network+MCTS average reward: 0.47, min: -0.32, max: 1.34, stdev: 0.26
Old bare network average reward: 0.28, min: -0.28, max: 1.24, stdev: 0.24
New bare network average reward: 0.29, min: -0.43, max: 1.31, stdev: 0.24
External policy "random" average reward: 0.25, min: -0.32, max: 1.22, stdev: 0.24
External policy "individual greedy" average reward: 0.52, min: -0.09, max: 1.23, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.00, max: 1.53, stdev: 0.23
New network won 147 and tied 20 out of 300 games (52.33% wins where ties are half wins)
Keeping the new network

Training iteration 8 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 38.83 seconds
Training examples lengths: [64795, 65039, 64860, 64704, 64923, 64703, 64600, 64654]
Total value: 221917.73
Training on 518278 examples
Training with 254 batches of size 2048
Epoch 1/10, Train Loss: 1.8433 (value: 0.0039, weighted value: 0.1962, policy: 1.6471, weighted policy: 1.6471), Train Mean Max: 0.4220
Epoch 2/10, Train Loss: 1.8233 (value: 0.0035, weighted value: 0.1762, policy: 1.6472, weighted policy: 1.6472), Train Mean Max: 0.4259
Epoch 3/10, Train Loss: 1.8081 (value: 0.0033, weighted value: 0.1627, policy: 1.6454, weighted policy: 1.6454), Train Mean Max: 0.4272
Epoch 4/10, Train Loss: 1.8035 (value: 0.0032, weighted value: 0.1595, policy: 1.6440, weighted policy: 1.6440), Train Mean Max: 0.4279
Epoch 5/10, Train Loss: 1.7891 (value: 0.0029, weighted value: 0.1462, policy: 1.6429, weighted policy: 1.6429), Train Mean Max: 0.4290
Epoch 6/10, Train Loss: 1.7858 (value: 0.0029, weighted value: 0.1435, policy: 1.6423, weighted policy: 1.6423), Train Mean Max: 0.4294
Epoch 7/10, Train Loss: 1.7734 (value: 0.0026, weighted value: 0.1320, policy: 1.6414, weighted policy: 1.6414), Train Mean Max: 0.4301
Epoch 8/10, Train Loss: 1.7771 (value: 0.0027, weighted value: 0.1364, policy: 1.6407, weighted policy: 1.6407), Train Mean Max: 0.4301
Epoch 9/10, Train Loss: 1.7653 (value: 0.0025, weighted value: 0.1251, policy: 1.6403, weighted policy: 1.6403), Train Mean Max: 0.4312
Epoch 10/10, Train Loss: 1.7577 (value: 0.0024, weighted value: 0.1178, policy: 1.6399, weighted policy: 1.6399), Train Mean Max: 0.4313
..training done in 49.46 seconds
..evaluation done in 12.18 seconds
Old network+MCTS average reward: 0.50, min: -0.31, max: 1.21, stdev: 0.24
New network+MCTS average reward: 0.50, min: -0.13, max: 1.35, stdev: 0.24
Old bare network average reward: 0.31, min: -0.37, max: 1.15, stdev: 0.24
New bare network average reward: 0.30, min: -0.46, max: 1.07, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.29, max: 1.08, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: -0.17, max: 1.38, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: -0.05, max: 1.34, stdev: 0.23
New network won 140 and tied 13 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 9 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 39.30 seconds
Training examples lengths: [64795, 65039, 64860, 64704, 64923, 64703, 64600, 64654, 65101]
Total value: 254471.19
Training on 583379 examples
Training with 285 batches of size 2048
Epoch 1/10, Train Loss: 1.8591 (value: 0.0050, weighted value: 0.2499, policy: 1.6092, weighted policy: 1.6092), Train Mean Max: 0.4304
Epoch 2/10, Train Loss: 1.8267 (value: 0.0044, weighted value: 0.2200, policy: 1.6067, weighted policy: 1.6067), Train Mean Max: 0.4364
Epoch 3/10, Train Loss: 1.8029 (value: 0.0039, weighted value: 0.1971, policy: 1.6057, weighted policy: 1.6057), Train Mean Max: 0.4389
Epoch 4/10, Train Loss: 1.7899 (value: 0.0037, weighted value: 0.1856, policy: 1.6044, weighted policy: 1.6044), Train Mean Max: 0.4403
Epoch 5/10, Train Loss: 1.7740 (value: 0.0034, weighted value: 0.1706, policy: 1.6034, weighted policy: 1.6034), Train Mean Max: 0.4419
Epoch 6/10, Train Loss: 1.7678 (value: 0.0033, weighted value: 0.1653, policy: 1.6024, weighted policy: 1.6024), Train Mean Max: 0.4423
Epoch 7/10, Train Loss: 1.7645 (value: 0.0033, weighted value: 0.1630, policy: 1.6015, weighted policy: 1.6015), Train Mean Max: 0.4426
Epoch 8/10, Train Loss: 1.7488 (value: 0.0030, weighted value: 0.1486, policy: 1.6002, weighted policy: 1.6002), Train Mean Max: 0.4436
Epoch 9/10, Train Loss: 1.7449 (value: 0.0029, weighted value: 0.1449, policy: 1.5999, weighted policy: 1.5999), Train Mean Max: 0.4441
Epoch 10/10, Train Loss: 1.7383 (value: 0.0028, weighted value: 0.1394, policy: 1.5989, weighted policy: 1.5989), Train Mean Max: 0.4444
..training done in 60.08 seconds
..evaluation done in 12.75 seconds
Old network+MCTS average reward: 0.52, min: -0.10, max: 1.31, stdev: 0.24
New network+MCTS average reward: 0.52, min: -0.17, max: 1.24, stdev: 0.24
Old bare network average reward: 0.33, min: -0.38, max: 0.94, stdev: 0.23
New bare network average reward: 0.33, min: -0.22, max: 1.05, stdev: 0.23
External policy "random" average reward: 0.28, min: -0.22, max: 1.12, stdev: 0.23
External policy "individual greedy" average reward: 0.56, min: -0.03, max: 1.22, stdev: 0.23
External policy "total greedy" average reward: 0.68, min: 0.10, max: 1.34, stdev: 0.23
New network won 153 and tied 11 out of 300 games (52.83% wins where ties are half wins)
Keeping the new network

Training iteration 10 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 39.34 seconds
Training examples lengths: [64795, 65039, 64860, 64704, 64923, 64703, 64600, 64654, 65101, 64821]
Total value: 287475.28
Training on 648200 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 1.7467 (value: 0.0037, weighted value: 0.1854, policy: 1.5613, weighted policy: 1.5613), Train Mean Max: 0.4524
Epoch 2/10, Train Loss: 1.7264 (value: 0.0033, weighted value: 0.1660, policy: 1.5604, weighted policy: 1.5604), Train Mean Max: 0.4554
Epoch 3/10, Train Loss: 1.7202 (value: 0.0032, weighted value: 0.1602, policy: 1.5600, weighted policy: 1.5600), Train Mean Max: 0.4562
Epoch 4/10, Train Loss: 1.7073 (value: 0.0030, weighted value: 0.1487, policy: 1.5587, weighted policy: 1.5587), Train Mean Max: 0.4571
Epoch 5/10, Train Loss: 1.7044 (value: 0.0029, weighted value: 0.1462, policy: 1.5582, weighted policy: 1.5582), Train Mean Max: 0.4575
Epoch 6/10, Train Loss: 1.7000 (value: 0.0028, weighted value: 0.1422, policy: 1.5579, weighted policy: 1.5579), Train Mean Max: 0.4578
Epoch 7/10, Train Loss: 1.6886 (value: 0.0026, weighted value: 0.1320, policy: 1.5566, weighted policy: 1.5566), Train Mean Max: 0.4585
Epoch 8/10, Train Loss: 1.6864 (value: 0.0026, weighted value: 0.1304, policy: 1.5560, weighted policy: 1.5560), Train Mean Max: 0.4585
Epoch 9/10, Train Loss: 1.6793 (value: 0.0025, weighted value: 0.1239, policy: 1.5554, weighted policy: 1.5554), Train Mean Max: 0.4591
Epoch 10/10, Train Loss: 1.6780 (value: 0.0025, weighted value: 0.1232, policy: 1.5549, weighted policy: 1.5549), Train Mean Max: 0.4591
..training done in 62.98 seconds
..evaluation done in 13.08 seconds
Old network+MCTS average reward: 0.51, min: -0.18, max: 1.08, stdev: 0.24
New network+MCTS average reward: 0.51, min: -0.30, max: 1.15, stdev: 0.24
Old bare network average reward: 0.32, min: -0.21, max: 0.89, stdev: 0.22
New bare network average reward: 0.32, min: -0.38, max: 0.88, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.31, max: 0.93, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: 0.03, max: 1.14, stdev: 0.21
External policy "total greedy" average reward: 0.66, min: 0.18, max: 1.22, stdev: 0.21
New network won 138 and tied 24 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 11 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 40.49 seconds
Training examples lengths: [65039, 64860, 64704, 64923, 64703, 64600, 64654, 65101, 64821, 65024]
Total value: 300690.08
Training on 648429 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 1.5713 (value: 0.0035, weighted value: 0.1752, policy: 1.3961, weighted policy: 1.3961), Train Mean Max: 0.4957
Epoch 2/10, Train Loss: 1.5486 (value: 0.0031, weighted value: 0.1561, policy: 1.3924, weighted policy: 1.3924), Train Mean Max: 0.5040
Epoch 3/10, Train Loss: 1.5329 (value: 0.0028, weighted value: 0.1424, policy: 1.3904, weighted policy: 1.3904), Train Mean Max: 0.5071
Epoch 4/10, Train Loss: 1.5258 (value: 0.0027, weighted value: 0.1364, policy: 1.3893, weighted policy: 1.3893), Train Mean Max: 0.5085
Epoch 5/10, Train Loss: 1.5216 (value: 0.0027, weighted value: 0.1332, policy: 1.3884, weighted policy: 1.3884), Train Mean Max: 0.5093
Epoch 6/10, Train Loss: 1.5165 (value: 0.0026, weighted value: 0.1293, policy: 1.3872, weighted policy: 1.3872), Train Mean Max: 0.5097
Epoch 7/10, Train Loss: 1.5087 (value: 0.0024, weighted value: 0.1219, policy: 1.3868, weighted policy: 1.3868), Train Mean Max: 0.5106
Epoch 8/10, Train Loss: 1.5014 (value: 0.0023, weighted value: 0.1158, policy: 1.3856, weighted policy: 1.3856), Train Mean Max: 0.5112
Epoch 9/10, Train Loss: 1.5023 (value: 0.0023, weighted value: 0.1174, policy: 1.3849, weighted policy: 1.3849), Train Mean Max: 0.5112
Epoch 10/10, Train Loss: 1.4938 (value: 0.0022, weighted value: 0.1095, policy: 1.3843, weighted policy: 1.3843), Train Mean Max: 0.5119
..training done in 71.17 seconds
..evaluation done in 14.03 seconds
Old network+MCTS average reward: 0.49, min: -0.07, max: 1.14, stdev: 0.23
New network+MCTS average reward: 0.50, min: -0.12, max: 1.19, stdev: 0.22
Old bare network average reward: 0.31, min: -0.26, max: 0.94, stdev: 0.23
New bare network average reward: 0.33, min: -0.21, max: 0.94, stdev: 0.23
External policy "random" average reward: 0.28, min: -0.28, max: 0.89, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: 0.00, max: 1.17, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.14, max: 1.26, stdev: 0.22
New network won 154 and tied 10 out of 300 games (53.00% wins where ties are half wins)
Keeping the new network

Training iteration 12 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 40.67 seconds
Training examples lengths: [64860, 64704, 64923, 64703, 64600, 64654, 65101, 64821, 65024, 64996]
Total value: 309393.99
Training on 648386 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 1.4392 (value: 0.0032, weighted value: 0.1619, policy: 1.2773, weighted policy: 1.2773), Train Mean Max: 0.5349
Epoch 2/10, Train Loss: 1.4165 (value: 0.0028, weighted value: 0.1415, policy: 1.2751, weighted policy: 1.2751), Train Mean Max: 0.5417
Epoch 3/10, Train Loss: 1.4069 (value: 0.0027, weighted value: 0.1333, policy: 1.2736, weighted policy: 1.2736), Train Mean Max: 0.5439
Epoch 4/10, Train Loss: 1.3993 (value: 0.0025, weighted value: 0.1262, policy: 1.2731, weighted policy: 1.2731), Train Mean Max: 0.5452
Epoch 5/10, Train Loss: 1.3935 (value: 0.0024, weighted value: 0.1216, policy: 1.2719, weighted policy: 1.2719), Train Mean Max: 0.5461
Epoch 6/10, Train Loss: 1.3855 (value: 0.0023, weighted value: 0.1149, policy: 1.2706, weighted policy: 1.2706), Train Mean Max: 0.5467
Epoch 7/10, Train Loss: 1.3829 (value: 0.0023, weighted value: 0.1128, policy: 1.2701, weighted policy: 1.2701), Train Mean Max: 0.5471
Epoch 8/10, Train Loss: 1.3779 (value: 0.0022, weighted value: 0.1083, policy: 1.2696, weighted policy: 1.2696), Train Mean Max: 0.5476
Epoch 9/10, Train Loss: 1.3776 (value: 0.0022, weighted value: 0.1085, policy: 1.2691, weighted policy: 1.2691), Train Mean Max: 0.5477
Epoch 10/10, Train Loss: 1.3695 (value: 0.0020, weighted value: 0.1016, policy: 1.2679, weighted policy: 1.2679), Train Mean Max: 0.5481
..training done in 70.10 seconds
..evaluation done in 13.77 seconds
Old network+MCTS average reward: 0.50, min: -0.21, max: 1.26, stdev: 0.24
New network+MCTS average reward: 0.51, min: -0.08, max: 1.31, stdev: 0.24
Old bare network average reward: 0.31, min: -0.32, max: 1.11, stdev: 0.24
New bare network average reward: 0.32, min: -0.24, max: 1.09, stdev: 0.24
External policy "random" average reward: 0.25, min: -0.37, max: 1.10, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: -0.02, max: 1.60, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: -0.08, max: 1.63, stdev: 0.23
New network won 146 and tied 22 out of 300 games (52.33% wins where ties are half wins)
Keeping the new network

Training iteration 13 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 40.45 seconds
Training examples lengths: [64704, 64923, 64703, 64600, 64654, 65101, 64821, 65024, 64996, 64921]
Total value: 316400.86
Training on 648447 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 1.3554 (value: 0.0030, weighted value: 0.1503, policy: 1.2050, weighted policy: 1.2050), Train Mean Max: 0.5601
Epoch 2/10, Train Loss: 1.3362 (value: 0.0027, weighted value: 0.1329, policy: 1.2033, weighted policy: 1.2033), Train Mean Max: 0.5648
Epoch 3/10, Train Loss: 1.3284 (value: 0.0025, weighted value: 0.1263, policy: 1.2021, weighted policy: 1.2021), Train Mean Max: 0.5664
Epoch 4/10, Train Loss: 1.3208 (value: 0.0024, weighted value: 0.1192, policy: 1.2016, weighted policy: 1.2016), Train Mean Max: 0.5676
Epoch 5/10, Train Loss: 1.3118 (value: 0.0022, weighted value: 0.1114, policy: 1.2004, weighted policy: 1.2004), Train Mean Max: 0.5687
Epoch 6/10, Train Loss: 1.3156 (value: 0.0023, weighted value: 0.1155, policy: 1.2001, weighted policy: 1.2001), Train Mean Max: 0.5687
Epoch 7/10, Train Loss: 1.3111 (value: 0.0022, weighted value: 0.1113, policy: 1.1998, weighted policy: 1.1998), Train Mean Max: 0.5693
Epoch 8/10, Train Loss: 1.3015 (value: 0.0021, weighted value: 0.1029, policy: 1.1986, weighted policy: 1.1986), Train Mean Max: 0.5699
Epoch 9/10, Train Loss: 1.2962 (value: 0.0020, weighted value: 0.0978, policy: 1.1984, weighted policy: 1.1984), Train Mean Max: 0.5701
Epoch 10/10, Train Loss: 1.2946 (value: 0.0019, weighted value: 0.0965, policy: 1.1980, weighted policy: 1.1980), Train Mean Max: 0.5706
..training done in 68.28 seconds
..evaluation done in 13.89 seconds
Old network+MCTS average reward: 0.50, min: -0.20, max: 1.27, stdev: 0.24
New network+MCTS average reward: 0.52, min: -0.27, max: 1.21, stdev: 0.24
Old bare network average reward: 0.33, min: -0.26, max: 1.10, stdev: 0.24
New bare network average reward: 0.33, min: -0.28, max: 0.92, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.29, max: 1.06, stdev: 0.22
External policy "individual greedy" average reward: 0.56, min: -0.06, max: 1.28, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: -0.02, max: 1.23, stdev: 0.22
New network won 142 and tied 27 out of 300 games (51.83% wins where ties are half wins)
Keeping the new network

Training iteration 14 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 40.42 seconds
Training examples lengths: [64923, 64703, 64600, 64654, 65101, 64821, 65024, 64996, 64921, 64566]
Total value: 320249.66
Training on 648309 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 1.2852 (value: 0.0028, weighted value: 0.1421, policy: 1.1431, weighted policy: 1.1431), Train Mean Max: 0.5805
Epoch 2/10, Train Loss: 1.2649 (value: 0.0025, weighted value: 0.1231, policy: 1.1418, weighted policy: 1.1418), Train Mean Max: 0.5848
Epoch 3/10, Train Loss: 1.2613 (value: 0.0024, weighted value: 0.1205, policy: 1.1409, weighted policy: 1.1409), Train Mean Max: 0.5861
Epoch 4/10, Train Loss: 1.2501 (value: 0.0022, weighted value: 0.1103, policy: 1.1398, weighted policy: 1.1398), Train Mean Max: 0.5875
Epoch 5/10, Train Loss: 1.2463 (value: 0.0021, weighted value: 0.1072, policy: 1.1391, weighted policy: 1.1391), Train Mean Max: 0.5882
Epoch 6/10, Train Loss: 1.2407 (value: 0.0020, weighted value: 0.1022, policy: 1.1385, weighted policy: 1.1385), Train Mean Max: 0.5888
Epoch 7/10, Train Loss: 1.2366 (value: 0.0020, weighted value: 0.0987, policy: 1.1379, weighted policy: 1.1379), Train Mean Max: 0.5893
Epoch 8/10, Train Loss: 1.2342 (value: 0.0019, weighted value: 0.0967, policy: 1.1375, weighted policy: 1.1375), Train Mean Max: 0.5896
Epoch 9/10, Train Loss: 1.2281 (value: 0.0018, weighted value: 0.0909, policy: 1.1372, weighted policy: 1.1372), Train Mean Max: 0.5902
Epoch 10/10, Train Loss: 1.2263 (value: 0.0018, weighted value: 0.0900, policy: 1.1363, weighted policy: 1.1363), Train Mean Max: 0.5903
..training done in 65.32 seconds
..evaluation done in 13.67 seconds
Old network+MCTS average reward: 0.50, min: -0.25, max: 1.11, stdev: 0.22
New network+MCTS average reward: 0.50, min: -0.19, max: 1.11, stdev: 0.22
Old bare network average reward: 0.32, min: -0.24, max: 1.04, stdev: 0.22
New bare network average reward: 0.33, min: -0.37, max: 1.04, stdev: 0.23
External policy "random" average reward: 0.24, min: -0.34, max: 0.88, stdev: 0.21
External policy "individual greedy" average reward: 0.52, min: -0.05, max: 1.26, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: 0.16, max: 1.26, stdev: 0.21
New network won 134 and tied 26 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 15 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 41.93 seconds
Training examples lengths: [64703, 64600, 64654, 65101, 64821, 65024, 64996, 64921, 64566, 64747]
Total value: 323838.01
Training on 648133 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 1.2853 (value: 0.0037, weighted value: 0.1867, policy: 1.0986, weighted policy: 1.0986), Train Mean Max: 0.5879
Epoch 2/10, Train Loss: 1.2523 (value: 0.0031, weighted value: 0.1567, policy: 1.0956, weighted policy: 1.0956), Train Mean Max: 0.5954
Epoch 3/10, Train Loss: 1.2354 (value: 0.0028, weighted value: 0.1409, policy: 1.0945, weighted policy: 1.0945), Train Mean Max: 0.5987
Epoch 4/10, Train Loss: 1.2301 (value: 0.0027, weighted value: 0.1364, policy: 1.0937, weighted policy: 1.0937), Train Mean Max: 0.6001
Epoch 5/10, Train Loss: 1.2181 (value: 0.0025, weighted value: 0.1257, policy: 1.0925, weighted policy: 1.0925), Train Mean Max: 0.6018
Epoch 6/10, Train Loss: 1.2097 (value: 0.0024, weighted value: 0.1180, policy: 1.0917, weighted policy: 1.0917), Train Mean Max: 0.6028
Epoch 7/10, Train Loss: 1.2034 (value: 0.0023, weighted value: 0.1125, policy: 1.0909, weighted policy: 1.0909), Train Mean Max: 0.6037
Epoch 8/10, Train Loss: 1.2020 (value: 0.0022, weighted value: 0.1118, policy: 1.0902, weighted policy: 1.0902), Train Mean Max: 0.6041
Epoch 9/10, Train Loss: 1.1928 (value: 0.0021, weighted value: 0.1032, policy: 1.0896, weighted policy: 1.0896), Train Mean Max: 0.6049
Epoch 10/10, Train Loss: 1.1898 (value: 0.0020, weighted value: 0.1011, policy: 1.0888, weighted policy: 1.0888), Train Mean Max: 0.6053
..training done in 64.85 seconds
..evaluation done in 12.83 seconds
Old network+MCTS average reward: 0.52, min: -0.09, max: 1.32, stdev: 0.24
New network+MCTS average reward: 0.53, min: -0.04, max: 1.27, stdev: 0.24
Old bare network average reward: 0.35, min: -0.29, max: 1.08, stdev: 0.24
New bare network average reward: 0.36, min: -0.31, max: 0.97, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.55, max: 0.93, stdev: 0.24
External policy "individual greedy" average reward: 0.55, min: -0.14, max: 1.18, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.07, max: 1.22, stdev: 0.22
New network won 139 and tied 24 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 16 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 40.32 seconds
Training examples lengths: [64600, 64654, 65101, 64821, 65024, 64996, 64921, 64566, 64747, 64617]
Total value: 327975.46
Training on 648047 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 1.1866 (value: 0.0028, weighted value: 0.1395, policy: 1.0471, weighted policy: 1.0471), Train Mean Max: 0.6126
Epoch 2/10, Train Loss: 1.1718 (value: 0.0025, weighted value: 0.1264, policy: 1.0454, weighted policy: 1.0454), Train Mean Max: 0.6158
Epoch 3/10, Train Loss: 1.1625 (value: 0.0024, weighted value: 0.1177, policy: 1.0448, weighted policy: 1.0448), Train Mean Max: 0.6174
Epoch 4/10, Train Loss: 1.1552 (value: 0.0022, weighted value: 0.1114, policy: 1.0438, weighted policy: 1.0438), Train Mean Max: 0.6186
Epoch 5/10, Train Loss: 1.1483 (value: 0.0021, weighted value: 0.1050, policy: 1.0433, weighted policy: 1.0433), Train Mean Max: 0.6196
Epoch 6/10, Train Loss: 1.1418 (value: 0.0020, weighted value: 0.0994, policy: 1.0424, weighted policy: 1.0424), Train Mean Max: 0.6201
Epoch 7/10, Train Loss: 1.1403 (value: 0.0020, weighted value: 0.0985, policy: 1.0418, weighted policy: 1.0418), Train Mean Max: 0.6207
Epoch 8/10, Train Loss: 1.1350 (value: 0.0019, weighted value: 0.0938, policy: 1.0411, weighted policy: 1.0411), Train Mean Max: 0.6213
Epoch 9/10, Train Loss: 1.1346 (value: 0.0019, weighted value: 0.0939, policy: 1.0407, weighted policy: 1.0407), Train Mean Max: 0.6214
Epoch 10/10, Train Loss: 1.1279 (value: 0.0018, weighted value: 0.0879, policy: 1.0400, weighted policy: 1.0400), Train Mean Max: 0.6218
..training done in 64.99 seconds
..evaluation done in 14.80 seconds
Old network+MCTS average reward: 0.53, min: -0.14, max: 1.25, stdev: 0.24
New network+MCTS average reward: 0.53, min: -0.14, max: 1.27, stdev: 0.23
Old bare network average reward: 0.35, min: -0.42, max: 1.00, stdev: 0.23
New bare network average reward: 0.35, min: -0.38, max: 1.18, stdev: 0.22
External policy "random" average reward: 0.27, min: -0.43, max: 0.86, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: -0.12, max: 1.23, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.17, max: 1.43, stdev: 0.21
New network won 133 and tied 34 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 17 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 41.30 seconds
Training examples lengths: [64654, 65101, 64821, 65024, 64996, 64921, 64566, 64747, 64617, 64661]
Total value: 331358.00
Training on 648108 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 1.1319 (value: 0.0026, weighted value: 0.1323, policy: 0.9996, weighted policy: 0.9996), Train Mean Max: 0.6287
Epoch 2/10, Train Loss: 1.1133 (value: 0.0023, weighted value: 0.1152, policy: 0.9981, weighted policy: 0.9981), Train Mean Max: 0.6319
Epoch 3/10, Train Loss: 1.1035 (value: 0.0021, weighted value: 0.1061, policy: 0.9975, weighted policy: 0.9975), Train Mean Max: 0.6338
Epoch 4/10, Train Loss: 1.1010 (value: 0.0021, weighted value: 0.1046, policy: 0.9964, weighted policy: 0.9964), Train Mean Max: 0.6344
Epoch 5/10, Train Loss: 1.0923 (value: 0.0019, weighted value: 0.0965, policy: 0.9958, weighted policy: 0.9958), Train Mean Max: 0.6355
Epoch 6/10, Train Loss: 1.0890 (value: 0.0019, weighted value: 0.0937, policy: 0.9953, weighted policy: 0.9953), Train Mean Max: 0.6359
Epoch 7/10, Train Loss: 1.0873 (value: 0.0018, weighted value: 0.0923, policy: 0.9950, weighted policy: 0.9950), Train Mean Max: 0.6366
Epoch 8/10, Train Loss: 1.0824 (value: 0.0018, weighted value: 0.0882, policy: 0.9942, weighted policy: 0.9942), Train Mean Max: 0.6369
Epoch 9/10, Train Loss: 1.0779 (value: 0.0017, weighted value: 0.0843, policy: 0.9936, weighted policy: 0.9936), Train Mean Max: 0.6374
Epoch 10/10, Train Loss: 1.0754 (value: 0.0017, weighted value: 0.0826, policy: 0.9928, weighted policy: 0.9928), Train Mean Max: 0.6376
..training done in 67.94 seconds
..evaluation done in 14.13 seconds
Old network+MCTS average reward: 0.54, min: -0.06, max: 1.05, stdev: 0.23
New network+MCTS average reward: 0.55, min: -0.21, max: 1.13, stdev: 0.23
Old bare network average reward: 0.37, min: -0.24, max: 1.15, stdev: 0.22
New bare network average reward: 0.38, min: -0.18, max: 0.94, stdev: 0.22
External policy "random" average reward: 0.26, min: -0.37, max: 0.89, stdev: 0.21
External policy "individual greedy" average reward: 0.55, min: -0.06, max: 1.27, stdev: 0.21
External policy "total greedy" average reward: 0.68, min: 0.16, max: 1.26, stdev: 0.20
New network won 133 and tied 48 out of 300 games (52.33% wins where ties are half wins)
Keeping the new network

Training iteration 18 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 42.24 seconds
Training examples lengths: [65101, 64821, 65024, 64996, 64921, 64566, 64747, 64617, 64661, 64869]
Total value: 333941.31
Training on 648323 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 1.0799 (value: 0.0025, weighted value: 0.1254, policy: 0.9545, weighted policy: 0.9545), Train Mean Max: 0.6439
Epoch 2/10, Train Loss: 1.0648 (value: 0.0022, weighted value: 0.1118, policy: 0.9530, weighted policy: 0.9530), Train Mean Max: 0.6469
Epoch 3/10, Train Loss: 1.0553 (value: 0.0021, weighted value: 0.1032, policy: 0.9521, weighted policy: 0.9521), Train Mean Max: 0.6484
Epoch 4/10, Train Loss: 1.0474 (value: 0.0019, weighted value: 0.0958, policy: 0.9516, weighted policy: 0.9516), Train Mean Max: 0.6498
Epoch 5/10, Train Loss: 1.0446 (value: 0.0019, weighted value: 0.0939, policy: 0.9507, weighted policy: 0.9507), Train Mean Max: 0.6503
Epoch 6/10, Train Loss: 1.0407 (value: 0.0018, weighted value: 0.0902, policy: 0.9504, weighted policy: 0.9504), Train Mean Max: 0.6510
Epoch 7/10, Train Loss: 1.0367 (value: 0.0017, weighted value: 0.0869, policy: 0.9498, weighted policy: 0.9498), Train Mean Max: 0.6514
Epoch 8/10, Train Loss: 1.0314 (value: 0.0017, weighted value: 0.0826, policy: 0.9488, weighted policy: 0.9488), Train Mean Max: 0.6520
Epoch 9/10, Train Loss: 1.0306 (value: 0.0016, weighted value: 0.0824, policy: 0.9482, weighted policy: 0.9482), Train Mean Max: 0.6522
Epoch 10/10, Train Loss: 1.0282 (value: 0.0016, weighted value: 0.0805, policy: 0.9477, weighted policy: 0.9477), Train Mean Max: 0.6524
..training done in 69.62 seconds
..evaluation done in 14.07 seconds
Old network+MCTS average reward: 0.53, min: -0.13, max: 1.18, stdev: 0.24
New network+MCTS average reward: 0.53, min: -0.26, max: 1.27, stdev: 0.25
Old bare network average reward: 0.36, min: -0.28, max: 1.08, stdev: 0.25
New bare network average reward: 0.36, min: -0.30, max: 1.06, stdev: 0.25
External policy "random" average reward: 0.25, min: -0.42, max: 0.96, stdev: 0.23
External policy "individual greedy" average reward: 0.52, min: -0.20, max: 1.23, stdev: 0.24
External policy "total greedy" average reward: 0.64, min: 0.01, max: 1.26, stdev: 0.23
New network won 133 and tied 36 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 19 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 41.62 seconds
Training examples lengths: [64821, 65024, 64996, 64921, 64566, 64747, 64617, 64661, 64869, 64438]
Total value: 335837.96
Training on 647660 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 1.0280 (value: 0.0024, weighted value: 0.1218, policy: 0.9062, weighted policy: 0.9062), Train Mean Max: 0.6591
Epoch 2/10, Train Loss: 1.0107 (value: 0.0021, weighted value: 0.1057, policy: 0.9050, weighted policy: 0.9050), Train Mean Max: 0.6626
Epoch 3/10, Train Loss: 1.0032 (value: 0.0020, weighted value: 0.0993, policy: 0.9039, weighted policy: 0.9039), Train Mean Max: 0.6640
Epoch 4/10, Train Loss: 0.9987 (value: 0.0019, weighted value: 0.0954, policy: 0.9032, weighted policy: 0.9032), Train Mean Max: 0.6652
Epoch 5/10, Train Loss: 0.9906 (value: 0.0018, weighted value: 0.0881, policy: 0.9025, weighted policy: 0.9025), Train Mean Max: 0.6661
Epoch 6/10, Train Loss: 0.9901 (value: 0.0018, weighted value: 0.0880, policy: 0.9021, weighted policy: 0.9021), Train Mean Max: 0.6666
Epoch 7/10, Train Loss: 0.9854 (value: 0.0017, weighted value: 0.0840, policy: 0.9015, weighted policy: 0.9015), Train Mean Max: 0.6671
Epoch 8/10, Train Loss: 0.9813 (value: 0.0016, weighted value: 0.0805, policy: 0.9007, weighted policy: 0.9007), Train Mean Max: 0.6677
Epoch 9/10, Train Loss: 0.9796 (value: 0.0016, weighted value: 0.0793, policy: 0.9003, weighted policy: 0.9003), Train Mean Max: 0.6681
Epoch 10/10, Train Loss: 0.9764 (value: 0.0015, weighted value: 0.0769, policy: 0.8995, weighted policy: 0.8995), Train Mean Max: 0.6684
..training done in 65.03 seconds
..evaluation done in 14.49 seconds
Old network+MCTS average reward: 0.51, min: -0.21, max: 1.27, stdev: 0.24
New network+MCTS average reward: 0.52, min: -0.37, max: 1.37, stdev: 0.24
Old bare network average reward: 0.35, min: -0.33, max: 1.24, stdev: 0.25
New bare network average reward: 0.35, min: -0.32, max: 1.09, stdev: 0.24
External policy "random" average reward: 0.25, min: -0.21, max: 0.94, stdev: 0.21
External policy "individual greedy" average reward: 0.51, min: -0.03, max: 1.29, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: -0.06, max: 1.31, stdev: 0.22
New network won 133 and tied 32 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 20 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 41.69 seconds
Training examples lengths: [65024, 64996, 64921, 64566, 64747, 64617, 64661, 64869, 64438, 64737]
Total value: 338012.69
Training on 647576 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 1.0406 (value: 0.0034, weighted value: 0.1685, policy: 0.8721, weighted policy: 0.8721), Train Mean Max: 0.6638
Epoch 2/10, Train Loss: 1.0062 (value: 0.0027, weighted value: 0.1368, policy: 0.8693, weighted policy: 0.8693), Train Mean Max: 0.6702
Epoch 3/10, Train Loss: 0.9928 (value: 0.0025, weighted value: 0.1244, policy: 0.8684, weighted policy: 0.8684), Train Mean Max: 0.6730
Epoch 4/10, Train Loss: 0.9826 (value: 0.0023, weighted value: 0.1154, policy: 0.8672, weighted policy: 0.8672), Train Mean Max: 0.6751
Epoch 5/10, Train Loss: 0.9765 (value: 0.0022, weighted value: 0.1100, policy: 0.8664, weighted policy: 0.8664), Train Mean Max: 0.6764
Epoch 6/10, Train Loss: 0.9668 (value: 0.0020, weighted value: 0.1022, policy: 0.8646, weighted policy: 0.8646), Train Mean Max: 0.6777
Epoch 7/10, Train Loss: 0.9620 (value: 0.0020, weighted value: 0.0979, policy: 0.8641, weighted policy: 0.8641), Train Mean Max: 0.6785
Epoch 8/10, Train Loss: 0.9578 (value: 0.0019, weighted value: 0.0944, policy: 0.8634, weighted policy: 0.8634), Train Mean Max: 0.6792
Epoch 9/10, Train Loss: 0.9534 (value: 0.0018, weighted value: 0.0910, policy: 0.8624, weighted policy: 0.8624), Train Mean Max: 0.6797
Epoch 10/10, Train Loss: 0.9491 (value: 0.0017, weighted value: 0.0871, policy: 0.8620, weighted policy: 0.8620), Train Mean Max: 0.6802
..training done in 70.02 seconds
..evaluation done in 14.58 seconds
Old network+MCTS average reward: 0.54, min: -0.06, max: 1.44, stdev: 0.22
New network+MCTS average reward: 0.54, min: -0.08, max: 1.38, stdev: 0.23
Old bare network average reward: 0.39, min: -0.13, max: 1.14, stdev: 0.23
New bare network average reward: 0.39, min: -0.18, max: 1.10, stdev: 0.23
External policy "random" average reward: 0.28, min: -0.31, max: 1.01, stdev: 0.21
External policy "individual greedy" average reward: 0.55, min: -0.06, max: 1.67, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.08, max: 1.73, stdev: 0.23
New network won 125 and tied 51 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to zgaz_checkpoints/1757654225_iter_20

Training iteration 21 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 42.86 seconds
Training examples lengths: [64996, 64921, 64566, 64747, 64617, 64661, 64869, 64438, 64737, 64808]
Total value: 342015.28
Training on 647360 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.9509 (value: 0.0025, weighted value: 0.1273, policy: 0.8236, weighted policy: 0.8236), Train Mean Max: 0.6864
Epoch 2/10, Train Loss: 0.9349 (value: 0.0022, weighted value: 0.1119, policy: 0.8230, weighted policy: 0.8230), Train Mean Max: 0.6896
Epoch 3/10, Train Loss: 0.9290 (value: 0.0021, weighted value: 0.1073, policy: 0.8216, weighted policy: 0.8216), Train Mean Max: 0.6909
Epoch 4/10, Train Loss: 0.9182 (value: 0.0019, weighted value: 0.0974, policy: 0.8208, weighted policy: 0.8208), Train Mean Max: 0.6924
Epoch 5/10, Train Loss: 0.9120 (value: 0.0018, weighted value: 0.0918, policy: 0.8202, weighted policy: 0.8202), Train Mean Max: 0.6933
Epoch 6/10, Train Loss: 0.9099 (value: 0.0018, weighted value: 0.0909, policy: 0.8190, weighted policy: 0.8190), Train Mean Max: 0.6940
Epoch 7/10, Train Loss: 0.9047 (value: 0.0017, weighted value: 0.0855, policy: 0.8192, weighted policy: 0.8192), Train Mean Max: 0.6948
Epoch 8/10, Train Loss: 0.9024 (value: 0.0017, weighted value: 0.0839, policy: 0.8185, weighted policy: 0.8185), Train Mean Max: 0.6951
Epoch 9/10, Train Loss: 0.8983 (value: 0.0016, weighted value: 0.0803, policy: 0.8179, weighted policy: 0.8179), Train Mean Max: 0.6956
Epoch 10/10, Train Loss: 0.8967 (value: 0.0016, weighted value: 0.0795, policy: 0.8172, weighted policy: 0.8172), Train Mean Max: 0.6960
..training done in 65.59 seconds
..evaluation done in 14.24 seconds
Old network+MCTS average reward: 0.54, min: -0.17, max: 1.29, stdev: 0.24
New network+MCTS average reward: 0.53, min: -0.23, max: 1.27, stdev: 0.25
Old bare network average reward: 0.39, min: -0.23, max: 1.08, stdev: 0.25
New bare network average reward: 0.39, min: -0.28, max: 1.21, stdev: 0.25
External policy "random" average reward: 0.26, min: -0.39, max: 0.87, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: -0.16, max: 1.26, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.14, max: 1.31, stdev: 0.23
New network won 115 and tied 47 out of 300 games (46.17% wins where ties are half wins)
Reverting to the old network

Training iteration 22 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 42.02 seconds
Training examples lengths: [64921, 64566, 64747, 64617, 64661, 64869, 64438, 64737, 64808, 64726]
Total value: 344580.78
Training on 647090 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.9630 (value: 0.0033, weighted value: 0.1671, policy: 0.7959, weighted policy: 0.7959), Train Mean Max: 0.6902
Epoch 2/10, Train Loss: 0.9313 (value: 0.0028, weighted value: 0.1380, policy: 0.7932, weighted policy: 0.7932), Train Mean Max: 0.6955
Epoch 3/10, Train Loss: 0.9159 (value: 0.0025, weighted value: 0.1245, policy: 0.7915, weighted policy: 0.7915), Train Mean Max: 0.6986
Epoch 4/10, Train Loss: 0.9063 (value: 0.0023, weighted value: 0.1159, policy: 0.7905, weighted policy: 0.7905), Train Mean Max: 0.7005
Epoch 5/10, Train Loss: 0.9002 (value: 0.0022, weighted value: 0.1110, policy: 0.7892, weighted policy: 0.7892), Train Mean Max: 0.7018
Epoch 6/10, Train Loss: 0.8925 (value: 0.0021, weighted value: 0.1043, policy: 0.7882, weighted policy: 0.7882), Train Mean Max: 0.7031
Epoch 7/10, Train Loss: 0.8861 (value: 0.0020, weighted value: 0.0991, policy: 0.7870, weighted policy: 0.7870), Train Mean Max: 0.7038
Epoch 8/10, Train Loss: 0.8804 (value: 0.0019, weighted value: 0.0943, policy: 0.7861, weighted policy: 0.7861), Train Mean Max: 0.7048
Epoch 9/10, Train Loss: 0.8755 (value: 0.0018, weighted value: 0.0900, policy: 0.7855, weighted policy: 0.7855), Train Mean Max: 0.7055
Epoch 10/10, Train Loss: 0.8729 (value: 0.0018, weighted value: 0.0875, policy: 0.7854, weighted policy: 0.7854), Train Mean Max: 0.7060
..training done in 65.29 seconds
..evaluation done in 14.49 seconds
Old network+MCTS average reward: 0.55, min: -0.11, max: 1.23, stdev: 0.23
New network+MCTS average reward: 0.55, min: -0.02, max: 1.20, stdev: 0.22
Old bare network average reward: 0.38, min: -0.24, max: 1.15, stdev: 0.23
New bare network average reward: 0.40, min: -0.17, max: 1.17, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.30, max: 0.92, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: 0.04, max: 1.34, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.10, max: 1.29, stdev: 0.22
New network won 126 and tied 53 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 23 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 43.21 seconds
Training examples lengths: [64566, 64747, 64617, 64661, 64869, 64438, 64737, 64808, 64726, 64650]
Total value: 346759.57
Training on 646819 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.8814 (value: 0.0025, weighted value: 0.1234, policy: 0.7580, weighted policy: 0.7580), Train Mean Max: 0.7097
Epoch 2/10, Train Loss: 0.8714 (value: 0.0023, weighted value: 0.1145, policy: 0.7568, weighted policy: 0.7568), Train Mean Max: 0.7119
Epoch 3/10, Train Loss: 0.8543 (value: 0.0020, weighted value: 0.0988, policy: 0.7555, weighted policy: 0.7555), Train Mean Max: 0.7138
Epoch 4/10, Train Loss: 0.8514 (value: 0.0019, weighted value: 0.0964, policy: 0.7550, weighted policy: 0.7550), Train Mean Max: 0.7147
Epoch 5/10, Train Loss: 0.8467 (value: 0.0018, weighted value: 0.0920, policy: 0.7547, weighted policy: 0.7547), Train Mean Max: 0.7156
Epoch 6/10, Train Loss: 0.8428 (value: 0.0018, weighted value: 0.0894, policy: 0.7535, weighted policy: 0.7535), Train Mean Max: 0.7162
Epoch 7/10, Train Loss: 0.8353 (value: 0.0017, weighted value: 0.0829, policy: 0.7524, weighted policy: 0.7524), Train Mean Max: 0.7170
Epoch 8/10, Train Loss: 0.8369 (value: 0.0017, weighted value: 0.0851, policy: 0.7518, weighted policy: 0.7518), Train Mean Max: 0.7171
Epoch 9/10, Train Loss: 0.8326 (value: 0.0016, weighted value: 0.0812, policy: 0.7514, weighted policy: 0.7514), Train Mean Max: 0.7177
Epoch 10/10, Train Loss: 0.8266 (value: 0.0015, weighted value: 0.0759, policy: 0.7507, weighted policy: 0.7507), Train Mean Max: 0.7182
..training done in 65.52 seconds
..evaluation done in 14.71 seconds
Old network+MCTS average reward: 0.55, min: -0.06, max: 1.34, stdev: 0.24
New network+MCTS average reward: 0.55, min: -0.27, max: 1.34, stdev: 0.23
Old bare network average reward: 0.38, min: -0.44, max: 1.09, stdev: 0.23
New bare network average reward: 0.40, min: -0.34, max: 1.10, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.39, max: 0.97, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: -0.01, max: 1.28, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.09, max: 1.38, stdev: 0.22
New network won 123 and tied 51 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 24 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 42.45 seconds
Training examples lengths: [64747, 64617, 64661, 64869, 64438, 64737, 64808, 64726, 64650, 64798]
Total value: 349087.94
Training on 647051 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.8978 (value: 0.0033, weighted value: 0.1630, policy: 0.7349, weighted policy: 0.7349), Train Mean Max: 0.7127
Epoch 2/10, Train Loss: 0.8697 (value: 0.0027, weighted value: 0.1366, policy: 0.7331, weighted policy: 0.7331), Train Mean Max: 0.7170
Epoch 3/10, Train Loss: 0.8556 (value: 0.0025, weighted value: 0.1244, policy: 0.7312, weighted policy: 0.7312), Train Mean Max: 0.7194
Epoch 4/10, Train Loss: 0.8446 (value: 0.0023, weighted value: 0.1146, policy: 0.7299, weighted policy: 0.7299), Train Mean Max: 0.7212
Epoch 5/10, Train Loss: 0.8364 (value: 0.0021, weighted value: 0.1073, policy: 0.7291, weighted policy: 0.7291), Train Mean Max: 0.7226
Epoch 6/10, Train Loss: 0.8298 (value: 0.0020, weighted value: 0.1015, policy: 0.7283, weighted policy: 0.7283), Train Mean Max: 0.7237
Epoch 7/10, Train Loss: 0.8209 (value: 0.0019, weighted value: 0.0943, policy: 0.7266, weighted policy: 0.7266), Train Mean Max: 0.7245
Epoch 8/10, Train Loss: 0.8196 (value: 0.0019, weighted value: 0.0934, policy: 0.7263, weighted policy: 0.7263), Train Mean Max: 0.7251
Epoch 9/10, Train Loss: 0.8145 (value: 0.0018, weighted value: 0.0892, policy: 0.7252, weighted policy: 0.7252), Train Mean Max: 0.7259
Epoch 10/10, Train Loss: 0.8114 (value: 0.0017, weighted value: 0.0866, policy: 0.7247, weighted policy: 0.7247), Train Mean Max: 0.7264
..training done in 64.23 seconds
..evaluation done in 15.24 seconds
Old network+MCTS average reward: 0.54, min: -0.01, max: 1.20, stdev: 0.24
New network+MCTS average reward: 0.54, min: -0.02, max: 1.18, stdev: 0.24
Old bare network average reward: 0.39, min: -0.26, max: 1.15, stdev: 0.25
New bare network average reward: 0.40, min: -0.26, max: 1.14, stdev: 0.25
External policy "random" average reward: 0.26, min: -0.41, max: 1.06, stdev: 0.24
External policy "individual greedy" average reward: 0.54, min: 0.02, max: 1.36, stdev: 0.24
External policy "total greedy" average reward: 0.65, min: 0.10, max: 1.34, stdev: 0.24
New network won 115 and tied 61 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 25 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 41.71 seconds
Training examples lengths: [64617, 64661, 64869, 64438, 64737, 64808, 64726, 64650, 64798, 64618]
Total value: 350898.02
Training on 646922 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.9080 (value: 0.0039, weighted value: 0.1967, policy: 0.7113, weighted policy: 0.7113), Train Mean Max: 0.7159
Epoch 2/10, Train Loss: 0.8696 (value: 0.0032, weighted value: 0.1617, policy: 0.7079, weighted policy: 0.7079), Train Mean Max: 0.7218
Epoch 3/10, Train Loss: 0.8459 (value: 0.0028, weighted value: 0.1405, policy: 0.7054, weighted policy: 0.7054), Train Mean Max: 0.7256
Epoch 4/10, Train Loss: 0.8373 (value: 0.0027, weighted value: 0.1330, policy: 0.7043, weighted policy: 0.7043), Train Mean Max: 0.7275
Epoch 5/10, Train Loss: 0.8218 (value: 0.0024, weighted value: 0.1191, policy: 0.7027, weighted policy: 0.7027), Train Mean Max: 0.7298
Epoch 6/10, Train Loss: 0.8156 (value: 0.0023, weighted value: 0.1140, policy: 0.7016, weighted policy: 0.7016), Train Mean Max: 0.7311
Epoch 7/10, Train Loss: 0.8082 (value: 0.0022, weighted value: 0.1080, policy: 0.7002, weighted policy: 0.7002), Train Mean Max: 0.7325
Epoch 8/10, Train Loss: 0.7990 (value: 0.0020, weighted value: 0.1002, policy: 0.6988, weighted policy: 0.6988), Train Mean Max: 0.7334
Epoch 9/10, Train Loss: 0.7958 (value: 0.0020, weighted value: 0.0976, policy: 0.6982, weighted policy: 0.6982), Train Mean Max: 0.7341
Epoch 10/10, Train Loss: 0.7889 (value: 0.0018, weighted value: 0.0913, policy: 0.6976, weighted policy: 0.6976), Train Mean Max: 0.7351
..training done in 64.92 seconds
..evaluation done in 14.25 seconds
Old network+MCTS average reward: 0.55, min: -0.16, max: 1.17, stdev: 0.24
New network+MCTS average reward: 0.56, min: -0.10, max: 1.24, stdev: 0.24
Old bare network average reward: 0.40, min: -0.23, max: 1.12, stdev: 0.24
New bare network average reward: 0.41, min: -0.16, max: 1.28, stdev: 0.25
External policy "random" average reward: 0.26, min: -0.28, max: 1.05, stdev: 0.24
External policy "individual greedy" average reward: 0.54, min: -0.08, max: 1.22, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: -0.07, max: 1.35, stdev: 0.23
New network won 131 and tied 49 out of 300 games (51.83% wins where ties are half wins)
Keeping the new network

Training iteration 26 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 43.68 seconds
Training examples lengths: [64661, 64869, 64438, 64737, 64808, 64726, 64650, 64798, 64618, 64720]
Total value: 352612.83
Training on 647025 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.8055 (value: 0.0026, weighted value: 0.1291, policy: 0.6764, weighted policy: 0.6764), Train Mean Max: 0.7375
Epoch 2/10, Train Loss: 0.7837 (value: 0.0022, weighted value: 0.1088, policy: 0.6749, weighted policy: 0.6749), Train Mean Max: 0.7399
Epoch 3/10, Train Loss: 0.7781 (value: 0.0021, weighted value: 0.1041, policy: 0.6739, weighted policy: 0.6739), Train Mean Max: 0.7413
Epoch 4/10, Train Loss: 0.7722 (value: 0.0020, weighted value: 0.0995, policy: 0.6728, weighted policy: 0.6728), Train Mean Max: 0.7423
Epoch 5/10, Train Loss: 0.7631 (value: 0.0018, weighted value: 0.0914, policy: 0.6717, weighted policy: 0.6717), Train Mean Max: 0.7434
Epoch 6/10, Train Loss: 0.7599 (value: 0.0018, weighted value: 0.0892, policy: 0.6707, weighted policy: 0.6707), Train Mean Max: 0.7439
Epoch 7/10, Train Loss: 0.7555 (value: 0.0017, weighted value: 0.0858, policy: 0.6697, weighted policy: 0.6697), Train Mean Max: 0.7446
Epoch 8/10, Train Loss: 0.7536 (value: 0.0017, weighted value: 0.0842, policy: 0.6693, weighted policy: 0.6693), Train Mean Max: 0.7451
Epoch 9/10, Train Loss: 0.7498 (value: 0.0016, weighted value: 0.0806, policy: 0.6692, weighted policy: 0.6692), Train Mean Max: 0.7455
Epoch 10/10, Train Loss: 0.7450 (value: 0.0015, weighted value: 0.0768, policy: 0.6683, weighted policy: 0.6683), Train Mean Max: 0.7461
..training done in 69.94 seconds
..evaluation done in 14.57 seconds
Old network+MCTS average reward: 0.56, min: -0.19, max: 1.05, stdev: 0.23
New network+MCTS average reward: 0.56, min: -0.17, max: 1.13, stdev: 0.23
Old bare network average reward: 0.41, min: -0.27, max: 1.06, stdev: 0.24
New bare network average reward: 0.41, min: -0.34, max: 0.99, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.33, max: 0.83, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.12, max: 1.07, stdev: 0.21
External policy "total greedy" average reward: 0.64, min: 0.07, max: 1.22, stdev: 0.21
New network won 108 and tied 69 out of 300 games (47.50% wins where ties are half wins)
Reverting to the old network

Training iteration 27 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 43.22 seconds
Training examples lengths: [64869, 64438, 64737, 64808, 64726, 64650, 64798, 64618, 64720, 64668]
Total value: 354106.26
Training on 647032 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.8202 (value: 0.0032, weighted value: 0.1599, policy: 0.6602, weighted policy: 0.6602), Train Mean Max: 0.7392
Epoch 2/10, Train Loss: 0.7896 (value: 0.0026, weighted value: 0.1318, policy: 0.6578, weighted policy: 0.6578), Train Mean Max: 0.7428
Epoch 3/10, Train Loss: 0.7777 (value: 0.0024, weighted value: 0.1220, policy: 0.6557, weighted policy: 0.6557), Train Mean Max: 0.7451
Epoch 4/10, Train Loss: 0.7672 (value: 0.0023, weighted value: 0.1129, policy: 0.6543, weighted policy: 0.6543), Train Mean Max: 0.7466
Epoch 5/10, Train Loss: 0.7603 (value: 0.0021, weighted value: 0.1074, policy: 0.6530, weighted policy: 0.6530), Train Mean Max: 0.7479
Epoch 6/10, Train Loss: 0.7518 (value: 0.0020, weighted value: 0.1001, policy: 0.6518, weighted policy: 0.6518), Train Mean Max: 0.7491
Epoch 7/10, Train Loss: 0.7460 (value: 0.0019, weighted value: 0.0947, policy: 0.6513, weighted policy: 0.6513), Train Mean Max: 0.7500
Epoch 8/10, Train Loss: 0.7427 (value: 0.0018, weighted value: 0.0923, policy: 0.6504, weighted policy: 0.6504), Train Mean Max: 0.7507
Epoch 9/10, Train Loss: 0.7357 (value: 0.0017, weighted value: 0.0860, policy: 0.6497, weighted policy: 0.6497), Train Mean Max: 0.7515
Epoch 10/10, Train Loss: 0.7326 (value: 0.0017, weighted value: 0.0840, policy: 0.6486, weighted policy: 0.6486), Train Mean Max: 0.7519
..training done in 70.19 seconds
..evaluation done in 14.53 seconds
Old network+MCTS average reward: 0.56, min: -0.06, max: 1.32, stdev: 0.23
New network+MCTS average reward: 0.57, min: -0.10, max: 1.29, stdev: 0.22
Old bare network average reward: 0.41, min: -0.15, max: 1.29, stdev: 0.24
New bare network average reward: 0.41, min: -0.24, max: 1.26, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.26, max: 1.07, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: 0.06, max: 1.38, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.07, max: 1.40, stdev: 0.22
New network won 119 and tied 69 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 28 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 43.74 seconds
Training examples lengths: [64438, 64737, 64808, 64726, 64650, 64798, 64618, 64720, 64668, 64896]
Total value: 356455.11
Training on 647059 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.7528 (value: 0.0025, weighted value: 0.1228, policy: 0.6300, weighted policy: 0.6300), Train Mean Max: 0.7543
Epoch 2/10, Train Loss: 0.7364 (value: 0.0021, weighted value: 0.1071, policy: 0.6293, weighted policy: 0.6293), Train Mean Max: 0.7563
Epoch 3/10, Train Loss: 0.7242 (value: 0.0019, weighted value: 0.0973, policy: 0.6269, weighted policy: 0.6269), Train Mean Max: 0.7576
Epoch 4/10, Train Loss: 0.7203 (value: 0.0019, weighted value: 0.0936, policy: 0.6266, weighted policy: 0.6266), Train Mean Max: 0.7585
Epoch 5/10, Train Loss: 0.7148 (value: 0.0018, weighted value: 0.0886, policy: 0.6262, weighted policy: 0.6262), Train Mean Max: 0.7594
Epoch 6/10, Train Loss: 0.7103 (value: 0.0017, weighted value: 0.0854, policy: 0.6249, weighted policy: 0.6249), Train Mean Max: 0.7600
Epoch 7/10, Train Loss: 0.7055 (value: 0.0016, weighted value: 0.0816, policy: 0.6239, weighted policy: 0.6239), Train Mean Max: 0.7607
Epoch 8/10, Train Loss: 0.7038 (value: 0.0016, weighted value: 0.0804, policy: 0.6233, weighted policy: 0.6233), Train Mean Max: 0.7610
Epoch 9/10, Train Loss: 0.6987 (value: 0.0015, weighted value: 0.0764, policy: 0.6223, weighted policy: 0.6223), Train Mean Max: 0.7616
Epoch 10/10, Train Loss: 0.6977 (value: 0.0015, weighted value: 0.0751, policy: 0.6225, weighted policy: 0.6225), Train Mean Max: 0.7618
..training done in 64.62 seconds
..evaluation done in 14.50 seconds
Old network+MCTS average reward: 0.57, min: -0.07, max: 1.25, stdev: 0.24
New network+MCTS average reward: 0.57, min: -0.08, max: 1.21, stdev: 0.23
Old bare network average reward: 0.42, min: -0.11, max: 1.21, stdev: 0.23
New bare network average reward: 0.43, min: -0.27, max: 1.13, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.41, max: 0.91, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: 0.03, max: 1.32, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.07, max: 1.37, stdev: 0.21
New network won 115 and tied 74 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 29 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 43.31 seconds
Training examples lengths: [64737, 64808, 64726, 64650, 64798, 64618, 64720, 64668, 64896, 64550]
Total value: 359046.12
Training on 647171 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.7156 (value: 0.0022, weighted value: 0.1103, policy: 0.6052, weighted policy: 0.6052), Train Mean Max: 0.7646
Epoch 2/10, Train Loss: 0.7082 (value: 0.0021, weighted value: 0.1026, policy: 0.6056, weighted policy: 0.6056), Train Mean Max: 0.7653
Epoch 3/10, Train Loss: 0.7222 (value: 0.0018, weighted value: 0.0920, policy: 0.6302, weighted policy: 0.6302), Train Mean Max: 0.7660
Epoch 4/10, Train Loss: 0.7004 (value: 0.0018, weighted value: 0.0914, policy: 0.6090, weighted policy: 0.6090), Train Mean Max: 0.7676
Epoch 5/10, Train Loss: 0.6998 (value: 0.0019, weighted value: 0.0935, policy: 0.6064, weighted policy: 0.6064), Train Mean Max: 0.7673
Epoch 6/10, Train Loss: 0.6940 (value: 0.0017, weighted value: 0.0862, policy: 0.6078, weighted policy: 0.6078), Train Mean Max: 0.7683
Epoch 7/10, Train Loss: 0.6990 (value: 0.0018, weighted value: 0.0919, policy: 0.6071, weighted policy: 0.6071), Train Mean Max: 0.7681
Epoch 8/10, Train Loss: 0.7080 (value: 0.0017, weighted value: 0.0875, policy: 0.6205, weighted policy: 0.6205), Train Mean Max: 0.7680
Epoch 9/10, Train Loss: 0.7108 (value: 0.0016, weighted value: 0.0796, policy: 0.6312, weighted policy: 0.6312), Train Mean Max: 0.7690
Epoch 10/10, Train Loss: 0.6927 (value: 0.0018, weighted value: 0.0924, policy: 0.6002, weighted policy: 0.6002), Train Mean Max: 0.7688
..training done in 64.73 seconds
..evaluation done in 14.29 seconds
Old network+MCTS average reward: 0.60, min: -0.04, max: 1.41, stdev: 0.22
New network+MCTS average reward: 0.59, min: 0.01, max: 1.34, stdev: 0.23
Old bare network average reward: 0.45, min: -0.12, max: 1.19, stdev: 0.24
New bare network average reward: 0.46, min: -0.19, max: 1.20, stdev: 0.23
External policy "random" average reward: 0.28, min: -0.21, max: 0.90, stdev: 0.22
External policy "individual greedy" average reward: 0.57, min: -0.04, max: 1.32, stdev: 0.23
External policy "total greedy" average reward: 0.68, min: 0.15, max: 1.34, stdev: 0.22
New network won 124 and tied 32 out of 300 games (46.67% wins where ties are half wins)
Reverting to the old network

Training iteration 30 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 43.69 seconds
Training examples lengths: [64808, 64726, 64650, 64798, 64618, 64720, 64668, 64896, 64550, 65106]
Total value: 361358.00
Training on 647540 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.7316 (value: 0.0029, weighted value: 0.1435, policy: 0.5881, weighted policy: 0.5881), Train Mean Max: 0.7663
Epoch 2/10, Train Loss: 0.7077 (value: 0.0024, weighted value: 0.1216, policy: 0.5861, weighted policy: 0.5861), Train Mean Max: 0.7691
Epoch 3/10, Train Loss: 0.6940 (value: 0.0022, weighted value: 0.1096, policy: 0.5844, weighted policy: 0.5844), Train Mean Max: 0.7708
Epoch 4/10, Train Loss: 0.6878 (value: 0.0021, weighted value: 0.1046, policy: 0.5831, weighted policy: 0.5831), Train Mean Max: 0.7720
Epoch 5/10, Train Loss: 0.6759 (value: 0.0019, weighted value: 0.0939, policy: 0.5819, weighted policy: 0.5819), Train Mean Max: 0.7734
Epoch 6/10, Train Loss: 0.6727 (value: 0.0018, weighted value: 0.0919, policy: 0.5808, weighted policy: 0.5808), Train Mean Max: 0.7741
Epoch 7/10, Train Loss: 0.6666 (value: 0.0017, weighted value: 0.0867, policy: 0.5799, weighted policy: 0.5799), Train Mean Max: 0.7750
Epoch 8/10, Train Loss: 0.6647 (value: 0.0017, weighted value: 0.0861, policy: 0.5787, weighted policy: 0.5787), Train Mean Max: 0.7754
Epoch 9/10, Train Loss: 0.6573 (value: 0.0016, weighted value: 0.0792, policy: 0.5782, weighted policy: 0.5782), Train Mean Max: 0.7763
Epoch 10/10, Train Loss: 0.6538 (value: 0.0015, weighted value: 0.0763, policy: 0.5775, weighted policy: 0.5775), Train Mean Max: 0.7770
..training done in 71.03 seconds
..evaluation done in 14.55 seconds
Old network+MCTS average reward: 0.57, min: -0.09, max: 1.31, stdev: 0.22
New network+MCTS average reward: 0.57, min: 0.01, max: 1.42, stdev: 0.21
Old bare network average reward: 0.43, min: -0.17, max: 1.25, stdev: 0.22
New bare network average reward: 0.43, min: -0.09, max: 1.34, stdev: 0.22
External policy "random" average reward: 0.27, min: -0.19, max: 0.96, stdev: 0.21
External policy "individual greedy" average reward: 0.53, min: -0.09, max: 1.40, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: 0.13, max: 1.63, stdev: 0.21
New network won 114 and tied 66 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 31 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 43.77 seconds
Training examples lengths: [64726, 64650, 64798, 64618, 64720, 64668, 64896, 64550, 65106, 64500]
Total value: 362635.66
Training on 647232 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.7536 (value: 0.0035, weighted value: 0.1760, policy: 0.5776, weighted policy: 0.5776), Train Mean Max: 0.7672
Epoch 2/10, Train Loss: 0.7225 (value: 0.0029, weighted value: 0.1471, policy: 0.5754, weighted policy: 0.5754), Train Mean Max: 0.7707
Epoch 3/10, Train Loss: 0.7018 (value: 0.0026, weighted value: 0.1281, policy: 0.5737, weighted policy: 0.5737), Train Mean Max: 0.7729
Epoch 4/10, Train Loss: 0.6901 (value: 0.0024, weighted value: 0.1190, policy: 0.5712, weighted policy: 0.5712), Train Mean Max: 0.7750
Epoch 5/10, Train Loss: 0.6773 (value: 0.0022, weighted value: 0.1080, policy: 0.5693, weighted policy: 0.5693), Train Mean Max: 0.7764
Epoch 6/10, Train Loss: 0.6742 (value: 0.0021, weighted value: 0.1051, policy: 0.5691, weighted policy: 0.5691), Train Mean Max: 0.7773
Epoch 7/10, Train Loss: 0.6674 (value: 0.0020, weighted value: 0.0985, policy: 0.5689, weighted policy: 0.5689), Train Mean Max: 0.7781
Epoch 8/10, Train Loss: 0.6616 (value: 0.0019, weighted value: 0.0936, policy: 0.5680, weighted policy: 0.5680), Train Mean Max: 0.7792
Epoch 9/10, Train Loss: 0.6543 (value: 0.0018, weighted value: 0.0886, policy: 0.5657, weighted policy: 0.5657), Train Mean Max: 0.7801
Epoch 10/10, Train Loss: 0.6560 (value: 0.0018, weighted value: 0.0902, policy: 0.5658, weighted policy: 0.5658), Train Mean Max: 0.7801
..training done in 65.50 seconds
..evaluation done in 15.13 seconds
Old network+MCTS average reward: 0.55, min: -0.17, max: 1.07, stdev: 0.23
New network+MCTS average reward: 0.56, min: -0.14, max: 1.25, stdev: 0.23
Old bare network average reward: 0.42, min: -0.17, max: 1.04, stdev: 0.23
New bare network average reward: 0.43, min: -0.23, max: 1.11, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.31, max: 0.92, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: -0.16, max: 1.28, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: 0.09, max: 1.41, stdev: 0.23
New network won 131 and tied 61 out of 300 games (53.83% wins where ties are half wins)
Keeping the new network

Training iteration 32 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.23 seconds
Training examples lengths: [64650, 64798, 64618, 64720, 64668, 64896, 64550, 65106, 64500, 65449]
Total value: 365372.98
Training on 647955 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.6661 (value: 0.0023, weighted value: 0.1154, policy: 0.5507, weighted policy: 0.5507), Train Mean Max: 0.7829
Epoch 2/10, Train Loss: 0.6512 (value: 0.0020, weighted value: 0.1024, policy: 0.5488, weighted policy: 0.5488), Train Mean Max: 0.7843
Epoch 3/10, Train Loss: 0.6436 (value: 0.0019, weighted value: 0.0964, policy: 0.5471, weighted policy: 0.5471), Train Mean Max: 0.7855
Epoch 4/10, Train Loss: 0.6391 (value: 0.0019, weighted value: 0.0931, policy: 0.5459, weighted policy: 0.5459), Train Mean Max: 0.7861
Epoch 5/10, Train Loss: 0.6315 (value: 0.0017, weighted value: 0.0863, policy: 0.5452, weighted policy: 0.5452), Train Mean Max: 0.7870
Epoch 6/10, Train Loss: 0.6300 (value: 0.0017, weighted value: 0.0855, policy: 0.5445, weighted policy: 0.5445), Train Mean Max: 0.7877
Epoch 7/10, Train Loss: 0.6223 (value: 0.0016, weighted value: 0.0798, policy: 0.5425, weighted policy: 0.5425), Train Mean Max: 0.7883
Epoch 8/10, Train Loss: 0.6214 (value: 0.0016, weighted value: 0.0786, policy: 0.5427, weighted policy: 0.5427), Train Mean Max: 0.7888
Epoch 9/10, Train Loss: 0.6179 (value: 0.0015, weighted value: 0.0762, policy: 0.5417, weighted policy: 0.5417), Train Mean Max: 0.7892
Epoch 10/10, Train Loss: 0.6132 (value: 0.0014, weighted value: 0.0717, policy: 0.5415, weighted policy: 0.5415), Train Mean Max: 0.7898
..training done in 71.22 seconds
..evaluation done in 15.35 seconds
Old network+MCTS average reward: 0.57, min: -0.08, max: 1.33, stdev: 0.23
New network+MCTS average reward: 0.58, min: -0.08, max: 1.27, stdev: 0.23
Old bare network average reward: 0.44, min: -0.17, max: 1.27, stdev: 0.23
New bare network average reward: 0.44, min: -0.13, max: 1.27, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.31, max: 1.00, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: -0.07, max: 1.37, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: -0.05, max: 1.44, stdev: 0.23
New network won 124 and tied 67 out of 300 games (52.50% wins where ties are half wins)
Keeping the new network

Training iteration 33 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 43.53 seconds
Training examples lengths: [64798, 64618, 64720, 64668, 64896, 64550, 65106, 64500, 65449, 65046]
Total value: 368022.92
Training on 648351 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.6337 (value: 0.0021, weighted value: 0.1041, policy: 0.5297, weighted policy: 0.5297), Train Mean Max: 0.7916
Epoch 2/10, Train Loss: 0.6234 (value: 0.0019, weighted value: 0.0950, policy: 0.5284, weighted policy: 0.5284), Train Mean Max: 0.7925
Epoch 3/10, Train Loss: 0.6142 (value: 0.0017, weighted value: 0.0875, policy: 0.5267, weighted policy: 0.5267), Train Mean Max: 0.7935
Epoch 4/10, Train Loss: 0.6076 (value: 0.0016, weighted value: 0.0814, policy: 0.5262, weighted policy: 0.5262), Train Mean Max: 0.7944
Epoch 5/10, Train Loss: 0.6039 (value: 0.0016, weighted value: 0.0793, policy: 0.5245, weighted policy: 0.5245), Train Mean Max: 0.7949
Epoch 6/10, Train Loss: 0.6017 (value: 0.0015, weighted value: 0.0769, policy: 0.5247, weighted policy: 0.5247), Train Mean Max: 0.7956
Epoch 7/10, Train Loss: 0.5974 (value: 0.0015, weighted value: 0.0736, policy: 0.5237, weighted policy: 0.5237), Train Mean Max: 0.7958
Epoch 8/10, Train Loss: 0.5941 (value: 0.0014, weighted value: 0.0712, policy: 0.5229, weighted policy: 0.5229), Train Mean Max: 0.7963
Epoch 9/10, Train Loss: 0.5934 (value: 0.0014, weighted value: 0.0710, policy: 0.5224, weighted policy: 0.5224), Train Mean Max: 0.7966
Epoch 10/10, Train Loss: 0.5870 (value: 0.0013, weighted value: 0.0660, policy: 0.5210, weighted policy: 0.5210), Train Mean Max: 0.7971
..training done in 64.61 seconds
..evaluation done in 14.89 seconds
Old network+MCTS average reward: 0.57, min: -0.06, max: 1.31, stdev: 0.21
New network+MCTS average reward: 0.56, min: -0.16, max: 1.31, stdev: 0.21
Old bare network average reward: 0.43, min: -0.17, max: 1.31, stdev: 0.22
New bare network average reward: 0.44, min: -0.17, max: 1.23, stdev: 0.21
External policy "random" average reward: 0.25, min: -0.30, max: 0.97, stdev: 0.21
External policy "individual greedy" average reward: 0.53, min: 0.03, max: 1.46, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: 0.04, max: 1.54, stdev: 0.22
New network won 104 and tied 76 out of 300 games (47.33% wins where ties are half wins)
Reverting to the old network

Training iteration 34 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.04 seconds
Training examples lengths: [64618, 64720, 64668, 64896, 64550, 65106, 64500, 65449, 65046, 64871]
Total value: 370456.62
Training on 648424 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.6545 (value: 0.0027, weighted value: 0.1362, policy: 0.5182, weighted policy: 0.5182), Train Mean Max: 0.7928
Epoch 2/10, Train Loss: 0.6312 (value: 0.0023, weighted value: 0.1152, policy: 0.5160, weighted policy: 0.5160), Train Mean Max: 0.7948
Epoch 3/10, Train Loss: 0.6199 (value: 0.0021, weighted value: 0.1064, policy: 0.5135, weighted policy: 0.5135), Train Mean Max: 0.7963
Epoch 4/10, Train Loss: 0.6107 (value: 0.0020, weighted value: 0.0985, policy: 0.5122, weighted policy: 0.5122), Train Mean Max: 0.7975
Epoch 5/10, Train Loss: 0.6027 (value: 0.0018, weighted value: 0.0911, policy: 0.5117, weighted policy: 0.5117), Train Mean Max: 0.7984
Epoch 6/10, Train Loss: 0.5979 (value: 0.0018, weighted value: 0.0875, policy: 0.5104, weighted policy: 0.5104), Train Mean Max: 0.7992
Epoch 7/10, Train Loss: 0.5942 (value: 0.0017, weighted value: 0.0847, policy: 0.5095, weighted policy: 0.5095), Train Mean Max: 0.7998
Epoch 8/10, Train Loss: 0.5894 (value: 0.0016, weighted value: 0.0813, policy: 0.5081, weighted policy: 0.5081), Train Mean Max: 0.8005
Epoch 9/10, Train Loss: 0.5850 (value: 0.0016, weighted value: 0.0776, policy: 0.5074, weighted policy: 0.5074), Train Mean Max: 0.8012
Epoch 10/10, Train Loss: 0.5803 (value: 0.0015, weighted value: 0.0738, policy: 0.5065, weighted policy: 0.5065), Train Mean Max: 0.8017
..training done in 65.15 seconds
..evaluation done in 15.26 seconds
Old network+MCTS average reward: 0.56, min: -0.22, max: 1.16, stdev: 0.22
New network+MCTS average reward: 0.57, min: -0.06, max: 1.15, stdev: 0.22
Old bare network average reward: 0.44, min: -0.29, max: 1.04, stdev: 0.22
New bare network average reward: 0.44, min: -0.25, max: 1.09, stdev: 0.23
External policy "random" average reward: 0.24, min: -0.40, max: 0.84, stdev: 0.21
External policy "individual greedy" average reward: 0.52, min: -0.04, max: 1.34, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: -0.02, max: 1.40, stdev: 0.22
New network won 134 and tied 61 out of 300 games (54.83% wins where ties are half wins)
Keeping the new network

Training iteration 35 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.63 seconds
Training examples lengths: [64720, 64668, 64896, 64550, 65106, 64500, 65449, 65046, 64871, 64915]
Total value: 373125.57
Training on 648721 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.5985 (value: 0.0021, weighted value: 0.1064, policy: 0.4921, weighted policy: 0.4921), Train Mean Max: 0.8035
Epoch 2/10, Train Loss: 0.5865 (value: 0.0019, weighted value: 0.0954, policy: 0.4911, weighted policy: 0.4911), Train Mean Max: 0.8050
Epoch 3/10, Train Loss: 0.5769 (value: 0.0018, weighted value: 0.0877, policy: 0.4893, weighted policy: 0.4893), Train Mean Max: 0.8059
Epoch 4/10, Train Loss: 0.5720 (value: 0.0017, weighted value: 0.0834, policy: 0.4887, weighted policy: 0.4887), Train Mean Max: 0.8069
Epoch 5/10, Train Loss: 0.5676 (value: 0.0016, weighted value: 0.0803, policy: 0.4873, weighted policy: 0.4873), Train Mean Max: 0.8075
Epoch 6/10, Train Loss: 0.5651 (value: 0.0016, weighted value: 0.0778, policy: 0.4873, weighted policy: 0.4873), Train Mean Max: 0.8080
Epoch 7/10, Train Loss: 0.5593 (value: 0.0015, weighted value: 0.0740, policy: 0.4853, weighted policy: 0.4853), Train Mean Max: 0.8088
Epoch 8/10, Train Loss: 0.5559 (value: 0.0014, weighted value: 0.0712, policy: 0.4847, weighted policy: 0.4847), Train Mean Max: 0.8093
Epoch 9/10, Train Loss: 0.5561 (value: 0.0014, weighted value: 0.0717, policy: 0.4844, weighted policy: 0.4844), Train Mean Max: 0.8096
Epoch 10/10, Train Loss: 0.5514 (value: 0.0013, weighted value: 0.0671, policy: 0.4843, weighted policy: 0.4843), Train Mean Max: 0.8102
..training done in 70.91 seconds
..evaluation done in 15.81 seconds
Old network+MCTS average reward: 0.59, min: 0.07, max: 1.29, stdev: 0.21
New network+MCTS average reward: 0.58, min: 0.03, max: 1.29, stdev: 0.21
Old bare network average reward: 0.46, min: -0.07, max: 1.05, stdev: 0.22
New bare network average reward: 0.47, min: -0.06, max: 1.09, stdev: 0.21
External policy "random" average reward: 0.28, min: -0.35, max: 0.97, stdev: 0.22
External policy "individual greedy" average reward: 0.56, min: -0.04, max: 1.17, stdev: 0.21
External policy "total greedy" average reward: 0.66, min: 0.16, max: 1.37, stdev: 0.21
New network won 97 and tied 99 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 36 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.64 seconds
Training examples lengths: [64668, 64896, 64550, 65106, 64500, 65449, 65046, 64871, 64915, 64727]
Total value: 375116.16
Training on 648728 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.6238 (value: 0.0028, weighted value: 0.1396, policy: 0.4842, weighted policy: 0.4842), Train Mean Max: 0.8040
Epoch 2/10, Train Loss: 0.5964 (value: 0.0023, weighted value: 0.1149, policy: 0.4815, weighted policy: 0.4815), Train Mean Max: 0.8064
Epoch 3/10, Train Loss: 0.5869 (value: 0.0021, weighted value: 0.1070, policy: 0.4800, weighted policy: 0.4800), Train Mean Max: 0.8079
Epoch 4/10, Train Loss: 0.5749 (value: 0.0019, weighted value: 0.0967, policy: 0.4782, weighted policy: 0.4782), Train Mean Max: 0.8091
Epoch 5/10, Train Loss: 0.5697 (value: 0.0018, weighted value: 0.0923, policy: 0.4775, weighted policy: 0.4775), Train Mean Max: 0.8101
Epoch 6/10, Train Loss: 0.5641 (value: 0.0018, weighted value: 0.0881, policy: 0.4760, weighted policy: 0.4760), Train Mean Max: 0.8108
Epoch 7/10, Train Loss: 0.5591 (value: 0.0017, weighted value: 0.0850, policy: 0.4742, weighted policy: 0.4742), Train Mean Max: 0.8115
Epoch 8/10, Train Loss: 0.5532 (value: 0.0016, weighted value: 0.0795, policy: 0.4737, weighted policy: 0.4737), Train Mean Max: 0.8124
Epoch 9/10, Train Loss: 0.5503 (value: 0.0016, weighted value: 0.0782, policy: 0.4721, weighted policy: 0.4721), Train Mean Max: 0.8128
Epoch 10/10, Train Loss: 0.5479 (value: 0.0015, weighted value: 0.0759, policy: 0.4720, weighted policy: 0.4720), Train Mean Max: 0.8135
..training done in 66.67 seconds
..evaluation done in 15.15 seconds
Old network+MCTS average reward: 0.56, min: -0.09, max: 1.24, stdev: 0.23
New network+MCTS average reward: 0.56, min: -0.08, max: 1.12, stdev: 0.22
Old bare network average reward: 0.42, min: -0.26, max: 1.06, stdev: 0.24
New bare network average reward: 0.42, min: -0.30, max: 1.06, stdev: 0.23
External policy "random" average reward: 0.24, min: -0.38, max: 1.00, stdev: 0.22
External policy "individual greedy" average reward: 0.51, min: -0.17, max: 1.19, stdev: 0.23
External policy "total greedy" average reward: 0.63, min: -0.03, max: 1.44, stdev: 0.23
New network won 121 and tied 73 out of 300 games (52.50% wins where ties are half wins)
Keeping the new network

Training iteration 37 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 43.44 seconds
Training examples lengths: [64896, 64550, 65106, 64500, 65449, 65046, 64871, 64915, 64727, 64758]
Total value: 376787.24
Training on 648818 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.5643 (value: 0.0020, weighted value: 0.1022, policy: 0.4621, weighted policy: 0.4621), Train Mean Max: 0.8150
Epoch 2/10, Train Loss: 0.5539 (value: 0.0019, weighted value: 0.0937, policy: 0.4602, weighted policy: 0.4602), Train Mean Max: 0.8161
Epoch 3/10, Train Loss: 0.5463 (value: 0.0018, weighted value: 0.0875, policy: 0.4587, weighted policy: 0.4587), Train Mean Max: 0.8170
Epoch 4/10, Train Loss: 0.5389 (value: 0.0016, weighted value: 0.0817, policy: 0.4572, weighted policy: 0.4572), Train Mean Max: 0.8178
Epoch 5/10, Train Loss: 0.5347 (value: 0.0016, weighted value: 0.0785, policy: 0.4563, weighted policy: 0.4563), Train Mean Max: 0.8185
Epoch 6/10, Train Loss: 0.5301 (value: 0.0015, weighted value: 0.0745, policy: 0.4556, weighted policy: 0.4556), Train Mean Max: 0.8193
Epoch 7/10, Train Loss: 0.5290 (value: 0.0015, weighted value: 0.0741, policy: 0.4549, weighted policy: 0.4549), Train Mean Max: 0.8198
Epoch 8/10, Train Loss: 0.5249 (value: 0.0014, weighted value: 0.0709, policy: 0.4540, weighted policy: 0.4540), Train Mean Max: 0.8202
Epoch 9/10, Train Loss: 0.5232 (value: 0.0014, weighted value: 0.0706, policy: 0.4525, weighted policy: 0.4525), Train Mean Max: 0.8207
Epoch 10/10, Train Loss: 0.5200 (value: 0.0013, weighted value: 0.0672, policy: 0.4529, weighted policy: 0.4529), Train Mean Max: 0.8211
..training done in 64.62 seconds
..evaluation done in 14.64 seconds
Old network+MCTS average reward: 0.58, min: -0.04, max: 1.14, stdev: 0.22
New network+MCTS average reward: 0.58, min: -0.05, max: 1.18, stdev: 0.22
Old bare network average reward: 0.47, min: -0.19, max: 1.16, stdev: 0.23
New bare network average reward: 0.47, min: -0.20, max: 1.15, stdev: 0.22
External policy "random" average reward: 0.27, min: -0.33, max: 1.18, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.04, max: 1.55, stdev: 0.24
External policy "total greedy" average reward: 0.67, min: 0.03, max: 1.64, stdev: 0.23
New network won 107 and tied 77 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 38 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.43 seconds
Training examples lengths: [64550, 65106, 64500, 65449, 65046, 64871, 64915, 64727, 64758, 64765]
Total value: 377355.43
Training on 648687 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.5898 (value: 0.0027, weighted value: 0.1338, policy: 0.4560, weighted policy: 0.4560), Train Mean Max: 0.8152
Epoch 2/10, Train Loss: 0.5649 (value: 0.0022, weighted value: 0.1118, policy: 0.4531, weighted policy: 0.4531), Train Mean Max: 0.8170
Epoch 3/10, Train Loss: 0.5554 (value: 0.0021, weighted value: 0.1032, policy: 0.4522, weighted policy: 0.4522), Train Mean Max: 0.8182
Epoch 4/10, Train Loss: 0.5459 (value: 0.0019, weighted value: 0.0957, policy: 0.4503, weighted policy: 0.4503), Train Mean Max: 0.8193
Epoch 5/10, Train Loss: 0.5408 (value: 0.0018, weighted value: 0.0923, policy: 0.4485, weighted policy: 0.4485), Train Mean Max: 0.8201
Epoch 6/10, Train Loss: 0.5326 (value: 0.0017, weighted value: 0.0855, policy: 0.4472, weighted policy: 0.4472), Train Mean Max: 0.8210
Epoch 7/10, Train Loss: 0.5280 (value: 0.0016, weighted value: 0.0813, policy: 0.4467, weighted policy: 0.4467), Train Mean Max: 0.8219
Epoch 8/10, Train Loss: 0.5246 (value: 0.0016, weighted value: 0.0792, policy: 0.4454, weighted policy: 0.4454), Train Mean Max: 0.8226
Epoch 9/10, Train Loss: 0.5210 (value: 0.0015, weighted value: 0.0770, policy: 0.4441, weighted policy: 0.4441), Train Mean Max: 0.8231
Epoch 10/10, Train Loss: 0.5177 (value: 0.0015, weighted value: 0.0737, policy: 0.4439, weighted policy: 0.4439), Train Mean Max: 0.8236
..training done in 64.16 seconds
..evaluation done in 14.85 seconds
Old network+MCTS average reward: 0.59, min: 0.05, max: 1.28, stdev: 0.21
New network+MCTS average reward: 0.59, min: 0.10, max: 1.24, stdev: 0.21
Old bare network average reward: 0.46, min: -0.36, max: 1.18, stdev: 0.22
New bare network average reward: 0.47, min: -0.20, max: 1.21, stdev: 0.21
External policy "random" average reward: 0.25, min: -0.34, max: 0.83, stdev: 0.21
External policy "individual greedy" average reward: 0.55, min: 0.00, max: 1.16, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.13, max: 1.33, stdev: 0.21
New network won 115 and tied 73 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 39 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.39 seconds
Training examples lengths: [65106, 64500, 65449, 65046, 64871, 64915, 64727, 64758, 64765, 64950]
Total value: 378559.11
Training on 649087 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.5425 (value: 0.0021, weighted value: 0.1037, policy: 0.4388, weighted policy: 0.4388), Train Mean Max: 0.8241
Epoch 2/10, Train Loss: 0.5317 (value: 0.0019, weighted value: 0.0953, policy: 0.4364, weighted policy: 0.4364), Train Mean Max: 0.8250
Epoch 3/10, Train Loss: 0.5226 (value: 0.0017, weighted value: 0.0870, policy: 0.4357, weighted policy: 0.4357), Train Mean Max: 0.8258
Epoch 4/10, Train Loss: 0.5161 (value: 0.0016, weighted value: 0.0816, policy: 0.4345, weighted policy: 0.4345), Train Mean Max: 0.8265
Epoch 5/10, Train Loss: 0.5158 (value: 0.0017, weighted value: 0.0827, policy: 0.4331, weighted policy: 0.4331), Train Mean Max: 0.8269
Epoch 6/10, Train Loss: 0.5066 (value: 0.0015, weighted value: 0.0739, policy: 0.4327, weighted policy: 0.4327), Train Mean Max: 0.8277
Epoch 7/10, Train Loss: 0.5034 (value: 0.0014, weighted value: 0.0720, policy: 0.4314, weighted policy: 0.4314), Train Mean Max: 0.8283
Epoch 8/10, Train Loss: 0.4994 (value: 0.0014, weighted value: 0.0691, policy: 0.4303, weighted policy: 0.4303), Train Mean Max: 0.8288
Epoch 9/10, Train Loss: 0.4983 (value: 0.0014, weighted value: 0.0677, policy: 0.4306, weighted policy: 0.4306), Train Mean Max: 0.8292
Epoch 10/10, Train Loss: 0.4982 (value: 0.0014, weighted value: 0.0678, policy: 0.4304, weighted policy: 0.4304), Train Mean Max: 0.8293
..training done in 70.22 seconds
..evaluation done in 15.30 seconds
Old network+MCTS average reward: 0.58, min: -0.03, max: 1.30, stdev: 0.21
New network+MCTS average reward: 0.59, min: 0.03, max: 1.29, stdev: 0.21
Old bare network average reward: 0.47, min: -0.05, max: 1.30, stdev: 0.23
New bare network average reward: 0.48, min: -0.04, max: 1.32, stdev: 0.22
External policy "random" average reward: 0.27, min: -0.37, max: 1.21, stdev: 0.24
External policy "individual greedy" average reward: 0.54, min: 0.05, max: 1.44, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.13, max: 1.53, stdev: 0.23
New network won 122 and tied 81 out of 300 games (54.17% wins where ties are half wins)
Keeping the new network

Training iteration 40 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.46 seconds
Training examples lengths: [64500, 65449, 65046, 64871, 64915, 64727, 64758, 64765, 64950, 65062]
Total value: 379888.16
Training on 649043 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.5207 (value: 0.0019, weighted value: 0.0968, policy: 0.4238, weighted policy: 0.4238), Train Mean Max: 0.8304
Epoch 2/10, Train Loss: 0.5097 (value: 0.0017, weighted value: 0.0874, policy: 0.4223, weighted policy: 0.4223), Train Mean Max: 0.8311
Epoch 3/10, Train Loss: 0.5003 (value: 0.0016, weighted value: 0.0801, policy: 0.4202, weighted policy: 0.4202), Train Mean Max: 0.8317
Epoch 4/10, Train Loss: 0.4960 (value: 0.0015, weighted value: 0.0764, policy: 0.4195, weighted policy: 0.4195), Train Mean Max: 0.8323
Epoch 5/10, Train Loss: 0.4911 (value: 0.0015, weighted value: 0.0726, policy: 0.4185, weighted policy: 0.4185), Train Mean Max: 0.8329
Epoch 6/10, Train Loss: 0.4869 (value: 0.0014, weighted value: 0.0695, policy: 0.4174, weighted policy: 0.4174), Train Mean Max: 0.8335
Epoch 7/10, Train Loss: 0.4836 (value: 0.0013, weighted value: 0.0672, policy: 0.4164, weighted policy: 0.4164), Train Mean Max: 0.8339
Epoch 8/10, Train Loss: 0.4851 (value: 0.0014, weighted value: 0.0686, policy: 0.4165, weighted policy: 0.4165), Train Mean Max: 0.8341
Epoch 9/10, Train Loss: 0.4780 (value: 0.0013, weighted value: 0.0628, policy: 0.4153, weighted policy: 0.4153), Train Mean Max: 0.8347
Epoch 10/10, Train Loss: 0.4769 (value: 0.0012, weighted value: 0.0622, policy: 0.4147, weighted policy: 0.4147), Train Mean Max: 0.8350
..training done in 65.76 seconds
..evaluation done in 14.82 seconds
Old network+MCTS average reward: 0.59, min: 0.00, max: 1.18, stdev: 0.20
New network+MCTS average reward: 0.59, min: 0.05, max: 1.22, stdev: 0.21
Old bare network average reward: 0.48, min: -0.04, max: 1.10, stdev: 0.21
New bare network average reward: 0.48, min: -0.04, max: 1.22, stdev: 0.22
External policy "random" average reward: 0.26, min: -0.34, max: 0.77, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: -0.03, max: 1.15, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.06, max: 1.20, stdev: 0.22
New network won 109 and tied 94 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to zgaz_checkpoints/1757654225_iter_40

Training iteration 41 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.25 seconds
Training examples lengths: [65449, 65046, 64871, 64915, 64727, 64758, 64765, 64950, 65062, 64685]
Total value: 381363.90
Training on 649228 examples
Training with 318 batches of size 2048
Epoch 1/10, Train Loss: 0.4988 (value: 0.0018, weighted value: 0.0916, policy: 0.4072, weighted policy: 0.4072), Train Mean Max: 0.8363
Epoch 2/10, Train Loss: 0.5138 (value: 0.0020, weighted value: 0.0998, policy: 0.4140, weighted policy: 0.4140), Train Mean Max: 0.8362
Epoch 3/10, Train Loss: 0.5103 (value: 0.0017, weighted value: 0.0869, policy: 0.4234, weighted policy: 0.4234), Train Mean Max: 0.8367
Epoch 4/10, Train Loss: 0.5051 (value: 0.0018, weighted value: 0.0895, policy: 0.4157, weighted policy: 0.4157), Train Mean Max: 0.8375
Epoch 5/10, Train Loss: 0.4804 (value: 0.0015, weighted value: 0.0753, policy: 0.4051, weighted policy: 0.4051), Train Mean Max: 0.8382
Epoch 6/10, Train Loss: 0.4778 (value: 0.0014, weighted value: 0.0705, policy: 0.4073, weighted policy: 0.4073), Train Mean Max: 0.8384
Epoch 7/10, Train Loss: 0.4727 (value: 0.0014, weighted value: 0.0677, policy: 0.4050, weighted policy: 0.4050), Train Mean Max: 0.8391
Epoch 8/10, Train Loss: 0.4722 (value: 0.0013, weighted value: 0.0671, policy: 0.4051, weighted policy: 0.4051), Train Mean Max: 0.8396
Epoch 9/10, Train Loss: 0.4714 (value: 0.0013, weighted value: 0.0668, policy: 0.4046, weighted policy: 0.4046), Train Mean Max: 0.8396
Epoch 10/10, Train Loss: 0.4701 (value: 0.0012, weighted value: 0.0624, policy: 0.4076, weighted policy: 0.4076), Train Mean Max: 0.8398
..training done in 71.13 seconds
..evaluation done in 15.54 seconds
Old network+MCTS average reward: 0.59, min: -0.02, max: 1.28, stdev: 0.21
New network+MCTS average reward: 0.58, min: -0.01, max: 1.25, stdev: 0.22
Old bare network average reward: 0.46, min: -0.46, max: 1.11, stdev: 0.23
New bare network average reward: 0.48, min: -0.26, max: 1.17, stdev: 0.22
External policy "random" average reward: 0.27, min: -0.26, max: 0.87, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.06, max: 1.36, stdev: 0.21
External policy "total greedy" average reward: 0.65, min: 0.09, max: 1.47, stdev: 0.21
New network won 117 and tied 58 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 42 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 43.23 seconds
Training examples lengths: [65046, 64871, 64915, 64727, 64758, 64765, 64950, 65062, 64685, 64920]
Total value: 382220.52
Training on 648699 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.5230 (value: 0.0024, weighted value: 0.1213, policy: 0.4017, weighted policy: 0.4017), Train Mean Max: 0.8366
Epoch 2/10, Train Loss: 0.5017 (value: 0.0021, weighted value: 0.1031, policy: 0.3985, weighted policy: 0.3985), Train Mean Max: 0.8379
Epoch 3/10, Train Loss: 0.4908 (value: 0.0019, weighted value: 0.0936, policy: 0.3972, weighted policy: 0.3972), Train Mean Max: 0.8390
Epoch 4/10, Train Loss: 0.4831 (value: 0.0018, weighted value: 0.0881, policy: 0.3950, weighted policy: 0.3950), Train Mean Max: 0.8395
Epoch 5/10, Train Loss: 0.4768 (value: 0.0017, weighted value: 0.0828, policy: 0.3941, weighted policy: 0.3941), Train Mean Max: 0.8404
Epoch 6/10, Train Loss: 0.4698 (value: 0.0015, weighted value: 0.0767, policy: 0.3931, weighted policy: 0.3931), Train Mean Max: 0.8413
Epoch 7/10, Train Loss: 0.4675 (value: 0.0015, weighted value: 0.0758, policy: 0.3917, weighted policy: 0.3917), Train Mean Max: 0.8417
Epoch 8/10, Train Loss: 0.4635 (value: 0.0014, weighted value: 0.0720, policy: 0.3915, weighted policy: 0.3915), Train Mean Max: 0.8423
Epoch 9/10, Train Loss: 0.4581 (value: 0.0014, weighted value: 0.0683, policy: 0.3899, weighted policy: 0.3899), Train Mean Max: 0.8431
Epoch 10/10, Train Loss: 0.4570 (value: 0.0013, weighted value: 0.0674, policy: 0.3896, weighted policy: 0.3896), Train Mean Max: 0.8434
..training done in 71.05 seconds
..evaluation done in 15.55 seconds
Old network+MCTS average reward: 0.61, min: -0.09, max: 1.24, stdev: 0.22
New network+MCTS average reward: 0.61, min: 0.06, max: 1.27, stdev: 0.21
Old bare network average reward: 0.48, min: -0.24, max: 1.17, stdev: 0.22
New bare network average reward: 0.49, min: -0.20, max: 1.17, stdev: 0.21
External policy "random" average reward: 0.26, min: -0.28, max: 0.90, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: -0.01, max: 1.16, stdev: 0.21
External policy "total greedy" average reward: 0.66, min: 0.08, max: 1.25, stdev: 0.21
New network won 99 and tied 113 out of 300 games (51.83% wins where ties are half wins)
Keeping the new network

Training iteration 43 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.22 seconds
Training examples lengths: [64871, 64915, 64727, 64758, 64765, 64950, 65062, 64685, 64920, 64696]
Total value: 383404.22
Training on 648349 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.4852 (value: 0.0020, weighted value: 0.0987, policy: 0.3865, weighted policy: 0.3865), Train Mean Max: 0.8434
Epoch 2/10, Train Loss: 0.4684 (value: 0.0017, weighted value: 0.0838, policy: 0.3845, weighted policy: 0.3845), Train Mean Max: 0.8445
Epoch 3/10, Train Loss: 0.4638 (value: 0.0016, weighted value: 0.0808, policy: 0.3830, weighted policy: 0.3830), Train Mean Max: 0.8449
Epoch 4/10, Train Loss: 0.4574 (value: 0.0015, weighted value: 0.0755, policy: 0.3818, weighted policy: 0.3818), Train Mean Max: 0.8457
Epoch 5/10, Train Loss: 0.4549 (value: 0.0015, weighted value: 0.0733, policy: 0.3816, weighted policy: 0.3816), Train Mean Max: 0.8461
Epoch 6/10, Train Loss: 0.4489 (value: 0.0014, weighted value: 0.0689, policy: 0.3800, weighted policy: 0.3800), Train Mean Max: 0.8467
Epoch 7/10, Train Loss: 0.4481 (value: 0.0014, weighted value: 0.0683, policy: 0.3798, weighted policy: 0.3798), Train Mean Max: 0.8469
Epoch 8/10, Train Loss: 0.4451 (value: 0.0013, weighted value: 0.0667, policy: 0.3784, weighted policy: 0.3784), Train Mean Max: 0.8474
Epoch 9/10, Train Loss: 0.4404 (value: 0.0013, weighted value: 0.0625, policy: 0.3779, weighted policy: 0.3779), Train Mean Max: 0.8479
Epoch 10/10, Train Loss: 0.4381 (value: 0.0012, weighted value: 0.0612, policy: 0.3769, weighted policy: 0.3769), Train Mean Max: 0.8482
..training done in 66.71 seconds
..evaluation done in 15.05 seconds
Old network+MCTS average reward: 0.60, min: -0.09, max: 1.39, stdev: 0.22
New network+MCTS average reward: 0.60, min: -0.07, max: 1.39, stdev: 0.22
Old bare network average reward: 0.49, min: -0.16, max: 1.24, stdev: 0.23
New bare network average reward: 0.49, min: -0.22, max: 1.31, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.44, max: 1.03, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: -0.03, max: 1.44, stdev: 0.22
External policy "total greedy" average reward: 0.67, min: 0.05, max: 1.48, stdev: 0.23
New network won 101 and tied 104 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 44 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.74 seconds
Training examples lengths: [64915, 64727, 64758, 64765, 64950, 65062, 64685, 64920, 64696, 64824]
Total value: 384336.98
Training on 648302 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.4664 (value: 0.0019, weighted value: 0.0929, policy: 0.3735, weighted policy: 0.3735), Train Mean Max: 0.8485
Epoch 2/10, Train Loss: 0.4507 (value: 0.0016, weighted value: 0.0786, policy: 0.3722, weighted policy: 0.3722), Train Mean Max: 0.8495
Epoch 3/10, Train Loss: 0.4467 (value: 0.0015, weighted value: 0.0755, policy: 0.3712, weighted policy: 0.3712), Train Mean Max: 0.8500
Epoch 4/10, Train Loss: 0.4429 (value: 0.0015, weighted value: 0.0730, policy: 0.3699, weighted policy: 0.3699), Train Mean Max: 0.8504
Epoch 5/10, Train Loss: 0.4391 (value: 0.0014, weighted value: 0.0695, policy: 0.3696, weighted policy: 0.3696), Train Mean Max: 0.8509
Epoch 6/10, Train Loss: 0.4331 (value: 0.0013, weighted value: 0.0658, policy: 0.3673, weighted policy: 0.3673), Train Mean Max: 0.8514
Epoch 7/10, Train Loss: 0.4308 (value: 0.0013, weighted value: 0.0645, policy: 0.3663, weighted policy: 0.3663), Train Mean Max: 0.8518
Epoch 8/10, Train Loss: 0.4299 (value: 0.0013, weighted value: 0.0634, policy: 0.3664, weighted policy: 0.3664), Train Mean Max: 0.8521
Epoch 9/10, Train Loss: 0.4254 (value: 0.0012, weighted value: 0.0595, policy: 0.3659, weighted policy: 0.3659), Train Mean Max: 0.8525
Epoch 10/10, Train Loss: 0.4243 (value: 0.0012, weighted value: 0.0598, policy: 0.3646, weighted policy: 0.3646), Train Mean Max: 0.8530
..training done in 67.09 seconds
..evaluation done in 15.23 seconds
Old network+MCTS average reward: 0.58, min: -0.08, max: 1.26, stdev: 0.21
New network+MCTS average reward: 0.58, min: -0.11, max: 1.27, stdev: 0.22
Old bare network average reward: 0.48, min: -0.08, max: 1.26, stdev: 0.23
New bare network average reward: 0.48, min: -0.21, max: 1.24, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.51, max: 0.97, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: -0.10, max: 1.20, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: -0.06, max: 1.35, stdev: 0.22
New network won 98 and tied 95 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 45 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.75 seconds
Training examples lengths: [64727, 64758, 64765, 64950, 65062, 64685, 64920, 64696, 64824, 64785]
Total value: 384982.76
Training on 648172 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.4929 (value: 0.0024, weighted value: 0.1197, policy: 0.3732, weighted policy: 0.3732), Train Mean Max: 0.8484
Epoch 2/10, Train Loss: 0.4708 (value: 0.0020, weighted value: 0.1016, policy: 0.3692, weighted policy: 0.3692), Train Mean Max: 0.8496
Epoch 3/10, Train Loss: 0.4599 (value: 0.0019, weighted value: 0.0927, policy: 0.3672, weighted policy: 0.3672), Train Mean Max: 0.8502
Epoch 4/10, Train Loss: 0.4526 (value: 0.0018, weighted value: 0.0876, policy: 0.3650, weighted policy: 0.3650), Train Mean Max: 0.8511
Epoch 5/10, Train Loss: 0.4443 (value: 0.0016, weighted value: 0.0801, policy: 0.3642, weighted policy: 0.3642), Train Mean Max: 0.8517
Epoch 6/10, Train Loss: 0.4407 (value: 0.0016, weighted value: 0.0780, policy: 0.3627, weighted policy: 0.3627), Train Mean Max: 0.8522
Epoch 7/10, Train Loss: 0.4365 (value: 0.0015, weighted value: 0.0738, policy: 0.3627, weighted policy: 0.3627), Train Mean Max: 0.8528
Epoch 8/10, Train Loss: 0.4326 (value: 0.0014, weighted value: 0.0719, policy: 0.3607, weighted policy: 0.3607), Train Mean Max: 0.8534
Epoch 9/10, Train Loss: 0.4294 (value: 0.0014, weighted value: 0.0688, policy: 0.3606, weighted policy: 0.3606), Train Mean Max: 0.8539
Epoch 10/10, Train Loss: 0.4258 (value: 0.0013, weighted value: 0.0665, policy: 0.3593, weighted policy: 0.3593), Train Mean Max: 0.8543
..training done in 68.96 seconds
..evaluation done in 15.29 seconds
Old network+MCTS average reward: 0.57, min: -0.06, max: 1.47, stdev: 0.23
New network+MCTS average reward: 0.58, min: 0.00, max: 1.44, stdev: 0.23
Old bare network average reward: 0.46, min: -0.06, max: 1.43, stdev: 0.23
New bare network average reward: 0.47, min: 0.00, max: 1.44, stdev: 0.23
External policy "random" average reward: 0.24, min: -0.29, max: 0.99, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: -0.02, max: 1.41, stdev: 0.23
External policy "total greedy" average reward: 0.63, min: 0.10, max: 1.54, stdev: 0.22
New network won 110 and tied 102 out of 300 games (53.67% wins where ties are half wins)
Keeping the new network

Training iteration 46 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 43.52 seconds
Training examples lengths: [64758, 64765, 64950, 65062, 64685, 64920, 64696, 64824, 64785, 64695]
Total value: 386118.83
Training on 648140 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.4548 (value: 0.0019, weighted value: 0.0962, policy: 0.3586, weighted policy: 0.3586), Train Mean Max: 0.8544
Epoch 2/10, Train Loss: 0.4419 (value: 0.0017, weighted value: 0.0866, policy: 0.3553, weighted policy: 0.3553), Train Mean Max: 0.8552
Epoch 3/10, Train Loss: 0.4356 (value: 0.0016, weighted value: 0.0809, policy: 0.3548, weighted policy: 0.3548), Train Mean Max: 0.8556
Epoch 4/10, Train Loss: 0.4282 (value: 0.0015, weighted value: 0.0754, policy: 0.3529, weighted policy: 0.3529), Train Mean Max: 0.8563
Epoch 5/10, Train Loss: 0.4243 (value: 0.0014, weighted value: 0.0724, policy: 0.3519, weighted policy: 0.3519), Train Mean Max: 0.8568
Epoch 6/10, Train Loss: 0.4212 (value: 0.0014, weighted value: 0.0699, policy: 0.3513, weighted policy: 0.3513), Train Mean Max: 0.8572
Epoch 7/10, Train Loss: 0.4182 (value: 0.0014, weighted value: 0.0683, policy: 0.3499, weighted policy: 0.3499), Train Mean Max: 0.8576
Epoch 8/10, Train Loss: 0.4124 (value: 0.0013, weighted value: 0.0636, policy: 0.3489, weighted policy: 0.3489), Train Mean Max: 0.8582
Epoch 9/10, Train Loss: 0.4130 (value: 0.0013, weighted value: 0.0637, policy: 0.3494, weighted policy: 0.3494), Train Mean Max: 0.8585
Epoch 10/10, Train Loss: 0.4112 (value: 0.0013, weighted value: 0.0630, policy: 0.3482, weighted policy: 0.3482), Train Mean Max: 0.8590
..training done in 67.13 seconds
..evaluation done in 15.54 seconds
Old network+MCTS average reward: 0.61, min: 0.10, max: 1.38, stdev: 0.22
New network+MCTS average reward: 0.61, min: 0.01, max: 1.40, stdev: 0.23
Old bare network average reward: 0.49, min: -0.08, max: 1.31, stdev: 0.23
New bare network average reward: 0.50, min: -0.08, max: 1.34, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.41, max: 0.97, stdev: 0.21
External policy "individual greedy" average reward: 0.55, min: -0.19, max: 1.40, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: 0.07, max: 1.62, stdev: 0.22
New network won 101 and tied 98 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 47 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.38 seconds
Training examples lengths: [64765, 64950, 65062, 64685, 64920, 64696, 64824, 64785, 64695, 64726]
Total value: 387761.77
Training on 648108 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.4338 (value: 0.0018, weighted value: 0.0884, policy: 0.3455, weighted policy: 0.3455), Train Mean Max: 0.8592
Epoch 2/10, Train Loss: 0.4233 (value: 0.0016, weighted value: 0.0793, policy: 0.3440, weighted policy: 0.3440), Train Mean Max: 0.8600
Epoch 3/10, Train Loss: 0.4185 (value: 0.0015, weighted value: 0.0766, policy: 0.3418, weighted policy: 0.3418), Train Mean Max: 0.8604
Epoch 4/10, Train Loss: 0.4116 (value: 0.0014, weighted value: 0.0698, policy: 0.3418, weighted policy: 0.3418), Train Mean Max: 0.8610
Epoch 5/10, Train Loss: 0.4104 (value: 0.0014, weighted value: 0.0705, policy: 0.3400, weighted policy: 0.3400), Train Mean Max: 0.8613
Epoch 6/10, Train Loss: 0.4034 (value: 0.0013, weighted value: 0.0645, policy: 0.3389, weighted policy: 0.3389), Train Mean Max: 0.8618
Epoch 7/10, Train Loss: 0.4022 (value: 0.0013, weighted value: 0.0637, policy: 0.3386, weighted policy: 0.3386), Train Mean Max: 0.8623
Epoch 8/10, Train Loss: 0.4012 (value: 0.0012, weighted value: 0.0624, policy: 0.3389, weighted policy: 0.3389), Train Mean Max: 0.8626
Epoch 9/10, Train Loss: 0.3963 (value: 0.0012, weighted value: 0.0596, policy: 0.3366, weighted policy: 0.3366), Train Mean Max: 0.8630
Epoch 10/10, Train Loss: 0.3952 (value: 0.0012, weighted value: 0.0589, policy: 0.3363, weighted policy: 0.3363), Train Mean Max: 0.8632
..training done in 65.34 seconds
..evaluation done in 15.82 seconds
Old network+MCTS average reward: 0.58, min: -0.18, max: 1.30, stdev: 0.22
New network+MCTS average reward: 0.58, min: -0.10, max: 1.30, stdev: 0.22
Old bare network average reward: 0.48, min: -0.19, max: 1.14, stdev: 0.23
New bare network average reward: 0.48, min: -0.18, max: 1.14, stdev: 0.22
External policy "random" average reward: 0.25, min: -0.28, max: 0.80, stdev: 0.22
External policy "individual greedy" average reward: 0.52, min: -0.25, max: 1.09, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: 0.01, max: 1.32, stdev: 0.22
New network won 95 and tied 107 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 48 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.06 seconds
Training examples lengths: [64950, 65062, 64685, 64920, 64696, 64824, 64785, 64695, 64726, 64801]
Total value: 389111.92
Training on 648144 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.4606 (value: 0.0024, weighted value: 0.1178, policy: 0.3429, weighted policy: 0.3429), Train Mean Max: 0.8591
Epoch 2/10, Train Loss: 0.4421 (value: 0.0020, weighted value: 0.1015, policy: 0.3407, weighted policy: 0.3407), Train Mean Max: 0.8601
Epoch 3/10, Train Loss: 0.4290 (value: 0.0018, weighted value: 0.0912, policy: 0.3378, weighted policy: 0.3378), Train Mean Max: 0.8610
Epoch 4/10, Train Loss: 0.4218 (value: 0.0017, weighted value: 0.0856, policy: 0.3362, weighted policy: 0.3362), Train Mean Max: 0.8617
Epoch 5/10, Train Loss: 0.4162 (value: 0.0016, weighted value: 0.0802, policy: 0.3360, weighted policy: 0.3360), Train Mean Max: 0.8623
Epoch 6/10, Train Loss: 0.4106 (value: 0.0015, weighted value: 0.0760, policy: 0.3346, weighted policy: 0.3346), Train Mean Max: 0.8628
Epoch 7/10, Train Loss: 0.4102 (value: 0.0015, weighted value: 0.0769, policy: 0.3333, weighted policy: 0.3333), Train Mean Max: 0.8633
Epoch 8/10, Train Loss: 0.4023 (value: 0.0014, weighted value: 0.0705, policy: 0.3319, weighted policy: 0.3319), Train Mean Max: 0.8638
Epoch 9/10, Train Loss: 0.3980 (value: 0.0013, weighted value: 0.0669, policy: 0.3312, weighted policy: 0.3312), Train Mean Max: 0.8645
Epoch 10/10, Train Loss: 0.3977 (value: 0.0013, weighted value: 0.0669, policy: 0.3308, weighted policy: 0.3308), Train Mean Max: 0.8649
..training done in 72.89 seconds
..evaluation done in 15.85 seconds
Old network+MCTS average reward: 0.61, min: -0.09, max: 1.53, stdev: 0.21
New network+MCTS average reward: 0.60, min: -0.11, max: 1.53, stdev: 0.21
Old bare network average reward: 0.51, min: -0.10, max: 1.50, stdev: 0.22
New bare network average reward: 0.50, min: -0.25, max: 1.53, stdev: 0.22
External policy "random" average reward: 0.27, min: -0.45, max: 1.24, stdev: 0.21
External policy "individual greedy" average reward: 0.56, min: -0.02, max: 1.52, stdev: 0.22
External policy "total greedy" average reward: 0.67, min: 0.13, max: 1.59, stdev: 0.22
New network won 94 and tied 97 out of 300 games (47.50% wins where ties are half wins)
Reverting to the old network

Training iteration 49 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.08 seconds
Training examples lengths: [65062, 64685, 64920, 64696, 64824, 64785, 64695, 64726, 64801, 64668]
Total value: 389408.19
Training on 647862 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.4869 (value: 0.0029, weighted value: 0.1449, policy: 0.3420, weighted policy: 0.3420), Train Mean Max: 0.8589
Epoch 2/10, Train Loss: 0.4584 (value: 0.0024, weighted value: 0.1191, policy: 0.3393, weighted policy: 0.3393), Train Mean Max: 0.8602
Epoch 3/10, Train Loss: 0.4421 (value: 0.0021, weighted value: 0.1067, policy: 0.3354, weighted policy: 0.3354), Train Mean Max: 0.8612
Epoch 4/10, Train Loss: 0.4321 (value: 0.0020, weighted value: 0.0985, policy: 0.3335, weighted policy: 0.3335), Train Mean Max: 0.8620
Epoch 5/10, Train Loss: 0.4242 (value: 0.0018, weighted value: 0.0917, policy: 0.3326, weighted policy: 0.3326), Train Mean Max: 0.8627
Epoch 6/10, Train Loss: 0.4175 (value: 0.0017, weighted value: 0.0873, policy: 0.3301, weighted policy: 0.3301), Train Mean Max: 0.8634
Epoch 7/10, Train Loss: 0.4103 (value: 0.0016, weighted value: 0.0819, policy: 0.3284, weighted policy: 0.3284), Train Mean Max: 0.8642
Epoch 8/10, Train Loss: 0.4067 (value: 0.0016, weighted value: 0.0785, policy: 0.3282, weighted policy: 0.3282), Train Mean Max: 0.8647
Epoch 9/10, Train Loss: 0.4037 (value: 0.0015, weighted value: 0.0763, policy: 0.3273, weighted policy: 0.3273), Train Mean Max: 0.8652
Epoch 10/10, Train Loss: 0.4001 (value: 0.0015, weighted value: 0.0745, policy: 0.3256, weighted policy: 0.3256), Train Mean Max: 0.8657
..training done in 70.03 seconds
..evaluation done in 15.99 seconds
Old network+MCTS average reward: 0.61, min: -0.02, max: 1.18, stdev: 0.21
New network+MCTS average reward: 0.60, min: -0.02, max: 1.18, stdev: 0.21
Old bare network average reward: 0.50, min: -0.10, max: 1.10, stdev: 0.22
New bare network average reward: 0.51, min: -0.05, max: 1.07, stdev: 0.22
External policy "random" average reward: 0.26, min: -0.26, max: 0.90, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: 0.04, max: 1.27, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.15, max: 1.44, stdev: 0.21
New network won 109 and tied 85 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 50 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.91 seconds
Training examples lengths: [64685, 64920, 64696, 64824, 64785, 64695, 64726, 64801, 64668, 64761]
Total value: 390076.14
Training on 647561 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.4228 (value: 0.0020, weighted value: 0.0978, policy: 0.3250, weighted policy: 0.3250), Train Mean Max: 0.8661
Epoch 2/10, Train Loss: 0.4139 (value: 0.0018, weighted value: 0.0893, policy: 0.3245, weighted policy: 0.3245), Train Mean Max: 0.8667
Epoch 3/10, Train Loss: 0.4049 (value: 0.0017, weighted value: 0.0837, policy: 0.3212, weighted policy: 0.3212), Train Mean Max: 0.8674
Epoch 4/10, Train Loss: 0.3978 (value: 0.0015, weighted value: 0.0774, policy: 0.3204, weighted policy: 0.3204), Train Mean Max: 0.8681
Epoch 5/10, Train Loss: 0.3954 (value: 0.0015, weighted value: 0.0758, policy: 0.3196, weighted policy: 0.3196), Train Mean Max: 0.8685
Epoch 6/10, Train Loss: 0.3912 (value: 0.0015, weighted value: 0.0727, policy: 0.3185, weighted policy: 0.3185), Train Mean Max: 0.8689
Epoch 7/10, Train Loss: 0.3859 (value: 0.0014, weighted value: 0.0682, policy: 0.3177, weighted policy: 0.3177), Train Mean Max: 0.8694
Epoch 8/10, Train Loss: 0.3837 (value: 0.0014, weighted value: 0.0678, policy: 0.3159, weighted policy: 0.3159), Train Mean Max: 0.8698
Epoch 9/10, Train Loss: 0.3815 (value: 0.0013, weighted value: 0.0659, policy: 0.3156, weighted policy: 0.3156), Train Mean Max: 0.8702
Epoch 10/10, Train Loss: 0.3781 (value: 0.0013, weighted value: 0.0627, policy: 0.3154, weighted policy: 0.3154), Train Mean Max: 0.8706
..training done in 72.34 seconds
..evaluation done in 14.92 seconds
Old network+MCTS average reward: 0.62, min: 0.00, max: 1.21, stdev: 0.22
New network+MCTS average reward: 0.62, min: 0.00, max: 1.31, stdev: 0.22
Old bare network average reward: 0.52, min: -0.09, max: 1.17, stdev: 0.23
New bare network average reward: 0.51, min: -0.04, max: 1.17, stdev: 0.22
External policy "random" average reward: 0.27, min: -0.24, max: 1.02, stdev: 0.21
External policy "individual greedy" average reward: 0.55, min: 0.03, max: 1.31, stdev: 0.22
External policy "total greedy" average reward: 0.67, min: 0.14, max: 1.31, stdev: 0.22
New network won 96 and tied 100 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 51 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 43.92 seconds
Training examples lengths: [64920, 64696, 64824, 64785, 64695, 64726, 64801, 64668, 64761, 64993]
Total value: 390722.39
Training on 647869 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.4535 (value: 0.0025, weighted value: 0.1255, policy: 0.3280, weighted policy: 0.3280), Train Mean Max: 0.8655
Epoch 2/10, Train Loss: 0.4322 (value: 0.0021, weighted value: 0.1073, policy: 0.3249, weighted policy: 0.3249), Train Mean Max: 0.8661
Epoch 3/10, Train Loss: 0.4192 (value: 0.0019, weighted value: 0.0969, policy: 0.3223, weighted policy: 0.3223), Train Mean Max: 0.8669
Epoch 4/10, Train Loss: 0.4136 (value: 0.0019, weighted value: 0.0930, policy: 0.3205, weighted policy: 0.3205), Train Mean Max: 0.8675
Epoch 5/10, Train Loss: 0.4035 (value: 0.0017, weighted value: 0.0847, policy: 0.3187, weighted policy: 0.3187), Train Mean Max: 0.8684
Epoch 6/10, Train Loss: 0.3987 (value: 0.0016, weighted value: 0.0812, policy: 0.3175, weighted policy: 0.3175), Train Mean Max: 0.8688
Epoch 7/10, Train Loss: 0.3963 (value: 0.0016, weighted value: 0.0802, policy: 0.3161, weighted policy: 0.3161), Train Mean Max: 0.8693
Epoch 8/10, Train Loss: 0.3891 (value: 0.0015, weighted value: 0.0736, policy: 0.3154, weighted policy: 0.3154), Train Mean Max: 0.8699
Epoch 9/10, Train Loss: 0.3855 (value: 0.0014, weighted value: 0.0713, policy: 0.3142, weighted policy: 0.3142), Train Mean Max: 0.8704
Epoch 10/10, Train Loss: 0.3834 (value: 0.0014, weighted value: 0.0696, policy: 0.3138, weighted policy: 0.3138), Train Mean Max: 0.8708
..training done in 65.54 seconds
..evaluation done in 14.97 seconds
Old network+MCTS average reward: 0.59, min: 0.04, max: 1.50, stdev: 0.22
New network+MCTS average reward: 0.59, min: 0.04, max: 1.39, stdev: 0.22
Old bare network average reward: 0.49, min: -0.22, max: 1.28, stdev: 0.23
New bare network average reward: 0.48, min: -0.15, max: 1.28, stdev: 0.22
External policy "random" average reward: 0.24, min: -0.46, max: 0.91, stdev: 0.23
External policy "individual greedy" average reward: 0.51, min: -0.15, max: 1.27, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.03, max: 1.54, stdev: 0.23
New network won 91 and tied 99 out of 300 games (46.83% wins where ties are half wins)
Reverting to the old network

Training iteration 52 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.43 seconds
Training examples lengths: [64696, 64824, 64785, 64695, 64726, 64801, 64668, 64761, 64993, 65089]
Total value: 391346.50
Training on 648038 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.4824 (value: 0.0030, weighted value: 0.1517, policy: 0.3306, weighted policy: 0.3306), Train Mean Max: 0.8647
Epoch 2/10, Train Loss: 0.4502 (value: 0.0025, weighted value: 0.1260, policy: 0.3242, weighted policy: 0.3242), Train Mean Max: 0.8657
Epoch 3/10, Train Loss: 0.4369 (value: 0.0023, weighted value: 0.1142, policy: 0.3227, weighted policy: 0.3227), Train Mean Max: 0.8666
Epoch 4/10, Train Loss: 0.4235 (value: 0.0021, weighted value: 0.1039, policy: 0.3195, weighted policy: 0.3195), Train Mean Max: 0.8673
Epoch 5/10, Train Loss: 0.4168 (value: 0.0020, weighted value: 0.0985, policy: 0.3184, weighted policy: 0.3184), Train Mean Max: 0.8680
Epoch 6/10, Train Loss: 0.4073 (value: 0.0018, weighted value: 0.0914, policy: 0.3159, weighted policy: 0.3159), Train Mean Max: 0.8687
Epoch 7/10, Train Loss: 0.4003 (value: 0.0017, weighted value: 0.0850, policy: 0.3153, weighted policy: 0.3153), Train Mean Max: 0.8694
Epoch 8/10, Train Loss: 0.3954 (value: 0.0016, weighted value: 0.0812, policy: 0.3142, weighted policy: 0.3142), Train Mean Max: 0.8699
Epoch 9/10, Train Loss: 0.3938 (value: 0.0016, weighted value: 0.0814, policy: 0.3123, weighted policy: 0.3123), Train Mean Max: 0.8705
Epoch 10/10, Train Loss: 0.3889 (value: 0.0015, weighted value: 0.0765, policy: 0.3124, weighted policy: 0.3124), Train Mean Max: 0.8708
..training done in 65.24 seconds
..evaluation done in 14.77 seconds
Old network+MCTS average reward: 0.61, min: 0.00, max: 1.48, stdev: 0.23
New network+MCTS average reward: 0.62, min: -0.03, max: 1.36, stdev: 0.22
Old bare network average reward: 0.51, min: -0.06, max: 1.26, stdev: 0.22
New bare network average reward: 0.51, min: -0.04, max: 1.26, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.33, max: 1.16, stdev: 0.23
External policy "individual greedy" average reward: 0.56, min: 0.02, max: 1.59, stdev: 0.22
External policy "total greedy" average reward: 0.68, min: 0.12, max: 1.49, stdev: 0.22
New network won 114 and tied 101 out of 300 games (54.83% wins where ties are half wins)
Keeping the new network

Training iteration 53 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.60 seconds
Training examples lengths: [64824, 64785, 64695, 64726, 64801, 64668, 64761, 64993, 65089, 64882]
Total value: 391560.77
Training on 648224 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.4119 (value: 0.0020, weighted value: 0.0988, policy: 0.3131, weighted policy: 0.3131), Train Mean Max: 0.8709
Epoch 2/10, Train Loss: 0.3997 (value: 0.0018, weighted value: 0.0890, policy: 0.3107, weighted policy: 0.3107), Train Mean Max: 0.8716
Epoch 3/10, Train Loss: 0.3919 (value: 0.0016, weighted value: 0.0825, policy: 0.3094, weighted policy: 0.3094), Train Mean Max: 0.8723
Epoch 4/10, Train Loss: 0.3877 (value: 0.0016, weighted value: 0.0795, policy: 0.3082, weighted policy: 0.3082), Train Mean Max: 0.8727
Epoch 5/10, Train Loss: 0.3828 (value: 0.0015, weighted value: 0.0768, policy: 0.3060, weighted policy: 0.3060), Train Mean Max: 0.8732
Epoch 6/10, Train Loss: 0.3777 (value: 0.0014, weighted value: 0.0722, policy: 0.3054, weighted policy: 0.3054), Train Mean Max: 0.8738
Epoch 7/10, Train Loss: 0.3746 (value: 0.0014, weighted value: 0.0699, policy: 0.3046, weighted policy: 0.3046), Train Mean Max: 0.8741
Epoch 8/10, Train Loss: 0.3706 (value: 0.0014, weighted value: 0.0677, policy: 0.3029, weighted policy: 0.3029), Train Mean Max: 0.8745
Epoch 9/10, Train Loss: 0.3682 (value: 0.0013, weighted value: 0.0653, policy: 0.3029, weighted policy: 0.3029), Train Mean Max: 0.8750
Epoch 10/10, Train Loss: 0.3662 (value: 0.0013, weighted value: 0.0638, policy: 0.3024, weighted policy: 0.3024), Train Mean Max: 0.8754
..training done in 63.99 seconds
..evaluation done in 15.38 seconds
Old network+MCTS average reward: 0.61, min: 0.07, max: 1.18, stdev: 0.21
New network+MCTS average reward: 0.61, min: 0.10, max: 1.17, stdev: 0.20
Old bare network average reward: 0.51, min: -0.05, max: 1.19, stdev: 0.22
New bare network average reward: 0.51, min: -0.09, max: 1.06, stdev: 0.21
External policy "random" average reward: 0.26, min: -0.30, max: 0.95, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: 0.01, max: 1.19, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.14, max: 1.27, stdev: 0.22
New network won 104 and tied 108 out of 300 games (52.67% wins where ties are half wins)
Keeping the new network

Training iteration 54 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.19 seconds
Training examples lengths: [64785, 64695, 64726, 64801, 64668, 64761, 64993, 65089, 64882, 64872]
Total value: 391997.66
Training on 648272 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3982 (value: 0.0019, weighted value: 0.0932, policy: 0.3051, weighted policy: 0.3051), Train Mean Max: 0.8751
Epoch 2/10, Train Loss: 0.3828 (value: 0.0016, weighted value: 0.0803, policy: 0.3025, weighted policy: 0.3025), Train Mean Max: 0.8756
Epoch 3/10, Train Loss: 0.3759 (value: 0.0015, weighted value: 0.0748, policy: 0.3011, weighted policy: 0.3011), Train Mean Max: 0.8760
Epoch 4/10, Train Loss: 0.3726 (value: 0.0015, weighted value: 0.0729, policy: 0.2997, weighted policy: 0.2997), Train Mean Max: 0.8764
Epoch 5/10, Train Loss: 0.3688 (value: 0.0014, weighted value: 0.0694, policy: 0.2994, weighted policy: 0.2994), Train Mean Max: 0.8769
Epoch 6/10, Train Loss: 0.3645 (value: 0.0013, weighted value: 0.0667, policy: 0.2978, weighted policy: 0.2978), Train Mean Max: 0.8771
Epoch 7/10, Train Loss: 0.3607 (value: 0.0013, weighted value: 0.0634, policy: 0.2973, weighted policy: 0.2973), Train Mean Max: 0.8776
Epoch 8/10, Train Loss: 0.3595 (value: 0.0013, weighted value: 0.0634, policy: 0.2961, weighted policy: 0.2961), Train Mean Max: 0.8779
Epoch 9/10, Train Loss: 0.3545 (value: 0.0012, weighted value: 0.0590, policy: 0.2955, weighted policy: 0.2955), Train Mean Max: 0.8783
Epoch 10/10, Train Loss: 0.3548 (value: 0.0012, weighted value: 0.0597, policy: 0.2951, weighted policy: 0.2951), Train Mean Max: 0.8785
..training done in 72.15 seconds
..evaluation done in 15.66 seconds
Old network+MCTS average reward: 0.60, min: 0.02, max: 1.16, stdev: 0.22
New network+MCTS average reward: 0.60, min: -0.03, max: 1.16, stdev: 0.21
Old bare network average reward: 0.51, min: -0.18, max: 1.08, stdev: 0.22
New bare network average reward: 0.52, min: -0.11, max: 1.11, stdev: 0.22
External policy "random" average reward: 0.27, min: -0.44, max: 0.83, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: 0.01, max: 1.26, stdev: 0.21
External policy "total greedy" average reward: 0.65, min: 0.12, max: 1.28, stdev: 0.21
New network won 95 and tied 112 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 55 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.44 seconds
Training examples lengths: [64695, 64726, 64801, 64668, 64761, 64993, 65089, 64882, 64872, 64903]
Total value: 393006.43
Training on 648390 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3833 (value: 0.0017, weighted value: 0.0868, policy: 0.2964, weighted policy: 0.2964), Train Mean Max: 0.8784
Epoch 2/10, Train Loss: 0.3711 (value: 0.0015, weighted value: 0.0758, policy: 0.2952, weighted policy: 0.2952), Train Mean Max: 0.8790
Epoch 3/10, Train Loss: 0.3623 (value: 0.0014, weighted value: 0.0700, policy: 0.2923, weighted policy: 0.2923), Train Mean Max: 0.8793
Epoch 4/10, Train Loss: 0.3612 (value: 0.0014, weighted value: 0.0702, policy: 0.2910, weighted policy: 0.2910), Train Mean Max: 0.8797
Epoch 5/10, Train Loss: 0.3562 (value: 0.0013, weighted value: 0.0652, policy: 0.2911, weighted policy: 0.2911), Train Mean Max: 0.8801
Epoch 6/10, Train Loss: 0.3528 (value: 0.0013, weighted value: 0.0631, policy: 0.2897, weighted policy: 0.2897), Train Mean Max: 0.8806
Epoch 7/10, Train Loss: 0.3500 (value: 0.0012, weighted value: 0.0608, policy: 0.2892, weighted policy: 0.2892), Train Mean Max: 0.8807
Epoch 8/10, Train Loss: 0.3466 (value: 0.0012, weighted value: 0.0588, policy: 0.2878, weighted policy: 0.2878), Train Mean Max: 0.8812
Epoch 9/10, Train Loss: 0.3462 (value: 0.0012, weighted value: 0.0584, policy: 0.2878, weighted policy: 0.2878), Train Mean Max: 0.8814
Epoch 10/10, Train Loss: 0.3425 (value: 0.0011, weighted value: 0.0559, policy: 0.2866, weighted policy: 0.2866), Train Mean Max: 0.8818
..training done in 71.43 seconds
..evaluation done in 16.06 seconds
Old network+MCTS average reward: 0.60, min: -0.15, max: 1.37, stdev: 0.21
New network+MCTS average reward: 0.60, min: 0.00, max: 1.35, stdev: 0.21
Old bare network average reward: 0.51, min: -0.11, max: 1.37, stdev: 0.22
New bare network average reward: 0.51, min: -0.15, max: 1.37, stdev: 0.22
External policy "random" average reward: 0.26, min: -0.26, max: 0.85, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: -0.03, max: 1.25, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.15, max: 1.42, stdev: 0.21
New network won 90 and tied 106 out of 300 games (47.67% wins where ties are half wins)
Reverting to the old network

Training iteration 56 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.17 seconds
Training examples lengths: [64726, 64801, 64668, 64761, 64993, 65089, 64882, 64872, 64903, 64854]
Total value: 393316.28
Training on 648549 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.4115 (value: 0.0023, weighted value: 0.1135, policy: 0.2980, weighted policy: 0.2980), Train Mean Max: 0.8781
Epoch 2/10, Train Loss: 0.3895 (value: 0.0019, weighted value: 0.0949, policy: 0.2946, weighted policy: 0.2946), Train Mean Max: 0.8787
Epoch 3/10, Train Loss: 0.3803 (value: 0.0018, weighted value: 0.0883, policy: 0.2920, weighted policy: 0.2920), Train Mean Max: 0.8793
Epoch 4/10, Train Loss: 0.3741 (value: 0.0017, weighted value: 0.0837, policy: 0.2904, weighted policy: 0.2904), Train Mean Max: 0.8797
Epoch 5/10, Train Loss: 0.3663 (value: 0.0015, weighted value: 0.0769, policy: 0.2894, weighted policy: 0.2894), Train Mean Max: 0.8803
Epoch 6/10, Train Loss: 0.3606 (value: 0.0015, weighted value: 0.0734, policy: 0.2871, weighted policy: 0.2871), Train Mean Max: 0.8806
Epoch 7/10, Train Loss: 0.3568 (value: 0.0014, weighted value: 0.0697, policy: 0.2871, weighted policy: 0.2871), Train Mean Max: 0.8811
Epoch 8/10, Train Loss: 0.3529 (value: 0.0014, weighted value: 0.0676, policy: 0.2853, weighted policy: 0.2853), Train Mean Max: 0.8817
Epoch 9/10, Train Loss: 0.3510 (value: 0.0013, weighted value: 0.0656, policy: 0.2854, weighted policy: 0.2854), Train Mean Max: 0.8818
Epoch 10/10, Train Loss: 0.3479 (value: 0.0013, weighted value: 0.0632, policy: 0.2847, weighted policy: 0.2847), Train Mean Max: 0.8823
..training done in 67.51 seconds
..evaluation done in 15.41 seconds
Old network+MCTS average reward: 0.62, min: -0.14, max: 1.34, stdev: 0.23
New network+MCTS average reward: 0.63, min: -0.08, max: 1.34, stdev: 0.23
Old bare network average reward: 0.53, min: -0.18, max: 1.13, stdev: 0.23
New bare network average reward: 0.54, min: -0.18, max: 1.34, stdev: 0.23
External policy "random" average reward: 0.30, min: -0.39, max: 0.99, stdev: 0.25
External policy "individual greedy" average reward: 0.56, min: -0.14, max: 1.41, stdev: 0.23
External policy "total greedy" average reward: 0.68, min: 0.05, max: 1.29, stdev: 0.22
New network won 90 and tied 114 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 57 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.84 seconds
Training examples lengths: [64801, 64668, 64761, 64993, 65089, 64882, 64872, 64903, 64854, 64761]
Total value: 393674.10
Training on 648584 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.4411 (value: 0.0028, weighted value: 0.1384, policy: 0.3027, weighted policy: 0.3027), Train Mean Max: 0.8775
Epoch 2/10, Train Loss: 0.4104 (value: 0.0023, weighted value: 0.1135, policy: 0.2970, weighted policy: 0.2970), Train Mean Max: 0.8783
Epoch 3/10, Train Loss: 0.3953 (value: 0.0020, weighted value: 0.1017, policy: 0.2936, weighted policy: 0.2936), Train Mean Max: 0.8789
Epoch 4/10, Train Loss: 0.3867 (value: 0.0019, weighted value: 0.0955, policy: 0.2912, weighted policy: 0.2912), Train Mean Max: 0.8793
Epoch 5/10, Train Loss: 0.3771 (value: 0.0018, weighted value: 0.0878, policy: 0.2892, weighted policy: 0.2892), Train Mean Max: 0.8798
Epoch 6/10, Train Loss: 0.3702 (value: 0.0016, weighted value: 0.0818, policy: 0.2884, weighted policy: 0.2884), Train Mean Max: 0.8805
Epoch 7/10, Train Loss: 0.3661 (value: 0.0016, weighted value: 0.0788, policy: 0.2873, weighted policy: 0.2873), Train Mean Max: 0.8808
Epoch 8/10, Train Loss: 0.3626 (value: 0.0015, weighted value: 0.0764, policy: 0.2862, weighted policy: 0.2862), Train Mean Max: 0.8811
Epoch 9/10, Train Loss: 0.3562 (value: 0.0014, weighted value: 0.0713, policy: 0.2849, weighted policy: 0.2849), Train Mean Max: 0.8817
Epoch 10/10, Train Loss: 0.3531 (value: 0.0014, weighted value: 0.0687, policy: 0.2844, weighted policy: 0.2844), Train Mean Max: 0.8820
..training done in 71.23 seconds
..evaluation done in 16.19 seconds
Old network+MCTS average reward: 0.63, min: 0.09, max: 1.35, stdev: 0.22
New network+MCTS average reward: 0.63, min: 0.10, max: 1.42, stdev: 0.21
Old bare network average reward: 0.53, min: 0.01, max: 1.35, stdev: 0.23
New bare network average reward: 0.54, min: -0.01, max: 1.18, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.21, max: 1.02, stdev: 0.22
External policy "individual greedy" average reward: 0.56, min: 0.00, max: 1.33, stdev: 0.22
External policy "total greedy" average reward: 0.67, min: 0.16, max: 1.39, stdev: 0.22
New network won 97 and tied 95 out of 300 games (48.17% wins where ties are half wins)
Reverting to the old network

Training iteration 58 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.05 seconds
Training examples lengths: [64668, 64761, 64993, 65089, 64882, 64872, 64903, 64854, 64761, 65127]
Total value: 394976.73
Training on 648910 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.4691 (value: 0.0033, weighted value: 0.1642, policy: 0.3049, weighted policy: 0.3049), Train Mean Max: 0.8768
Epoch 2/10, Train Loss: 0.4312 (value: 0.0026, weighted value: 0.1314, policy: 0.2998, weighted policy: 0.2998), Train Mean Max: 0.8778
Epoch 3/10, Train Loss: 0.4133 (value: 0.0024, weighted value: 0.1184, policy: 0.2949, weighted policy: 0.2949), Train Mean Max: 0.8784
Epoch 4/10, Train Loss: 0.3981 (value: 0.0021, weighted value: 0.1065, policy: 0.2917, weighted policy: 0.2917), Train Mean Max: 0.8792
Epoch 5/10, Train Loss: 0.3869 (value: 0.0019, weighted value: 0.0966, policy: 0.2903, weighted policy: 0.2903), Train Mean Max: 0.8797
Epoch 6/10, Train Loss: 0.3818 (value: 0.0019, weighted value: 0.0931, policy: 0.2887, weighted policy: 0.2887), Train Mean Max: 0.8800
Epoch 7/10, Train Loss: 0.3746 (value: 0.0017, weighted value: 0.0871, policy: 0.2875, weighted policy: 0.2875), Train Mean Max: 0.8805
Epoch 8/10, Train Loss: 0.3688 (value: 0.0017, weighted value: 0.0833, policy: 0.2855, weighted policy: 0.2855), Train Mean Max: 0.8811
Epoch 9/10, Train Loss: 0.3630 (value: 0.0016, weighted value: 0.0788, policy: 0.2842, weighted policy: 0.2842), Train Mean Max: 0.8815
Epoch 10/10, Train Loss: 0.3599 (value: 0.0015, weighted value: 0.0760, policy: 0.2838, weighted policy: 0.2838), Train Mean Max: 0.8820
..training done in 67.25 seconds
..evaluation done in 15.66 seconds
Old network+MCTS average reward: 0.61, min: -0.07, max: 1.22, stdev: 0.23
New network+MCTS average reward: 0.61, min: 0.05, max: 1.27, stdev: 0.23
Old bare network average reward: 0.51, min: -0.07, max: 1.15, stdev: 0.23
New bare network average reward: 0.52, min: 0.02, max: 1.16, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.32, max: 1.06, stdev: 0.24
External policy "individual greedy" average reward: 0.54, min: -0.16, max: 1.33, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: -0.01, max: 1.36, stdev: 0.23
New network won 109 and tied 99 out of 300 games (52.83% wins where ties are half wins)
Keeping the new network

Training iteration 59 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.19 seconds
Training examples lengths: [64761, 64993, 65089, 64882, 64872, 64903, 64854, 64761, 65127, 64774]
Total value: 396390.32
Training on 649016 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3846 (value: 0.0020, weighted value: 0.0985, policy: 0.2861, weighted policy: 0.2861), Train Mean Max: 0.8819
Epoch 2/10, Train Loss: 0.3743 (value: 0.0018, weighted value: 0.0903, policy: 0.2840, weighted policy: 0.2840), Train Mean Max: 0.8823
Epoch 3/10, Train Loss: 0.3649 (value: 0.0017, weighted value: 0.0828, policy: 0.2821, weighted policy: 0.2821), Train Mean Max: 0.8830
Epoch 4/10, Train Loss: 0.3596 (value: 0.0016, weighted value: 0.0794, policy: 0.2802, weighted policy: 0.2802), Train Mean Max: 0.8833
Epoch 5/10, Train Loss: 0.3542 (value: 0.0015, weighted value: 0.0750, policy: 0.2792, weighted policy: 0.2792), Train Mean Max: 0.8838
Epoch 6/10, Train Loss: 0.3494 (value: 0.0014, weighted value: 0.0715, policy: 0.2779, weighted policy: 0.2779), Train Mean Max: 0.8841
Epoch 7/10, Train Loss: 0.3484 (value: 0.0014, weighted value: 0.0704, policy: 0.2780, weighted policy: 0.2780), Train Mean Max: 0.8846
Epoch 8/10, Train Loss: 0.3438 (value: 0.0014, weighted value: 0.0680, policy: 0.2758, weighted policy: 0.2758), Train Mean Max: 0.8849
Epoch 9/10, Train Loss: 0.3404 (value: 0.0013, weighted value: 0.0649, policy: 0.2755, weighted policy: 0.2755), Train Mean Max: 0.8854
Epoch 10/10, Train Loss: 0.3370 (value: 0.0012, weighted value: 0.0624, policy: 0.2747, weighted policy: 0.2747), Train Mean Max: 0.8856
..training done in 68.37 seconds
..evaluation done in 15.18 seconds
Old network+MCTS average reward: 0.58, min: -0.07, max: 1.22, stdev: 0.22
New network+MCTS average reward: 0.59, min: 0.04, max: 1.30, stdev: 0.22
Old bare network average reward: 0.50, min: -0.17, max: 1.14, stdev: 0.23
New bare network average reward: 0.50, min: -0.27, max: 1.21, stdev: 0.23
External policy "random" average reward: 0.23, min: -0.36, max: 0.94, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: -0.11, max: 1.33, stdev: 0.23
External policy "total greedy" average reward: 0.63, min: 0.03, max: 1.37, stdev: 0.23
New network won 107 and tied 117 out of 300 games (55.17% wins where ties are half wins)
Keeping the new network

Training iteration 60 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.06 seconds
Training examples lengths: [64993, 65089, 64882, 64872, 64903, 64854, 64761, 65127, 64774, 64803]
Total value: 396527.81
Training on 649058 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3665 (value: 0.0018, weighted value: 0.0893, policy: 0.2772, weighted policy: 0.2772), Train Mean Max: 0.8856
Epoch 2/10, Train Loss: 0.3559 (value: 0.0016, weighted value: 0.0805, policy: 0.2754, weighted policy: 0.2754), Train Mean Max: 0.8861
Epoch 3/10, Train Loss: 0.3495 (value: 0.0015, weighted value: 0.0757, policy: 0.2738, weighted policy: 0.2738), Train Mean Max: 0.8864
Epoch 4/10, Train Loss: 0.3420 (value: 0.0014, weighted value: 0.0694, policy: 0.2727, weighted policy: 0.2727), Train Mean Max: 0.8868
Epoch 5/10, Train Loss: 0.3398 (value: 0.0014, weighted value: 0.0683, policy: 0.2715, weighted policy: 0.2715), Train Mean Max: 0.8872
Epoch 6/10, Train Loss: 0.3354 (value: 0.0013, weighted value: 0.0651, policy: 0.2703, weighted policy: 0.2703), Train Mean Max: 0.8877
Epoch 7/10, Train Loss: 0.3325 (value: 0.0013, weighted value: 0.0630, policy: 0.2695, weighted policy: 0.2695), Train Mean Max: 0.8879
Epoch 8/10, Train Loss: 0.3324 (value: 0.0013, weighted value: 0.0626, policy: 0.2697, weighted policy: 0.2697), Train Mean Max: 0.8882
Epoch 9/10, Train Loss: 0.3267 (value: 0.0012, weighted value: 0.0592, policy: 0.2675, weighted policy: 0.2675), Train Mean Max: 0.8886
Epoch 10/10, Train Loss: 0.3276 (value: 0.0012, weighted value: 0.0602, policy: 0.2674, weighted policy: 0.2674), Train Mean Max: 0.8889
..training done in 71.23 seconds
..evaluation done in 15.47 seconds
Old network+MCTS average reward: 0.61, min: 0.01, max: 1.37, stdev: 0.23
New network+MCTS average reward: 0.61, min: -0.08, max: 1.37, stdev: 0.22
Old bare network average reward: 0.52, min: -0.08, max: 1.37, stdev: 0.22
New bare network average reward: 0.53, min: -0.08, max: 1.37, stdev: 0.22
External policy "random" average reward: 0.26, min: -0.27, max: 1.08, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.08, max: 1.44, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.14, max: 1.42, stdev: 0.22
New network won 86 and tied 109 out of 300 games (46.83% wins where ties are half wins)
Reverting to the old network
Saving intermediate checkpoint to zgaz_checkpoints/1757654225_iter_60

Training iteration 61 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.03 seconds
Training examples lengths: [65089, 64882, 64872, 64903, 64854, 64761, 65127, 64774, 64803, 64597]
Total value: 397387.04
Training on 648662 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3950 (value: 0.0023, weighted value: 0.1151, policy: 0.2799, weighted policy: 0.2799), Train Mean Max: 0.8850
Epoch 2/10, Train Loss: 0.3750 (value: 0.0020, weighted value: 0.0981, policy: 0.2769, weighted policy: 0.2769), Train Mean Max: 0.8857
Epoch 3/10, Train Loss: 0.3669 (value: 0.0018, weighted value: 0.0914, policy: 0.2755, weighted policy: 0.2755), Train Mean Max: 0.8861
Epoch 4/10, Train Loss: 0.3556 (value: 0.0017, weighted value: 0.0842, policy: 0.2715, weighted policy: 0.2715), Train Mean Max: 0.8866
Epoch 5/10, Train Loss: 0.3495 (value: 0.0016, weighted value: 0.0790, policy: 0.2705, weighted policy: 0.2705), Train Mean Max: 0.8871
Epoch 6/10, Train Loss: 0.3457 (value: 0.0015, weighted value: 0.0751, policy: 0.2706, weighted policy: 0.2706), Train Mean Max: 0.8877
Epoch 7/10, Train Loss: 0.3431 (value: 0.0015, weighted value: 0.0738, policy: 0.2693, weighted policy: 0.2693), Train Mean Max: 0.8879
Epoch 8/10, Train Loss: 0.3371 (value: 0.0014, weighted value: 0.0692, policy: 0.2680, weighted policy: 0.2680), Train Mean Max: 0.8882
Epoch 9/10, Train Loss: 0.3347 (value: 0.0014, weighted value: 0.0682, policy: 0.2665, weighted policy: 0.2665), Train Mean Max: 0.8886
Epoch 10/10, Train Loss: 0.3318 (value: 0.0013, weighted value: 0.0657, policy: 0.2661, weighted policy: 0.2661), Train Mean Max: 0.8890
..training done in 70.32 seconds
..evaluation done in 15.65 seconds
Old network+MCTS average reward: 0.60, min: 0.00, max: 1.24, stdev: 0.23
New network+MCTS average reward: 0.61, min: 0.04, max: 1.26, stdev: 0.23
Old bare network average reward: 0.52, min: -0.03, max: 1.17, stdev: 0.22
New bare network average reward: 0.52, min: -0.04, max: 1.22, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.37, max: 0.93, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: -0.07, max: 1.44, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.04, max: 1.55, stdev: 0.23
New network won 110 and tied 107 out of 300 games (54.50% wins where ties are half wins)
Keeping the new network

Training iteration 62 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.70 seconds
Training examples lengths: [64882, 64872, 64903, 64854, 64761, 65127, 64774, 64803, 64597, 64880]
Total value: 397779.98
Training on 648453 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3590 (value: 0.0018, weighted value: 0.0892, policy: 0.2698, weighted policy: 0.2698), Train Mean Max: 0.8886
Epoch 2/10, Train Loss: 0.3469 (value: 0.0016, weighted value: 0.0797, policy: 0.2671, weighted policy: 0.2671), Train Mean Max: 0.8892
Epoch 3/10, Train Loss: 0.3410 (value: 0.0015, weighted value: 0.0767, policy: 0.2643, weighted policy: 0.2643), Train Mean Max: 0.8896
Epoch 4/10, Train Loss: 0.3351 (value: 0.0014, weighted value: 0.0719, policy: 0.2632, weighted policy: 0.2632), Train Mean Max: 0.8900
Epoch 5/10, Train Loss: 0.3315 (value: 0.0014, weighted value: 0.0697, policy: 0.2618, weighted policy: 0.2618), Train Mean Max: 0.8904
Epoch 6/10, Train Loss: 0.3267 (value: 0.0013, weighted value: 0.0652, policy: 0.2615, weighted policy: 0.2615), Train Mean Max: 0.8907
Epoch 7/10, Train Loss: 0.3259 (value: 0.0013, weighted value: 0.0651, policy: 0.2608, weighted policy: 0.2608), Train Mean Max: 0.8908
Epoch 8/10, Train Loss: 0.3227 (value: 0.0013, weighted value: 0.0627, policy: 0.2600, weighted policy: 0.2600), Train Mean Max: 0.8914
Epoch 9/10, Train Loss: 0.3191 (value: 0.0012, weighted value: 0.0601, policy: 0.2590, weighted policy: 0.2590), Train Mean Max: 0.8918
Epoch 10/10, Train Loss: 0.3177 (value: 0.0012, weighted value: 0.0589, policy: 0.2588, weighted policy: 0.2588), Train Mean Max: 0.8922
..training done in 70.60 seconds
..evaluation done in 15.39 seconds
Old network+MCTS average reward: 0.61, min: 0.02, max: 1.37, stdev: 0.22
New network+MCTS average reward: 0.62, min: 0.07, max: 1.37, stdev: 0.22
Old bare network average reward: 0.53, min: -0.01, max: 1.27, stdev: 0.22
New bare network average reward: 0.54, min: -0.02, max: 1.33, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.38, max: 0.96, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: -0.06, max: 1.20, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: 0.15, max: 1.41, stdev: 0.23
New network won 89 and tied 126 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 63 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.01 seconds
Training examples lengths: [64872, 64903, 64854, 64761, 65127, 64774, 64803, 64597, 64880, 64964]
Total value: 397864.58
Training on 648535 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3498 (value: 0.0017, weighted value: 0.0866, policy: 0.2632, weighted policy: 0.2632), Train Mean Max: 0.8915
Epoch 2/10, Train Loss: 0.3383 (value: 0.0015, weighted value: 0.0769, policy: 0.2614, weighted policy: 0.2614), Train Mean Max: 0.8919
Epoch 3/10, Train Loss: 0.3304 (value: 0.0014, weighted value: 0.0716, policy: 0.2588, weighted policy: 0.2588), Train Mean Max: 0.8924
Epoch 4/10, Train Loss: 0.3252 (value: 0.0014, weighted value: 0.0685, policy: 0.2567, weighted policy: 0.2567), Train Mean Max: 0.8927
Epoch 5/10, Train Loss: 0.3233 (value: 0.0013, weighted value: 0.0664, policy: 0.2569, weighted policy: 0.2569), Train Mean Max: 0.8929
Epoch 6/10, Train Loss: 0.3185 (value: 0.0013, weighted value: 0.0629, policy: 0.2556, weighted policy: 0.2556), Train Mean Max: 0.8933
Epoch 7/10, Train Loss: 0.3164 (value: 0.0012, weighted value: 0.0613, policy: 0.2550, weighted policy: 0.2550), Train Mean Max: 0.8935
Epoch 8/10, Train Loss: 0.3154 (value: 0.0012, weighted value: 0.0603, policy: 0.2551, weighted policy: 0.2551), Train Mean Max: 0.8939
Epoch 9/10, Train Loss: 0.3117 (value: 0.0012, weighted value: 0.0585, policy: 0.2532, weighted policy: 0.2532), Train Mean Max: 0.8943
Epoch 10/10, Train Loss: 0.3096 (value: 0.0011, weighted value: 0.0567, policy: 0.2528, weighted policy: 0.2528), Train Mean Max: 0.8944
..training done in 65.52 seconds
..evaluation done in 15.65 seconds
Old network+MCTS average reward: 0.63, min: 0.02, max: 1.43, stdev: 0.22
New network+MCTS average reward: 0.63, min: 0.02, max: 1.43, stdev: 0.23
Old bare network average reward: 0.55, min: 0.01, max: 1.26, stdev: 0.23
New bare network average reward: 0.55, min: -0.02, max: 1.31, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.31, max: 0.90, stdev: 0.22
External policy "individual greedy" average reward: 0.56, min: -0.01, max: 1.40, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: 0.06, max: 1.37, stdev: 0.23
New network won 92 and tied 115 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 64 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.84 seconds
Training examples lengths: [64903, 64854, 64761, 65127, 64774, 64803, 64597, 64880, 64964, 64617]
Total value: 398302.19
Training on 648280 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3778 (value: 0.0022, weighted value: 0.1108, policy: 0.2670, weighted policy: 0.2670), Train Mean Max: 0.8909
Epoch 2/10, Train Loss: 0.3588 (value: 0.0019, weighted value: 0.0956, policy: 0.2632, weighted policy: 0.2632), Train Mean Max: 0.8917
Epoch 3/10, Train Loss: 0.3493 (value: 0.0018, weighted value: 0.0886, policy: 0.2607, weighted policy: 0.2607), Train Mean Max: 0.8920
Epoch 4/10, Train Loss: 0.3407 (value: 0.0016, weighted value: 0.0817, policy: 0.2590, weighted policy: 0.2590), Train Mean Max: 0.8922
Epoch 5/10, Train Loss: 0.3328 (value: 0.0015, weighted value: 0.0761, policy: 0.2567, weighted policy: 0.2567), Train Mean Max: 0.8926
Epoch 6/10, Train Loss: 0.3306 (value: 0.0015, weighted value: 0.0748, policy: 0.2559, weighted policy: 0.2559), Train Mean Max: 0.8931
Epoch 7/10, Train Loss: 0.3253 (value: 0.0014, weighted value: 0.0705, policy: 0.2549, weighted policy: 0.2549), Train Mean Max: 0.8933
Epoch 8/10, Train Loss: 0.3210 (value: 0.0013, weighted value: 0.0673, policy: 0.2537, weighted policy: 0.2537), Train Mean Max: 0.8937
Epoch 9/10, Train Loss: 0.3183 (value: 0.0013, weighted value: 0.0653, policy: 0.2531, weighted policy: 0.2531), Train Mean Max: 0.8940
Epoch 10/10, Train Loss: 0.3145 (value: 0.0013, weighted value: 0.0628, policy: 0.2518, weighted policy: 0.2518), Train Mean Max: 0.8944
..training done in 63.95 seconds
..evaluation done in 15.43 seconds
Old network+MCTS average reward: 0.62, min: -0.19, max: 1.39, stdev: 0.25
New network+MCTS average reward: 0.63, min: -0.17, max: 1.37, stdev: 0.25
Old bare network average reward: 0.53, min: -0.20, max: 1.38, stdev: 0.25
New bare network average reward: 0.54, min: -0.28, max: 1.38, stdev: 0.26
External policy "random" average reward: 0.27, min: -0.56, max: 1.06, stdev: 0.24
External policy "individual greedy" average reward: 0.55, min: -0.14, max: 1.25, stdev: 0.26
External policy "total greedy" average reward: 0.67, min: -0.19, max: 1.39, stdev: 0.25
New network won 97 and tied 118 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network

Training iteration 65 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.03 seconds
Training examples lengths: [64854, 64761, 65127, 64774, 64803, 64597, 64880, 64964, 64617, 64702]
Total value: 398253.00
Training on 648079 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3457 (value: 0.0018, weighted value: 0.0885, policy: 0.2572, weighted policy: 0.2572), Train Mean Max: 0.8939
Epoch 2/10, Train Loss: 0.3335 (value: 0.0016, weighted value: 0.0779, policy: 0.2557, weighted policy: 0.2557), Train Mean Max: 0.8943
Epoch 3/10, Train Loss: 0.3275 (value: 0.0015, weighted value: 0.0746, policy: 0.2529, weighted policy: 0.2529), Train Mean Max: 0.8948
Epoch 4/10, Train Loss: 0.3224 (value: 0.0014, weighted value: 0.0702, policy: 0.2522, weighted policy: 0.2522), Train Mean Max: 0.8950
Epoch 5/10, Train Loss: 0.3188 (value: 0.0014, weighted value: 0.0678, policy: 0.2510, weighted policy: 0.2510), Train Mean Max: 0.8954
Epoch 6/10, Train Loss: 0.3141 (value: 0.0013, weighted value: 0.0644, policy: 0.2497, weighted policy: 0.2497), Train Mean Max: 0.8956
Epoch 7/10, Train Loss: 0.3119 (value: 0.0013, weighted value: 0.0637, policy: 0.2482, weighted policy: 0.2482), Train Mean Max: 0.8959
Epoch 8/10, Train Loss: 0.3089 (value: 0.0012, weighted value: 0.0602, policy: 0.2487, weighted policy: 0.2487), Train Mean Max: 0.8963
Epoch 9/10, Train Loss: 0.3074 (value: 0.0012, weighted value: 0.0591, policy: 0.2482, weighted policy: 0.2482), Train Mean Max: 0.8966
Epoch 10/10, Train Loss: 0.3050 (value: 0.0011, weighted value: 0.0571, policy: 0.2478, weighted policy: 0.2478), Train Mean Max: 0.8969
..training done in 70.19 seconds
..evaluation done in 15.39 seconds
Old network+MCTS average reward: 0.60, min: 0.00, max: 1.32, stdev: 0.22
New network+MCTS average reward: 0.61, min: 0.00, max: 1.30, stdev: 0.22
Old bare network average reward: 0.52, min: 0.00, max: 1.23, stdev: 0.23
New bare network average reward: 0.53, min: -0.19, max: 1.19, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.36, max: 0.89, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: -0.02, max: 1.38, stdev: 0.24
External policy "total greedy" average reward: 0.65, min: 0.09, max: 1.42, stdev: 0.24
New network won 97 and tied 122 out of 300 games (52.67% wins where ties are half wins)
Keeping the new network

Training iteration 66 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.45 seconds
Training examples lengths: [64761, 65127, 64774, 64803, 64597, 64880, 64964, 64617, 64702, 64689]
Total value: 398705.15
Training on 647914 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3364 (value: 0.0017, weighted value: 0.0847, policy: 0.2517, weighted policy: 0.2517), Train Mean Max: 0.8964
Epoch 2/10, Train Loss: 0.3218 (value: 0.0015, weighted value: 0.0726, policy: 0.2492, weighted policy: 0.2492), Train Mean Max: 0.8968
Epoch 3/10, Train Loss: 0.3178 (value: 0.0014, weighted value: 0.0690, policy: 0.2488, weighted policy: 0.2488), Train Mean Max: 0.8971
Epoch 4/10, Train Loss: 0.3138 (value: 0.0013, weighted value: 0.0666, policy: 0.2472, weighted policy: 0.2472), Train Mean Max: 0.8974
Epoch 5/10, Train Loss: 0.3122 (value: 0.0013, weighted value: 0.0661, policy: 0.2461, weighted policy: 0.2461), Train Mean Max: 0.8976
Epoch 6/10, Train Loss: 0.3038 (value: 0.0012, weighted value: 0.0595, policy: 0.2443, weighted policy: 0.2443), Train Mean Max: 0.8980
Epoch 7/10, Train Loss: 0.3032 (value: 0.0012, weighted value: 0.0594, policy: 0.2438, weighted policy: 0.2438), Train Mean Max: 0.8984
Epoch 8/10, Train Loss: 0.3016 (value: 0.0012, weighted value: 0.0580, policy: 0.2436, weighted policy: 0.2436), Train Mean Max: 0.8984
Epoch 9/10, Train Loss: 0.2993 (value: 0.0011, weighted value: 0.0569, policy: 0.2424, weighted policy: 0.2424), Train Mean Max: 0.8989
Epoch 10/10, Train Loss: 0.2962 (value: 0.0011, weighted value: 0.0542, policy: 0.2419, weighted policy: 0.2419), Train Mean Max: 0.8991
..training done in 64.48 seconds
..evaluation done in 15.26 seconds
Old network+MCTS average reward: 0.64, min: 0.08, max: 1.40, stdev: 0.23
New network+MCTS average reward: 0.64, min: 0.15, max: 1.40, stdev: 0.23
Old bare network average reward: 0.55, min: 0.03, max: 1.30, stdev: 0.24
New bare network average reward: 0.54, min: -0.08, max: 1.17, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.32, max: 1.05, stdev: 0.24
External policy "individual greedy" average reward: 0.56, min: -0.10, max: 1.21, stdev: 0.24
External policy "total greedy" average reward: 0.67, min: 0.11, max: 1.31, stdev: 0.23
New network won 108 and tied 101 out of 300 games (52.83% wins where ties are half wins)
Keeping the new network

Training iteration 67 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.66 seconds
Training examples lengths: [65127, 64774, 64803, 64597, 64880, 64964, 64617, 64702, 64689, 65171]
Total value: 399435.75
Training on 648324 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3311 (value: 0.0017, weighted value: 0.0836, policy: 0.2475, weighted policy: 0.2475), Train Mean Max: 0.8983
Epoch 2/10, Train Loss: 0.3159 (value: 0.0014, weighted value: 0.0716, policy: 0.2443, weighted policy: 0.2443), Train Mean Max: 0.8989
Epoch 3/10, Train Loss: 0.3116 (value: 0.0014, weighted value: 0.0683, policy: 0.2433, weighted policy: 0.2433), Train Mean Max: 0.8992
Epoch 4/10, Train Loss: 0.3081 (value: 0.0013, weighted value: 0.0656, policy: 0.2424, weighted policy: 0.2424), Train Mean Max: 0.8992
Epoch 5/10, Train Loss: 0.3029 (value: 0.0012, weighted value: 0.0617, policy: 0.2412, weighted policy: 0.2412), Train Mean Max: 0.8996
Epoch 6/10, Train Loss: 0.3009 (value: 0.0012, weighted value: 0.0602, policy: 0.2407, weighted policy: 0.2407), Train Mean Max: 0.8999
Epoch 7/10, Train Loss: 0.2965 (value: 0.0012, weighted value: 0.0581, policy: 0.2384, weighted policy: 0.2384), Train Mean Max: 0.9003
Epoch 8/10, Train Loss: 0.2953 (value: 0.0011, weighted value: 0.0561, policy: 0.2392, weighted policy: 0.2392), Train Mean Max: 0.9005
Epoch 9/10, Train Loss: 0.2952 (value: 0.0011, weighted value: 0.0566, policy: 0.2386, weighted policy: 0.2386), Train Mean Max: 0.9007
Epoch 10/10, Train Loss: 0.2907 (value: 0.0011, weighted value: 0.0540, policy: 0.2366, weighted policy: 0.2366), Train Mean Max: 0.9010
..training done in 67.44 seconds
..evaluation done in 15.68 seconds
Old network+MCTS average reward: 0.60, min: 0.05, max: 1.22, stdev: 0.22
New network+MCTS average reward: 0.60, min: 0.09, max: 1.22, stdev: 0.22
Old bare network average reward: 0.52, min: -0.07, max: 1.14, stdev: 0.23
New bare network average reward: 0.53, min: -0.07, max: 1.16, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.33, max: 0.85, stdev: 0.22
External policy "individual greedy" average reward: 0.52, min: -0.05, max: 1.16, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.10, max: 1.35, stdev: 0.23
New network won 89 and tied 125 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 68 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.73 seconds
Training examples lengths: [64774, 64803, 64597, 64880, 64964, 64617, 64702, 64689, 65171, 64832]
Total value: 399368.98
Training on 648029 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3238 (value: 0.0016, weighted value: 0.0802, policy: 0.2436, weighted policy: 0.2436), Train Mean Max: 0.9003
Epoch 2/10, Train Loss: 0.3109 (value: 0.0014, weighted value: 0.0708, policy: 0.2401, weighted policy: 0.2401), Train Mean Max: 0.9008
Epoch 3/10, Train Loss: 0.3046 (value: 0.0013, weighted value: 0.0666, policy: 0.2380, weighted policy: 0.2380), Train Mean Max: 0.9011
Epoch 4/10, Train Loss: 0.2996 (value: 0.0013, weighted value: 0.0627, policy: 0.2368, weighted policy: 0.2368), Train Mean Max: 0.9014
Epoch 5/10, Train Loss: 0.2995 (value: 0.0013, weighted value: 0.0631, policy: 0.2364, weighted policy: 0.2364), Train Mean Max: 0.9016
Epoch 6/10, Train Loss: 0.2935 (value: 0.0012, weighted value: 0.0579, policy: 0.2356, weighted policy: 0.2356), Train Mean Max: 0.9019
Epoch 7/10, Train Loss: 0.2913 (value: 0.0011, weighted value: 0.0564, policy: 0.2349, weighted policy: 0.2349), Train Mean Max: 0.9021
Epoch 8/10, Train Loss: 0.2922 (value: 0.0012, weighted value: 0.0582, policy: 0.2340, weighted policy: 0.2340), Train Mean Max: 0.9024
Epoch 9/10, Train Loss: 0.2851 (value: 0.0010, weighted value: 0.0520, policy: 0.2331, weighted policy: 0.2331), Train Mean Max: 0.9027
Epoch 10/10, Train Loss: 0.2874 (value: 0.0011, weighted value: 0.0543, policy: 0.2332, weighted policy: 0.2332), Train Mean Max: 0.9029
..training done in 68.42 seconds
..evaluation done in 16.18 seconds
Old network+MCTS average reward: 0.61, min: 0.07, max: 1.31, stdev: 0.23
New network+MCTS average reward: 0.60, min: 0.04, max: 1.47, stdev: 0.23
Old bare network average reward: 0.52, min: -0.17, max: 1.26, stdev: 0.24
New bare network average reward: 0.53, min: -0.17, max: 1.26, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.31, max: 1.06, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: -0.06, max: 1.46, stdev: 0.24
External policy "total greedy" average reward: 0.66, min: 0.07, max: 1.57, stdev: 0.23
New network won 89 and tied 115 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 69 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.22 seconds
Training examples lengths: [64803, 64597, 64880, 64964, 64617, 64702, 64689, 65171, 64832, 64497]
Total value: 399691.25
Training on 647752 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3554 (value: 0.0021, weighted value: 0.1061, policy: 0.2493, weighted policy: 0.2493), Train Mean Max: 0.8994
Epoch 2/10, Train Loss: 0.3324 (value: 0.0018, weighted value: 0.0884, policy: 0.2440, weighted policy: 0.2440), Train Mean Max: 0.8999
Epoch 3/10, Train Loss: 0.3250 (value: 0.0017, weighted value: 0.0833, policy: 0.2417, weighted policy: 0.2417), Train Mean Max: 0.9003
Epoch 4/10, Train Loss: 0.3180 (value: 0.0016, weighted value: 0.0779, policy: 0.2400, weighted policy: 0.2400), Train Mean Max: 0.9005
Epoch 5/10, Train Loss: 0.3099 (value: 0.0014, weighted value: 0.0723, policy: 0.2376, weighted policy: 0.2376), Train Mean Max: 0.9010
Epoch 6/10, Train Loss: 0.3064 (value: 0.0014, weighted value: 0.0695, policy: 0.2369, weighted policy: 0.2369), Train Mean Max: 0.9012
Epoch 7/10, Train Loss: 0.3007 (value: 0.0013, weighted value: 0.0656, policy: 0.2351, weighted policy: 0.2351), Train Mean Max: 0.9016
Epoch 8/10, Train Loss: 0.3000 (value: 0.0013, weighted value: 0.0655, policy: 0.2345, weighted policy: 0.2345), Train Mean Max: 0.9018
Epoch 9/10, Train Loss: 0.2951 (value: 0.0012, weighted value: 0.0606, policy: 0.2345, weighted policy: 0.2345), Train Mean Max: 0.9022
Epoch 10/10, Train Loss: 0.2947 (value: 0.0012, weighted value: 0.0609, policy: 0.2338, weighted policy: 0.2338), Train Mean Max: 0.9023
..training done in 72.04 seconds
..evaluation done in 15.73 seconds
Old network+MCTS average reward: 0.61, min: 0.02, max: 1.58, stdev: 0.22
New network+MCTS average reward: 0.62, min: 0.07, max: 1.62, stdev: 0.22
Old bare network average reward: 0.53, min: -0.06, max: 1.53, stdev: 0.22
New bare network average reward: 0.53, min: -0.06, max: 1.47, stdev: 0.22
External policy "random" average reward: 0.28, min: -0.27, max: 1.29, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: 0.00, max: 1.70, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.16, max: 1.69, stdev: 0.23
New network won 104 and tied 105 out of 300 games (52.17% wins where ties are half wins)
Keeping the new network

Training iteration 70 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.11 seconds
Training examples lengths: [64597, 64880, 64964, 64617, 64702, 64689, 65171, 64832, 64497, 65060]
Total value: 400342.58
Training on 648009 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3240 (value: 0.0017, weighted value: 0.0844, policy: 0.2396, weighted policy: 0.2396), Train Mean Max: 0.9019
Epoch 2/10, Train Loss: 0.3134 (value: 0.0015, weighted value: 0.0769, policy: 0.2366, weighted policy: 0.2366), Train Mean Max: 0.9023
Epoch 3/10, Train Loss: 0.3053 (value: 0.0014, weighted value: 0.0708, policy: 0.2345, weighted policy: 0.2345), Train Mean Max: 0.9024
Epoch 4/10, Train Loss: 0.3021 (value: 0.0014, weighted value: 0.0682, policy: 0.2338, weighted policy: 0.2338), Train Mean Max: 0.9028
Epoch 5/10, Train Loss: 0.2965 (value: 0.0013, weighted value: 0.0645, policy: 0.2320, weighted policy: 0.2320), Train Mean Max: 0.9031
Epoch 6/10, Train Loss: 0.2937 (value: 0.0012, weighted value: 0.0625, policy: 0.2312, weighted policy: 0.2312), Train Mean Max: 0.9034
Epoch 7/10, Train Loss: 0.2926 (value: 0.0012, weighted value: 0.0619, policy: 0.2308, weighted policy: 0.2308), Train Mean Max: 0.9035
Epoch 8/10, Train Loss: 0.2871 (value: 0.0012, weighted value: 0.0578, policy: 0.2292, weighted policy: 0.2292), Train Mean Max: 0.9038
Epoch 9/10, Train Loss: 0.2855 (value: 0.0011, weighted value: 0.0556, policy: 0.2298, weighted policy: 0.2298), Train Mean Max: 0.9042
Epoch 10/10, Train Loss: 0.2844 (value: 0.0011, weighted value: 0.0552, policy: 0.2291, weighted policy: 0.2291), Train Mean Max: 0.9045
..training done in 71.81 seconds
..evaluation done in 15.59 seconds
Old network+MCTS average reward: 0.63, min: -0.18, max: 1.43, stdev: 0.22
New network+MCTS average reward: 0.64, min: -0.06, max: 1.43, stdev: 0.22
Old bare network average reward: 0.56, min: -0.18, max: 1.43, stdev: 0.23
New bare network average reward: 0.55, min: -0.31, max: 1.36, stdev: 0.23
External policy "random" average reward: 0.28, min: -0.33, max: 1.10, stdev: 0.24
External policy "individual greedy" average reward: 0.56, min: -0.02, max: 1.56, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: 0.01, max: 1.45, stdev: 0.23
New network won 85 and tied 130 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 71 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.37 seconds
Training examples lengths: [64880, 64964, 64617, 64702, 64689, 65171, 64832, 64497, 65060, 64767]
Total value: 400772.61
Training on 648179 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3193 (value: 0.0017, weighted value: 0.0852, policy: 0.2340, weighted policy: 0.2340), Train Mean Max: 0.9037
Epoch 2/10, Train Loss: 0.3052 (value: 0.0015, weighted value: 0.0725, policy: 0.2327, weighted policy: 0.2327), Train Mean Max: 0.9042
Epoch 3/10, Train Loss: 0.2988 (value: 0.0014, weighted value: 0.0692, policy: 0.2297, weighted policy: 0.2297), Train Mean Max: 0.9045
Epoch 4/10, Train Loss: 0.2948 (value: 0.0013, weighted value: 0.0664, policy: 0.2284, weighted policy: 0.2284), Train Mean Max: 0.9048
Epoch 5/10, Train Loss: 0.2932 (value: 0.0013, weighted value: 0.0642, policy: 0.2290, weighted policy: 0.2290), Train Mean Max: 0.9049
Epoch 6/10, Train Loss: 0.2873 (value: 0.0012, weighted value: 0.0607, policy: 0.2266, weighted policy: 0.2266), Train Mean Max: 0.9052
Epoch 7/10, Train Loss: 0.2842 (value: 0.0012, weighted value: 0.0582, policy: 0.2261, weighted policy: 0.2261), Train Mean Max: 0.9056
Epoch 8/10, Train Loss: 0.2814 (value: 0.0011, weighted value: 0.0560, policy: 0.2254, weighted policy: 0.2254), Train Mean Max: 0.9057
Epoch 9/10, Train Loss: 0.2818 (value: 0.0011, weighted value: 0.0570, policy: 0.2248, weighted policy: 0.2248), Train Mean Max: 0.9059
Epoch 10/10, Train Loss: 0.2791 (value: 0.0011, weighted value: 0.0546, policy: 0.2245, weighted policy: 0.2245), Train Mean Max: 0.9062
..training done in 65.84 seconds
..evaluation done in 15.77 seconds
Old network+MCTS average reward: 0.60, min: 0.00, max: 1.31, stdev: 0.22
New network+MCTS average reward: 0.61, min: 0.06, max: 1.38, stdev: 0.21
Old bare network average reward: 0.52, min: 0.00, max: 1.30, stdev: 0.22
New bare network average reward: 0.53, min: -0.07, max: 1.30, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.23, max: 0.99, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: 0.03, max: 1.27, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: -0.07, max: 1.31, stdev: 0.22
New network won 89 and tied 146 out of 300 games (54.00% wins where ties are half wins)
Keeping the new network

Training iteration 72 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.97 seconds
Training examples lengths: [64964, 64617, 64702, 64689, 65171, 64832, 64497, 65060, 64767, 64650]
Total value: 401239.02
Training on 647949 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3092 (value: 0.0016, weighted value: 0.0785, policy: 0.2307, weighted policy: 0.2307), Train Mean Max: 0.9055
Epoch 2/10, Train Loss: 0.2987 (value: 0.0014, weighted value: 0.0714, policy: 0.2274, weighted policy: 0.2274), Train Mean Max: 0.9061
Epoch 3/10, Train Loss: 0.2912 (value: 0.0013, weighted value: 0.0664, policy: 0.2248, weighted policy: 0.2248), Train Mean Max: 0.9064
Epoch 4/10, Train Loss: 0.2884 (value: 0.0013, weighted value: 0.0637, policy: 0.2247, weighted policy: 0.2247), Train Mean Max: 0.9065
Epoch 5/10, Train Loss: 0.2855 (value: 0.0012, weighted value: 0.0611, policy: 0.2245, weighted policy: 0.2245), Train Mean Max: 0.9068
Epoch 6/10, Train Loss: 0.2821 (value: 0.0012, weighted value: 0.0599, policy: 0.2223, weighted policy: 0.2223), Train Mean Max: 0.9071
Epoch 7/10, Train Loss: 0.2791 (value: 0.0011, weighted value: 0.0574, policy: 0.2218, weighted policy: 0.2218), Train Mean Max: 0.9074
Epoch 8/10, Train Loss: 0.2759 (value: 0.0011, weighted value: 0.0544, policy: 0.2215, weighted policy: 0.2215), Train Mean Max: 0.9076
Epoch 9/10, Train Loss: 0.2767 (value: 0.0011, weighted value: 0.0552, policy: 0.2215, weighted policy: 0.2215), Train Mean Max: 0.9079
Epoch 10/10, Train Loss: 0.2721 (value: 0.0011, weighted value: 0.0527, policy: 0.2194, weighted policy: 0.2194), Train Mean Max: 0.9081
..training done in 65.97 seconds
..evaluation done in 15.58 seconds
Old network+MCTS average reward: 0.65, min: 0.06, max: 1.26, stdev: 0.22
New network+MCTS average reward: 0.65, min: 0.06, max: 1.19, stdev: 0.21
Old bare network average reward: 0.57, min: -0.07, max: 1.19, stdev: 0.23
New bare network average reward: 0.57, min: -0.12, max: 1.19, stdev: 0.22
External policy "random" average reward: 0.29, min: -0.30, max: 0.96, stdev: 0.24
External policy "individual greedy" average reward: 0.58, min: 0.06, max: 1.36, stdev: 0.23
External policy "total greedy" average reward: 0.68, min: 0.18, max: 1.33, stdev: 0.22
New network won 79 and tied 141 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 73 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.70 seconds
Training examples lengths: [64617, 64702, 64689, 65171, 64832, 64497, 65060, 64767, 64650, 64722]
Total value: 402272.83
Training on 647707 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3383 (value: 0.0020, weighted value: 0.1024, policy: 0.2358, weighted policy: 0.2358), Train Mean Max: 0.9048
Epoch 2/10, Train Loss: 0.3219 (value: 0.0018, weighted value: 0.0904, policy: 0.2316, weighted policy: 0.2316), Train Mean Max: 0.9052
Epoch 3/10, Train Loss: 0.3110 (value: 0.0016, weighted value: 0.0822, policy: 0.2287, weighted policy: 0.2287), Train Mean Max: 0.9057
Epoch 4/10, Train Loss: 0.3033 (value: 0.0015, weighted value: 0.0771, policy: 0.2262, weighted policy: 0.2262), Train Mean Max: 0.9060
Epoch 5/10, Train Loss: 0.2983 (value: 0.0015, weighted value: 0.0737, policy: 0.2246, weighted policy: 0.2246), Train Mean Max: 0.9063
Epoch 6/10, Train Loss: 0.2915 (value: 0.0014, weighted value: 0.0682, policy: 0.2233, weighted policy: 0.2233), Train Mean Max: 0.9066
Epoch 7/10, Train Loss: 0.2908 (value: 0.0014, weighted value: 0.0680, policy: 0.2228, weighted policy: 0.2228), Train Mean Max: 0.9068
Epoch 8/10, Train Loss: 0.2859 (value: 0.0013, weighted value: 0.0642, policy: 0.2218, weighted policy: 0.2218), Train Mean Max: 0.9073
Epoch 9/10, Train Loss: 0.2839 (value: 0.0012, weighted value: 0.0621, policy: 0.2218, weighted policy: 0.2218), Train Mean Max: 0.9073
Epoch 10/10, Train Loss: 0.2786 (value: 0.0012, weighted value: 0.0588, policy: 0.2198, weighted policy: 0.2198), Train Mean Max: 0.9077
..training done in 66.83 seconds
..evaluation done in 16.48 seconds
Old network+MCTS average reward: 0.63, min: 0.10, max: 1.29, stdev: 0.21
New network+MCTS average reward: 0.63, min: -0.04, max: 1.24, stdev: 0.22
Old bare network average reward: 0.55, min: -0.02, max: 1.18, stdev: 0.22
New bare network average reward: 0.55, min: -0.10, max: 1.30, stdev: 0.23
External policy "random" average reward: 0.29, min: -0.26, max: 1.01, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: 0.03, max: 1.16, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.07, max: 1.24, stdev: 0.23
New network won 91 and tied 123 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 74 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.75 seconds
Training examples lengths: [64702, 64689, 65171, 64832, 64497, 65060, 64767, 64650, 64722, 64750]
Total value: 403264.00
Training on 647840 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3128 (value: 0.0017, weighted value: 0.0859, policy: 0.2269, weighted policy: 0.2269), Train Mean Max: 0.9069
Epoch 2/10, Train Loss: 0.3012 (value: 0.0015, weighted value: 0.0767, policy: 0.2245, weighted policy: 0.2245), Train Mean Max: 0.9074
Epoch 3/10, Train Loss: 0.2933 (value: 0.0014, weighted value: 0.0710, policy: 0.2223, weighted policy: 0.2223), Train Mean Max: 0.9077
Epoch 4/10, Train Loss: 0.2887 (value: 0.0014, weighted value: 0.0680, policy: 0.2207, weighted policy: 0.2207), Train Mean Max: 0.9080
Epoch 5/10, Train Loss: 0.2851 (value: 0.0013, weighted value: 0.0656, policy: 0.2195, weighted policy: 0.2195), Train Mean Max: 0.9085
Epoch 6/10, Train Loss: 0.2820 (value: 0.0013, weighted value: 0.0633, policy: 0.2187, weighted policy: 0.2187), Train Mean Max: 0.9086
Epoch 7/10, Train Loss: 0.2792 (value: 0.0012, weighted value: 0.0611, policy: 0.2180, weighted policy: 0.2180), Train Mean Max: 0.9088
Epoch 8/10, Train Loss: 0.2766 (value: 0.0012, weighted value: 0.0596, policy: 0.2170, weighted policy: 0.2170), Train Mean Max: 0.9090
Epoch 9/10, Train Loss: 0.2747 (value: 0.0012, weighted value: 0.0578, policy: 0.2169, weighted policy: 0.2169), Train Mean Max: 0.9093
Epoch 10/10, Train Loss: 0.2710 (value: 0.0011, weighted value: 0.0549, policy: 0.2161, weighted policy: 0.2161), Train Mean Max: 0.9094
..training done in 65.49 seconds
..evaluation done in 15.59 seconds
Old network+MCTS average reward: 0.61, min: 0.06, max: 1.59, stdev: 0.20
New network+MCTS average reward: 0.61, min: 0.06, max: 1.58, stdev: 0.20
Old bare network average reward: 0.54, min: 0.03, max: 1.29, stdev: 0.21
New bare network average reward: 0.54, min: 0.03, max: 1.44, stdev: 0.21
External policy "random" average reward: 0.27, min: -0.19, max: 1.07, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: 0.09, max: 1.67, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: 0.12, max: 1.68, stdev: 0.21
New network won 89 and tied 116 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 75 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.28 seconds
Training examples lengths: [64689, 65171, 64832, 64497, 65060, 64767, 64650, 64722, 64750, 64942]
Total value: 404008.24
Training on 648080 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3446 (value: 0.0022, weighted value: 0.1104, policy: 0.2342, weighted policy: 0.2342), Train Mean Max: 0.9059
Epoch 2/10, Train Loss: 0.3251 (value: 0.0019, weighted value: 0.0959, policy: 0.2291, weighted policy: 0.2291), Train Mean Max: 0.9064
Epoch 3/10, Train Loss: 0.3106 (value: 0.0017, weighted value: 0.0852, policy: 0.2254, weighted policy: 0.2254), Train Mean Max: 0.9069
Epoch 4/10, Train Loss: 0.3054 (value: 0.0016, weighted value: 0.0805, policy: 0.2249, weighted policy: 0.2249), Train Mean Max: 0.9070
Epoch 5/10, Train Loss: 0.3005 (value: 0.0016, weighted value: 0.0783, policy: 0.2222, weighted policy: 0.2222), Train Mean Max: 0.9074
Epoch 6/10, Train Loss: 0.2919 (value: 0.0014, weighted value: 0.0706, policy: 0.2213, weighted policy: 0.2213), Train Mean Max: 0.9077
Epoch 7/10, Train Loss: 0.2900 (value: 0.0014, weighted value: 0.0702, policy: 0.2198, weighted policy: 0.2198), Train Mean Max: 0.9081
Epoch 8/10, Train Loss: 0.2860 (value: 0.0013, weighted value: 0.0668, policy: 0.2192, weighted policy: 0.2192), Train Mean Max: 0.9081
Epoch 9/10, Train Loss: 0.2832 (value: 0.0013, weighted value: 0.0650, policy: 0.2182, weighted policy: 0.2182), Train Mean Max: 0.9084
Epoch 10/10, Train Loss: 0.2798 (value: 0.0012, weighted value: 0.0623, policy: 0.2175, weighted policy: 0.2175), Train Mean Max: 0.9087
..training done in 64.87 seconds
..evaluation done in 15.34 seconds
Old network+MCTS average reward: 0.61, min: 0.03, max: 1.28, stdev: 0.21
New network+MCTS average reward: 0.61, min: 0.00, max: 1.19, stdev: 0.21
Old bare network average reward: 0.53, min: -0.16, max: 1.07, stdev: 0.21
New bare network average reward: 0.53, min: -0.07, max: 1.18, stdev: 0.21
External policy "random" average reward: 0.25, min: -0.23, max: 0.89, stdev: 0.21
External policy "individual greedy" average reward: 0.53, min: 0.01, max: 1.15, stdev: 0.20
External policy "total greedy" average reward: 0.65, min: 0.04, max: 1.28, stdev: 0.20
New network won 96 and tied 116 out of 300 games (51.33% wins where ties are half wins)
Keeping the new network

Training iteration 76 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.67 seconds
Training examples lengths: [65171, 64832, 64497, 65060, 64767, 64650, 64722, 64750, 64942, 64710]
Total value: 404619.29
Training on 648101 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3112 (value: 0.0017, weighted value: 0.0875, policy: 0.2238, weighted policy: 0.2238), Train Mean Max: 0.9079
Epoch 2/10, Train Loss: 0.2987 (value: 0.0016, weighted value: 0.0781, policy: 0.2206, weighted policy: 0.2206), Train Mean Max: 0.9082
Epoch 3/10, Train Loss: 0.2936 (value: 0.0015, weighted value: 0.0740, policy: 0.2196, weighted policy: 0.2196), Train Mean Max: 0.9085
Epoch 4/10, Train Loss: 0.2871 (value: 0.0014, weighted value: 0.0683, policy: 0.2188, weighted policy: 0.2188), Train Mean Max: 0.9088
Epoch 5/10, Train Loss: 0.2848 (value: 0.0013, weighted value: 0.0672, policy: 0.2176, weighted policy: 0.2176), Train Mean Max: 0.9091
Epoch 6/10, Train Loss: 0.2808 (value: 0.0013, weighted value: 0.0638, policy: 0.2170, weighted policy: 0.2170), Train Mean Max: 0.9093
Epoch 7/10, Train Loss: 0.2783 (value: 0.0013, weighted value: 0.0633, policy: 0.2150, weighted policy: 0.2150), Train Mean Max: 0.9096
Epoch 8/10, Train Loss: 0.2740 (value: 0.0012, weighted value: 0.0588, policy: 0.2151, weighted policy: 0.2151), Train Mean Max: 0.9098
Epoch 9/10, Train Loss: 0.2734 (value: 0.0012, weighted value: 0.0595, policy: 0.2139, weighted policy: 0.2139), Train Mean Max: 0.9101
Epoch 10/10, Train Loss: 0.2708 (value: 0.0011, weighted value: 0.0572, policy: 0.2137, weighted policy: 0.2137), Train Mean Max: 0.9105
..training done in 65.56 seconds
..evaluation done in 15.92 seconds
Old network+MCTS average reward: 0.64, min: 0.17, max: 1.43, stdev: 0.23
New network+MCTS average reward: 0.64, min: 0.04, max: 1.43, stdev: 0.22
Old bare network average reward: 0.57, min: 0.04, max: 1.43, stdev: 0.24
New bare network average reward: 0.57, min: 0.06, max: 1.43, stdev: 0.23
External policy "random" average reward: 0.29, min: -0.38, max: 1.00, stdev: 0.24
External policy "individual greedy" average reward: 0.56, min: -0.08, max: 1.36, stdev: 0.24
External policy "total greedy" average reward: 0.67, min: 0.09, max: 1.44, stdev: 0.22
New network won 88 and tied 136 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network

Training iteration 77 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.20 seconds
Training examples lengths: [64832, 64497, 65060, 64767, 64650, 64722, 64750, 64942, 64710, 64900]
Total value: 404858.29
Training on 647830 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3038 (value: 0.0017, weighted value: 0.0830, policy: 0.2209, weighted policy: 0.2209), Train Mean Max: 0.9096
Epoch 2/10, Train Loss: 0.2891 (value: 0.0014, weighted value: 0.0719, policy: 0.2173, weighted policy: 0.2173), Train Mean Max: 0.9102
Epoch 3/10, Train Loss: 0.2838 (value: 0.0014, weighted value: 0.0683, policy: 0.2155, weighted policy: 0.2155), Train Mean Max: 0.9105
Epoch 4/10, Train Loss: 0.2809 (value: 0.0013, weighted value: 0.0668, policy: 0.2142, weighted policy: 0.2142), Train Mean Max: 0.9105
Epoch 5/10, Train Loss: 0.2763 (value: 0.0013, weighted value: 0.0630, policy: 0.2133, weighted policy: 0.2133), Train Mean Max: 0.9108
Epoch 6/10, Train Loss: 0.2742 (value: 0.0012, weighted value: 0.0616, policy: 0.2126, weighted policy: 0.2126), Train Mean Max: 0.9112
Epoch 7/10, Train Loss: 0.2705 (value: 0.0012, weighted value: 0.0589, policy: 0.2116, weighted policy: 0.2116), Train Mean Max: 0.9114
Epoch 8/10, Train Loss: 0.2684 (value: 0.0011, weighted value: 0.0574, policy: 0.2110, weighted policy: 0.2110), Train Mean Max: 0.9117
Epoch 9/10, Train Loss: 0.2654 (value: 0.0011, weighted value: 0.0557, policy: 0.2096, weighted policy: 0.2096), Train Mean Max: 0.9121
Epoch 10/10, Train Loss: 0.2640 (value: 0.0011, weighted value: 0.0538, policy: 0.2102, weighted policy: 0.2102), Train Mean Max: 0.9122
..training done in 72.52 seconds
..evaluation done in 16.15 seconds
Old network+MCTS average reward: 0.61, min: -0.12, max: 1.19, stdev: 0.22
New network+MCTS average reward: 0.62, min: 0.01, max: 1.20, stdev: 0.22
Old bare network average reward: 0.53, min: -0.12, max: 1.19, stdev: 0.23
New bare network average reward: 0.54, min: -0.12, max: 1.20, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.31, max: 0.88, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: 0.08, max: 1.36, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.09, max: 1.43, stdev: 0.22
New network won 99 and tied 116 out of 300 games (52.33% wins where ties are half wins)
Keeping the new network

Training iteration 78 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.96 seconds
Training examples lengths: [64497, 65060, 64767, 64650, 64722, 64750, 64942, 64710, 64900, 64765]
Total value: 405004.83
Training on 647763 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2990 (value: 0.0016, weighted value: 0.0808, policy: 0.2182, weighted policy: 0.2182), Train Mean Max: 0.9110
Epoch 2/10, Train Loss: 0.2859 (value: 0.0014, weighted value: 0.0709, policy: 0.2150, weighted policy: 0.2150), Train Mean Max: 0.9116
Epoch 3/10, Train Loss: 0.2809 (value: 0.0013, weighted value: 0.0670, policy: 0.2138, weighted policy: 0.2138), Train Mean Max: 0.9118
Epoch 4/10, Train Loss: 0.2758 (value: 0.0013, weighted value: 0.0639, policy: 0.2118, weighted policy: 0.2118), Train Mean Max: 0.9120
Epoch 5/10, Train Loss: 0.2710 (value: 0.0012, weighted value: 0.0615, policy: 0.2095, weighted policy: 0.2095), Train Mean Max: 0.9123
Epoch 6/10, Train Loss: 0.2697 (value: 0.0012, weighted value: 0.0590, policy: 0.2108, weighted policy: 0.2108), Train Mean Max: 0.9124
Epoch 7/10, Train Loss: 0.2649 (value: 0.0011, weighted value: 0.0571, policy: 0.2077, weighted policy: 0.2077), Train Mean Max: 0.9128
Epoch 8/10, Train Loss: 0.2634 (value: 0.0011, weighted value: 0.0549, policy: 0.2085, weighted policy: 0.2085), Train Mean Max: 0.9128
Epoch 9/10, Train Loss: 0.2621 (value: 0.0011, weighted value: 0.0542, policy: 0.2079, weighted policy: 0.2079), Train Mean Max: 0.9132
Epoch 10/10, Train Loss: 0.2611 (value: 0.0011, weighted value: 0.0536, policy: 0.2075, weighted policy: 0.2075), Train Mean Max: 0.9133
..training done in 66.66 seconds
..evaluation done in 15.28 seconds
Old network+MCTS average reward: 0.62, min: 0.08, max: 1.24, stdev: 0.21
New network+MCTS average reward: 0.62, min: 0.09, max: 1.24, stdev: 0.20
Old bare network average reward: 0.54, min: 0.05, max: 1.24, stdev: 0.22
New bare network average reward: 0.55, min: 0.04, max: 1.24, stdev: 0.21
External policy "random" average reward: 0.26, min: -0.24, max: 0.91, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: 0.05, max: 1.28, stdev: 0.21
External policy "total greedy" average reward: 0.66, min: 0.16, max: 1.32, stdev: 0.20
New network won 98 and tied 105 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 79 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.64 seconds
Training examples lengths: [65060, 64767, 64650, 64722, 64750, 64942, 64710, 64900, 64765, 64890]
Total value: 404881.94
Training on 648156 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2945 (value: 0.0016, weighted value: 0.0784, policy: 0.2161, weighted policy: 0.2161), Train Mean Max: 0.9123
Epoch 2/10, Train Loss: 0.2830 (value: 0.0014, weighted value: 0.0700, policy: 0.2131, weighted policy: 0.2131), Train Mean Max: 0.9126
Epoch 3/10, Train Loss: 0.2770 (value: 0.0013, weighted value: 0.0662, policy: 0.2108, weighted policy: 0.2108), Train Mean Max: 0.9130
Epoch 4/10, Train Loss: 0.2731 (value: 0.0013, weighted value: 0.0641, policy: 0.2089, weighted policy: 0.2089), Train Mean Max: 0.9132
Epoch 5/10, Train Loss: 0.2671 (value: 0.0012, weighted value: 0.0593, policy: 0.2078, weighted policy: 0.2078), Train Mean Max: 0.9134
Epoch 6/10, Train Loss: 0.2646 (value: 0.0011, weighted value: 0.0572, policy: 0.2074, weighted policy: 0.2074), Train Mean Max: 0.9135
Epoch 7/10, Train Loss: 0.2633 (value: 0.0011, weighted value: 0.0568, policy: 0.2065, weighted policy: 0.2065), Train Mean Max: 0.9137
Epoch 8/10, Train Loss: 0.2605 (value: 0.0011, weighted value: 0.0551, policy: 0.2053, weighted policy: 0.2053), Train Mean Max: 0.9140
Epoch 9/10, Train Loss: 0.2605 (value: 0.0011, weighted value: 0.0550, policy: 0.2055, weighted policy: 0.2055), Train Mean Max: 0.9141
Epoch 10/10, Train Loss: 0.2557 (value: 0.0010, weighted value: 0.0513, policy: 0.2043, weighted policy: 0.2043), Train Mean Max: 0.9142
..training done in 72.24 seconds
..evaluation done in 16.02 seconds
Old network+MCTS average reward: 0.61, min: 0.03, max: 1.22, stdev: 0.20
New network+MCTS average reward: 0.62, min: -0.08, max: 1.19, stdev: 0.21
Old bare network average reward: 0.54, min: -0.19, max: 1.15, stdev: 0.21
New bare network average reward: 0.54, min: 0.00, max: 1.17, stdev: 0.21
External policy "random" average reward: 0.26, min: -0.31, max: 0.98, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.03, max: 1.20, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.06, max: 1.29, stdev: 0.22
New network won 96 and tied 125 out of 300 games (52.83% wins where ties are half wins)
Keeping the new network

Training iteration 80 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.32 seconds
Training examples lengths: [64767, 64650, 64722, 64750, 64942, 64710, 64900, 64765, 64890, 64852]
Total value: 405472.30
Training on 647948 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2906 (value: 0.0016, weighted value: 0.0782, policy: 0.2124, weighted policy: 0.2124), Train Mean Max: 0.9134
Epoch 2/10, Train Loss: 0.2768 (value: 0.0013, weighted value: 0.0675, policy: 0.2094, weighted policy: 0.2094), Train Mean Max: 0.9139
Epoch 3/10, Train Loss: 0.2745 (value: 0.0013, weighted value: 0.0668, policy: 0.2077, weighted policy: 0.2077), Train Mean Max: 0.9141
Epoch 4/10, Train Loss: 0.2670 (value: 0.0012, weighted value: 0.0618, policy: 0.2052, weighted policy: 0.2052), Train Mean Max: 0.9143
Epoch 5/10, Train Loss: 0.2636 (value: 0.0012, weighted value: 0.0588, policy: 0.2048, weighted policy: 0.2048), Train Mean Max: 0.9146
Epoch 6/10, Train Loss: 0.2618 (value: 0.0012, weighted value: 0.0577, policy: 0.2041, weighted policy: 0.2041), Train Mean Max: 0.9148
Epoch 7/10, Train Loss: 0.2598 (value: 0.0011, weighted value: 0.0561, policy: 0.2037, weighted policy: 0.2037), Train Mean Max: 0.9149
Epoch 8/10, Train Loss: 0.2578 (value: 0.0011, weighted value: 0.0546, policy: 0.2032, weighted policy: 0.2032), Train Mean Max: 0.9151
Epoch 9/10, Train Loss: 0.2557 (value: 0.0011, weighted value: 0.0540, policy: 0.2017, weighted policy: 0.2017), Train Mean Max: 0.9154
Epoch 10/10, Train Loss: 0.2525 (value: 0.0010, weighted value: 0.0506, policy: 0.2020, weighted policy: 0.2020), Train Mean Max: 0.9156
..training done in 70.65 seconds
..evaluation done in 16.51 seconds
Old network+MCTS average reward: 0.62, min: -0.06, max: 1.35, stdev: 0.21
New network+MCTS average reward: 0.62, min: -0.15, max: 1.35, stdev: 0.22
Old bare network average reward: 0.54, min: -0.24, max: 1.35, stdev: 0.23
New bare network average reward: 0.54, min: -0.28, max: 1.35, stdev: 0.22
External policy "random" average reward: 0.25, min: -0.23, max: 0.85, stdev: 0.21
External policy "individual greedy" average reward: 0.55, min: -0.04, max: 1.26, stdev: 0.21
External policy "total greedy" average reward: 0.65, min: 0.01, max: 1.26, stdev: 0.22
New network won 86 and tied 134 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to zgaz_checkpoints/1757654225_iter_80

Training iteration 81 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.68 seconds
Training examples lengths: [64650, 64722, 64750, 64942, 64710, 64900, 64765, 64890, 64852, 64931]
Total value: 405773.44
Training on 648112 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2873 (value: 0.0015, weighted value: 0.0771, policy: 0.2102, weighted policy: 0.2102), Train Mean Max: 0.9146
Epoch 2/10, Train Loss: 0.2778 (value: 0.0014, weighted value: 0.0707, policy: 0.2071, weighted policy: 0.2071), Train Mean Max: 0.9150
Epoch 3/10, Train Loss: 0.2718 (value: 0.0013, weighted value: 0.0670, policy: 0.2048, weighted policy: 0.2048), Train Mean Max: 0.9152
Epoch 4/10, Train Loss: 0.2647 (value: 0.0012, weighted value: 0.0616, policy: 0.2031, weighted policy: 0.2031), Train Mean Max: 0.9155
Epoch 5/10, Train Loss: 0.2619 (value: 0.0012, weighted value: 0.0594, policy: 0.2025, weighted policy: 0.2025), Train Mean Max: 0.9156
Epoch 6/10, Train Loss: 0.2593 (value: 0.0011, weighted value: 0.0572, policy: 0.2021, weighted policy: 0.2021), Train Mean Max: 0.9159
Epoch 7/10, Train Loss: 0.2582 (value: 0.0011, weighted value: 0.0571, policy: 0.2011, weighted policy: 0.2011), Train Mean Max: 0.9160
Epoch 8/10, Train Loss: 0.2540 (value: 0.0011, weighted value: 0.0539, policy: 0.2001, weighted policy: 0.2001), Train Mean Max: 0.9163
Epoch 9/10, Train Loss: 0.2527 (value: 0.0011, weighted value: 0.0530, policy: 0.1997, weighted policy: 0.1997), Train Mean Max: 0.9164
Epoch 10/10, Train Loss: 0.2512 (value: 0.0010, weighted value: 0.0521, policy: 0.1991, weighted policy: 0.1991), Train Mean Max: 0.9167
..training done in 71.91 seconds
..evaluation done in 16.04 seconds
Old network+MCTS average reward: 0.60, min: -0.06, max: 1.16, stdev: 0.22
New network+MCTS average reward: 0.60, min: 0.02, max: 1.18, stdev: 0.22
Old bare network average reward: 0.53, min: -0.09, max: 1.10, stdev: 0.23
New bare network average reward: 0.52, min: -0.09, max: 1.17, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.25, max: 0.89, stdev: 0.22
External policy "individual greedy" average reward: 0.52, min: 0.03, max: 1.22, stdev: 0.23
External policy "total greedy" average reward: 0.63, min: 0.15, max: 1.25, stdev: 0.22
New network won 99 and tied 111 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 82 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.06 seconds
Training examples lengths: [64722, 64750, 64942, 64710, 64900, 64765, 64890, 64852, 64931, 64551]
Total value: 405978.64
Training on 648013 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2855 (value: 0.0015, weighted value: 0.0751, policy: 0.2103, weighted policy: 0.2103), Train Mean Max: 0.9155
Epoch 2/10, Train Loss: 0.2740 (value: 0.0014, weighted value: 0.0690, policy: 0.2050, weighted policy: 0.2050), Train Mean Max: 0.9159
Epoch 3/10, Train Loss: 0.2669 (value: 0.0013, weighted value: 0.0631, policy: 0.2039, weighted policy: 0.2039), Train Mean Max: 0.9162
Epoch 4/10, Train Loss: 0.2634 (value: 0.0012, weighted value: 0.0615, policy: 0.2019, weighted policy: 0.2019), Train Mean Max: 0.9164
Epoch 5/10, Train Loss: 0.2609 (value: 0.0012, weighted value: 0.0603, policy: 0.2006, weighted policy: 0.2006), Train Mean Max: 0.9165
Epoch 6/10, Train Loss: 0.2561 (value: 0.0011, weighted value: 0.0557, policy: 0.2003, weighted policy: 0.2003), Train Mean Max: 0.9168
Epoch 7/10, Train Loss: 0.2565 (value: 0.0011, weighted value: 0.0568, policy: 0.1997, weighted policy: 0.1997), Train Mean Max: 0.9168
Epoch 8/10, Train Loss: 0.2522 (value: 0.0011, weighted value: 0.0526, policy: 0.1996, weighted policy: 0.1996), Train Mean Max: 0.9171
Epoch 9/10, Train Loss: 0.2509 (value: 0.0011, weighted value: 0.0531, policy: 0.1978, weighted policy: 0.1978), Train Mean Max: 0.9173
Epoch 10/10, Train Loss: 0.2488 (value: 0.0010, weighted value: 0.0519, policy: 0.1969, weighted policy: 0.1969), Train Mean Max: 0.9176
..training done in 67.67 seconds
..evaluation done in 16.28 seconds
Old network+MCTS average reward: 0.63, min: 0.05, max: 1.64, stdev: 0.23
New network+MCTS average reward: 0.64, min: 0.09, max: 1.47, stdev: 0.22
Old bare network average reward: 0.55, min: 0.05, max: 1.31, stdev: 0.23
New bare network average reward: 0.56, min: 0.06, max: 1.32, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.31, max: 1.22, stdev: 0.24
External policy "individual greedy" average reward: 0.55, min: -0.08, max: 1.56, stdev: 0.24
External policy "total greedy" average reward: 0.66, min: -0.06, max: 1.57, stdev: 0.23
New network won 110 and tied 108 out of 300 games (54.67% wins where ties are half wins)
Keeping the new network

Training iteration 83 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.35 seconds
Training examples lengths: [64750, 64942, 64710, 64900, 64765, 64890, 64852, 64931, 64551, 65113]
Total value: 406093.07
Training on 648404 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2823 (value: 0.0015, weighted value: 0.0760, policy: 0.2064, weighted policy: 0.2064), Train Mean Max: 0.9164
Epoch 2/10, Train Loss: 0.2705 (value: 0.0013, weighted value: 0.0674, policy: 0.2030, weighted policy: 0.2030), Train Mean Max: 0.9168
Epoch 3/10, Train Loss: 0.2637 (value: 0.0013, weighted value: 0.0636, policy: 0.2001, weighted policy: 0.2001), Train Mean Max: 0.9170
Epoch 4/10, Train Loss: 0.2631 (value: 0.0013, weighted value: 0.0635, policy: 0.1995, weighted policy: 0.1995), Train Mean Max: 0.9172
Epoch 5/10, Train Loss: 0.2555 (value: 0.0011, weighted value: 0.0567, policy: 0.1987, weighted policy: 0.1987), Train Mean Max: 0.9174
Epoch 6/10, Train Loss: 0.2542 (value: 0.0011, weighted value: 0.0562, policy: 0.1980, weighted policy: 0.1980), Train Mean Max: 0.9176
Epoch 7/10, Train Loss: 0.2521 (value: 0.0011, weighted value: 0.0553, policy: 0.1968, weighted policy: 0.1968), Train Mean Max: 0.9178
Epoch 8/10, Train Loss: 0.2517 (value: 0.0011, weighted value: 0.0553, policy: 0.1964, weighted policy: 0.1964), Train Mean Max: 0.9180
Epoch 9/10, Train Loss: 0.2468 (value: 0.0010, weighted value: 0.0512, policy: 0.1955, weighted policy: 0.1955), Train Mean Max: 0.9181
Epoch 10/10, Train Loss: 0.2480 (value: 0.0010, weighted value: 0.0522, policy: 0.1958, weighted policy: 0.1958), Train Mean Max: 0.9183
..training done in 71.47 seconds
..evaluation done in 15.68 seconds
Old network+MCTS average reward: 0.62, min: 0.03, max: 1.61, stdev: 0.21
New network+MCTS average reward: 0.61, min: -0.06, max: 1.55, stdev: 0.21
Old bare network average reward: 0.54, min: -0.08, max: 1.64, stdev: 0.22
New bare network average reward: 0.54, min: -0.12, max: 1.55, stdev: 0.22
External policy "random" average reward: 0.26, min: -0.25, max: 1.44, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: -0.06, max: 1.54, stdev: 0.21
External policy "total greedy" average reward: 0.65, min: 0.09, max: 1.70, stdev: 0.22
New network won 76 and tied 128 out of 300 games (46.67% wins where ties are half wins)
Reverting to the old network

Training iteration 84 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.44 seconds
Training examples lengths: [64942, 64710, 64900, 64765, 64890, 64852, 64931, 64551, 65113, 64762]
Total value: 406604.52
Training on 648416 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3129 (value: 0.0020, weighted value: 0.1006, policy: 0.2122, weighted policy: 0.2122), Train Mean Max: 0.9153
Epoch 2/10, Train Loss: 0.2915 (value: 0.0017, weighted value: 0.0839, policy: 0.2077, weighted policy: 0.2077), Train Mean Max: 0.9159
Epoch 3/10, Train Loss: 0.2831 (value: 0.0016, weighted value: 0.0791, policy: 0.2040, weighted policy: 0.2040), Train Mean Max: 0.9162
Epoch 4/10, Train Loss: 0.2756 (value: 0.0015, weighted value: 0.0735, policy: 0.2021, weighted policy: 0.2021), Train Mean Max: 0.9165
Epoch 5/10, Train Loss: 0.2690 (value: 0.0014, weighted value: 0.0685, policy: 0.2005, weighted policy: 0.2005), Train Mean Max: 0.9167
Epoch 6/10, Train Loss: 0.2655 (value: 0.0013, weighted value: 0.0665, policy: 0.1990, weighted policy: 0.1990), Train Mean Max: 0.9171
Epoch 7/10, Train Loss: 0.2614 (value: 0.0013, weighted value: 0.0639, policy: 0.1975, weighted policy: 0.1975), Train Mean Max: 0.9172
Epoch 8/10, Train Loss: 0.2583 (value: 0.0012, weighted value: 0.0611, policy: 0.1972, weighted policy: 0.1972), Train Mean Max: 0.9175
Epoch 9/10, Train Loss: 0.2563 (value: 0.0012, weighted value: 0.0591, policy: 0.1971, weighted policy: 0.1971), Train Mean Max: 0.9177
Epoch 10/10, Train Loss: 0.2531 (value: 0.0012, weighted value: 0.0577, policy: 0.1954, weighted policy: 0.1954), Train Mean Max: 0.9178
..training done in 70.98 seconds
..evaluation done in 16.19 seconds
Old network+MCTS average reward: 0.61, min: -0.01, max: 1.39, stdev: 0.22
New network+MCTS average reward: 0.61, min: 0.05, max: 1.42, stdev: 0.22
Old bare network average reward: 0.53, min: -0.02, max: 1.39, stdev: 0.23
New bare network average reward: 0.53, min: -0.07, max: 1.39, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.47, max: 0.92, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: 0.03, max: 1.34, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.13, max: 1.43, stdev: 0.21
New network won 90 and tied 132 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network

Training iteration 85 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.71 seconds
Training examples lengths: [64710, 64900, 64765, 64890, 64852, 64931, 64551, 65113, 64762, 64615]
Total value: 406711.85
Training on 648089 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2844 (value: 0.0016, weighted value: 0.0814, policy: 0.2030, weighted policy: 0.2030), Train Mean Max: 0.9170
Epoch 2/10, Train Loss: 0.2757 (value: 0.0015, weighted value: 0.0745, policy: 0.2011, weighted policy: 0.2011), Train Mean Max: 0.9174
Epoch 3/10, Train Loss: 0.2664 (value: 0.0014, weighted value: 0.0689, policy: 0.1975, weighted policy: 0.1975), Train Mean Max: 0.9176
Epoch 4/10, Train Loss: 0.2633 (value: 0.0013, weighted value: 0.0666, policy: 0.1967, weighted policy: 0.1967), Train Mean Max: 0.9179
Epoch 5/10, Train Loss: 0.2588 (value: 0.0013, weighted value: 0.0627, policy: 0.1962, weighted policy: 0.1962), Train Mean Max: 0.9181
Epoch 6/10, Train Loss: 0.2550 (value: 0.0012, weighted value: 0.0596, policy: 0.1954, weighted policy: 0.1954), Train Mean Max: 0.9184
Epoch 7/10, Train Loss: 0.2545 (value: 0.0012, weighted value: 0.0597, policy: 0.1948, weighted policy: 0.1948), Train Mean Max: 0.9185
Epoch 8/10, Train Loss: 0.2511 (value: 0.0011, weighted value: 0.0570, policy: 0.1941, weighted policy: 0.1941), Train Mean Max: 0.9187
Epoch 9/10, Train Loss: 0.2487 (value: 0.0011, weighted value: 0.0557, policy: 0.1931, weighted policy: 0.1931), Train Mean Max: 0.9190
Epoch 10/10, Train Loss: 0.2471 (value: 0.0011, weighted value: 0.0548, policy: 0.1923, weighted policy: 0.1923), Train Mean Max: 0.9192
..training done in 65.60 seconds
..evaluation done in 15.76 seconds
Old network+MCTS average reward: 0.62, min: 0.08, max: 1.30, stdev: 0.21
New network+MCTS average reward: 0.62, min: 0.06, max: 1.31, stdev: 0.21
Old bare network average reward: 0.55, min: -0.08, max: 1.31, stdev: 0.22
New bare network average reward: 0.55, min: -0.06, max: 1.31, stdev: 0.22
External policy "random" average reward: 0.27, min: -0.34, max: 0.99, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.05, max: 1.30, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.07, max: 1.29, stdev: 0.22
New network won 79 and tied 141 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 86 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.91 seconds
Training examples lengths: [64900, 64765, 64890, 64852, 64931, 64551, 65113, 64762, 64615, 64862]
Total value: 407003.62
Training on 648241 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3184 (value: 0.0021, weighted value: 0.1068, policy: 0.2116, weighted policy: 0.2116), Train Mean Max: 0.9155
Epoch 2/10, Train Loss: 0.2969 (value: 0.0018, weighted value: 0.0906, policy: 0.2062, weighted policy: 0.2062), Train Mean Max: 0.9162
Epoch 3/10, Train Loss: 0.2863 (value: 0.0017, weighted value: 0.0833, policy: 0.2030, weighted policy: 0.2030), Train Mean Max: 0.9165
Epoch 4/10, Train Loss: 0.2790 (value: 0.0016, weighted value: 0.0783, policy: 0.2008, weighted policy: 0.2008), Train Mean Max: 0.9167
Epoch 5/10, Train Loss: 0.2735 (value: 0.0015, weighted value: 0.0743, policy: 0.1991, weighted policy: 0.1991), Train Mean Max: 0.9170
Epoch 6/10, Train Loss: 0.2669 (value: 0.0014, weighted value: 0.0697, policy: 0.1972, weighted policy: 0.1972), Train Mean Max: 0.9174
Epoch 7/10, Train Loss: 0.2645 (value: 0.0014, weighted value: 0.0679, policy: 0.1966, weighted policy: 0.1966), Train Mean Max: 0.9176
Epoch 8/10, Train Loss: 0.2610 (value: 0.0013, weighted value: 0.0645, policy: 0.1965, weighted policy: 0.1965), Train Mean Max: 0.9179
Epoch 9/10, Train Loss: 0.2595 (value: 0.0013, weighted value: 0.0634, policy: 0.1962, weighted policy: 0.1962), Train Mean Max: 0.9180
Epoch 10/10, Train Loss: 0.2543 (value: 0.0012, weighted value: 0.0599, policy: 0.1945, weighted policy: 0.1945), Train Mean Max: 0.9184
..training done in 65.14 seconds
..evaluation done in 15.91 seconds
Old network+MCTS average reward: 0.61, min: 0.09, max: 1.16, stdev: 0.22
New network+MCTS average reward: 0.61, min: 0.06, max: 1.13, stdev: 0.22
Old bare network average reward: 0.54, min: 0.04, max: 1.12, stdev: 0.22
New bare network average reward: 0.55, min: 0.05, max: 1.12, stdev: 0.22
External policy "random" average reward: 0.26, min: -0.26, max: 0.86, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: 0.04, max: 1.25, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.11, max: 1.27, stdev: 0.22
New network won 85 and tied 131 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 87 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.35 seconds
Training examples lengths: [64765, 64890, 64852, 64931, 64551, 65113, 64762, 64615, 64862, 64866]
Total value: 407692.53
Training on 648207 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2866 (value: 0.0017, weighted value: 0.0838, policy: 0.2027, weighted policy: 0.2027), Train Mean Max: 0.9173
Epoch 2/10, Train Loss: 0.2744 (value: 0.0015, weighted value: 0.0757, policy: 0.1987, weighted policy: 0.1987), Train Mean Max: 0.9177
Epoch 3/10, Train Loss: 0.2678 (value: 0.0014, weighted value: 0.0715, policy: 0.1964, weighted policy: 0.1964), Train Mean Max: 0.9180
Epoch 4/10, Train Loss: 0.2651 (value: 0.0014, weighted value: 0.0686, policy: 0.1964, weighted policy: 0.1964), Train Mean Max: 0.9181
Epoch 5/10, Train Loss: 0.2596 (value: 0.0013, weighted value: 0.0657, policy: 0.1940, weighted policy: 0.1940), Train Mean Max: 0.9186
Epoch 6/10, Train Loss: 0.2559 (value: 0.0012, weighted value: 0.0618, policy: 0.1941, weighted policy: 0.1941), Train Mean Max: 0.9186
Epoch 7/10, Train Loss: 0.2526 (value: 0.0012, weighted value: 0.0591, policy: 0.1935, weighted policy: 0.1935), Train Mean Max: 0.9191
Epoch 8/10, Train Loss: 0.2509 (value: 0.0012, weighted value: 0.0597, policy: 0.1912, weighted policy: 0.1912), Train Mean Max: 0.9193
Epoch 9/10, Train Loss: 0.2482 (value: 0.0011, weighted value: 0.0569, policy: 0.1914, weighted policy: 0.1914), Train Mean Max: 0.9195
Epoch 10/10, Train Loss: 0.2458 (value: 0.0011, weighted value: 0.0554, policy: 0.1904, weighted policy: 0.1904), Train Mean Max: 0.9199
..training done in 72.93 seconds
..evaluation done in 15.69 seconds
Old network+MCTS average reward: 0.62, min: 0.01, max: 1.44, stdev: 0.22
New network+MCTS average reward: 0.61, min: 0.01, max: 1.41, stdev: 0.22
Old bare network average reward: 0.54, min: -0.13, max: 1.32, stdev: 0.23
New bare network average reward: 0.55, min: -0.07, max: 1.34, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.24, max: 0.89, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: -0.08, max: 1.28, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.06, max: 1.43, stdev: 0.22
New network won 77 and tied 136 out of 300 games (48.33% wins where ties are half wins)
Reverting to the old network

Training iteration 88 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.37 seconds
Training examples lengths: [64890, 64852, 64931, 64551, 65113, 64762, 64615, 64862, 64866, 64922]
Total value: 408436.97
Training on 648364 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3143 (value: 0.0021, weighted value: 0.1050, policy: 0.2093, weighted policy: 0.2093), Train Mean Max: 0.9162
Epoch 2/10, Train Loss: 0.2977 (value: 0.0019, weighted value: 0.0931, policy: 0.2047, weighted policy: 0.2047), Train Mean Max: 0.9168
Epoch 3/10, Train Loss: 0.2869 (value: 0.0017, weighted value: 0.0854, policy: 0.2015, weighted policy: 0.2015), Train Mean Max: 0.9173
Epoch 4/10, Train Loss: 0.2788 (value: 0.0016, weighted value: 0.0793, policy: 0.1994, weighted policy: 0.1994), Train Mean Max: 0.9174
Epoch 5/10, Train Loss: 0.2718 (value: 0.0015, weighted value: 0.0751, policy: 0.1967, weighted policy: 0.1967), Train Mean Max: 0.9179
Epoch 6/10, Train Loss: 0.2673 (value: 0.0014, weighted value: 0.0714, policy: 0.1959, weighted policy: 0.1959), Train Mean Max: 0.9180
Epoch 7/10, Train Loss: 0.2651 (value: 0.0014, weighted value: 0.0705, policy: 0.1946, weighted policy: 0.1946), Train Mean Max: 0.9182
Epoch 8/10, Train Loss: 0.2594 (value: 0.0013, weighted value: 0.0656, policy: 0.1938, weighted policy: 0.1938), Train Mean Max: 0.9185
Epoch 9/10, Train Loss: 0.2554 (value: 0.0013, weighted value: 0.0628, policy: 0.1926, weighted policy: 0.1926), Train Mean Max: 0.9188
Epoch 10/10, Train Loss: 0.2543 (value: 0.0012, weighted value: 0.0622, policy: 0.1922, weighted policy: 0.1922), Train Mean Max: 0.9191
..training done in 65.58 seconds
..evaluation done in 17.18 seconds
Old network+MCTS average reward: 0.63, min: 0.01, max: 1.69, stdev: 0.24
New network+MCTS average reward: 0.63, min: 0.00, max: 1.69, stdev: 0.24
Old bare network average reward: 0.57, min: 0.01, max: 1.68, stdev: 0.24
New bare network average reward: 0.56, min: 0.01, max: 1.52, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.28, max: 1.30, stdev: 0.25
External policy "individual greedy" average reward: 0.54, min: -0.12, max: 1.67, stdev: 0.24
External policy "total greedy" average reward: 0.65, min: 0.06, max: 1.45, stdev: 0.24
New network won 84 and tied 132 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 89 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.41 seconds
Training examples lengths: [64852, 64931, 64551, 65113, 64762, 64615, 64862, 64866, 64922, 64767]
Total value: 409213.06
Training on 648241 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2852 (value: 0.0017, weighted value: 0.0848, policy: 0.2005, weighted policy: 0.2005), Train Mean Max: 0.9181
Epoch 2/10, Train Loss: 0.2730 (value: 0.0015, weighted value: 0.0764, policy: 0.1966, weighted policy: 0.1966), Train Mean Max: 0.9184
Epoch 3/10, Train Loss: 0.2678 (value: 0.0014, weighted value: 0.0725, policy: 0.1953, weighted policy: 0.1953), Train Mean Max: 0.9188
Epoch 4/10, Train Loss: 0.2621 (value: 0.0014, weighted value: 0.0686, policy: 0.1935, weighted policy: 0.1935), Train Mean Max: 0.9191
Epoch 5/10, Train Loss: 0.2577 (value: 0.0013, weighted value: 0.0647, policy: 0.1931, weighted policy: 0.1931), Train Mean Max: 0.9193
Epoch 6/10, Train Loss: 0.2549 (value: 0.0013, weighted value: 0.0637, policy: 0.1912, weighted policy: 0.1912), Train Mean Max: 0.9196
Epoch 7/10, Train Loss: 0.2517 (value: 0.0012, weighted value: 0.0606, policy: 0.1912, weighted policy: 0.1912), Train Mean Max: 0.9197
Epoch 8/10, Train Loss: 0.2505 (value: 0.0012, weighted value: 0.0594, policy: 0.1911, weighted policy: 0.1911), Train Mean Max: 0.9200
Epoch 9/10, Train Loss: 0.2464 (value: 0.0011, weighted value: 0.0571, policy: 0.1892, weighted policy: 0.1892), Train Mean Max: 0.9203
Epoch 10/10, Train Loss: 0.2454 (value: 0.0011, weighted value: 0.0563, policy: 0.1891, weighted policy: 0.1891), Train Mean Max: 0.9205
..training done in 65.11 seconds
..evaluation done in 15.99 seconds
Old network+MCTS average reward: 0.61, min: 0.10, max: 1.32, stdev: 0.22
New network+MCTS average reward: 0.61, min: 0.09, max: 1.33, stdev: 0.23
Old bare network average reward: 0.53, min: -0.01, max: 1.32, stdev: 0.23
New bare network average reward: 0.54, min: -0.01, max: 1.29, stdev: 0.22
External policy "random" average reward: 0.24, min: -0.42, max: 0.90, stdev: 0.23
External policy "individual greedy" average reward: 0.52, min: 0.05, max: 1.32, stdev: 0.22
External policy "total greedy" average reward: 0.63, min: 0.17, max: 1.28, stdev: 0.22
New network won 92 and tied 114 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 90 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.15 seconds
Training examples lengths: [64931, 64551, 65113, 64762, 64615, 64862, 64866, 64922, 64767, 64971]
Total value: 409728.53
Training on 648360 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3185 (value: 0.0022, weighted value: 0.1101, policy: 0.2083, weighted policy: 0.2083), Train Mean Max: 0.9169
Epoch 2/10, Train Loss: 0.2948 (value: 0.0019, weighted value: 0.0928, policy: 0.2020, weighted policy: 0.2020), Train Mean Max: 0.9175
Epoch 3/10, Train Loss: 0.2858 (value: 0.0017, weighted value: 0.0863, policy: 0.1995, weighted policy: 0.1995), Train Mean Max: 0.9177
Epoch 4/10, Train Loss: 0.2791 (value: 0.0016, weighted value: 0.0814, policy: 0.1977, weighted policy: 0.1977), Train Mean Max: 0.9182
Epoch 5/10, Train Loss: 0.2709 (value: 0.0015, weighted value: 0.0757, policy: 0.1952, weighted policy: 0.1952), Train Mean Max: 0.9185
Epoch 6/10, Train Loss: 0.2683 (value: 0.0015, weighted value: 0.0740, policy: 0.1943, weighted policy: 0.1943), Train Mean Max: 0.9185
Epoch 7/10, Train Loss: 0.2617 (value: 0.0014, weighted value: 0.0687, policy: 0.1930, weighted policy: 0.1930), Train Mean Max: 0.9189
Epoch 8/10, Train Loss: 0.2594 (value: 0.0013, weighted value: 0.0671, policy: 0.1923, weighted policy: 0.1923), Train Mean Max: 0.9193
Epoch 9/10, Train Loss: 0.2571 (value: 0.0013, weighted value: 0.0653, policy: 0.1918, weighted policy: 0.1918), Train Mean Max: 0.9196
Epoch 10/10, Train Loss: 0.2519 (value: 0.0012, weighted value: 0.0606, policy: 0.1913, weighted policy: 0.1913), Train Mean Max: 0.9197
..training done in 72.09 seconds
..evaluation done in 16.01 seconds
Old network+MCTS average reward: 0.63, min: -0.05, max: 1.43, stdev: 0.21
New network+MCTS average reward: 0.63, min: -0.05, max: 1.43, stdev: 0.21
Old bare network average reward: 0.55, min: -0.14, max: 1.24, stdev: 0.22
New bare network average reward: 0.56, min: -0.14, max: 1.27, stdev: 0.22
External policy "random" average reward: 0.27, min: -0.35, max: 0.88, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.01, max: 1.14, stdev: 0.21
External policy "total greedy" average reward: 0.65, min: 0.00, max: 1.31, stdev: 0.21
New network won 89 and tied 128 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 91 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.42 seconds
Training examples lengths: [64551, 65113, 64762, 64615, 64862, 64866, 64922, 64767, 64971, 64699]
Total value: 410330.64
Training on 648128 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2848 (value: 0.0017, weighted value: 0.0864, policy: 0.1984, weighted policy: 0.1984), Train Mean Max: 0.9187
Epoch 2/10, Train Loss: 0.2733 (value: 0.0016, weighted value: 0.0778, policy: 0.1955, weighted policy: 0.1955), Train Mean Max: 0.9193
Epoch 3/10, Train Loss: 0.2680 (value: 0.0015, weighted value: 0.0739, policy: 0.1940, weighted policy: 0.1940), Train Mean Max: 0.9195
Epoch 4/10, Train Loss: 0.2582 (value: 0.0013, weighted value: 0.0675, policy: 0.1907, weighted policy: 0.1907), Train Mean Max: 0.9199
Epoch 5/10, Train Loss: 0.2553 (value: 0.0013, weighted value: 0.0648, policy: 0.1905, weighted policy: 0.1905), Train Mean Max: 0.9202
Epoch 6/10, Train Loss: 0.2522 (value: 0.0013, weighted value: 0.0629, policy: 0.1893, weighted policy: 0.1893), Train Mean Max: 0.9206
Epoch 7/10, Train Loss: 0.2510 (value: 0.0012, weighted value: 0.0622, policy: 0.1888, weighted policy: 0.1888), Train Mean Max: 0.9207
Epoch 8/10, Train Loss: 0.2476 (value: 0.0012, weighted value: 0.0589, policy: 0.1887, weighted policy: 0.1887), Train Mean Max: 0.9208
Epoch 9/10, Train Loss: 0.2441 (value: 0.0011, weighted value: 0.0574, policy: 0.1868, weighted policy: 0.1868), Train Mean Max: 0.9212
Epoch 10/10, Train Loss: 0.2438 (value: 0.0011, weighted value: 0.0571, policy: 0.1867, weighted policy: 0.1867), Train Mean Max: 0.9213
..training done in 70.53 seconds
..evaluation done in 16.59 seconds
Old network+MCTS average reward: 0.65, min: 0.06, max: 1.41, stdev: 0.23
New network+MCTS average reward: 0.64, min: 0.02, max: 1.41, stdev: 0.23
Old bare network average reward: 0.58, min: -0.06, max: 1.41, stdev: 0.24
New bare network average reward: 0.58, min: 0.03, max: 1.41, stdev: 0.24
External policy "random" average reward: 0.29, min: -0.31, max: 1.01, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: -0.09, max: 1.36, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.13, max: 1.62, stdev: 0.23
New network won 71 and tied 136 out of 300 games (46.33% wins where ties are half wins)
Reverting to the old network

Training iteration 92 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.73 seconds
Training examples lengths: [65113, 64762, 64615, 64862, 64866, 64922, 64767, 64971, 64699, 64741]
Total value: 410855.61
Training on 648318 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3159 (value: 0.0022, weighted value: 0.1093, policy: 0.2066, weighted policy: 0.2066), Train Mean Max: 0.9176
Epoch 2/10, Train Loss: 0.2938 (value: 0.0018, weighted value: 0.0924, policy: 0.2014, weighted policy: 0.2014), Train Mean Max: 0.9181
Epoch 3/10, Train Loss: 0.2825 (value: 0.0017, weighted value: 0.0863, policy: 0.1961, weighted policy: 0.1961), Train Mean Max: 0.9188
Epoch 4/10, Train Loss: 0.2757 (value: 0.0016, weighted value: 0.0814, policy: 0.1943, weighted policy: 0.1943), Train Mean Max: 0.9189
Epoch 5/10, Train Loss: 0.2693 (value: 0.0015, weighted value: 0.0757, policy: 0.1936, weighted policy: 0.1936), Train Mean Max: 0.9192
Epoch 6/10, Train Loss: 0.2626 (value: 0.0014, weighted value: 0.0714, policy: 0.1912, weighted policy: 0.1912), Train Mean Max: 0.9195
Epoch 7/10, Train Loss: 0.2603 (value: 0.0014, weighted value: 0.0697, policy: 0.1906, weighted policy: 0.1906), Train Mean Max: 0.9198
Epoch 8/10, Train Loss: 0.2565 (value: 0.0013, weighted value: 0.0662, policy: 0.1904, weighted policy: 0.1904), Train Mean Max: 0.9200
Epoch 9/10, Train Loss: 0.2535 (value: 0.0013, weighted value: 0.0635, policy: 0.1900, weighted policy: 0.1900), Train Mean Max: 0.9204
Epoch 10/10, Train Loss: 0.2513 (value: 0.0013, weighted value: 0.0628, policy: 0.1885, weighted policy: 0.1885), Train Mean Max: 0.9207
..training done in 72.99 seconds
..evaluation done in 16.15 seconds
Old network+MCTS average reward: 0.62, min: 0.06, max: 1.22, stdev: 0.20
New network+MCTS average reward: 0.61, min: -0.02, max: 1.24, stdev: 0.21
Old bare network average reward: 0.54, min: -0.18, max: 1.15, stdev: 0.21
New bare network average reward: 0.55, min: -0.06, max: 1.15, stdev: 0.21
External policy "random" average reward: 0.23, min: -0.52, max: 0.85, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: -0.01, max: 1.32, stdev: 0.21
External policy "total greedy" average reward: 0.64, min: 0.02, max: 1.44, stdev: 0.21
New network won 78 and tied 124 out of 300 games (46.67% wins where ties are half wins)
Reverting to the old network

Training iteration 93 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.24 seconds
Training examples lengths: [64762, 64615, 64862, 64866, 64922, 64767, 64971, 64699, 64741, 64583]
Total value: 411149.10
Training on 647788 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3424 (value: 0.0026, weighted value: 0.1278, policy: 0.2146, weighted policy: 0.2146), Train Mean Max: 0.9164
Epoch 2/10, Train Loss: 0.3144 (value: 0.0022, weighted value: 0.1079, policy: 0.2065, weighted policy: 0.2065), Train Mean Max: 0.9173
Epoch 3/10, Train Loss: 0.3018 (value: 0.0020, weighted value: 0.0987, policy: 0.2031, weighted policy: 0.2031), Train Mean Max: 0.9176
Epoch 4/10, Train Loss: 0.2901 (value: 0.0018, weighted value: 0.0908, policy: 0.1993, weighted policy: 0.1993), Train Mean Max: 0.9182
Epoch 5/10, Train Loss: 0.2815 (value: 0.0017, weighted value: 0.0848, policy: 0.1967, weighted policy: 0.1967), Train Mean Max: 0.9183
Epoch 6/10, Train Loss: 0.2761 (value: 0.0016, weighted value: 0.0805, policy: 0.1957, weighted policy: 0.1957), Train Mean Max: 0.9185
Epoch 7/10, Train Loss: 0.2722 (value: 0.0016, weighted value: 0.0780, policy: 0.1942, weighted policy: 0.1942), Train Mean Max: 0.9187
Epoch 8/10, Train Loss: 0.2643 (value: 0.0014, weighted value: 0.0716, policy: 0.1927, weighted policy: 0.1927), Train Mean Max: 0.9192
Epoch 9/10, Train Loss: 0.2623 (value: 0.0014, weighted value: 0.0709, policy: 0.1913, weighted policy: 0.1913), Train Mean Max: 0.9193
Epoch 10/10, Train Loss: 0.2596 (value: 0.0014, weighted value: 0.0694, policy: 0.1901, weighted policy: 0.1901), Train Mean Max: 0.9197
..training done in 65.68 seconds
..evaluation done in 15.77 seconds
Old network+MCTS average reward: 0.61, min: -0.03, max: 1.35, stdev: 0.22
New network+MCTS average reward: 0.61, min: -0.01, max: 1.42, stdev: 0.22
Old bare network average reward: 0.55, min: -0.10, max: 1.38, stdev: 0.22
New bare network average reward: 0.55, min: -0.10, max: 1.28, stdev: 0.22
External policy "random" average reward: 0.24, min: -0.40, max: 0.96, stdev: 0.23
External policy "individual greedy" average reward: 0.52, min: -0.14, max: 1.35, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: -0.13, max: 1.42, stdev: 0.22
New network won 84 and tied 134 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 94 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.06 seconds
Training examples lengths: [64615, 64862, 64866, 64922, 64767, 64971, 64699, 64741, 64583, 64416]
Total value: 410410.65
Training on 647442 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2870 (value: 0.0017, weighted value: 0.0869, policy: 0.2001, weighted policy: 0.2001), Train Mean Max: 0.9183
Epoch 2/10, Train Loss: 0.2775 (value: 0.0016, weighted value: 0.0792, policy: 0.1983, weighted policy: 0.1983), Train Mean Max: 0.9187
Epoch 3/10, Train Loss: 0.2703 (value: 0.0015, weighted value: 0.0750, policy: 0.1953, weighted policy: 0.1953), Train Mean Max: 0.9190
Epoch 4/10, Train Loss: 0.2671 (value: 0.0015, weighted value: 0.0727, policy: 0.1945, weighted policy: 0.1945), Train Mean Max: 0.9191
Epoch 5/10, Train Loss: 0.2608 (value: 0.0014, weighted value: 0.0683, policy: 0.1925, weighted policy: 0.1925), Train Mean Max: 0.9194
Epoch 6/10, Train Loss: 0.2560 (value: 0.0013, weighted value: 0.0651, policy: 0.1909, weighted policy: 0.1909), Train Mean Max: 0.9199
Epoch 7/10, Train Loss: 0.2549 (value: 0.0013, weighted value: 0.0635, policy: 0.1915, weighted policy: 0.1915), Train Mean Max: 0.9200
Epoch 8/10, Train Loss: 0.2503 (value: 0.0012, weighted value: 0.0611, policy: 0.1892, weighted policy: 0.1892), Train Mean Max: 0.9203
Epoch 9/10, Train Loss: 0.2479 (value: 0.0012, weighted value: 0.0588, policy: 0.1890, weighted policy: 0.1890), Train Mean Max: 0.9204
Epoch 10/10, Train Loss: 0.2484 (value: 0.0012, weighted value: 0.0579, policy: 0.1905, weighted policy: 0.1905), Train Mean Max: 0.9207
..training done in 72.13 seconds
..evaluation done in 16.13 seconds
Old network+MCTS average reward: 0.61, min: 0.04, max: 1.30, stdev: 0.21
New network+MCTS average reward: 0.62, min: 0.04, max: 1.28, stdev: 0.21
Old bare network average reward: 0.55, min: -0.04, max: 1.25, stdev: 0.23
New bare network average reward: 0.55, min: -0.04, max: 1.24, stdev: 0.22
External policy "random" average reward: 0.26, min: -0.26, max: 0.97, stdev: 0.22
External policy "individual greedy" average reward: 0.52, min: -0.08, max: 1.19, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: 0.04, max: 1.25, stdev: 0.21
New network won 94 and tied 133 out of 300 games (53.50% wins where ties are half wins)
Keeping the new network

Training iteration 95 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.73 seconds
Training examples lengths: [64862, 64866, 64922, 64767, 64971, 64699, 64741, 64583, 64416, 64619]
Total value: 410890.41
Training on 647446 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2775 (value: 0.0016, weighted value: 0.0809, policy: 0.1966, weighted policy: 0.1966), Train Mean Max: 0.9196
Epoch 2/10, Train Loss: 0.2669 (value: 0.0014, weighted value: 0.0718, policy: 0.1952, weighted policy: 0.1952), Train Mean Max: 0.9200
Epoch 3/10, Train Loss: 0.2625 (value: 0.0014, weighted value: 0.0714, policy: 0.1911, weighted policy: 0.1911), Train Mean Max: 0.9205
Epoch 4/10, Train Loss: 0.2560 (value: 0.0013, weighted value: 0.0650, policy: 0.1910, weighted policy: 0.1910), Train Mean Max: 0.9206
Epoch 5/10, Train Loss: 0.2516 (value: 0.0012, weighted value: 0.0618, policy: 0.1898, weighted policy: 0.1898), Train Mean Max: 0.9209
Epoch 6/10, Train Loss: 0.2492 (value: 0.0012, weighted value: 0.0615, policy: 0.1877, weighted policy: 0.1877), Train Mean Max: 0.9212
Epoch 7/10, Train Loss: 0.2444 (value: 0.0011, weighted value: 0.0571, policy: 0.1872, weighted policy: 0.1872), Train Mean Max: 0.9215
Epoch 8/10, Train Loss: 0.2432 (value: 0.0011, weighted value: 0.0564, policy: 0.1868, weighted policy: 0.1868), Train Mean Max: 0.9216
Epoch 9/10, Train Loss: 0.2426 (value: 0.0011, weighted value: 0.0561, policy: 0.1865, weighted policy: 0.1865), Train Mean Max: 0.9217
Epoch 10/10, Train Loss: 0.2407 (value: 0.0011, weighted value: 0.0545, policy: 0.1862, weighted policy: 0.1862), Train Mean Max: 0.9219
..training done in 68.69 seconds
..evaluation done in 16.18 seconds
Old network+MCTS average reward: 0.63, min: 0.04, max: 1.62, stdev: 0.23
New network+MCTS average reward: 0.63, min: 0.08, max: 1.62, stdev: 0.22
Old bare network average reward: 0.56, min: 0.04, max: 1.62, stdev: 0.23
New bare network average reward: 0.56, min: -0.11, max: 1.62, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.27, max: 1.02, stdev: 0.24
External policy "individual greedy" average reward: 0.54, min: -0.06, max: 1.64, stdev: 0.24
External policy "total greedy" average reward: 0.66, min: 0.08, max: 1.56, stdev: 0.23
New network won 86 and tied 126 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 96 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.05 seconds
Training examples lengths: [64866, 64922, 64767, 64971, 64699, 64741, 64583, 64416, 64619, 64494]
Total value: 410960.82
Training on 647078 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.3112 (value: 0.0021, weighted value: 0.1057, policy: 0.2055, weighted policy: 0.2055), Train Mean Max: 0.9187
Epoch 2/10, Train Loss: 0.2880 (value: 0.0018, weighted value: 0.0887, policy: 0.1993, weighted policy: 0.1993), Train Mean Max: 0.9192
Epoch 3/10, Train Loss: 0.2780 (value: 0.0017, weighted value: 0.0833, policy: 0.1947, weighted policy: 0.1947), Train Mean Max: 0.9196
Epoch 4/10, Train Loss: 0.2698 (value: 0.0015, weighted value: 0.0768, policy: 0.1930, weighted policy: 0.1930), Train Mean Max: 0.9198
Epoch 5/10, Train Loss: 0.2679 (value: 0.0015, weighted value: 0.0768, policy: 0.1911, weighted policy: 0.1911), Train Mean Max: 0.9201
Epoch 6/10, Train Loss: 0.2576 (value: 0.0014, weighted value: 0.0675, policy: 0.1900, weighted policy: 0.1900), Train Mean Max: 0.9202
Epoch 7/10, Train Loss: 0.2569 (value: 0.0013, weighted value: 0.0671, policy: 0.1899, weighted policy: 0.1899), Train Mean Max: 0.9204
Epoch 8/10, Train Loss: 0.2528 (value: 0.0013, weighted value: 0.0641, policy: 0.1887, weighted policy: 0.1887), Train Mean Max: 0.9207
Epoch 9/10, Train Loss: 0.2493 (value: 0.0012, weighted value: 0.0619, policy: 0.1875, weighted policy: 0.1875), Train Mean Max: 0.9211
Epoch 10/10, Train Loss: 0.2499 (value: 0.0012, weighted value: 0.0618, policy: 0.1882, weighted policy: 0.1882), Train Mean Max: 0.9212
..training done in 71.00 seconds
..evaluation done in 15.83 seconds
Old network+MCTS average reward: 0.62, min: 0.06, max: 1.85, stdev: 0.23
New network+MCTS average reward: 0.62, min: 0.09, max: 1.85, stdev: 0.23
Old bare network average reward: 0.55, min: -0.04, max: 1.69, stdev: 0.24
New bare network average reward: 0.55, min: -0.06, max: 1.69, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.25, max: 1.40, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: 0.06, max: 1.51, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.13, max: 1.75, stdev: 0.23
New network won 88 and tied 126 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 97 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.30 seconds
Training examples lengths: [64922, 64767, 64971, 64699, 64741, 64583, 64416, 64619, 64494, 64740]
Total value: 410883.86
Training on 646952 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2779 (value: 0.0016, weighted value: 0.0813, policy: 0.1966, weighted policy: 0.1966), Train Mean Max: 0.9200
Epoch 2/10, Train Loss: 0.2668 (value: 0.0015, weighted value: 0.0741, policy: 0.1927, weighted policy: 0.1927), Train Mean Max: 0.9205
Epoch 3/10, Train Loss: 0.2600 (value: 0.0014, weighted value: 0.0691, policy: 0.1909, weighted policy: 0.1909), Train Mean Max: 0.9207
Epoch 4/10, Train Loss: 0.2565 (value: 0.0013, weighted value: 0.0669, policy: 0.1895, weighted policy: 0.1895), Train Mean Max: 0.9209
Epoch 5/10, Train Loss: 0.2509 (value: 0.0013, weighted value: 0.0626, policy: 0.1883, weighted policy: 0.1883), Train Mean Max: 0.9213
Epoch 6/10, Train Loss: 0.2474 (value: 0.0012, weighted value: 0.0595, policy: 0.1879, weighted policy: 0.1879), Train Mean Max: 0.9215
Epoch 7/10, Train Loss: 0.2466 (value: 0.0012, weighted value: 0.0596, policy: 0.1870, weighted policy: 0.1870), Train Mean Max: 0.9216
Epoch 8/10, Train Loss: 0.2433 (value: 0.0011, weighted value: 0.0569, policy: 0.1864, weighted policy: 0.1864), Train Mean Max: 0.9219
Epoch 9/10, Train Loss: 0.2409 (value: 0.0011, weighted value: 0.0558, policy: 0.1851, weighted policy: 0.1851), Train Mean Max: 0.9223
Epoch 10/10, Train Loss: 0.2415 (value: 0.0011, weighted value: 0.0559, policy: 0.1856, weighted policy: 0.1856), Train Mean Max: 0.9223
..training done in 65.95 seconds
..evaluation done in 16.04 seconds
Old network+MCTS average reward: 0.63, min: -0.05, max: 1.34, stdev: 0.22
New network+MCTS average reward: 0.64, min: 0.06, max: 1.31, stdev: 0.22
Old bare network average reward: 0.57, min: -0.05, max: 1.24, stdev: 0.22
New bare network average reward: 0.57, min: -0.01, max: 1.24, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.50, max: 0.99, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: -0.13, max: 1.24, stdev: 0.22
External policy "total greedy" average reward: 0.67, min: -0.03, max: 1.38, stdev: 0.22
New network won 85 and tied 130 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 98 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.68 seconds
Training examples lengths: [64767, 64971, 64699, 64741, 64583, 64416, 64619, 64494, 64740, 64839]
Total value: 411298.14
Training on 646869 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2739 (value: 0.0016, weighted value: 0.0787, policy: 0.1953, weighted policy: 0.1953), Train Mean Max: 0.9210
Epoch 2/10, Train Loss: 0.2595 (value: 0.0014, weighted value: 0.0682, policy: 0.1913, weighted policy: 0.1913), Train Mean Max: 0.9214
Epoch 3/10, Train Loss: 0.2531 (value: 0.0013, weighted value: 0.0644, policy: 0.1887, weighted policy: 0.1887), Train Mean Max: 0.9217
Epoch 4/10, Train Loss: 0.2514 (value: 0.0013, weighted value: 0.0642, policy: 0.1872, weighted policy: 0.1872), Train Mean Max: 0.9219
Epoch 5/10, Train Loss: 0.2470 (value: 0.0012, weighted value: 0.0602, policy: 0.1867, weighted policy: 0.1867), Train Mean Max: 0.9221
Epoch 6/10, Train Loss: 0.2430 (value: 0.0012, weighted value: 0.0583, policy: 0.1848, weighted policy: 0.1848), Train Mean Max: 0.9225
Epoch 7/10, Train Loss: 0.2401 (value: 0.0011, weighted value: 0.0555, policy: 0.1846, weighted policy: 0.1846), Train Mean Max: 0.9226
Epoch 8/10, Train Loss: 0.2402 (value: 0.0011, weighted value: 0.0550, policy: 0.1852, weighted policy: 0.1852), Train Mean Max: 0.9226
Epoch 9/10, Train Loss: 0.2368 (value: 0.0011, weighted value: 0.0528, policy: 0.1840, weighted policy: 0.1840), Train Mean Max: 0.9230
Epoch 10/10, Train Loss: 0.2348 (value: 0.0010, weighted value: 0.0519, policy: 0.1828, weighted policy: 0.1828), Train Mean Max: 0.9232
..training done in 65.95 seconds
..evaluation done in 15.65 seconds
Old network+MCTS average reward: 0.65, min: -0.16, max: 1.42, stdev: 0.22
New network+MCTS average reward: 0.66, min: -0.14, max: 1.42, stdev: 0.22
Old bare network average reward: 0.59, min: -0.52, max: 1.27, stdev: 0.24
New bare network average reward: 0.59, min: -0.52, max: 1.35, stdev: 0.23
External policy "random" average reward: 0.28, min: -0.43, max: 0.94, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: -0.20, max: 1.24, stdev: 0.24
External policy "total greedy" average reward: 0.66, min: 0.00, max: 1.29, stdev: 0.23
New network won 90 and tied 124 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 99 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.86 seconds
Training examples lengths: [64971, 64699, 64741, 64583, 64416, 64619, 64494, 64740, 64839, 64692]
Total value: 411878.30
Training on 646794 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2694 (value: 0.0015, weighted value: 0.0771, policy: 0.1922, weighted policy: 0.1922), Train Mean Max: 0.9218
Epoch 2/10, Train Loss: 0.2537 (value: 0.0013, weighted value: 0.0654, policy: 0.1883, weighted policy: 0.1883), Train Mean Max: 0.9224
Epoch 3/10, Train Loss: 0.2513 (value: 0.0013, weighted value: 0.0643, policy: 0.1870, weighted policy: 0.1870), Train Mean Max: 0.9226
Epoch 4/10, Train Loss: 0.2444 (value: 0.0012, weighted value: 0.0597, policy: 0.1847, weighted policy: 0.1847), Train Mean Max: 0.9229
Epoch 5/10, Train Loss: 0.2426 (value: 0.0012, weighted value: 0.0589, policy: 0.1837, weighted policy: 0.1837), Train Mean Max: 0.9231
Epoch 6/10, Train Loss: 0.2409 (value: 0.0011, weighted value: 0.0570, policy: 0.1840, weighted policy: 0.1840), Train Mean Max: 0.9232
Epoch 7/10, Train Loss: 0.2376 (value: 0.0011, weighted value: 0.0543, policy: 0.1833, weighted policy: 0.1833), Train Mean Max: 0.9234
Epoch 8/10, Train Loss: 0.2353 (value: 0.0011, weighted value: 0.0533, policy: 0.1820, weighted policy: 0.1820), Train Mean Max: 0.9237
Epoch 9/10, Train Loss: 0.2328 (value: 0.0010, weighted value: 0.0511, policy: 0.1817, weighted policy: 0.1817), Train Mean Max: 0.9240
Epoch 10/10, Train Loss: 0.2320 (value: 0.0010, weighted value: 0.0505, policy: 0.1814, weighted policy: 0.1814), Train Mean Max: 0.9241
..training done in 66.17 seconds
..evaluation done in 15.84 seconds
Old network+MCTS average reward: 0.62, min: 0.01, max: 1.56, stdev: 0.24
New network+MCTS average reward: 0.62, min: 0.01, max: 1.68, stdev: 0.23
Old bare network average reward: 0.55, min: -0.10, max: 1.44, stdev: 0.24
New bare network average reward: 0.55, min: 0.01, max: 1.54, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.23, max: 1.26, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: -0.02, max: 1.35, stdev: 0.24
External policy "total greedy" average reward: 0.64, min: 0.04, max: 1.60, stdev: 0.23
New network won 82 and tied 144 out of 300 games (51.33% wins where ties are half wins)
Keeping the new network

Training iteration 100 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.64 seconds
Training examples lengths: [64699, 64741, 64583, 64416, 64619, 64494, 64740, 64839, 64692, 64845]
Total value: 411599.14
Training on 646668 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2638 (value: 0.0014, weighted value: 0.0725, policy: 0.1914, weighted policy: 0.1914), Train Mean Max: 0.9226
Epoch 2/10, Train Loss: 0.2546 (value: 0.0013, weighted value: 0.0667, policy: 0.1880, weighted policy: 0.1880), Train Mean Max: 0.9231
Epoch 3/10, Train Loss: 0.2477 (value: 0.0013, weighted value: 0.0630, policy: 0.1846, weighted policy: 0.1846), Train Mean Max: 0.9234
Epoch 4/10, Train Loss: 0.2437 (value: 0.0012, weighted value: 0.0604, policy: 0.1833, weighted policy: 0.1833), Train Mean Max: 0.9236
Epoch 5/10, Train Loss: 0.2390 (value: 0.0011, weighted value: 0.0558, policy: 0.1833, weighted policy: 0.1833), Train Mean Max: 0.9237
Epoch 6/10, Train Loss: 0.2371 (value: 0.0011, weighted value: 0.0551, policy: 0.1820, weighted policy: 0.1820), Train Mean Max: 0.9240
Epoch 7/10, Train Loss: 0.2340 (value: 0.0011, weighted value: 0.0532, policy: 0.1808, weighted policy: 0.1808), Train Mean Max: 0.9242
Epoch 8/10, Train Loss: 0.2342 (value: 0.0011, weighted value: 0.0527, policy: 0.1815, weighted policy: 0.1815), Train Mean Max: 0.9243
Epoch 9/10, Train Loss: 0.2320 (value: 0.0010, weighted value: 0.0514, policy: 0.1807, weighted policy: 0.1807), Train Mean Max: 0.9246
Epoch 10/10, Train Loss: 0.2286 (value: 0.0010, weighted value: 0.0492, policy: 0.1794, weighted policy: 0.1794), Train Mean Max: 0.9248
..training done in 72.74 seconds
..evaluation done in 15.95 seconds
Old network+MCTS average reward: 0.62, min: 0.01, max: 1.26, stdev: 0.23
New network+MCTS average reward: 0.62, min: 0.08, max: 1.24, stdev: 0.22
Old bare network average reward: 0.55, min: -0.01, max: 1.26, stdev: 0.22
New bare network average reward: 0.56, min: -0.06, max: 1.24, stdev: 0.23
External policy "random" average reward: 0.24, min: -0.34, max: 0.93, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: -0.19, max: 1.16, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: 0.11, max: 1.21, stdev: 0.22
New network won 69 and tied 151 out of 300 games (48.17% wins where ties are half wins)
Reverting to the old network
Saving intermediate checkpoint to zgaz_checkpoints/1757654225_iter_100

Training iteration 101 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.86 seconds
Training examples lengths: [64741, 64583, 64416, 64619, 64494, 64740, 64839, 64692, 64845, 64857]
Total value: 412169.94
Training on 646826 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2981 (value: 0.0020, weighted value: 0.0976, policy: 0.2005, weighted policy: 0.2005), Train Mean Max: 0.9212
Epoch 2/10, Train Loss: 0.2785 (value: 0.0017, weighted value: 0.0852, policy: 0.1932, weighted policy: 0.1932), Train Mean Max: 0.9218
Epoch 3/10, Train Loss: 0.2673 (value: 0.0015, weighted value: 0.0765, policy: 0.1908, weighted policy: 0.1908), Train Mean Max: 0.9220
Epoch 4/10, Train Loss: 0.2596 (value: 0.0014, weighted value: 0.0711, policy: 0.1885, weighted policy: 0.1885), Train Mean Max: 0.9223
Epoch 5/10, Train Loss: 0.2536 (value: 0.0013, weighted value: 0.0672, policy: 0.1864, weighted policy: 0.1864), Train Mean Max: 0.9226
Epoch 6/10, Train Loss: 0.2517 (value: 0.0013, weighted value: 0.0660, policy: 0.1858, weighted policy: 0.1858), Train Mean Max: 0.9229
Epoch 7/10, Train Loss: 0.2458 (value: 0.0012, weighted value: 0.0616, policy: 0.1842, weighted policy: 0.1842), Train Mean Max: 0.9230
Epoch 8/10, Train Loss: 0.2434 (value: 0.0012, weighted value: 0.0603, policy: 0.1831, weighted policy: 0.1831), Train Mean Max: 0.9234
Epoch 9/10, Train Loss: 0.2412 (value: 0.0012, weighted value: 0.0576, policy: 0.1837, weighted policy: 0.1837), Train Mean Max: 0.9234
Epoch 10/10, Train Loss: 0.2384 (value: 0.0011, weighted value: 0.0567, policy: 0.1816, weighted policy: 0.1816), Train Mean Max: 0.9238
..training done in 66.30 seconds
..evaluation done in 16.59 seconds
Old network+MCTS average reward: 0.62, min: -0.08, max: 1.19, stdev: 0.22
New network+MCTS average reward: 0.63, min: -0.08, max: 1.21, stdev: 0.22
Old bare network average reward: 0.56, min: -0.05, max: 1.19, stdev: 0.23
New bare network average reward: 0.57, min: -0.05, max: 1.19, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.47, max: 0.86, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: -0.07, max: 1.16, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: -0.01, max: 1.27, stdev: 0.22
New network won 81 and tied 133 out of 300 games (49.17% wins where ties are half wins)
Reverting to the old network

Training iteration 102 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.33 seconds
Training examples lengths: [64583, 64416, 64619, 64494, 64740, 64839, 64692, 64845, 64857, 64646]
Total value: 412590.43
Training on 646731 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.3288 (value: 0.0024, weighted value: 0.1211, policy: 0.2077, weighted policy: 0.2077), Train Mean Max: 0.9197
Epoch 2/10, Train Loss: 0.3010 (value: 0.0020, weighted value: 0.1001, policy: 0.2009, weighted policy: 0.2009), Train Mean Max: 0.9205
Epoch 3/10, Train Loss: 0.2872 (value: 0.0018, weighted value: 0.0912, policy: 0.1960, weighted policy: 0.1960), Train Mean Max: 0.9209
Epoch 4/10, Train Loss: 0.2753 (value: 0.0017, weighted value: 0.0834, policy: 0.1918, weighted policy: 0.1918), Train Mean Max: 0.9213
Epoch 5/10, Train Loss: 0.2706 (value: 0.0016, weighted value: 0.0804, policy: 0.1902, weighted policy: 0.1902), Train Mean Max: 0.9213
Epoch 6/10, Train Loss: 0.2625 (value: 0.0015, weighted value: 0.0733, policy: 0.1892, weighted policy: 0.1892), Train Mean Max: 0.9217
Epoch 7/10, Train Loss: 0.2596 (value: 0.0014, weighted value: 0.0714, policy: 0.1882, weighted policy: 0.1882), Train Mean Max: 0.9220
Epoch 8/10, Train Loss: 0.2523 (value: 0.0013, weighted value: 0.0656, policy: 0.1866, weighted policy: 0.1866), Train Mean Max: 0.9221
Epoch 9/10, Train Loss: 0.2512 (value: 0.0013, weighted value: 0.0662, policy: 0.1850, weighted policy: 0.1850), Train Mean Max: 0.9223
Epoch 10/10, Train Loss: 0.2470 (value: 0.0012, weighted value: 0.0614, policy: 0.1856, weighted policy: 0.1856), Train Mean Max: 0.9224
..training done in 66.80 seconds
..evaluation done in 16.94 seconds
Old network+MCTS average reward: 0.65, min: 0.18, max: 1.42, stdev: 0.22
New network+MCTS average reward: 0.65, min: 0.01, max: 1.41, stdev: 0.22
Old bare network average reward: 0.58, min: -0.02, max: 1.36, stdev: 0.22
New bare network average reward: 0.58, min: -0.02, max: 1.36, stdev: 0.23
External policy "random" average reward: 0.28, min: -0.31, max: 1.16, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: -0.01, max: 1.41, stdev: 0.23
External policy "total greedy" average reward: 0.68, min: 0.12, max: 1.45, stdev: 0.22
New network won 80 and tied 133 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 103 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.81 seconds
Training examples lengths: [64416, 64619, 64494, 64740, 64839, 64692, 64845, 64857, 64646, 64532]
Total value: 412884.75
Training on 646680 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.3594 (value: 0.0028, weighted value: 0.1423, policy: 0.2170, weighted policy: 0.2170), Train Mean Max: 0.9186
Epoch 2/10, Train Loss: 0.3252 (value: 0.0024, weighted value: 0.1191, policy: 0.2062, weighted policy: 0.2062), Train Mean Max: 0.9195
Epoch 3/10, Train Loss: 0.3021 (value: 0.0021, weighted value: 0.1028, policy: 0.1994, weighted policy: 0.1994), Train Mean Max: 0.9198
Epoch 4/10, Train Loss: 0.2910 (value: 0.0019, weighted value: 0.0957, policy: 0.1953, weighted policy: 0.1953), Train Mean Max: 0.9200
Epoch 5/10, Train Loss: 0.2824 (value: 0.0018, weighted value: 0.0893, policy: 0.1931, weighted policy: 0.1931), Train Mean Max: 0.9204
Epoch 6/10, Train Loss: 0.2761 (value: 0.0017, weighted value: 0.0830, policy: 0.1931, weighted policy: 0.1931), Train Mean Max: 0.9204
Epoch 7/10, Train Loss: 0.2697 (value: 0.0016, weighted value: 0.0797, policy: 0.1900, weighted policy: 0.1900), Train Mean Max: 0.9208
Epoch 8/10, Train Loss: 0.2633 (value: 0.0015, weighted value: 0.0741, policy: 0.1892, weighted policy: 0.1892), Train Mean Max: 0.9210
Epoch 9/10, Train Loss: 0.2598 (value: 0.0014, weighted value: 0.0715, policy: 0.1883, weighted policy: 0.1883), Train Mean Max: 0.9212
Epoch 10/10, Train Loss: 0.2554 (value: 0.0014, weighted value: 0.0679, policy: 0.1875, weighted policy: 0.1875), Train Mean Max: 0.9214
..training done in 71.89 seconds
..evaluation done in 16.59 seconds
Old network+MCTS average reward: 0.65, min: 0.06, max: 1.24, stdev: 0.21
New network+MCTS average reward: 0.66, min: 0.01, max: 1.31, stdev: 0.21
Old bare network average reward: 0.60, min: -0.04, max: 1.22, stdev: 0.22
New bare network average reward: 0.59, min: -0.02, max: 1.22, stdev: 0.22
External policy "random" average reward: 0.26, min: -0.29, max: 0.97, stdev: 0.23
External policy "individual greedy" average reward: 0.56, min: -0.02, max: 1.21, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: 0.02, max: 1.21, stdev: 0.22
New network won 97 and tied 130 out of 300 games (54.00% wins where ties are half wins)
Keeping the new network

Training iteration 104 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.65 seconds
Training examples lengths: [64619, 64494, 64740, 64839, 64692, 64845, 64857, 64646, 64532, 64589]
Total value: 413844.66
Training on 646853 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2840 (value: 0.0018, weighted value: 0.0890, policy: 0.1950, weighted policy: 0.1950), Train Mean Max: 0.9207
Epoch 2/10, Train Loss: 0.2707 (value: 0.0016, weighted value: 0.0787, policy: 0.1920, weighted policy: 0.1920), Train Mean Max: 0.9212
Epoch 3/10, Train Loss: 0.2645 (value: 0.0015, weighted value: 0.0757, policy: 0.1888, weighted policy: 0.1888), Train Mean Max: 0.9215
Epoch 4/10, Train Loss: 0.2577 (value: 0.0014, weighted value: 0.0711, policy: 0.1866, weighted policy: 0.1866), Train Mean Max: 0.9218
Epoch 5/10, Train Loss: 0.2544 (value: 0.0014, weighted value: 0.0679, policy: 0.1865, weighted policy: 0.1865), Train Mean Max: 0.9219
Epoch 6/10, Train Loss: 0.2499 (value: 0.0013, weighted value: 0.0641, policy: 0.1858, weighted policy: 0.1858), Train Mean Max: 0.9223
Epoch 7/10, Train Loss: 0.2474 (value: 0.0013, weighted value: 0.0633, policy: 0.1841, weighted policy: 0.1841), Train Mean Max: 0.9225
Epoch 8/10, Train Loss: 0.2469 (value: 0.0012, weighted value: 0.0622, policy: 0.1848, weighted policy: 0.1848), Train Mean Max: 0.9226
Epoch 9/10, Train Loss: 0.2414 (value: 0.0012, weighted value: 0.0587, policy: 0.1827, weighted policy: 0.1827), Train Mean Max: 0.9231
Epoch 10/10, Train Loss: 0.2390 (value: 0.0011, weighted value: 0.0569, policy: 0.1822, weighted policy: 0.1822), Train Mean Max: 0.9233
..training done in 65.72 seconds
..evaluation done in 17.27 seconds
Old network+MCTS average reward: 0.61, min: -0.06, max: 1.26, stdev: 0.22
New network+MCTS average reward: 0.61, min: 0.07, max: 1.35, stdev: 0.21
Old bare network average reward: 0.54, min: -0.04, max: 1.18, stdev: 0.22
New bare network average reward: 0.54, min: 0.00, max: 1.29, stdev: 0.22
External policy "random" average reward: 0.23, min: -0.41, max: 1.12, stdev: 0.22
External policy "individual greedy" average reward: 0.51, min: -0.12, max: 1.37, stdev: 0.22
External policy "total greedy" average reward: 0.62, min: 0.10, max: 1.36, stdev: 0.20
New network won 79 and tied 135 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 105 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.06 seconds
Training examples lengths: [64494, 64740, 64839, 64692, 64845, 64857, 64646, 64532, 64589, 65056]
Total value: 414642.01
Training on 647290 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3127 (value: 0.0022, weighted value: 0.1098, policy: 0.2029, weighted policy: 0.2029), Train Mean Max: 0.9197
Epoch 2/10, Train Loss: 0.2967 (value: 0.0019, weighted value: 0.0974, policy: 0.1992, weighted policy: 0.1992), Train Mean Max: 0.9202
Epoch 3/10, Train Loss: 0.2817 (value: 0.0018, weighted value: 0.0878, policy: 0.1939, weighted policy: 0.1939), Train Mean Max: 0.9208
Epoch 4/10, Train Loss: 0.2739 (value: 0.0016, weighted value: 0.0824, policy: 0.1916, weighted policy: 0.1916), Train Mean Max: 0.9208
Epoch 5/10, Train Loss: 0.2686 (value: 0.0016, weighted value: 0.0786, policy: 0.1901, weighted policy: 0.1901), Train Mean Max: 0.9212
Epoch 6/10, Train Loss: 0.2638 (value: 0.0015, weighted value: 0.0749, policy: 0.1888, weighted policy: 0.1888), Train Mean Max: 0.9214
Epoch 7/10, Train Loss: 0.2587 (value: 0.0014, weighted value: 0.0719, policy: 0.1868, weighted policy: 0.1868), Train Mean Max: 0.9216
Epoch 8/10, Train Loss: 0.2544 (value: 0.0014, weighted value: 0.0682, policy: 0.1862, weighted policy: 0.1862), Train Mean Max: 0.9220
Epoch 9/10, Train Loss: 0.2533 (value: 0.0013, weighted value: 0.0671, policy: 0.1862, weighted policy: 0.1862), Train Mean Max: 0.9219
Epoch 10/10, Train Loss: 0.2486 (value: 0.0013, weighted value: 0.0634, policy: 0.1852, weighted policy: 0.1852), Train Mean Max: 0.9224
..training done in 65.00 seconds
..evaluation done in 16.30 seconds
Old network+MCTS average reward: 0.65, min: 0.06, max: 1.48, stdev: 0.22
New network+MCTS average reward: 0.65, min: 0.06, max: 1.48, stdev: 0.22
Old bare network average reward: 0.59, min: -0.07, max: 1.48, stdev: 0.23
New bare network average reward: 0.59, min: -0.10, max: 1.48, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.30, max: 1.14, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.14, max: 1.35, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.06, max: 1.44, stdev: 0.23
New network won 90 and tied 122 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 106 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.02 seconds
Training examples lengths: [64740, 64839, 64692, 64845, 64857, 64646, 64532, 64589, 65056, 64750]
Total value: 415553.81
Training on 647546 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2753 (value: 0.0017, weighted value: 0.0825, policy: 0.1928, weighted policy: 0.1928), Train Mean Max: 0.9216
Epoch 2/10, Train Loss: 0.2655 (value: 0.0015, weighted value: 0.0763, policy: 0.1892, weighted policy: 0.1892), Train Mean Max: 0.9220
Epoch 3/10, Train Loss: 0.2585 (value: 0.0014, weighted value: 0.0715, policy: 0.1870, weighted policy: 0.1870), Train Mean Max: 0.9223
Epoch 4/10, Train Loss: 0.2551 (value: 0.0014, weighted value: 0.0687, policy: 0.1864, weighted policy: 0.1864), Train Mean Max: 0.9225
Epoch 5/10, Train Loss: 0.2506 (value: 0.0013, weighted value: 0.0667, policy: 0.1839, weighted policy: 0.1839), Train Mean Max: 0.9230
Epoch 6/10, Train Loss: 0.2448 (value: 0.0012, weighted value: 0.0617, policy: 0.1831, weighted policy: 0.1831), Train Mean Max: 0.9231
Epoch 7/10, Train Loss: 0.2432 (value: 0.0012, weighted value: 0.0611, policy: 0.1821, weighted policy: 0.1821), Train Mean Max: 0.9234
Epoch 8/10, Train Loss: 0.2419 (value: 0.0012, weighted value: 0.0596, policy: 0.1823, weighted policy: 0.1823), Train Mean Max: 0.9235
Epoch 9/10, Train Loss: 0.2381 (value: 0.0011, weighted value: 0.0567, policy: 0.1814, weighted policy: 0.1814), Train Mean Max: 0.9239
Epoch 10/10, Train Loss: 0.2371 (value: 0.0011, weighted value: 0.0561, policy: 0.1811, weighted policy: 0.1811), Train Mean Max: 0.9241
..training done in 71.84 seconds
..evaluation done in 16.43 seconds
Old network+MCTS average reward: 0.66, min: 0.04, max: 1.36, stdev: 0.23
New network+MCTS average reward: 0.66, min: -0.05, max: 1.36, stdev: 0.23
Old bare network average reward: 0.60, min: -0.21, max: 1.30, stdev: 0.24
New bare network average reward: 0.60, min: -0.05, max: 1.30, stdev: 0.23
External policy "random" average reward: 0.28, min: -0.48, max: 0.92, stdev: 0.24
External policy "individual greedy" average reward: 0.57, min: 0.01, max: 1.32, stdev: 0.23
External policy "total greedy" average reward: 0.69, min: 0.04, max: 1.49, stdev: 0.23
New network won 90 and tied 122 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 107 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.27 seconds
Training examples lengths: [64839, 64692, 64845, 64857, 64646, 64532, 64589, 65056, 64750, 64718]
Total value: 415893.18
Training on 647524 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2677 (value: 0.0016, weighted value: 0.0779, policy: 0.1898, weighted policy: 0.1898), Train Mean Max: 0.9226
Epoch 2/10, Train Loss: 0.2581 (value: 0.0014, weighted value: 0.0694, policy: 0.1888, weighted policy: 0.1888), Train Mean Max: 0.9231
Epoch 3/10, Train Loss: 0.2530 (value: 0.0014, weighted value: 0.0679, policy: 0.1851, weighted policy: 0.1851), Train Mean Max: 0.9235
Epoch 4/10, Train Loss: 0.2441 (value: 0.0012, weighted value: 0.0620, policy: 0.1821, weighted policy: 0.1821), Train Mean Max: 0.9237
Epoch 5/10, Train Loss: 0.2424 (value: 0.0012, weighted value: 0.0600, policy: 0.1824, weighted policy: 0.1824), Train Mean Max: 0.9238
Epoch 6/10, Train Loss: 0.2394 (value: 0.0012, weighted value: 0.0579, policy: 0.1815, weighted policy: 0.1815), Train Mean Max: 0.9241
Epoch 7/10, Train Loss: 0.2372 (value: 0.0012, weighted value: 0.0578, policy: 0.1794, weighted policy: 0.1794), Train Mean Max: 0.9244
Epoch 8/10, Train Loss: 0.2340 (value: 0.0011, weighted value: 0.0547, policy: 0.1793, weighted policy: 0.1793), Train Mean Max: 0.9246
Epoch 9/10, Train Loss: 0.2324 (value: 0.0011, weighted value: 0.0536, policy: 0.1789, weighted policy: 0.1789), Train Mean Max: 0.9249
Epoch 10/10, Train Loss: 0.2320 (value: 0.0011, weighted value: 0.0528, policy: 0.1792, weighted policy: 0.1792), Train Mean Max: 0.9249
..training done in 72.87 seconds
..evaluation done in 16.60 seconds
Old network+MCTS average reward: 0.66, min: 0.09, max: 1.35, stdev: 0.22
New network+MCTS average reward: 0.66, min: 0.09, max: 1.35, stdev: 0.22
Old bare network average reward: 0.59, min: 0.09, max: 1.27, stdev: 0.22
New bare network average reward: 0.59, min: 0.06, max: 1.35, stdev: 0.22
External policy "random" average reward: 0.29, min: -0.35, max: 1.04, stdev: 0.23
External policy "individual greedy" average reward: 0.56, min: 0.07, max: 1.36, stdev: 0.22
External policy "total greedy" average reward: 0.68, min: 0.22, max: 1.47, stdev: 0.21
New network won 81 and tied 132 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 108 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.56 seconds
Training examples lengths: [64692, 64845, 64857, 64646, 64532, 64589, 65056, 64750, 64718, 64857]
Total value: 416320.73
Training on 647542 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3014 (value: 0.0020, weighted value: 0.1013, policy: 0.2002, weighted policy: 0.2002), Train Mean Max: 0.9214
Epoch 2/10, Train Loss: 0.2795 (value: 0.0017, weighted value: 0.0864, policy: 0.1931, weighted policy: 0.1931), Train Mean Max: 0.9221
Epoch 3/10, Train Loss: 0.2687 (value: 0.0016, weighted value: 0.0806, policy: 0.1881, weighted policy: 0.1881), Train Mean Max: 0.9226
Epoch 4/10, Train Loss: 0.2613 (value: 0.0015, weighted value: 0.0749, policy: 0.1864, weighted policy: 0.1864), Train Mean Max: 0.9226
Epoch 5/10, Train Loss: 0.2559 (value: 0.0014, weighted value: 0.0702, policy: 0.1858, weighted policy: 0.1858), Train Mean Max: 0.9228
Epoch 6/10, Train Loss: 0.2521 (value: 0.0014, weighted value: 0.0680, policy: 0.1841, weighted policy: 0.1841), Train Mean Max: 0.9232
Epoch 7/10, Train Loss: 0.2495 (value: 0.0013, weighted value: 0.0658, policy: 0.1837, weighted policy: 0.1837), Train Mean Max: 0.9234
Epoch 8/10, Train Loss: 0.2434 (value: 0.0012, weighted value: 0.0618, policy: 0.1816, weighted policy: 0.1816), Train Mean Max: 0.9237
Epoch 9/10, Train Loss: 0.2428 (value: 0.0012, weighted value: 0.0613, policy: 0.1815, weighted policy: 0.1815), Train Mean Max: 0.9238
Epoch 10/10, Train Loss: 0.2403 (value: 0.0012, weighted value: 0.0602, policy: 0.1801, weighted policy: 0.1801), Train Mean Max: 0.9241
..training done in 70.04 seconds
..evaluation done in 15.74 seconds
Old network+MCTS average reward: 0.64, min: 0.00, max: 1.45, stdev: 0.23
New network+MCTS average reward: 0.64, min: -0.06, max: 1.45, stdev: 0.23
Old bare network average reward: 0.57, min: -0.06, max: 1.39, stdev: 0.24
New bare network average reward: 0.58, min: -0.12, max: 1.39, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.31, max: 1.03, stdev: 0.25
External policy "individual greedy" average reward: 0.54, min: -0.02, max: 1.34, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.14, max: 1.45, stdev: 0.22
New network won 66 and tied 147 out of 300 games (46.50% wins where ties are half wins)
Reverting to the old network

Training iteration 109 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.97 seconds
Training examples lengths: [64845, 64857, 64646, 64532, 64589, 65056, 64750, 64718, 64857, 64797]
Total value: 416327.80
Training on 647647 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3293 (value: 0.0024, weighted value: 0.1212, policy: 0.2082, weighted policy: 0.2082), Train Mean Max: 0.9203
Epoch 2/10, Train Loss: 0.2984 (value: 0.0020, weighted value: 0.1001, policy: 0.1983, weighted policy: 0.1983), Train Mean Max: 0.9212
Epoch 3/10, Train Loss: 0.2863 (value: 0.0019, weighted value: 0.0932, policy: 0.1931, weighted policy: 0.1931), Train Mean Max: 0.9215
Epoch 4/10, Train Loss: 0.2765 (value: 0.0017, weighted value: 0.0861, policy: 0.1904, weighted policy: 0.1904), Train Mean Max: 0.9216
Epoch 5/10, Train Loss: 0.2691 (value: 0.0016, weighted value: 0.0800, policy: 0.1891, weighted policy: 0.1891), Train Mean Max: 0.9219
Epoch 6/10, Train Loss: 0.2624 (value: 0.0015, weighted value: 0.0752, policy: 0.1872, weighted policy: 0.1872), Train Mean Max: 0.9222
Epoch 7/10, Train Loss: 0.2587 (value: 0.0015, weighted value: 0.0731, policy: 0.1856, weighted policy: 0.1856), Train Mean Max: 0.9225
Epoch 8/10, Train Loss: 0.2525 (value: 0.0014, weighted value: 0.0682, policy: 0.1843, weighted policy: 0.1843), Train Mean Max: 0.9228
Epoch 9/10, Train Loss: 0.2500 (value: 0.0013, weighted value: 0.0664, policy: 0.1836, weighted policy: 0.1836), Train Mean Max: 0.9230
Epoch 10/10, Train Loss: 0.2479 (value: 0.0013, weighted value: 0.0647, policy: 0.1832, weighted policy: 0.1832), Train Mean Max: 0.9231
..training done in 70.34 seconds
..evaluation done in 15.99 seconds
Old network+MCTS average reward: 0.65, min: -0.09, max: 1.28, stdev: 0.21
New network+MCTS average reward: 0.65, min: -0.15, max: 1.29, stdev: 0.22
Old bare network average reward: 0.59, min: -0.14, max: 1.27, stdev: 0.22
New bare network average reward: 0.59, min: -0.10, max: 1.27, stdev: 0.22
External policy "random" average reward: 0.28, min: -0.26, max: 0.84, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: -0.05, max: 1.23, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.11, max: 1.44, stdev: 0.21
New network won 93 and tied 121 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 110 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.90 seconds
Training examples lengths: [64857, 64646, 64532, 64589, 65056, 64750, 64718, 64857, 64797, 64715]
Total value: 417508.28
Training on 647517 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2756 (value: 0.0016, weighted value: 0.0823, policy: 0.1932, weighted policy: 0.1932), Train Mean Max: 0.9223
Epoch 2/10, Train Loss: 0.2653 (value: 0.0015, weighted value: 0.0760, policy: 0.1893, weighted policy: 0.1893), Train Mean Max: 0.9227
Epoch 3/10, Train Loss: 0.2587 (value: 0.0014, weighted value: 0.0714, policy: 0.1872, weighted policy: 0.1872), Train Mean Max: 0.9228
Epoch 4/10, Train Loss: 0.2522 (value: 0.0014, weighted value: 0.0675, policy: 0.1847, weighted policy: 0.1847), Train Mean Max: 0.9233
Epoch 5/10, Train Loss: 0.2480 (value: 0.0013, weighted value: 0.0655, policy: 0.1824, weighted policy: 0.1824), Train Mean Max: 0.9236
Epoch 6/10, Train Loss: 0.2456 (value: 0.0013, weighted value: 0.0630, policy: 0.1826, weighted policy: 0.1826), Train Mean Max: 0.9236
Epoch 7/10, Train Loss: 0.2409 (value: 0.0012, weighted value: 0.0594, policy: 0.1815, weighted policy: 0.1815), Train Mean Max: 0.9240
Epoch 8/10, Train Loss: 0.2415 (value: 0.0012, weighted value: 0.0602, policy: 0.1813, weighted policy: 0.1813), Train Mean Max: 0.9241
Epoch 9/10, Train Loss: 0.2370 (value: 0.0011, weighted value: 0.0574, policy: 0.1796, weighted policy: 0.1796), Train Mean Max: 0.9244
Epoch 10/10, Train Loss: 0.2347 (value: 0.0011, weighted value: 0.0545, policy: 0.1802, weighted policy: 0.1802), Train Mean Max: 0.9246
..training done in 65.79 seconds
..evaluation done in 15.88 seconds
Old network+MCTS average reward: 0.62, min: 0.06, max: 1.24, stdev: 0.22
New network+MCTS average reward: 0.62, min: 0.06, max: 1.27, stdev: 0.22
Old bare network average reward: 0.55, min: -0.05, max: 1.24, stdev: 0.22
New bare network average reward: 0.55, min: 0.04, max: 1.24, stdev: 0.22
External policy "random" average reward: 0.24, min: -0.43, max: 0.94, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: -0.13, max: 1.24, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.10, max: 1.34, stdev: 0.23
New network won 90 and tied 120 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 111 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.14 seconds
Training examples lengths: [64646, 64532, 64589, 65056, 64750, 64718, 64857, 64797, 64715, 64759]
Total value: 417204.39
Training on 647419 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2662 (value: 0.0015, weighted value: 0.0765, policy: 0.1897, weighted policy: 0.1897), Train Mean Max: 0.9235
Epoch 2/10, Train Loss: 0.2574 (value: 0.0014, weighted value: 0.0706, policy: 0.1868, weighted policy: 0.1868), Train Mean Max: 0.9234
Epoch 3/10, Train Loss: 0.2502 (value: 0.0013, weighted value: 0.0656, policy: 0.1846, weighted policy: 0.1846), Train Mean Max: 0.9238
Epoch 4/10, Train Loss: 0.2463 (value: 0.0013, weighted value: 0.0635, policy: 0.1828, weighted policy: 0.1828), Train Mean Max: 0.9242
Epoch 5/10, Train Loss: 0.2411 (value: 0.0012, weighted value: 0.0602, policy: 0.1809, weighted policy: 0.1809), Train Mean Max: 0.9245
Epoch 6/10, Train Loss: 0.2387 (value: 0.0012, weighted value: 0.0576, policy: 0.1810, weighted policy: 0.1810), Train Mean Max: 0.9246
Epoch 7/10, Train Loss: 0.2370 (value: 0.0011, weighted value: 0.0567, policy: 0.1803, weighted policy: 0.1803), Train Mean Max: 0.9247
Epoch 8/10, Train Loss: 0.2355 (value: 0.0011, weighted value: 0.0561, policy: 0.1794, weighted policy: 0.1794), Train Mean Max: 0.9250
Epoch 9/10, Train Loss: 0.2318 (value: 0.0011, weighted value: 0.0534, policy: 0.1784, weighted policy: 0.1784), Train Mean Max: 0.9252
Epoch 10/10, Train Loss: 0.2308 (value: 0.0010, weighted value: 0.0525, policy: 0.1783, weighted policy: 0.1783), Train Mean Max: 0.9253
..training done in 71.20 seconds
..evaluation done in 16.56 seconds
Old network+MCTS average reward: 0.64, min: 0.05, max: 1.33, stdev: 0.23
New network+MCTS average reward: 0.64, min: 0.13, max: 1.31, stdev: 0.23
Old bare network average reward: 0.57, min: 0.05, max: 1.20, stdev: 0.23
New bare network average reward: 0.57, min: -0.07, max: 1.28, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.44, max: 0.94, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.04, max: 1.24, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.17, max: 1.27, stdev: 0.22
New network won 93 and tied 114 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 112 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.59 seconds
Training examples lengths: [64532, 64589, 65056, 64750, 64718, 64857, 64797, 64715, 64759, 65010]
Total value: 418117.78
Training on 647783 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2599 (value: 0.0014, weighted value: 0.0720, policy: 0.1879, weighted policy: 0.1879), Train Mean Max: 0.9244
Epoch 2/10, Train Loss: 0.2501 (value: 0.0013, weighted value: 0.0666, policy: 0.1835, weighted policy: 0.1835), Train Mean Max: 0.9247
Epoch 3/10, Train Loss: 0.2447 (value: 0.0013, weighted value: 0.0631, policy: 0.1817, weighted policy: 0.1817), Train Mean Max: 0.9251
Epoch 4/10, Train Loss: 0.2395 (value: 0.0012, weighted value: 0.0594, policy: 0.1801, weighted policy: 0.1801), Train Mean Max: 0.9252
Epoch 5/10, Train Loss: 0.2371 (value: 0.0011, weighted value: 0.0573, policy: 0.1798, weighted policy: 0.1798), Train Mean Max: 0.9254
Epoch 6/10, Train Loss: 0.2341 (value: 0.0011, weighted value: 0.0565, policy: 0.1775, weighted policy: 0.1775), Train Mean Max: 0.9258
Epoch 7/10, Train Loss: 0.2316 (value: 0.0011, weighted value: 0.0546, policy: 0.1770, weighted policy: 0.1770), Train Mean Max: 0.9259
Epoch 8/10, Train Loss: 0.2278 (value: 0.0010, weighted value: 0.0519, policy: 0.1760, weighted policy: 0.1760), Train Mean Max: 0.9260
Epoch 9/10, Train Loss: 0.2283 (value: 0.0010, weighted value: 0.0515, policy: 0.1769, weighted policy: 0.1769), Train Mean Max: 0.9262
Epoch 10/10, Train Loss: 0.2250 (value: 0.0010, weighted value: 0.0491, policy: 0.1759, weighted policy: 0.1759), Train Mean Max: 0.9265
..training done in 72.17 seconds
..evaluation done in 16.01 seconds
Old network+MCTS average reward: 0.63, min: 0.05, max: 1.25, stdev: 0.21
New network+MCTS average reward: 0.63, min: -0.03, max: 1.24, stdev: 0.21
Old bare network average reward: 0.57, min: -0.01, max: 1.19, stdev: 0.22
New bare network average reward: 0.57, min: -0.03, max: 1.19, stdev: 0.22
External policy "random" average reward: 0.27, min: -0.28, max: 1.04, stdev: 0.22
External policy "individual greedy" average reward: 0.52, min: -0.03, max: 1.28, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.06, max: 1.31, stdev: 0.21
New network won 92 and tied 127 out of 300 games (51.83% wins where ties are half wins)
Keeping the new network

Training iteration 113 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.62 seconds
Training examples lengths: [64589, 65056, 64750, 64718, 64857, 64797, 64715, 64759, 65010, 64561]
Total value: 418533.00
Training on 647812 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2560 (value: 0.0014, weighted value: 0.0713, policy: 0.1847, weighted policy: 0.1847), Train Mean Max: 0.9252
Epoch 2/10, Train Loss: 0.2472 (value: 0.0013, weighted value: 0.0651, policy: 0.1821, weighted policy: 0.1821), Train Mean Max: 0.9255
Epoch 3/10, Train Loss: 0.2392 (value: 0.0012, weighted value: 0.0607, policy: 0.1785, weighted policy: 0.1785), Train Mean Max: 0.9261
Epoch 4/10, Train Loss: 0.2364 (value: 0.0012, weighted value: 0.0580, policy: 0.1784, weighted policy: 0.1784), Train Mean Max: 0.9260
Epoch 5/10, Train Loss: 0.2331 (value: 0.0011, weighted value: 0.0562, policy: 0.1769, weighted policy: 0.1769), Train Mean Max: 0.9262
Epoch 6/10, Train Loss: 0.2310 (value: 0.0011, weighted value: 0.0557, policy: 0.1754, weighted policy: 0.1754), Train Mean Max: 0.9263
Epoch 7/10, Train Loss: 0.2271 (value: 0.0010, weighted value: 0.0514, policy: 0.1757, weighted policy: 0.1757), Train Mean Max: 0.9266
Epoch 8/10, Train Loss: 0.2263 (value: 0.0010, weighted value: 0.0516, policy: 0.1747, weighted policy: 0.1747), Train Mean Max: 0.9267
Epoch 9/10, Train Loss: 0.2239 (value: 0.0010, weighted value: 0.0501, policy: 0.1738, weighted policy: 0.1738), Train Mean Max: 0.9270
Epoch 10/10, Train Loss: 0.2216 (value: 0.0010, weighted value: 0.0475, policy: 0.1741, weighted policy: 0.1741), Train Mean Max: 0.9272
..training done in 69.78 seconds
..evaluation done in 16.27 seconds
Old network+MCTS average reward: 0.66, min: 0.07, max: 1.31, stdev: 0.23
New network+MCTS average reward: 0.66, min: 0.09, max: 1.31, stdev: 0.23
Old bare network average reward: 0.59, min: -0.06, max: 1.31, stdev: 0.23
New bare network average reward: 0.59, min: -0.06, max: 1.31, stdev: 0.24
External policy "random" average reward: 0.28, min: -0.27, max: 0.94, stdev: 0.23
External policy "individual greedy" average reward: 0.56, min: 0.01, max: 1.45, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: 0.18, max: 1.42, stdev: 0.22
New network won 84 and tied 134 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 114 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.56 seconds
Training examples lengths: [65056, 64750, 64718, 64857, 64797, 64715, 64759, 65010, 64561, 64854]
Total value: 418947.34
Training on 648077 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2569 (value: 0.0015, weighted value: 0.0732, policy: 0.1836, weighted policy: 0.1836), Train Mean Max: 0.9255
Epoch 2/10, Train Loss: 0.2459 (value: 0.0013, weighted value: 0.0648, policy: 0.1811, weighted policy: 0.1811), Train Mean Max: 0.9259
Epoch 3/10, Train Loss: 0.2411 (value: 0.0012, weighted value: 0.0620, policy: 0.1791, weighted policy: 0.1791), Train Mean Max: 0.9262
Epoch 4/10, Train Loss: 0.2331 (value: 0.0011, weighted value: 0.0574, policy: 0.1757, weighted policy: 0.1757), Train Mean Max: 0.9266
Epoch 5/10, Train Loss: 0.2324 (value: 0.0011, weighted value: 0.0562, policy: 0.1762, weighted policy: 0.1762), Train Mean Max: 0.9266
Epoch 6/10, Train Loss: 0.2280 (value: 0.0011, weighted value: 0.0536, policy: 0.1744, weighted policy: 0.1744), Train Mean Max: 0.9268
Epoch 7/10, Train Loss: 0.2272 (value: 0.0011, weighted value: 0.0531, policy: 0.1741, weighted policy: 0.1741), Train Mean Max: 0.9271
Epoch 8/10, Train Loss: 0.2248 (value: 0.0010, weighted value: 0.0508, policy: 0.1739, weighted policy: 0.1739), Train Mean Max: 0.9272
Epoch 9/10, Train Loss: 0.2228 (value: 0.0010, weighted value: 0.0486, policy: 0.1742, weighted policy: 0.1742), Train Mean Max: 0.9275
Epoch 10/10, Train Loss: 0.2236 (value: 0.0010, weighted value: 0.0494, policy: 0.1742, weighted policy: 0.1742), Train Mean Max: 0.9275
..training done in 71.33 seconds
..evaluation done in 16.86 seconds
Old network+MCTS average reward: 0.66, min: 0.14, max: 1.29, stdev: 0.22
New network+MCTS average reward: 0.66, min: 0.08, max: 1.34, stdev: 0.22
Old bare network average reward: 0.60, min: 0.02, max: 1.31, stdev: 0.24
New bare network average reward: 0.60, min: 0.06, max: 1.34, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.32, max: 1.10, stdev: 0.23
External policy "individual greedy" average reward: 0.56, min: -0.06, max: 1.29, stdev: 0.24
External policy "total greedy" average reward: 0.67, min: 0.18, max: 1.31, stdev: 0.23
New network won 88 and tied 140 out of 300 games (52.67% wins where ties are half wins)
Keeping the new network

Training iteration 115 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.29 seconds
Training examples lengths: [64750, 64718, 64857, 64797, 64715, 64759, 65010, 64561, 64854, 65032]
Total value: 419440.50
Training on 648053 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2552 (value: 0.0014, weighted value: 0.0713, policy: 0.1840, weighted policy: 0.1840), Train Mean Max: 0.9260
Epoch 2/10, Train Loss: 0.2424 (value: 0.0013, weighted value: 0.0630, policy: 0.1794, weighted policy: 0.1794), Train Mean Max: 0.9264
Epoch 3/10, Train Loss: 0.2387 (value: 0.0012, weighted value: 0.0611, policy: 0.1775, weighted policy: 0.1775), Train Mean Max: 0.9265
Epoch 4/10, Train Loss: 0.2337 (value: 0.0012, weighted value: 0.0584, policy: 0.1753, weighted policy: 0.1753), Train Mean Max: 0.9268
Epoch 5/10, Train Loss: 0.2289 (value: 0.0011, weighted value: 0.0538, policy: 0.1751, weighted policy: 0.1751), Train Mean Max: 0.9270
Epoch 6/10, Train Loss: 0.2303 (value: 0.0011, weighted value: 0.0560, policy: 0.1742, weighted policy: 0.1742), Train Mean Max: 0.9271
Epoch 7/10, Train Loss: 0.2246 (value: 0.0010, weighted value: 0.0504, policy: 0.1742, weighted policy: 0.1742), Train Mean Max: 0.9273
Epoch 8/10, Train Loss: 0.2227 (value: 0.0010, weighted value: 0.0497, policy: 0.1730, weighted policy: 0.1730), Train Mean Max: 0.9276
Epoch 9/10, Train Loss: 0.2220 (value: 0.0010, weighted value: 0.0490, policy: 0.1730, weighted policy: 0.1730), Train Mean Max: 0.9278
Epoch 10/10, Train Loss: 0.2214 (value: 0.0010, weighted value: 0.0490, policy: 0.1725, weighted policy: 0.1725), Train Mean Max: 0.9279
..training done in 70.64 seconds
..evaluation done in 16.76 seconds
Old network+MCTS average reward: 0.65, min: 0.17, max: 1.29, stdev: 0.21
New network+MCTS average reward: 0.65, min: 0.19, max: 1.29, stdev: 0.21
Old bare network average reward: 0.58, min: 0.02, max: 1.29, stdev: 0.23
New bare network average reward: 0.58, min: 0.06, max: 1.29, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.46, max: 0.94, stdev: 0.21
External policy "individual greedy" average reward: 0.55, min: 0.04, max: 1.31, stdev: 0.22
External policy "total greedy" average reward: 0.67, min: 0.18, max: 1.40, stdev: 0.22
New network won 89 and tied 126 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 116 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.96 seconds
Training examples lengths: [64718, 64857, 64797, 64715, 64759, 65010, 64561, 64854, 65032, 65186]
Total value: 420106.26
Training on 648489 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2530 (value: 0.0014, weighted value: 0.0706, policy: 0.1824, weighted policy: 0.1824), Train Mean Max: 0.9262
Epoch 2/10, Train Loss: 0.2429 (value: 0.0013, weighted value: 0.0643, policy: 0.1786, weighted policy: 0.1786), Train Mean Max: 0.9267
Epoch 3/10, Train Loss: 0.2360 (value: 0.0012, weighted value: 0.0589, policy: 0.1771, weighted policy: 0.1771), Train Mean Max: 0.9269
Epoch 4/10, Train Loss: 0.2330 (value: 0.0012, weighted value: 0.0576, policy: 0.1755, weighted policy: 0.1755), Train Mean Max: 0.9272
Epoch 5/10, Train Loss: 0.2281 (value: 0.0011, weighted value: 0.0542, policy: 0.1739, weighted policy: 0.1739), Train Mean Max: 0.9275
Epoch 6/10, Train Loss: 0.2261 (value: 0.0011, weighted value: 0.0529, policy: 0.1732, weighted policy: 0.1732), Train Mean Max: 0.9277
Epoch 7/10, Train Loss: 0.2248 (value: 0.0010, weighted value: 0.0520, policy: 0.1728, weighted policy: 0.1728), Train Mean Max: 0.9278
Epoch 8/10, Train Loss: 0.2234 (value: 0.0010, weighted value: 0.0498, policy: 0.1736, weighted policy: 0.1736), Train Mean Max: 0.9279
Epoch 9/10, Train Loss: 0.2202 (value: 0.0010, weighted value: 0.0485, policy: 0.1717, weighted policy: 0.1717), Train Mean Max: 0.9281
Epoch 10/10, Train Loss: 0.2194 (value: 0.0010, weighted value: 0.0483, policy: 0.1711, weighted policy: 0.1711), Train Mean Max: 0.9282
..training done in 69.58 seconds
..evaluation done in 17.17 seconds
Old network+MCTS average reward: 0.65, min: -0.08, max: 1.31, stdev: 0.22
New network+MCTS average reward: 0.65, min: -0.04, max: 1.29, stdev: 0.22
Old bare network average reward: 0.59, min: -0.09, max: 1.34, stdev: 0.23
New bare network average reward: 0.59, min: -0.06, max: 1.25, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.29, max: 0.96, stdev: 0.24
External policy "individual greedy" average reward: 0.53, min: -0.14, max: 1.28, stdev: 0.24
External policy "total greedy" average reward: 0.65, min: -0.08, max: 1.47, stdev: 0.24
New network won 81 and tied 152 out of 300 games (52.33% wins where ties are half wins)
Keeping the new network

Training iteration 117 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.84 seconds
Training examples lengths: [64857, 64797, 64715, 64759, 65010, 64561, 64854, 65032, 65186, 64781]
Total value: 420752.83
Training on 648552 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2517 (value: 0.0014, weighted value: 0.0705, policy: 0.1812, weighted policy: 0.1812), Train Mean Max: 0.9271
Epoch 2/10, Train Loss: 0.2405 (value: 0.0012, weighted value: 0.0621, policy: 0.1785, weighted policy: 0.1785), Train Mean Max: 0.9274
Epoch 3/10, Train Loss: 0.2353 (value: 0.0012, weighted value: 0.0605, policy: 0.1748, weighted policy: 0.1748), Train Mean Max: 0.9278
Epoch 4/10, Train Loss: 0.2300 (value: 0.0011, weighted value: 0.0561, policy: 0.1739, weighted policy: 0.1739), Train Mean Max: 0.9278
Epoch 5/10, Train Loss: 0.2273 (value: 0.0011, weighted value: 0.0550, policy: 0.1723, weighted policy: 0.1723), Train Mean Max: 0.9282
Epoch 6/10, Train Loss: 0.2233 (value: 0.0010, weighted value: 0.0519, policy: 0.1714, weighted policy: 0.1714), Train Mean Max: 0.9284
Epoch 7/10, Train Loss: 0.2244 (value: 0.0010, weighted value: 0.0520, policy: 0.1723, weighted policy: 0.1723), Train Mean Max: 0.9283
Epoch 8/10, Train Loss: 0.2202 (value: 0.0010, weighted value: 0.0493, policy: 0.1709, weighted policy: 0.1709), Train Mean Max: 0.9286
Epoch 9/10, Train Loss: 0.2185 (value: 0.0010, weighted value: 0.0478, policy: 0.1707, weighted policy: 0.1707), Train Mean Max: 0.9287
Epoch 10/10, Train Loss: 0.2176 (value: 0.0009, weighted value: 0.0472, policy: 0.1704, weighted policy: 0.1704), Train Mean Max: 0.9288
..training done in 72.41 seconds
..evaluation done in 16.54 seconds
Old network+MCTS average reward: 0.63, min: 0.01, max: 1.26, stdev: 0.23
New network+MCTS average reward: 0.64, min: 0.03, max: 1.35, stdev: 0.22
Old bare network average reward: 0.58, min: 0.00, max: 1.19, stdev: 0.23
New bare network average reward: 0.57, min: 0.00, max: 1.19, stdev: 0.23
External policy "random" average reward: 0.24, min: -0.42, max: 0.89, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: 0.01, max: 1.19, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.11, max: 1.33, stdev: 0.24
New network won 85 and tied 142 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network

Training iteration 118 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.04 seconds
Training examples lengths: [64797, 64715, 64759, 65010, 64561, 64854, 65032, 65186, 64781, 64723]
Total value: 421166.02
Training on 648418 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2505 (value: 0.0014, weighted value: 0.0710, policy: 0.1795, weighted policy: 0.1795), Train Mean Max: 0.9275
Epoch 2/10, Train Loss: 0.2390 (value: 0.0012, weighted value: 0.0625, policy: 0.1765, weighted policy: 0.1765), Train Mean Max: 0.9280
Epoch 3/10, Train Loss: 0.2323 (value: 0.0012, weighted value: 0.0586, policy: 0.1737, weighted policy: 0.1737), Train Mean Max: 0.9282
Epoch 4/10, Train Loss: 0.2294 (value: 0.0011, weighted value: 0.0572, policy: 0.1722, weighted policy: 0.1722), Train Mean Max: 0.9285
Epoch 5/10, Train Loss: 0.2263 (value: 0.0011, weighted value: 0.0546, policy: 0.1717, weighted policy: 0.1717), Train Mean Max: 0.9286
Epoch 6/10, Train Loss: 0.2223 (value: 0.0011, weighted value: 0.0530, policy: 0.1693, weighted policy: 0.1693), Train Mean Max: 0.9288
Epoch 7/10, Train Loss: 0.2215 (value: 0.0010, weighted value: 0.0512, policy: 0.1703, weighted policy: 0.1703), Train Mean Max: 0.9290
Epoch 8/10, Train Loss: 0.2183 (value: 0.0010, weighted value: 0.0490, policy: 0.1694, weighted policy: 0.1694), Train Mean Max: 0.9293
Epoch 9/10, Train Loss: 0.2182 (value: 0.0010, weighted value: 0.0490, policy: 0.1691, weighted policy: 0.1691), Train Mean Max: 0.9293
Epoch 10/10, Train Loss: 0.2152 (value: 0.0009, weighted value: 0.0462, policy: 0.1690, weighted policy: 0.1690), Train Mean Max: 0.9295
..training done in 65.79 seconds
..evaluation done in 16.68 seconds
Old network+MCTS average reward: 0.66, min: 0.10, max: 1.29, stdev: 0.23
New network+MCTS average reward: 0.66, min: 0.09, max: 1.25, stdev: 0.23
Old bare network average reward: 0.59, min: -0.07, max: 1.26, stdev: 0.23
New bare network average reward: 0.60, min: 0.04, max: 1.27, stdev: 0.24
External policy "random" average reward: 0.28, min: -0.36, max: 1.05, stdev: 0.22
External policy "individual greedy" average reward: 0.56, min: 0.06, max: 1.27, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: 0.18, max: 1.32, stdev: 0.22
New network won 67 and tied 151 out of 300 games (47.50% wins where ties are half wins)
Reverting to the old network

Training iteration 119 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.56 seconds
Training examples lengths: [64715, 64759, 65010, 64561, 64854, 65032, 65186, 64781, 64723, 64650]
Total value: 421698.93
Training on 648271 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2835 (value: 0.0019, weighted value: 0.0939, policy: 0.1896, weighted policy: 0.1896), Train Mean Max: 0.9260
Epoch 2/10, Train Loss: 0.2641 (value: 0.0016, weighted value: 0.0813, policy: 0.1828, weighted policy: 0.1828), Train Mean Max: 0.9267
Epoch 3/10, Train Loss: 0.2511 (value: 0.0015, weighted value: 0.0732, policy: 0.1779, weighted policy: 0.1779), Train Mean Max: 0.9273
Epoch 4/10, Train Loss: 0.2441 (value: 0.0014, weighted value: 0.0685, policy: 0.1755, weighted policy: 0.1755), Train Mean Max: 0.9274
Epoch 5/10, Train Loss: 0.2391 (value: 0.0013, weighted value: 0.0647, policy: 0.1744, weighted policy: 0.1744), Train Mean Max: 0.9275
Epoch 6/10, Train Loss: 0.2353 (value: 0.0012, weighted value: 0.0623, policy: 0.1729, weighted policy: 0.1729), Train Mean Max: 0.9279
Epoch 7/10, Train Loss: 0.2334 (value: 0.0012, weighted value: 0.0615, policy: 0.1719, weighted policy: 0.1719), Train Mean Max: 0.9280
Epoch 8/10, Train Loss: 0.2272 (value: 0.0011, weighted value: 0.0560, policy: 0.1712, weighted policy: 0.1712), Train Mean Max: 0.9283
Epoch 9/10, Train Loss: 0.2260 (value: 0.0011, weighted value: 0.0561, policy: 0.1700, weighted policy: 0.1700), Train Mean Max: 0.9286
Epoch 10/10, Train Loss: 0.2252 (value: 0.0011, weighted value: 0.0544, policy: 0.1708, weighted policy: 0.1708), Train Mean Max: 0.9286
..training done in 71.54 seconds
..evaluation done in 15.97 seconds
Old network+MCTS average reward: 0.63, min: -0.09, max: 1.18, stdev: 0.22
New network+MCTS average reward: 0.64, min: -0.06, max: 1.18, stdev: 0.22
Old bare network average reward: 0.57, min: -0.13, max: 1.17, stdev: 0.23
New bare network average reward: 0.58, min: -0.10, max: 1.17, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.32, max: 0.91, stdev: 0.21
External policy "individual greedy" average reward: 0.53, min: -0.09, max: 1.06, stdev: 0.22
External policy "total greedy" average reward: 0.63, min: -0.09, max: 1.28, stdev: 0.23
New network won 89 and tied 142 out of 300 games (53.33% wins where ties are half wins)
Keeping the new network

Training iteration 120 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.26 seconds
Training examples lengths: [64759, 65010, 64561, 64854, 65032, 65186, 64781, 64723, 64650, 64785]
Total value: 421797.57
Training on 648341 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2568 (value: 0.0015, weighted value: 0.0761, policy: 0.1807, weighted policy: 0.1807), Train Mean Max: 0.9271
Epoch 2/10, Train Loss: 0.2457 (value: 0.0014, weighted value: 0.0679, policy: 0.1778, weighted policy: 0.1778), Train Mean Max: 0.9274
Epoch 3/10, Train Loss: 0.2374 (value: 0.0013, weighted value: 0.0629, policy: 0.1745, weighted policy: 0.1745), Train Mean Max: 0.9280
Epoch 4/10, Train Loss: 0.2342 (value: 0.0012, weighted value: 0.0614, policy: 0.1729, weighted policy: 0.1729), Train Mean Max: 0.9282
Epoch 5/10, Train Loss: 0.2307 (value: 0.0012, weighted value: 0.0596, policy: 0.1711, weighted policy: 0.1711), Train Mean Max: 0.9284
Epoch 6/10, Train Loss: 0.2279 (value: 0.0011, weighted value: 0.0567, policy: 0.1712, weighted policy: 0.1712), Train Mean Max: 0.9284
Epoch 7/10, Train Loss: 0.2242 (value: 0.0011, weighted value: 0.0539, policy: 0.1703, weighted policy: 0.1703), Train Mean Max: 0.9287
Epoch 8/10, Train Loss: 0.2234 (value: 0.0011, weighted value: 0.0534, policy: 0.1700, weighted policy: 0.1700), Train Mean Max: 0.9287
Epoch 9/10, Train Loss: 0.2205 (value: 0.0010, weighted value: 0.0514, policy: 0.1691, weighted policy: 0.1691), Train Mean Max: 0.9291
Epoch 10/10, Train Loss: 0.2176 (value: 0.0010, weighted value: 0.0496, policy: 0.1680, weighted policy: 0.1680), Train Mean Max: 0.9292
..training done in 65.70 seconds
..evaluation done in 16.44 seconds
Old network+MCTS average reward: 0.66, min: 0.04, max: 1.32, stdev: 0.22
New network+MCTS average reward: 0.66, min: 0.04, max: 1.31, stdev: 0.22
Old bare network average reward: 0.60, min: 0.04, max: 1.31, stdev: 0.23
New bare network average reward: 0.60, min: 0.00, max: 1.31, stdev: 0.22
External policy "random" average reward: 0.28, min: -0.35, max: 1.16, stdev: 0.22
External policy "individual greedy" average reward: 0.56, min: -0.03, max: 1.32, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: 0.07, max: 1.38, stdev: 0.22
New network won 67 and tied 156 out of 300 games (48.33% wins where ties are half wins)
Reverting to the old network
Saving intermediate checkpoint to zgaz_checkpoints/1757654225_iter_120

Training iteration 121 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.41 seconds
Training examples lengths: [65010, 64561, 64854, 65032, 65186, 64781, 64723, 64650, 64785, 64487]
Total value: 421761.37
Training on 648069 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2870 (value: 0.0019, weighted value: 0.0974, policy: 0.1896, weighted policy: 0.1896), Train Mean Max: 0.9257
Epoch 2/10, Train Loss: 0.2680 (value: 0.0017, weighted value: 0.0846, policy: 0.1834, weighted policy: 0.1834), Train Mean Max: 0.9263
Epoch 3/10, Train Loss: 0.2563 (value: 0.0016, weighted value: 0.0775, policy: 0.1788, weighted policy: 0.1788), Train Mean Max: 0.9267
Epoch 4/10, Train Loss: 0.2496 (value: 0.0015, weighted value: 0.0732, policy: 0.1764, weighted policy: 0.1764), Train Mean Max: 0.9270
Epoch 5/10, Train Loss: 0.2438 (value: 0.0014, weighted value: 0.0683, policy: 0.1755, weighted policy: 0.1755), Train Mean Max: 0.9273
Epoch 6/10, Train Loss: 0.2407 (value: 0.0013, weighted value: 0.0663, policy: 0.1743, weighted policy: 0.1743), Train Mean Max: 0.9275
Epoch 7/10, Train Loss: 0.2343 (value: 0.0012, weighted value: 0.0617, policy: 0.1726, weighted policy: 0.1726), Train Mean Max: 0.9277
Epoch 8/10, Train Loss: 0.2346 (value: 0.0012, weighted value: 0.0622, policy: 0.1724, weighted policy: 0.1724), Train Mean Max: 0.9279
Epoch 9/10, Train Loss: 0.2293 (value: 0.0011, weighted value: 0.0567, policy: 0.1726, weighted policy: 0.1726), Train Mean Max: 0.9279
Epoch 10/10, Train Loss: 0.2279 (value: 0.0011, weighted value: 0.0567, policy: 0.1712, weighted policy: 0.1712), Train Mean Max: 0.9284
..training done in 65.06 seconds
..evaluation done in 16.34 seconds
Old network+MCTS average reward: 0.65, min: 0.05, max: 1.31, stdev: 0.23
New network+MCTS average reward: 0.65, min: 0.02, max: 1.31, stdev: 0.23
Old bare network average reward: 0.59, min: 0.00, max: 1.27, stdev: 0.23
New bare network average reward: 0.59, min: -0.07, max: 1.31, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.35, max: 0.97, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: 0.06, max: 1.33, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.17, max: 1.31, stdev: 0.22
New network won 82 and tied 143 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 122 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.23 seconds
Training examples lengths: [64561, 64854, 65032, 65186, 64781, 64723, 64650, 64785, 64487, 64971]
Total value: 422235.81
Training on 648030 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2617 (value: 0.0016, weighted value: 0.0798, policy: 0.1820, weighted policy: 0.1820), Train Mean Max: 0.9267
Epoch 2/10, Train Loss: 0.2470 (value: 0.0014, weighted value: 0.0702, policy: 0.1768, weighted policy: 0.1768), Train Mean Max: 0.9274
Epoch 3/10, Train Loss: 0.2410 (value: 0.0013, weighted value: 0.0654, policy: 0.1756, weighted policy: 0.1756), Train Mean Max: 0.9275
Epoch 4/10, Train Loss: 0.2373 (value: 0.0013, weighted value: 0.0642, policy: 0.1731, weighted policy: 0.1731), Train Mean Max: 0.9278
Epoch 5/10, Train Loss: 0.2295 (value: 0.0012, weighted value: 0.0583, policy: 0.1713, weighted policy: 0.1713), Train Mean Max: 0.9283
Epoch 6/10, Train Loss: 0.2314 (value: 0.0012, weighted value: 0.0597, policy: 0.1717, weighted policy: 0.1717), Train Mean Max: 0.9282
Epoch 7/10, Train Loss: 0.2267 (value: 0.0011, weighted value: 0.0555, policy: 0.1712, weighted policy: 0.1712), Train Mean Max: 0.9284
Epoch 8/10, Train Loss: 0.2255 (value: 0.0011, weighted value: 0.0551, policy: 0.1704, weighted policy: 0.1704), Train Mean Max: 0.9287
Epoch 9/10, Train Loss: 0.2222 (value: 0.0011, weighted value: 0.0529, policy: 0.1693, weighted policy: 0.1693), Train Mean Max: 0.9289
Epoch 10/10, Train Loss: 0.2208 (value: 0.0010, weighted value: 0.0524, policy: 0.1684, weighted policy: 0.1684), Train Mean Max: 0.9292
..training done in 65.10 seconds
..evaluation done in 16.23 seconds
Old network+MCTS average reward: 0.64, min: 0.00, max: 1.30, stdev: 0.23
New network+MCTS average reward: 0.64, min: -0.19, max: 1.30, stdev: 0.23
Old bare network average reward: 0.58, min: -0.04, max: 1.30, stdev: 0.24
New bare network average reward: 0.58, min: -0.14, max: 1.30, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.36, max: 1.06, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: 0.05, max: 1.20, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.01, max: 1.24, stdev: 0.22
New network won 86 and tied 134 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 123 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.59 seconds
Training examples lengths: [64854, 65032, 65186, 64781, 64723, 64650, 64785, 64487, 64971, 65233]
Total value: 423684.21
Training on 648702 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2510 (value: 0.0014, weighted value: 0.0721, policy: 0.1788, weighted policy: 0.1788), Train Mean Max: 0.9275
Epoch 2/10, Train Loss: 0.2418 (value: 0.0013, weighted value: 0.0671, policy: 0.1748, weighted policy: 0.1748), Train Mean Max: 0.9280
Epoch 3/10, Train Loss: 0.2357 (value: 0.0013, weighted value: 0.0628, policy: 0.1728, weighted policy: 0.1728), Train Mean Max: 0.9284
Epoch 4/10, Train Loss: 0.2298 (value: 0.0012, weighted value: 0.0585, policy: 0.1712, weighted policy: 0.1712), Train Mean Max: 0.9286
Epoch 5/10, Train Loss: 0.2261 (value: 0.0011, weighted value: 0.0568, policy: 0.1693, weighted policy: 0.1693), Train Mean Max: 0.9289
Epoch 6/10, Train Loss: 0.2236 (value: 0.0011, weighted value: 0.0545, policy: 0.1691, weighted policy: 0.1691), Train Mean Max: 0.9291
Epoch 7/10, Train Loss: 0.2225 (value: 0.0011, weighted value: 0.0545, policy: 0.1680, weighted policy: 0.1680), Train Mean Max: 0.9292
Epoch 8/10, Train Loss: 0.2197 (value: 0.0010, weighted value: 0.0514, policy: 0.1683, weighted policy: 0.1683), Train Mean Max: 0.9294
Epoch 9/10, Train Loss: 0.2177 (value: 0.0010, weighted value: 0.0509, policy: 0.1668, weighted policy: 0.1668), Train Mean Max: 0.9297
Epoch 10/10, Train Loss: 0.2162 (value: 0.0010, weighted value: 0.0488, policy: 0.1675, weighted policy: 0.1675), Train Mean Max: 0.9299
..training done in 71.41 seconds
..evaluation done in 16.28 seconds
Old network+MCTS average reward: 0.64, min: -0.07, max: 1.43, stdev: 0.22
New network+MCTS average reward: 0.64, min: 0.15, max: 1.35, stdev: 0.21
Old bare network average reward: 0.58, min: -0.06, max: 1.13, stdev: 0.23
New bare network average reward: 0.58, min: -0.06, max: 1.13, stdev: 0.22
External policy "random" average reward: 0.28, min: -0.37, max: 1.08, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: 0.06, max: 1.18, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.06, max: 1.34, stdev: 0.22
New network won 79 and tied 142 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 124 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.39 seconds
Training examples lengths: [65032, 65186, 64781, 64723, 64650, 64785, 64487, 64971, 65233, 64930]
Total value: 423896.36
Training on 648778 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2488 (value: 0.0014, weighted value: 0.0713, policy: 0.1774, weighted policy: 0.1774), Train Mean Max: 0.9285
Epoch 2/10, Train Loss: 0.2376 (value: 0.0013, weighted value: 0.0648, policy: 0.1728, weighted policy: 0.1728), Train Mean Max: 0.9290
Epoch 3/10, Train Loss: 0.2318 (value: 0.0012, weighted value: 0.0622, policy: 0.1696, weighted policy: 0.1696), Train Mean Max: 0.9294
Epoch 4/10, Train Loss: 0.2276 (value: 0.0012, weighted value: 0.0577, policy: 0.1699, weighted policy: 0.1699), Train Mean Max: 0.9295
Epoch 5/10, Train Loss: 0.2234 (value: 0.0011, weighted value: 0.0553, policy: 0.1681, weighted policy: 0.1681), Train Mean Max: 0.9298
Epoch 6/10, Train Loss: 0.2208 (value: 0.0011, weighted value: 0.0543, policy: 0.1665, weighted policy: 0.1665), Train Mean Max: 0.9300
Epoch 7/10, Train Loss: 0.2185 (value: 0.0011, weighted value: 0.0527, policy: 0.1658, weighted policy: 0.1658), Train Mean Max: 0.9302
Epoch 8/10, Train Loss: 0.2151 (value: 0.0010, weighted value: 0.0496, policy: 0.1655, weighted policy: 0.1655), Train Mean Max: 0.9304
Epoch 9/10, Train Loss: 0.2162 (value: 0.0010, weighted value: 0.0503, policy: 0.1658, weighted policy: 0.1658), Train Mean Max: 0.9304
Epoch 10/10, Train Loss: 0.2141 (value: 0.0010, weighted value: 0.0481, policy: 0.1661, weighted policy: 0.1661), Train Mean Max: 0.9306
..training done in 65.66 seconds
..evaluation done in 16.01 seconds
Old network+MCTS average reward: 0.64, min: 0.04, max: 1.43, stdev: 0.22
New network+MCTS average reward: 0.63, min: 0.04, max: 1.37, stdev: 0.22
Old bare network average reward: 0.57, min: -0.01, max: 1.43, stdev: 0.23
New bare network average reward: 0.57, min: -0.15, max: 1.37, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.28, max: 0.97, stdev: 0.22
External policy "individual greedy" average reward: 0.52, min: -0.04, max: 1.13, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: 0.06, max: 1.26, stdev: 0.22
New network won 69 and tied 144 out of 300 games (47.00% wins where ties are half wins)
Reverting to the old network

Training iteration 125 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.01 seconds
Training examples lengths: [65186, 64781, 64723, 64650, 64785, 64487, 64971, 65233, 64930, 64793]
Total value: 423961.36
Training on 648539 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2805 (value: 0.0019, weighted value: 0.0944, policy: 0.1861, weighted policy: 0.1861), Train Mean Max: 0.9269
Epoch 2/10, Train Loss: 0.2625 (value: 0.0016, weighted value: 0.0818, policy: 0.1808, weighted policy: 0.1808), Train Mean Max: 0.9275
Epoch 3/10, Train Loss: 0.2494 (value: 0.0015, weighted value: 0.0738, policy: 0.1756, weighted policy: 0.1756), Train Mean Max: 0.9280
Epoch 4/10, Train Loss: 0.2444 (value: 0.0014, weighted value: 0.0714, policy: 0.1730, weighted policy: 0.1730), Train Mean Max: 0.9283
Epoch 5/10, Train Loss: 0.2386 (value: 0.0013, weighted value: 0.0671, policy: 0.1714, weighted policy: 0.1714), Train Mean Max: 0.9285
Epoch 6/10, Train Loss: 0.2329 (value: 0.0013, weighted value: 0.0626, policy: 0.1704, weighted policy: 0.1704), Train Mean Max: 0.9289
Epoch 7/10, Train Loss: 0.2293 (value: 0.0012, weighted value: 0.0601, policy: 0.1692, weighted policy: 0.1692), Train Mean Max: 0.9289
Epoch 8/10, Train Loss: 0.2289 (value: 0.0012, weighted value: 0.0593, policy: 0.1696, weighted policy: 0.1696), Train Mean Max: 0.9291
Epoch 9/10, Train Loss: 0.2269 (value: 0.0012, weighted value: 0.0580, policy: 0.1689, weighted policy: 0.1689), Train Mean Max: 0.9293
Epoch 10/10, Train Loss: 0.2207 (value: 0.0011, weighted value: 0.0540, policy: 0.1667, weighted policy: 0.1667), Train Mean Max: 0.9298
..training done in 71.65 seconds
..evaluation done in 15.95 seconds
Old network+MCTS average reward: 0.66, min: 0.08, max: 1.26, stdev: 0.22
New network+MCTS average reward: 0.66, min: 0.06, max: 1.34, stdev: 0.22
Old bare network average reward: 0.60, min: 0.01, max: 1.20, stdev: 0.23
New bare network average reward: 0.60, min: 0.03, max: 1.20, stdev: 0.22
External policy "random" average reward: 0.26, min: -0.27, max: 0.84, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: -0.10, max: 1.14, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: 0.12, max: 1.18, stdev: 0.22
New network won 98 and tied 139 out of 300 games (55.83% wins where ties are half wins)
Keeping the new network

Training iteration 126 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.28 seconds
Training examples lengths: [64781, 64723, 64650, 64785, 64487, 64971, 65233, 64930, 64793, 64719]
Total value: 424118.92
Training on 648072 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2568 (value: 0.0016, weighted value: 0.0778, policy: 0.1790, weighted policy: 0.1790), Train Mean Max: 0.9280
Epoch 2/10, Train Loss: 0.2440 (value: 0.0014, weighted value: 0.0689, policy: 0.1752, weighted policy: 0.1752), Train Mean Max: 0.9285
Epoch 3/10, Train Loss: 0.2362 (value: 0.0013, weighted value: 0.0640, policy: 0.1722, weighted policy: 0.1722), Train Mean Max: 0.9290
Epoch 4/10, Train Loss: 0.2322 (value: 0.0012, weighted value: 0.0616, policy: 0.1706, weighted policy: 0.1706), Train Mean Max: 0.9292
Epoch 5/10, Train Loss: 0.2288 (value: 0.0012, weighted value: 0.0600, policy: 0.1688, weighted policy: 0.1688), Train Mean Max: 0.9294
Epoch 6/10, Train Loss: 0.2246 (value: 0.0011, weighted value: 0.0561, policy: 0.1684, weighted policy: 0.1684), Train Mean Max: 0.9294
Epoch 7/10, Train Loss: 0.2242 (value: 0.0011, weighted value: 0.0560, policy: 0.1682, weighted policy: 0.1682), Train Mean Max: 0.9296
Epoch 8/10, Train Loss: 0.2214 (value: 0.0011, weighted value: 0.0538, policy: 0.1676, weighted policy: 0.1676), Train Mean Max: 0.9298
Epoch 9/10, Train Loss: 0.2189 (value: 0.0010, weighted value: 0.0521, policy: 0.1667, weighted policy: 0.1667), Train Mean Max: 0.9300
Epoch 10/10, Train Loss: 0.2169 (value: 0.0010, weighted value: 0.0506, policy: 0.1663, weighted policy: 0.1663), Train Mean Max: 0.9302
..training done in 72.95 seconds
..evaluation done in 16.32 seconds
Old network+MCTS average reward: 0.65, min: 0.03, max: 1.51, stdev: 0.23
New network+MCTS average reward: 0.65, min: 0.00, max: 1.54, stdev: 0.23
Old bare network average reward: 0.59, min: 0.03, max: 1.51, stdev: 0.23
New bare network average reward: 0.59, min: 0.03, max: 1.51, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.36, max: 0.95, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: -0.13, max: 1.12, stdev: 0.24
External policy "total greedy" average reward: 0.65, min: 0.02, max: 1.32, stdev: 0.23
New network won 88 and tied 134 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 127 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.04 seconds
Training examples lengths: [64723, 64650, 64785, 64487, 64971, 65233, 64930, 64793, 64719, 64626]
Total value: 424593.96
Training on 647917 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2495 (value: 0.0015, weighted value: 0.0731, policy: 0.1765, weighted policy: 0.1765), Train Mean Max: 0.9288
Epoch 2/10, Train Loss: 0.2409 (value: 0.0013, weighted value: 0.0669, policy: 0.1740, weighted policy: 0.1740), Train Mean Max: 0.9291
Epoch 3/10, Train Loss: 0.2353 (value: 0.0013, weighted value: 0.0638, policy: 0.1715, weighted policy: 0.1715), Train Mean Max: 0.9294
Epoch 4/10, Train Loss: 0.2289 (value: 0.0012, weighted value: 0.0599, policy: 0.1691, weighted policy: 0.1691), Train Mean Max: 0.9298
Epoch 5/10, Train Loss: 0.2244 (value: 0.0011, weighted value: 0.0563, policy: 0.1681, weighted policy: 0.1681), Train Mean Max: 0.9299
Epoch 6/10, Train Loss: 0.2219 (value: 0.0011, weighted value: 0.0546, policy: 0.1673, weighted policy: 0.1673), Train Mean Max: 0.9302
Epoch 7/10, Train Loss: 0.2193 (value: 0.0011, weighted value: 0.0534, policy: 0.1659, weighted policy: 0.1659), Train Mean Max: 0.9304
Epoch 8/10, Train Loss: 0.2194 (value: 0.0011, weighted value: 0.0532, policy: 0.1662, weighted policy: 0.1662), Train Mean Max: 0.9305
Epoch 9/10, Train Loss: 0.2146 (value: 0.0010, weighted value: 0.0490, policy: 0.1655, weighted policy: 0.1655), Train Mean Max: 0.9307
Epoch 10/10, Train Loss: 0.2142 (value: 0.0010, weighted value: 0.0499, policy: 0.1644, weighted policy: 0.1644), Train Mean Max: 0.9308
..training done in 72.38 seconds
..evaluation done in 16.46 seconds
Old network+MCTS average reward: 0.64, min: 0.12, max: 1.43, stdev: 0.22
New network+MCTS average reward: 0.65, min: 0.11, max: 1.43, stdev: 0.22
Old bare network average reward: 0.59, min: 0.07, max: 1.33, stdev: 0.23
New bare network average reward: 0.59, min: 0.10, max: 1.44, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.30, max: 1.20, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: -0.12, max: 1.49, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.03, max: 1.55, stdev: 0.22
New network won 77 and tied 142 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 128 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.49 seconds
Training examples lengths: [64650, 64785, 64487, 64971, 65233, 64930, 64793, 64719, 64626, 64748]
Total value: 424674.98
Training on 647942 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2820 (value: 0.0019, weighted value: 0.0963, policy: 0.1856, weighted policy: 0.1856), Train Mean Max: 0.9273
Epoch 2/10, Train Loss: 0.2618 (value: 0.0016, weighted value: 0.0818, policy: 0.1800, weighted policy: 0.1800), Train Mean Max: 0.9280
Epoch 3/10, Train Loss: 0.2526 (value: 0.0015, weighted value: 0.0761, policy: 0.1765, weighted policy: 0.1765), Train Mean Max: 0.9283
Epoch 4/10, Train Loss: 0.2451 (value: 0.0014, weighted value: 0.0718, policy: 0.1733, weighted policy: 0.1733), Train Mean Max: 0.9286
Epoch 5/10, Train Loss: 0.2370 (value: 0.0013, weighted value: 0.0667, policy: 0.1704, weighted policy: 0.1704), Train Mean Max: 0.9290
Epoch 6/10, Train Loss: 0.2331 (value: 0.0013, weighted value: 0.0637, policy: 0.1694, weighted policy: 0.1694), Train Mean Max: 0.9292
Epoch 7/10, Train Loss: 0.2316 (value: 0.0012, weighted value: 0.0620, policy: 0.1696, weighted policy: 0.1696), Train Mean Max: 0.9293
Epoch 8/10, Train Loss: 0.2272 (value: 0.0012, weighted value: 0.0591, policy: 0.1681, weighted policy: 0.1681), Train Mean Max: 0.9296
Epoch 9/10, Train Loss: 0.2252 (value: 0.0011, weighted value: 0.0572, policy: 0.1680, weighted policy: 0.1680), Train Mean Max: 0.9297
Epoch 10/10, Train Loss: 0.2225 (value: 0.0011, weighted value: 0.0550, policy: 0.1675, weighted policy: 0.1675), Train Mean Max: 0.9300
..training done in 71.60 seconds
..evaluation done in 16.26 seconds
Old network+MCTS average reward: 0.65, min: -0.18, max: 1.36, stdev: 0.23
New network+MCTS average reward: 0.65, min: -0.23, max: 1.27, stdev: 0.23
Old bare network average reward: 0.58, min: -0.28, max: 1.23, stdev: 0.23
New bare network average reward: 0.59, min: -0.28, max: 1.24, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.35, max: 1.06, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: -0.10, max: 1.34, stdev: 0.24
External policy "total greedy" average reward: 0.66, min: 0.00, max: 1.36, stdev: 0.23
New network won 79 and tied 140 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 129 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.44 seconds
Training examples lengths: [64785, 64487, 64971, 65233, 64930, 64793, 64719, 64626, 64748, 64679]
Total value: 425538.68
Training on 647971 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3128 (value: 0.0023, weighted value: 0.1173, policy: 0.1955, weighted policy: 0.1955), Train Mean Max: 0.9261
Epoch 2/10, Train Loss: 0.2853 (value: 0.0020, weighted value: 0.1003, policy: 0.1850, weighted policy: 0.1850), Train Mean Max: 0.9270
Epoch 3/10, Train Loss: 0.2681 (value: 0.0018, weighted value: 0.0889, policy: 0.1792, weighted policy: 0.1792), Train Mean Max: 0.9274
Epoch 4/10, Train Loss: 0.2580 (value: 0.0016, weighted value: 0.0823, policy: 0.1757, weighted policy: 0.1757), Train Mean Max: 0.9278
Epoch 5/10, Train Loss: 0.2504 (value: 0.0015, weighted value: 0.0764, policy: 0.1740, weighted policy: 0.1740), Train Mean Max: 0.9279
Epoch 6/10, Train Loss: 0.2472 (value: 0.0015, weighted value: 0.0747, policy: 0.1726, weighted policy: 0.1726), Train Mean Max: 0.9282
Epoch 7/10, Train Loss: 0.2407 (value: 0.0014, weighted value: 0.0690, policy: 0.1717, weighted policy: 0.1717), Train Mean Max: 0.9283
Epoch 8/10, Train Loss: 0.2360 (value: 0.0013, weighted value: 0.0653, policy: 0.1707, weighted policy: 0.1707), Train Mean Max: 0.9287
Epoch 9/10, Train Loss: 0.2361 (value: 0.0013, weighted value: 0.0652, policy: 0.1709, weighted policy: 0.1709), Train Mean Max: 0.9287
Epoch 10/10, Train Loss: 0.2312 (value: 0.0012, weighted value: 0.0612, policy: 0.1700, weighted policy: 0.1700), Train Mean Max: 0.9291
..training done in 65.26 seconds
..evaluation done in 16.10 seconds
Old network+MCTS average reward: 0.65, min: 0.10, max: 1.28, stdev: 0.21
New network+MCTS average reward: 0.65, min: 0.11, max: 1.28, stdev: 0.21
Old bare network average reward: 0.58, min: -0.13, max: 1.28, stdev: 0.22
New bare network average reward: 0.59, min: -0.13, max: 1.28, stdev: 0.21
External policy "random" average reward: 0.26, min: -0.35, max: 0.99, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: 0.01, max: 1.22, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.02, max: 1.26, stdev: 0.21
New network won 76 and tied 147 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 130 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.75 seconds
Training examples lengths: [64487, 64971, 65233, 64930, 64793, 64719, 64626, 64748, 64679, 64778]
Total value: 425998.14
Training on 647964 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3421 (value: 0.0028, weighted value: 0.1383, policy: 0.2038, weighted policy: 0.2038), Train Mean Max: 0.9250
Epoch 2/10, Train Loss: 0.3020 (value: 0.0023, weighted value: 0.1129, policy: 0.1891, weighted policy: 0.1891), Train Mean Max: 0.9261
Epoch 3/10, Train Loss: 0.2861 (value: 0.0020, weighted value: 0.1025, policy: 0.1837, weighted policy: 0.1837), Train Mean Max: 0.9264
Epoch 4/10, Train Loss: 0.2707 (value: 0.0018, weighted value: 0.0918, policy: 0.1789, weighted policy: 0.1789), Train Mean Max: 0.9268
Epoch 5/10, Train Loss: 0.2639 (value: 0.0017, weighted value: 0.0864, policy: 0.1775, weighted policy: 0.1775), Train Mean Max: 0.9269
Epoch 6/10, Train Loss: 0.2570 (value: 0.0016, weighted value: 0.0812, policy: 0.1758, weighted policy: 0.1758), Train Mean Max: 0.9272
Epoch 7/10, Train Loss: 0.2505 (value: 0.0015, weighted value: 0.0766, policy: 0.1739, weighted policy: 0.1739), Train Mean Max: 0.9276
Epoch 8/10, Train Loss: 0.2469 (value: 0.0015, weighted value: 0.0727, policy: 0.1742, weighted policy: 0.1742), Train Mean Max: 0.9277
Epoch 9/10, Train Loss: 0.2427 (value: 0.0014, weighted value: 0.0699, policy: 0.1728, weighted policy: 0.1728), Train Mean Max: 0.9280
Epoch 10/10, Train Loss: 0.2372 (value: 0.0013, weighted value: 0.0662, policy: 0.1710, weighted policy: 0.1710), Train Mean Max: 0.9284
..training done in 67.78 seconds
..evaluation done in 16.47 seconds
Old network+MCTS average reward: 0.66, min: 0.07, max: 1.36, stdev: 0.22
New network+MCTS average reward: 0.67, min: 0.07, max: 1.46, stdev: 0.22
Old bare network average reward: 0.60, min: 0.07, max: 1.23, stdev: 0.22
New bare network average reward: 0.60, min: 0.02, max: 1.32, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.24, max: 0.88, stdev: 0.22
External policy "individual greedy" average reward: 0.56, min: 0.00, max: 1.39, stdev: 0.22
External policy "total greedy" average reward: 0.67, min: 0.02, max: 1.43, stdev: 0.22
New network won 102 and tied 114 out of 300 games (53.00% wins where ties are half wins)
Keeping the new network

Training iteration 131 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.95 seconds
Training examples lengths: [64971, 65233, 64930, 64793, 64719, 64626, 64748, 64679, 64778, 64698]
Total value: 426619.98
Training on 648175 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2668 (value: 0.0017, weighted value: 0.0856, policy: 0.1812, weighted policy: 0.1812), Train Mean Max: 0.9269
Epoch 2/10, Train Loss: 0.2541 (value: 0.0015, weighted value: 0.0768, policy: 0.1773, weighted policy: 0.1773), Train Mean Max: 0.9275
Epoch 3/10, Train Loss: 0.2470 (value: 0.0015, weighted value: 0.0731, policy: 0.1739, weighted policy: 0.1739), Train Mean Max: 0.9278
Epoch 4/10, Train Loss: 0.2411 (value: 0.0014, weighted value: 0.0694, policy: 0.1718, weighted policy: 0.1718), Train Mean Max: 0.9281
Epoch 5/10, Train Loss: 0.2367 (value: 0.0013, weighted value: 0.0655, policy: 0.1712, weighted policy: 0.1712), Train Mean Max: 0.9283
Epoch 6/10, Train Loss: 0.2349 (value: 0.0013, weighted value: 0.0638, policy: 0.1711, weighted policy: 0.1711), Train Mean Max: 0.9285
Epoch 7/10, Train Loss: 0.2310 (value: 0.0012, weighted value: 0.0607, policy: 0.1703, weighted policy: 0.1703), Train Mean Max: 0.9288
Epoch 8/10, Train Loss: 0.2295 (value: 0.0012, weighted value: 0.0600, policy: 0.1695, weighted policy: 0.1695), Train Mean Max: 0.9289
Epoch 9/10, Train Loss: 0.2256 (value: 0.0011, weighted value: 0.0567, policy: 0.1689, weighted policy: 0.1689), Train Mean Max: 0.9291
Epoch 10/10, Train Loss: 0.2238 (value: 0.0011, weighted value: 0.0561, policy: 0.1677, weighted policy: 0.1677), Train Mean Max: 0.9295
..training done in 66.96 seconds
..evaluation done in 16.58 seconds
Old network+MCTS average reward: 0.67, min: 0.15, max: 1.33, stdev: 0.22
New network+MCTS average reward: 0.66, min: 0.17, max: 1.33, stdev: 0.23
Old bare network average reward: 0.61, min: 0.07, max: 1.33, stdev: 0.23
New bare network average reward: 0.60, min: 0.03, max: 1.29, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.26, max: 0.97, stdev: 0.21
External policy "individual greedy" average reward: 0.55, min: -0.06, max: 1.19, stdev: 0.22
External policy "total greedy" average reward: 0.67, min: 0.09, max: 1.39, stdev: 0.23
New network won 67 and tied 155 out of 300 games (48.17% wins where ties are half wins)
Reverting to the old network

Training iteration 132 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.59 seconds
Training examples lengths: [65233, 64930, 64793, 64719, 64626, 64748, 64679, 64778, 64698, 64846]
Total value: 426380.75
Training on 648050 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2949 (value: 0.0021, weighted value: 0.1057, policy: 0.1892, weighted policy: 0.1892), Train Mean Max: 0.9254
Epoch 2/10, Train Loss: 0.2762 (value: 0.0018, weighted value: 0.0920, policy: 0.1842, weighted policy: 0.1842), Train Mean Max: 0.9259
Epoch 3/10, Train Loss: 0.2656 (value: 0.0017, weighted value: 0.0852, policy: 0.1803, weighted policy: 0.1803), Train Mean Max: 0.9264
Epoch 4/10, Train Loss: 0.2581 (value: 0.0016, weighted value: 0.0806, policy: 0.1775, weighted policy: 0.1775), Train Mean Max: 0.9266
Epoch 5/10, Train Loss: 0.2490 (value: 0.0015, weighted value: 0.0738, policy: 0.1752, weighted policy: 0.1752), Train Mean Max: 0.9270
Epoch 6/10, Train Loss: 0.2457 (value: 0.0014, weighted value: 0.0714, policy: 0.1743, weighted policy: 0.1743), Train Mean Max: 0.9273
Epoch 7/10, Train Loss: 0.2423 (value: 0.0014, weighted value: 0.0688, policy: 0.1735, weighted policy: 0.1735), Train Mean Max: 0.9275
Epoch 8/10, Train Loss: 0.2386 (value: 0.0013, weighted value: 0.0665, policy: 0.1721, weighted policy: 0.1721), Train Mean Max: 0.9278
Epoch 9/10, Train Loss: 0.2353 (value: 0.0013, weighted value: 0.0640, policy: 0.1714, weighted policy: 0.1714), Train Mean Max: 0.9280
Epoch 10/10, Train Loss: 0.2307 (value: 0.0012, weighted value: 0.0601, policy: 0.1706, weighted policy: 0.1706), Train Mean Max: 0.9282
..training done in 71.10 seconds
..evaluation done in 16.12 seconds
Old network+MCTS average reward: 0.64, min: 0.10, max: 1.35, stdev: 0.21
New network+MCTS average reward: 0.64, min: 0.06, max: 1.29, stdev: 0.21
Old bare network average reward: 0.59, min: 0.06, max: 1.26, stdev: 0.21
New bare network average reward: 0.58, min: 0.02, max: 1.25, stdev: 0.22
External policy "random" average reward: 0.26, min: -0.31, max: 0.81, stdev: 0.21
External policy "individual greedy" average reward: 0.53, min: -0.06, max: 1.23, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.04, max: 1.21, stdev: 0.21
New network won 93 and tied 138 out of 300 games (54.00% wins where ties are half wins)
Keeping the new network

Training iteration 133 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.18 seconds
Training examples lengths: [64930, 64793, 64719, 64626, 64748, 64679, 64778, 64698, 64846, 64423]
Total value: 425628.58
Training on 647240 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2606 (value: 0.0016, weighted value: 0.0800, policy: 0.1806, weighted policy: 0.1806), Train Mean Max: 0.9271
Epoch 2/10, Train Loss: 0.2506 (value: 0.0014, weighted value: 0.0724, policy: 0.1782, weighted policy: 0.1782), Train Mean Max: 0.9273
Epoch 3/10, Train Loss: 0.2453 (value: 0.0014, weighted value: 0.0684, policy: 0.1769, weighted policy: 0.1769), Train Mean Max: 0.9276
Epoch 4/10, Train Loss: 0.2401 (value: 0.0013, weighted value: 0.0653, policy: 0.1748, weighted policy: 0.1748), Train Mean Max: 0.9279
Epoch 5/10, Train Loss: 0.2385 (value: 0.0013, weighted value: 0.0655, policy: 0.1730, weighted policy: 0.1730), Train Mean Max: 0.9279
Epoch 6/10, Train Loss: 0.2331 (value: 0.0012, weighted value: 0.0599, policy: 0.1731, weighted policy: 0.1731), Train Mean Max: 0.9283
Epoch 7/10, Train Loss: 0.2288 (value: 0.0012, weighted value: 0.0580, policy: 0.1708, weighted policy: 0.1708), Train Mean Max: 0.9285
Epoch 8/10, Train Loss: 0.2261 (value: 0.0011, weighted value: 0.0569, policy: 0.1692, weighted policy: 0.1692), Train Mean Max: 0.9288
Epoch 9/10, Train Loss: 0.2268 (value: 0.0011, weighted value: 0.0569, policy: 0.1699, weighted policy: 0.1699), Train Mean Max: 0.9289
Epoch 10/10, Train Loss: 0.2214 (value: 0.0011, weighted value: 0.0536, policy: 0.1678, weighted policy: 0.1678), Train Mean Max: 0.9295
..training done in 70.73 seconds
..evaluation done in 16.48 seconds
Old network+MCTS average reward: 0.65, min: -0.03, max: 1.50, stdev: 0.23
New network+MCTS average reward: 0.65, min: -0.05, max: 1.54, stdev: 0.23
Old bare network average reward: 0.60, min: -0.07, max: 1.50, stdev: 0.23
New bare network average reward: 0.59, min: -0.25, max: 1.54, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.44, max: 0.89, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: -0.03, max: 1.44, stdev: 0.24
External policy "total greedy" average reward: 0.65, min: 0.09, max: 1.56, stdev: 0.23
New network won 82 and tied 132 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 134 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.21 seconds
Training examples lengths: [64793, 64719, 64626, 64748, 64679, 64778, 64698, 64846, 64423, 64831]
Total value: 426873.58
Training on 647141 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2894 (value: 0.0020, weighted value: 0.0999, policy: 0.1894, weighted policy: 0.1894), Train Mean Max: 0.9255
Epoch 2/10, Train Loss: 0.2709 (value: 0.0018, weighted value: 0.0882, policy: 0.1827, weighted policy: 0.1827), Train Mean Max: 0.9261
Epoch 3/10, Train Loss: 0.2587 (value: 0.0016, weighted value: 0.0803, policy: 0.1784, weighted policy: 0.1784), Train Mean Max: 0.9266
Epoch 4/10, Train Loss: 0.2522 (value: 0.0015, weighted value: 0.0757, policy: 0.1765, weighted policy: 0.1765), Train Mean Max: 0.9269
Epoch 5/10, Train Loss: 0.2459 (value: 0.0014, weighted value: 0.0713, policy: 0.1747, weighted policy: 0.1747), Train Mean Max: 0.9272
Epoch 6/10, Train Loss: 0.2425 (value: 0.0014, weighted value: 0.0691, policy: 0.1734, weighted policy: 0.1734), Train Mean Max: 0.9275
Epoch 7/10, Train Loss: 0.2374 (value: 0.0013, weighted value: 0.0654, policy: 0.1720, weighted policy: 0.1720), Train Mean Max: 0.9277
Epoch 8/10, Train Loss: 0.2349 (value: 0.0013, weighted value: 0.0635, policy: 0.1714, weighted policy: 0.1714), Train Mean Max: 0.9280
Epoch 9/10, Train Loss: 0.2321 (value: 0.0012, weighted value: 0.0607, policy: 0.1715, weighted policy: 0.1715), Train Mean Max: 0.9282
Epoch 10/10, Train Loss: 0.2284 (value: 0.0012, weighted value: 0.0590, policy: 0.1694, weighted policy: 0.1694), Train Mean Max: 0.9285
..training done in 69.89 seconds
..evaluation done in 16.72 seconds
Old network+MCTS average reward: 0.65, min: 0.17, max: 1.31, stdev: 0.22
New network+MCTS average reward: 0.65, min: 0.10, max: 1.31, stdev: 0.21
Old bare network average reward: 0.59, min: 0.03, max: 1.31, stdev: 0.22
New bare network average reward: 0.59, min: 0.01, max: 1.27, stdev: 0.22
External policy "random" average reward: 0.26, min: -0.29, max: 0.80, stdev: 0.21
External policy "individual greedy" average reward: 0.53, min: -0.07, max: 1.19, stdev: 0.21
External policy "total greedy" average reward: 0.65, min: 0.15, max: 1.34, stdev: 0.22
New network won 97 and tied 138 out of 300 games (55.33% wins where ties are half wins)
Keeping the new network

Training iteration 135 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.87 seconds
Training examples lengths: [64719, 64626, 64748, 64679, 64778, 64698, 64846, 64423, 64831, 64729]
Total value: 427330.44
Training on 647077 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2578 (value: 0.0016, weighted value: 0.0793, policy: 0.1785, weighted policy: 0.1785), Train Mean Max: 0.9272
Epoch 2/10, Train Loss: 0.2460 (value: 0.0014, weighted value: 0.0713, policy: 0.1746, weighted policy: 0.1746), Train Mean Max: 0.9278
Epoch 3/10, Train Loss: 0.2408 (value: 0.0014, weighted value: 0.0681, policy: 0.1727, weighted policy: 0.1727), Train Mean Max: 0.9280
Epoch 4/10, Train Loss: 0.2343 (value: 0.0013, weighted value: 0.0627, policy: 0.1716, weighted policy: 0.1716), Train Mean Max: 0.9283
Epoch 5/10, Train Loss: 0.2340 (value: 0.0013, weighted value: 0.0629, policy: 0.1711, weighted policy: 0.1711), Train Mean Max: 0.9284
Epoch 6/10, Train Loss: 0.2284 (value: 0.0012, weighted value: 0.0595, policy: 0.1690, weighted policy: 0.1690), Train Mean Max: 0.9288
Epoch 7/10, Train Loss: 0.2279 (value: 0.0012, weighted value: 0.0593, policy: 0.1687, weighted policy: 0.1687), Train Mean Max: 0.9289
Epoch 8/10, Train Loss: 0.2235 (value: 0.0011, weighted value: 0.0554, policy: 0.1681, weighted policy: 0.1681), Train Mean Max: 0.9291
Epoch 9/10, Train Loss: 0.2207 (value: 0.0011, weighted value: 0.0531, policy: 0.1676, weighted policy: 0.1676), Train Mean Max: 0.9293
Epoch 10/10, Train Loss: 0.2195 (value: 0.0010, weighted value: 0.0519, policy: 0.1676, weighted policy: 0.1676), Train Mean Max: 0.9296
..training done in 66.23 seconds
..evaluation done in 17.25 seconds
Old network+MCTS average reward: 0.65, min: -0.05, max: 1.24, stdev: 0.24
New network+MCTS average reward: 0.65, min: -0.11, max: 1.31, stdev: 0.24
Old bare network average reward: 0.59, min: -0.06, max: 1.24, stdev: 0.24
New bare network average reward: 0.59, min: -0.07, max: 1.21, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.26, max: 0.84, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: -0.03, max: 1.17, stdev: 0.24
External policy "total greedy" average reward: 0.65, min: 0.04, max: 1.31, stdev: 0.23
New network won 91 and tied 129 out of 300 games (51.83% wins where ties are half wins)
Keeping the new network

Training iteration 136 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.70 seconds
Training examples lengths: [64626, 64748, 64679, 64778, 64698, 64846, 64423, 64831, 64729, 64879]
Total value: 427609.48
Training on 647237 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2528 (value: 0.0015, weighted value: 0.0748, policy: 0.1780, weighted policy: 0.1780), Train Mean Max: 0.9284
Epoch 2/10, Train Loss: 0.2431 (value: 0.0013, weighted value: 0.0669, policy: 0.1762, weighted policy: 0.1762), Train Mean Max: 0.9287
Epoch 3/10, Train Loss: 0.2347 (value: 0.0012, weighted value: 0.0620, policy: 0.1726, weighted policy: 0.1726), Train Mean Max: 0.9291
Epoch 4/10, Train Loss: 0.2320 (value: 0.0012, weighted value: 0.0608, policy: 0.1712, weighted policy: 0.1712), Train Mean Max: 0.9293
Epoch 5/10, Train Loss: 0.2281 (value: 0.0012, weighted value: 0.0578, policy: 0.1703, weighted policy: 0.1703), Train Mean Max: 0.9295
Epoch 6/10, Train Loss: 0.2259 (value: 0.0011, weighted value: 0.0567, policy: 0.1692, weighted policy: 0.1692), Train Mean Max: 0.9298
Epoch 7/10, Train Loss: 0.2222 (value: 0.0011, weighted value: 0.0550, policy: 0.1673, weighted policy: 0.1673), Train Mean Max: 0.9300
Epoch 8/10, Train Loss: 0.2203 (value: 0.0010, weighted value: 0.0522, policy: 0.1681, weighted policy: 0.1681), Train Mean Max: 0.9302
Epoch 9/10, Train Loss: 0.2170 (value: 0.0010, weighted value: 0.0503, policy: 0.1667, weighted policy: 0.1667), Train Mean Max: 0.9303
Epoch 10/10, Train Loss: 0.2192 (value: 0.0010, weighted value: 0.0504, policy: 0.1687, weighted policy: 0.1687), Train Mean Max: 0.9304
..training done in 70.67 seconds
..evaluation done in 16.12 seconds
Old network+MCTS average reward: 0.66, min: 0.03, max: 1.28, stdev: 0.21
New network+MCTS average reward: 0.66, min: 0.09, max: 1.28, stdev: 0.21
Old bare network average reward: 0.60, min: -0.01, max: 1.22, stdev: 0.21
New bare network average reward: 0.60, min: -0.01, max: 1.28, stdev: 0.22
External policy "random" average reward: 0.26, min: -0.37, max: 0.89, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.02, max: 1.25, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.12, max: 1.26, stdev: 0.21
New network won 64 and tied 146 out of 300 games (45.67% wins where ties are half wins)
Reverting to the old network

Training iteration 137 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.49 seconds
Training examples lengths: [64748, 64679, 64778, 64698, 64846, 64423, 64831, 64729, 64879, 64845]
Total value: 427623.42
Training on 647456 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2846 (value: 0.0019, weighted value: 0.0963, policy: 0.1882, weighted policy: 0.1882), Train Mean Max: 0.9270
Epoch 2/10, Train Loss: 0.2628 (value: 0.0016, weighted value: 0.0825, policy: 0.1804, weighted policy: 0.1804), Train Mean Max: 0.9278
Epoch 3/10, Train Loss: 0.2520 (value: 0.0015, weighted value: 0.0771, policy: 0.1749, weighted policy: 0.1749), Train Mean Max: 0.9283
Epoch 4/10, Train Loss: 0.2425 (value: 0.0014, weighted value: 0.0700, policy: 0.1725, weighted policy: 0.1725), Train Mean Max: 0.9285
Epoch 5/10, Train Loss: 0.2395 (value: 0.0014, weighted value: 0.0675, policy: 0.1720, weighted policy: 0.1720), Train Mean Max: 0.9287
Epoch 6/10, Train Loss: 0.2359 (value: 0.0013, weighted value: 0.0655, policy: 0.1704, weighted policy: 0.1704), Train Mean Max: 0.9290
Epoch 7/10, Train Loss: 0.2293 (value: 0.0012, weighted value: 0.0603, policy: 0.1690, weighted policy: 0.1690), Train Mean Max: 0.9294
Epoch 8/10, Train Loss: 0.2287 (value: 0.0012, weighted value: 0.0597, policy: 0.1690, weighted policy: 0.1690), Train Mean Max: 0.9294
Epoch 9/10, Train Loss: 0.2262 (value: 0.0012, weighted value: 0.0576, policy: 0.1687, weighted policy: 0.1687), Train Mean Max: 0.9295
Epoch 10/10, Train Loss: 0.2223 (value: 0.0011, weighted value: 0.0557, policy: 0.1666, weighted policy: 0.1666), Train Mean Max: 0.9298
..training done in 71.56 seconds
..evaluation done in 16.87 seconds
Old network+MCTS average reward: 0.63, min: 0.08, max: 1.39, stdev: 0.24
New network+MCTS average reward: 0.64, min: 0.06, max: 1.34, stdev: 0.24
Old bare network average reward: 0.58, min: 0.01, max: 1.39, stdev: 0.25
New bare network average reward: 0.58, min: -0.06, max: 1.34, stdev: 0.25
External policy "random" average reward: 0.24, min: -0.39, max: 0.86, stdev: 0.22
External policy "individual greedy" average reward: 0.51, min: -0.07, max: 1.18, stdev: 0.23
External policy "total greedy" average reward: 0.63, min: 0.09, max: 1.33, stdev: 0.23
New network won 81 and tied 141 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 138 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.23 seconds
Training examples lengths: [64679, 64778, 64698, 64846, 64423, 64831, 64729, 64879, 64845, 64603]
Total value: 427840.28
Training on 647311 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2546 (value: 0.0015, weighted value: 0.0767, policy: 0.1779, weighted policy: 0.1779), Train Mean Max: 0.9283
Epoch 2/10, Train Loss: 0.2425 (value: 0.0014, weighted value: 0.0684, policy: 0.1741, weighted policy: 0.1741), Train Mean Max: 0.9289
Epoch 3/10, Train Loss: 0.2353 (value: 0.0013, weighted value: 0.0640, policy: 0.1713, weighted policy: 0.1713), Train Mean Max: 0.9292
Epoch 4/10, Train Loss: 0.2308 (value: 0.0012, weighted value: 0.0617, policy: 0.1691, weighted policy: 0.1691), Train Mean Max: 0.9295
Epoch 5/10, Train Loss: 0.2277 (value: 0.0012, weighted value: 0.0585, policy: 0.1692, weighted policy: 0.1692), Train Mean Max: 0.9296
Epoch 6/10, Train Loss: 0.2249 (value: 0.0011, weighted value: 0.0571, policy: 0.1678, weighted policy: 0.1678), Train Mean Max: 0.9299
Epoch 7/10, Train Loss: 0.2237 (value: 0.0011, weighted value: 0.0560, policy: 0.1677, weighted policy: 0.1677), Train Mean Max: 0.9300
Epoch 8/10, Train Loss: 0.2206 (value: 0.0011, weighted value: 0.0536, policy: 0.1670, weighted policy: 0.1670), Train Mean Max: 0.9301
Epoch 9/10, Train Loss: 0.2189 (value: 0.0010, weighted value: 0.0512, policy: 0.1677, weighted policy: 0.1677), Train Mean Max: 0.9302
Epoch 10/10, Train Loss: 0.2159 (value: 0.0010, weighted value: 0.0503, policy: 0.1656, weighted policy: 0.1656), Train Mean Max: 0.9307
..training done in 66.27 seconds
..evaluation done in 16.55 seconds
Old network+MCTS average reward: 0.67, min: 0.15, max: 1.44, stdev: 0.22
New network+MCTS average reward: 0.67, min: 0.18, max: 1.49, stdev: 0.22
Old bare network average reward: 0.61, min: 0.03, max: 1.49, stdev: 0.23
New bare network average reward: 0.61, min: 0.02, max: 1.42, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.27, max: 0.83, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: 0.02, max: 1.19, stdev: 0.21
External policy "total greedy" average reward: 0.67, min: 0.13, max: 1.31, stdev: 0.21
New network won 110 and tied 116 out of 300 games (56.00% wins where ties are half wins)
Keeping the new network

Training iteration 139 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.86 seconds
Training examples lengths: [64778, 64698, 64846, 64423, 64831, 64729, 64879, 64845, 64603, 64759]
Total value: 427939.66
Training on 647391 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2494 (value: 0.0015, weighted value: 0.0735, policy: 0.1760, weighted policy: 0.1760), Train Mean Max: 0.9291
Epoch 2/10, Train Loss: 0.2348 (value: 0.0012, weighted value: 0.0619, policy: 0.1729, weighted policy: 0.1729), Train Mean Max: 0.9297
Epoch 3/10, Train Loss: 0.2309 (value: 0.0012, weighted value: 0.0609, policy: 0.1701, weighted policy: 0.1701), Train Mean Max: 0.9301
Epoch 4/10, Train Loss: 0.2269 (value: 0.0012, weighted value: 0.0585, policy: 0.1683, weighted policy: 0.1683), Train Mean Max: 0.9301
Epoch 5/10, Train Loss: 0.2207 (value: 0.0011, weighted value: 0.0540, policy: 0.1667, weighted policy: 0.1667), Train Mean Max: 0.9304
Epoch 6/10, Train Loss: 0.2207 (value: 0.0011, weighted value: 0.0540, policy: 0.1667, weighted policy: 0.1667), Train Mean Max: 0.9304
Epoch 7/10, Train Loss: 0.2197 (value: 0.0010, weighted value: 0.0524, policy: 0.1673, weighted policy: 0.1673), Train Mean Max: 0.9306
Epoch 8/10, Train Loss: 0.2143 (value: 0.0010, weighted value: 0.0489, policy: 0.1654, weighted policy: 0.1654), Train Mean Max: 0.9310
Epoch 9/10, Train Loss: 0.2163 (value: 0.0010, weighted value: 0.0507, policy: 0.1655, weighted policy: 0.1655), Train Mean Max: 0.9310
Epoch 10/10, Train Loss: 0.2107 (value: 0.0009, weighted value: 0.0472, policy: 0.1635, weighted policy: 0.1635), Train Mean Max: 0.9314
..training done in 69.06 seconds
..evaluation done in 16.34 seconds
Old network+MCTS average reward: 0.66, min: 0.05, max: 1.57, stdev: 0.23
New network+MCTS average reward: 0.66, min: 0.00, max: 1.57, stdev: 0.23
Old bare network average reward: 0.61, min: -0.22, max: 1.47, stdev: 0.24
New bare network average reward: 0.60, min: -0.22, max: 1.57, stdev: 0.24
External policy "random" average reward: 0.28, min: -0.32, max: 1.06, stdev: 0.24
External policy "individual greedy" average reward: 0.55, min: -0.04, max: 1.34, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.03, max: 1.45, stdev: 0.22
New network won 80 and tied 134 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 140 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.73 seconds
Training examples lengths: [64698, 64846, 64423, 64831, 64729, 64879, 64845, 64603, 64759, 64781]
Total value: 428328.30
Training on 647394 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2784 (value: 0.0018, weighted value: 0.0925, policy: 0.1859, weighted policy: 0.1859), Train Mean Max: 0.9279
Epoch 2/10, Train Loss: 0.2572 (value: 0.0016, weighted value: 0.0788, policy: 0.1784, weighted policy: 0.1784), Train Mean Max: 0.9286
Epoch 3/10, Train Loss: 0.2480 (value: 0.0015, weighted value: 0.0733, policy: 0.1748, weighted policy: 0.1748), Train Mean Max: 0.9288
Epoch 4/10, Train Loss: 0.2404 (value: 0.0014, weighted value: 0.0681, policy: 0.1723, weighted policy: 0.1723), Train Mean Max: 0.9291
Epoch 5/10, Train Loss: 0.2351 (value: 0.0013, weighted value: 0.0644, policy: 0.1707, weighted policy: 0.1707), Train Mean Max: 0.9293
Epoch 6/10, Train Loss: 0.2339 (value: 0.0013, weighted value: 0.0643, policy: 0.1695, weighted policy: 0.1695), Train Mean Max: 0.9296
Epoch 7/10, Train Loss: 0.2276 (value: 0.0012, weighted value: 0.0593, policy: 0.1684, weighted policy: 0.1684), Train Mean Max: 0.9298
Epoch 8/10, Train Loss: 0.2240 (value: 0.0011, weighted value: 0.0565, policy: 0.1675, weighted policy: 0.1675), Train Mean Max: 0.9300
Epoch 9/10, Train Loss: 0.2227 (value: 0.0011, weighted value: 0.0564, policy: 0.1663, weighted policy: 0.1663), Train Mean Max: 0.9304
Epoch 10/10, Train Loss: 0.2182 (value: 0.0010, weighted value: 0.0524, policy: 0.1659, weighted policy: 0.1659), Train Mean Max: 0.9305
..training done in 66.50 seconds
..evaluation done in 16.58 seconds
Old network+MCTS average reward: 0.66, min: -0.07, max: 1.23, stdev: 0.22
New network+MCTS average reward: 0.66, min: -0.07, max: 1.19, stdev: 0.22
Old bare network average reward: 0.60, min: -0.07, max: 1.19, stdev: 0.23
New bare network average reward: 0.60, min: -0.16, max: 1.23, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.42, max: 0.91, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: -0.04, max: 1.18, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: -0.12, max: 1.17, stdev: 0.22
New network won 90 and tied 127 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to zgaz_checkpoints/1757654225_iter_140

Training iteration 141 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.63 seconds
Training examples lengths: [64846, 64423, 64831, 64729, 64879, 64845, 64603, 64759, 64781, 64832]
Total value: 428989.38
Training on 647528 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2519 (value: 0.0015, weighted value: 0.0754, policy: 0.1765, weighted policy: 0.1765), Train Mean Max: 0.9291
Epoch 2/10, Train Loss: 0.2402 (value: 0.0014, weighted value: 0.0678, policy: 0.1724, weighted policy: 0.1724), Train Mean Max: 0.9296
Epoch 3/10, Train Loss: 0.2341 (value: 0.0013, weighted value: 0.0635, policy: 0.1705, weighted policy: 0.1705), Train Mean Max: 0.9299
Epoch 4/10, Train Loss: 0.2287 (value: 0.0012, weighted value: 0.0601, policy: 0.1687, weighted policy: 0.1687), Train Mean Max: 0.9301
Epoch 5/10, Train Loss: 0.2250 (value: 0.0012, weighted value: 0.0580, policy: 0.1670, weighted policy: 0.1670), Train Mean Max: 0.9305
Epoch 6/10, Train Loss: 0.2233 (value: 0.0011, weighted value: 0.0563, policy: 0.1670, weighted policy: 0.1670), Train Mean Max: 0.9305
Epoch 7/10, Train Loss: 0.2205 (value: 0.0011, weighted value: 0.0549, policy: 0.1657, weighted policy: 0.1657), Train Mean Max: 0.9309
Epoch 8/10, Train Loss: 0.2157 (value: 0.0010, weighted value: 0.0517, policy: 0.1640, weighted policy: 0.1640), Train Mean Max: 0.9311
Epoch 9/10, Train Loss: 0.2165 (value: 0.0010, weighted value: 0.0514, policy: 0.1651, weighted policy: 0.1651), Train Mean Max: 0.9310
Epoch 10/10, Train Loss: 0.2121 (value: 0.0010, weighted value: 0.0489, policy: 0.1632, weighted policy: 0.1632), Train Mean Max: 0.9314
..training done in 66.58 seconds
..evaluation done in 16.07 seconds
Old network+MCTS average reward: 0.66, min: 0.06, max: 1.41, stdev: 0.23
New network+MCTS average reward: 0.66, min: -0.06, max: 1.41, stdev: 0.23
Old bare network average reward: 0.59, min: -0.06, max: 1.37, stdev: 0.23
New bare network average reward: 0.60, min: -0.01, max: 1.41, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.36, max: 0.92, stdev: 0.21
External policy "individual greedy" average reward: 0.53, min: 0.02, max: 1.34, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.09, max: 1.36, stdev: 0.21
New network won 88 and tied 135 out of 300 games (51.83% wins where ties are half wins)
Keeping the new network

Training iteration 142 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.49 seconds
Training examples lengths: [64423, 64831, 64729, 64879, 64845, 64603, 64759, 64781, 64832, 64857]
Total value: 429775.37
Training on 647539 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2463 (value: 0.0014, weighted value: 0.0716, policy: 0.1747, weighted policy: 0.1747), Train Mean Max: 0.9298
Epoch 2/10, Train Loss: 0.2325 (value: 0.0013, weighted value: 0.0626, policy: 0.1700, weighted policy: 0.1700), Train Mean Max: 0.9305
Epoch 3/10, Train Loss: 0.2293 (value: 0.0012, weighted value: 0.0608, policy: 0.1684, weighted policy: 0.1684), Train Mean Max: 0.9307
Epoch 4/10, Train Loss: 0.2232 (value: 0.0011, weighted value: 0.0574, policy: 0.1658, weighted policy: 0.1658), Train Mean Max: 0.9310
Epoch 5/10, Train Loss: 0.2201 (value: 0.0011, weighted value: 0.0553, policy: 0.1648, weighted policy: 0.1648), Train Mean Max: 0.9311
Epoch 6/10, Train Loss: 0.2171 (value: 0.0011, weighted value: 0.0527, policy: 0.1644, weighted policy: 0.1644), Train Mean Max: 0.9313
Epoch 7/10, Train Loss: 0.2149 (value: 0.0010, weighted value: 0.0513, policy: 0.1635, weighted policy: 0.1635), Train Mean Max: 0.9315
Epoch 8/10, Train Loss: 0.2130 (value: 0.0010, weighted value: 0.0499, policy: 0.1631, weighted policy: 0.1631), Train Mean Max: 0.9318
Epoch 9/10, Train Loss: 0.2117 (value: 0.0010, weighted value: 0.0490, policy: 0.1627, weighted policy: 0.1627), Train Mean Max: 0.9320
Epoch 10/10, Train Loss: 0.2102 (value: 0.0010, weighted value: 0.0477, policy: 0.1625, weighted policy: 0.1625), Train Mean Max: 0.9320
..training done in 71.87 seconds
..evaluation done in 16.11 seconds
Old network+MCTS average reward: 0.64, min: 0.10, max: 1.20, stdev: 0.21
New network+MCTS average reward: 0.64, min: 0.13, max: 1.20, stdev: 0.20
Old bare network average reward: 0.58, min: 0.06, max: 1.20, stdev: 0.22
New bare network average reward: 0.59, min: 0.13, max: 1.20, stdev: 0.21
External policy "random" average reward: 0.26, min: -0.38, max: 1.06, stdev: 0.23
External policy "individual greedy" average reward: 0.52, min: 0.02, max: 1.17, stdev: 0.21
External policy "total greedy" average reward: 0.63, min: 0.20, max: 1.33, stdev: 0.21
New network won 87 and tied 119 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 143 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.32 seconds
Training examples lengths: [64831, 64729, 64879, 64845, 64603, 64759, 64781, 64832, 64857, 64545]
Total value: 430558.19
Training on 647661 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2754 (value: 0.0018, weighted value: 0.0920, policy: 0.1834, weighted policy: 0.1834), Train Mean Max: 0.9285
Epoch 2/10, Train Loss: 0.2553 (value: 0.0016, weighted value: 0.0784, policy: 0.1769, weighted policy: 0.1769), Train Mean Max: 0.9292
Epoch 3/10, Train Loss: 0.2453 (value: 0.0014, weighted value: 0.0722, policy: 0.1731, weighted policy: 0.1731), Train Mean Max: 0.9298
Epoch 4/10, Train Loss: 0.2373 (value: 0.0014, weighted value: 0.0676, policy: 0.1697, weighted policy: 0.1697), Train Mean Max: 0.9302
Epoch 5/10, Train Loss: 0.2344 (value: 0.0013, weighted value: 0.0663, policy: 0.1681, weighted policy: 0.1681), Train Mean Max: 0.9303
Epoch 6/10, Train Loss: 0.2294 (value: 0.0012, weighted value: 0.0623, policy: 0.1671, weighted policy: 0.1671), Train Mean Max: 0.9305
Epoch 7/10, Train Loss: 0.2229 (value: 0.0011, weighted value: 0.0575, policy: 0.1655, weighted policy: 0.1655), Train Mean Max: 0.9307
Epoch 8/10, Train Loss: 0.2222 (value: 0.0011, weighted value: 0.0573, policy: 0.1649, weighted policy: 0.1649), Train Mean Max: 0.9309
Epoch 9/10, Train Loss: 0.2198 (value: 0.0011, weighted value: 0.0551, policy: 0.1647, weighted policy: 0.1647), Train Mean Max: 0.9312
Epoch 10/10, Train Loss: 0.2174 (value: 0.0011, weighted value: 0.0535, policy: 0.1638, weighted policy: 0.1638), Train Mean Max: 0.9314
..training done in 70.77 seconds
..evaluation done in 16.03 seconds
Old network+MCTS average reward: 0.66, min: 0.07, max: 1.34, stdev: 0.22
New network+MCTS average reward: 0.66, min: 0.08, max: 1.31, stdev: 0.22
Old bare network average reward: 0.61, min: -0.11, max: 1.34, stdev: 0.23
New bare network average reward: 0.61, min: -0.11, max: 1.21, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.32, max: 0.93, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: -0.02, max: 1.09, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.12, max: 1.27, stdev: 0.22
New network won 91 and tied 140 out of 300 games (53.67% wins where ties are half wins)
Keeping the new network

Training iteration 144 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.00 seconds
Training examples lengths: [64729, 64879, 64845, 64603, 64759, 64781, 64832, 64857, 64545, 64636]
Total value: 430226.51
Training on 647466 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2484 (value: 0.0015, weighted value: 0.0748, policy: 0.1735, weighted policy: 0.1735), Train Mean Max: 0.9299
Epoch 2/10, Train Loss: 0.2374 (value: 0.0013, weighted value: 0.0666, policy: 0.1708, weighted policy: 0.1708), Train Mean Max: 0.9302
Epoch 3/10, Train Loss: 0.2292 (value: 0.0013, weighted value: 0.0627, policy: 0.1666, weighted policy: 0.1666), Train Mean Max: 0.9308
Epoch 4/10, Train Loss: 0.2274 (value: 0.0012, weighted value: 0.0609, policy: 0.1665, weighted policy: 0.1665), Train Mean Max: 0.9308
Epoch 5/10, Train Loss: 0.2222 (value: 0.0011, weighted value: 0.0574, policy: 0.1648, weighted policy: 0.1648), Train Mean Max: 0.9311
Epoch 6/10, Train Loss: 0.2197 (value: 0.0011, weighted value: 0.0559, policy: 0.1638, weighted policy: 0.1638), Train Mean Max: 0.9314
Epoch 7/10, Train Loss: 0.2160 (value: 0.0011, weighted value: 0.0529, policy: 0.1631, weighted policy: 0.1631), Train Mean Max: 0.9316
Epoch 8/10, Train Loss: 0.2160 (value: 0.0011, weighted value: 0.0529, policy: 0.1631, weighted policy: 0.1631), Train Mean Max: 0.9318
Epoch 9/10, Train Loss: 0.2127 (value: 0.0010, weighted value: 0.0502, policy: 0.1625, weighted policy: 0.1625), Train Mean Max: 0.9319
Epoch 10/10, Train Loss: 0.2108 (value: 0.0010, weighted value: 0.0493, policy: 0.1615, weighted policy: 0.1615), Train Mean Max: 0.9322
..training done in 68.43 seconds
..evaluation done in 16.13 seconds
Old network+MCTS average reward: 0.65, min: 0.14, max: 1.42, stdev: 0.22
New network+MCTS average reward: 0.65, min: 0.07, max: 1.37, stdev: 0.23
Old bare network average reward: 0.60, min: -0.06, max: 1.37, stdev: 0.23
New bare network average reward: 0.60, min: 0.01, max: 1.43, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.32, max: 1.00, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: 0.06, max: 1.38, stdev: 0.21
External policy "total greedy" average reward: 0.66, min: 0.16, max: 1.37, stdev: 0.21
New network won 80 and tied 145 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 145 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.59 seconds
Training examples lengths: [64879, 64845, 64603, 64759, 64781, 64832, 64857, 64545, 64636, 65016]
Total value: 430755.63
Training on 647753 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2436 (value: 0.0014, weighted value: 0.0718, policy: 0.1719, weighted policy: 0.1719), Train Mean Max: 0.9305
Epoch 2/10, Train Loss: 0.2309 (value: 0.0012, weighted value: 0.0624, policy: 0.1685, weighted policy: 0.1685), Train Mean Max: 0.9310
Epoch 3/10, Train Loss: 0.2263 (value: 0.0012, weighted value: 0.0598, policy: 0.1665, weighted policy: 0.1665), Train Mean Max: 0.9312
Epoch 4/10, Train Loss: 0.2214 (value: 0.0011, weighted value: 0.0573, policy: 0.1641, weighted policy: 0.1641), Train Mean Max: 0.9318
Epoch 5/10, Train Loss: 0.2171 (value: 0.0011, weighted value: 0.0538, policy: 0.1633, weighted policy: 0.1633), Train Mean Max: 0.9320
Epoch 6/10, Train Loss: 0.2172 (value: 0.0011, weighted value: 0.0545, policy: 0.1626, weighted policy: 0.1626), Train Mean Max: 0.9321
Epoch 7/10, Train Loss: 0.2123 (value: 0.0010, weighted value: 0.0505, policy: 0.1618, weighted policy: 0.1618), Train Mean Max: 0.9323
Epoch 8/10, Train Loss: 0.2123 (value: 0.0010, weighted value: 0.0507, policy: 0.1616, weighted policy: 0.1616), Train Mean Max: 0.9324
Epoch 9/10, Train Loss: 0.2092 (value: 0.0010, weighted value: 0.0480, policy: 0.1612, weighted policy: 0.1612), Train Mean Max: 0.9327
Epoch 10/10, Train Loss: 0.2087 (value: 0.0010, weighted value: 0.0477, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9328
..training done in 71.47 seconds
..evaluation done in 15.87 seconds
Old network+MCTS average reward: 0.65, min: 0.12, max: 1.45, stdev: 0.24
New network+MCTS average reward: 0.64, min: 0.12, max: 1.46, stdev: 0.24
Old bare network average reward: 0.59, min: 0.00, max: 1.40, stdev: 0.24
New bare network average reward: 0.58, min: 0.05, max: 1.38, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.38, max: 1.02, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: -0.02, max: 1.31, stdev: 0.24
External policy "total greedy" average reward: 0.66, min: 0.07, max: 1.53, stdev: 0.23
New network won 79 and tied 134 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 146 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.85 seconds
Training examples lengths: [64845, 64603, 64759, 64781, 64832, 64857, 64545, 64636, 65016, 64683]
Total value: 430727.14
Training on 647557 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2756 (value: 0.0019, weighted value: 0.0926, policy: 0.1830, weighted policy: 0.1830), Train Mean Max: 0.9288
Epoch 2/10, Train Loss: 0.2551 (value: 0.0016, weighted value: 0.0789, policy: 0.1762, weighted policy: 0.1762), Train Mean Max: 0.9297
Epoch 3/10, Train Loss: 0.2447 (value: 0.0015, weighted value: 0.0728, policy: 0.1719, weighted policy: 0.1719), Train Mean Max: 0.9301
Epoch 4/10, Train Loss: 0.2357 (value: 0.0014, weighted value: 0.0675, policy: 0.1682, weighted policy: 0.1682), Train Mean Max: 0.9305
Epoch 5/10, Train Loss: 0.2329 (value: 0.0013, weighted value: 0.0655, policy: 0.1674, weighted policy: 0.1674), Train Mean Max: 0.9306
Epoch 6/10, Train Loss: 0.2274 (value: 0.0012, weighted value: 0.0609, policy: 0.1665, weighted policy: 0.1665), Train Mean Max: 0.9308
Epoch 7/10, Train Loss: 0.2256 (value: 0.0012, weighted value: 0.0606, policy: 0.1651, weighted policy: 0.1651), Train Mean Max: 0.9312
Epoch 8/10, Train Loss: 0.2216 (value: 0.0011, weighted value: 0.0574, policy: 0.1643, weighted policy: 0.1643), Train Mean Max: 0.9312
Epoch 9/10, Train Loss: 0.2181 (value: 0.0011, weighted value: 0.0546, policy: 0.1635, weighted policy: 0.1635), Train Mean Max: 0.9315
Epoch 10/10, Train Loss: 0.2182 (value: 0.0011, weighted value: 0.0545, policy: 0.1638, weighted policy: 0.1638), Train Mean Max: 0.9317
..training done in 69.81 seconds
..evaluation done in 16.24 seconds
Old network+MCTS average reward: 0.66, min: 0.02, max: 1.38, stdev: 0.22
New network+MCTS average reward: 0.65, min: 0.06, max: 1.38, stdev: 0.23
Old bare network average reward: 0.59, min: -0.15, max: 1.38, stdev: 0.24
New bare network average reward: 0.60, min: 0.02, max: 1.31, stdev: 0.24
External policy "random" average reward: 0.24, min: -0.30, max: 0.94, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: 0.03, max: 1.28, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.16, max: 1.37, stdev: 0.22
New network won 75 and tied 135 out of 300 games (47.50% wins where ties are half wins)
Reverting to the old network

Training iteration 147 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.27 seconds
Training examples lengths: [64603, 64759, 64781, 64832, 64857, 64545, 64636, 65016, 64683, 64697]
Total value: 430914.85
Training on 647409 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3056 (value: 0.0023, weighted value: 0.1141, policy: 0.1915, weighted policy: 0.1915), Train Mean Max: 0.9273
Epoch 2/10, Train Loss: 0.2741 (value: 0.0018, weighted value: 0.0921, policy: 0.1820, weighted policy: 0.1820), Train Mean Max: 0.9285
Epoch 3/10, Train Loss: 0.2594 (value: 0.0017, weighted value: 0.0842, policy: 0.1752, weighted policy: 0.1752), Train Mean Max: 0.9289
Epoch 4/10, Train Loss: 0.2521 (value: 0.0016, weighted value: 0.0784, policy: 0.1737, weighted policy: 0.1737), Train Mean Max: 0.9292
Epoch 5/10, Train Loss: 0.2478 (value: 0.0016, weighted value: 0.0778, policy: 0.1700, weighted policy: 0.1700), Train Mean Max: 0.9294
Epoch 6/10, Train Loss: 0.2390 (value: 0.0014, weighted value: 0.0698, policy: 0.1693, weighted policy: 0.1693), Train Mean Max: 0.9297
Epoch 7/10, Train Loss: 0.2335 (value: 0.0013, weighted value: 0.0658, policy: 0.1677, weighted policy: 0.1677), Train Mean Max: 0.9300
Epoch 8/10, Train Loss: 0.2317 (value: 0.0013, weighted value: 0.0643, policy: 0.1675, weighted policy: 0.1675), Train Mean Max: 0.9302
Epoch 9/10, Train Loss: 0.2270 (value: 0.0012, weighted value: 0.0617, policy: 0.1653, weighted policy: 0.1653), Train Mean Max: 0.9305
Epoch 10/10, Train Loss: 0.2266 (value: 0.0012, weighted value: 0.0612, policy: 0.1655, weighted policy: 0.1655), Train Mean Max: 0.9305
..training done in 67.65 seconds
..evaluation done in 16.57 seconds
Old network+MCTS average reward: 0.65, min: 0.01, max: 1.42, stdev: 0.23
New network+MCTS average reward: 0.65, min: 0.03, max: 1.35, stdev: 0.23
Old bare network average reward: 0.59, min: -0.01, max: 1.42, stdev: 0.24
New bare network average reward: 0.59, min: -0.15, max: 1.35, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.24, max: 0.98, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: -0.02, max: 1.38, stdev: 0.24
External policy "total greedy" average reward: 0.65, min: 0.03, max: 1.38, stdev: 0.23
New network won 88 and tied 132 out of 300 games (51.33% wins where ties are half wins)
Keeping the new network

Training iteration 148 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 48.39 seconds
Training examples lengths: [64759, 64781, 64832, 64857, 64545, 64636, 65016, 64683, 64697, 64891]
Total value: 431427.81
Training on 647697 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2551 (value: 0.0016, weighted value: 0.0788, policy: 0.1763, weighted policy: 0.1763), Train Mean Max: 0.9291
Epoch 2/10, Train Loss: 0.2419 (value: 0.0014, weighted value: 0.0716, policy: 0.1703, weighted policy: 0.1703), Train Mean Max: 0.9299
Epoch 3/10, Train Loss: 0.2352 (value: 0.0013, weighted value: 0.0667, policy: 0.1686, weighted policy: 0.1686), Train Mean Max: 0.9302
Epoch 4/10, Train Loss: 0.2329 (value: 0.0013, weighted value: 0.0665, policy: 0.1664, weighted policy: 0.1664), Train Mean Max: 0.9304
Epoch 5/10, Train Loss: 0.2274 (value: 0.0012, weighted value: 0.0608, policy: 0.1666, weighted policy: 0.1666), Train Mean Max: 0.9305
Epoch 6/10, Train Loss: 0.2243 (value: 0.0012, weighted value: 0.0592, policy: 0.1650, weighted policy: 0.1650), Train Mean Max: 0.9310
Epoch 7/10, Train Loss: 0.2232 (value: 0.0012, weighted value: 0.0581, policy: 0.1651, weighted policy: 0.1651), Train Mean Max: 0.9309
Epoch 8/10, Train Loss: 0.2171 (value: 0.0011, weighted value: 0.0538, policy: 0.1632, weighted policy: 0.1632), Train Mean Max: 0.9315
Epoch 9/10, Train Loss: 0.2178 (value: 0.0011, weighted value: 0.0552, policy: 0.1625, weighted policy: 0.1625), Train Mean Max: 0.9316
Epoch 10/10, Train Loss: 0.2146 (value: 0.0010, weighted value: 0.0517, policy: 0.1629, weighted policy: 0.1629), Train Mean Max: 0.9316
..training done in 71.12 seconds
..evaluation done in 16.15 seconds
Old network+MCTS average reward: 0.65, min: 0.14, max: 1.37, stdev: 0.22
New network+MCTS average reward: 0.65, min: 0.08, max: 1.37, stdev: 0.22
Old bare network average reward: 0.60, min: 0.01, max: 1.31, stdev: 0.23
New bare network average reward: 0.60, min: 0.08, max: 1.26, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.43, max: 0.97, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: 0.06, max: 1.23, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.07, max: 1.47, stdev: 0.22
New network won 67 and tied 154 out of 300 games (48.00% wins where ties are half wins)
Reverting to the old network

Training iteration 149 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.81 seconds
Training examples lengths: [64781, 64832, 64857, 64545, 64636, 65016, 64683, 64697, 64891, 64806]
Total value: 431576.66
Training on 647744 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2805 (value: 0.0019, weighted value: 0.0970, policy: 0.1835, weighted policy: 0.1835), Train Mean Max: 0.9279
Epoch 2/10, Train Loss: 0.2642 (value: 0.0017, weighted value: 0.0857, policy: 0.1785, weighted policy: 0.1785), Train Mean Max: 0.9284
Epoch 3/10, Train Loss: 0.2538 (value: 0.0016, weighted value: 0.0801, policy: 0.1737, weighted policy: 0.1737), Train Mean Max: 0.9289
Epoch 4/10, Train Loss: 0.2444 (value: 0.0015, weighted value: 0.0738, policy: 0.1706, weighted policy: 0.1706), Train Mean Max: 0.9294
Epoch 5/10, Train Loss: 0.2405 (value: 0.0014, weighted value: 0.0707, policy: 0.1698, weighted policy: 0.1698), Train Mean Max: 0.9295
Epoch 6/10, Train Loss: 0.2354 (value: 0.0013, weighted value: 0.0671, policy: 0.1683, weighted policy: 0.1683), Train Mean Max: 0.9298
Epoch 7/10, Train Loss: 0.2311 (value: 0.0013, weighted value: 0.0634, policy: 0.1677, weighted policy: 0.1677), Train Mean Max: 0.9301
Epoch 8/10, Train Loss: 0.2277 (value: 0.0012, weighted value: 0.0611, policy: 0.1666, weighted policy: 0.1666), Train Mean Max: 0.9304
Epoch 9/10, Train Loss: 0.2269 (value: 0.0012, weighted value: 0.0613, policy: 0.1656, weighted policy: 0.1656), Train Mean Max: 0.9304
Epoch 10/10, Train Loss: 0.2220 (value: 0.0011, weighted value: 0.0567, policy: 0.1652, weighted policy: 0.1652), Train Mean Max: 0.9308
..training done in 66.40 seconds
..evaluation done in 15.61 seconds
Old network+MCTS average reward: 0.65, min: 0.11, max: 1.25, stdev: 0.22
New network+MCTS average reward: 0.65, min: 0.10, max: 1.27, stdev: 0.22
Old bare network average reward: 0.60, min: -0.03, max: 1.25, stdev: 0.22
New bare network average reward: 0.60, min: -0.03, max: 1.27, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.54, max: 0.79, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: -0.08, max: 1.21, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.06, max: 1.35, stdev: 0.23
New network won 69 and tied 144 out of 300 games (47.00% wins where ties are half wins)
Reverting to the old network

Training iteration 150 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.69 seconds
Training examples lengths: [64832, 64857, 64545, 64636, 65016, 64683, 64697, 64891, 64806, 64591]
Total value: 431508.88
Training on 647554 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3104 (value: 0.0024, weighted value: 0.1183, policy: 0.1921, weighted policy: 0.1921), Train Mean Max: 0.9265
Epoch 2/10, Train Loss: 0.2845 (value: 0.0020, weighted value: 0.1001, policy: 0.1844, weighted policy: 0.1844), Train Mean Max: 0.9274
Epoch 3/10, Train Loss: 0.2697 (value: 0.0018, weighted value: 0.0915, policy: 0.1782, weighted policy: 0.1782), Train Mean Max: 0.9277
Epoch 4/10, Train Loss: 0.2600 (value: 0.0017, weighted value: 0.0840, policy: 0.1760, weighted policy: 0.1760), Train Mean Max: 0.9280
Epoch 5/10, Train Loss: 0.2513 (value: 0.0016, weighted value: 0.0786, policy: 0.1727, weighted policy: 0.1727), Train Mean Max: 0.9284
Epoch 6/10, Train Loss: 0.2467 (value: 0.0015, weighted value: 0.0760, policy: 0.1707, weighted policy: 0.1707), Train Mean Max: 0.9285
Epoch 7/10, Train Loss: 0.2412 (value: 0.0014, weighted value: 0.0709, policy: 0.1703, weighted policy: 0.1703), Train Mean Max: 0.9288
Epoch 8/10, Train Loss: 0.2378 (value: 0.0014, weighted value: 0.0683, policy: 0.1695, weighted policy: 0.1695), Train Mean Max: 0.9290
Epoch 9/10, Train Loss: 0.2342 (value: 0.0013, weighted value: 0.0656, policy: 0.1687, weighted policy: 0.1687), Train Mean Max: 0.9293
Epoch 10/10, Train Loss: 0.2297 (value: 0.0012, weighted value: 0.0615, policy: 0.1682, weighted policy: 0.1682), Train Mean Max: 0.9296
..training done in 66.62 seconds
..evaluation done in 15.90 seconds
Old network+MCTS average reward: 0.67, min: 0.06, max: 1.21, stdev: 0.21
New network+MCTS average reward: 0.66, min: 0.11, max: 1.23, stdev: 0.21
Old bare network average reward: 0.60, min: -0.04, max: 1.21, stdev: 0.22
New bare network average reward: 0.61, min: 0.00, max: 1.17, stdev: 0.21
External policy "random" average reward: 0.28, min: -0.28, max: 0.85, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: -0.08, max: 1.10, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.05, max: 1.31, stdev: 0.21
New network won 78 and tied 128 out of 300 games (47.33% wins where ties are half wins)
Reverting to the old network

Training iteration 151 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.66 seconds
Training examples lengths: [64857, 64545, 64636, 65016, 64683, 64697, 64891, 64806, 64591, 64917]
Total value: 431903.57
Training on 647639 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3371 (value: 0.0027, weighted value: 0.1365, policy: 0.2007, weighted policy: 0.2007), Train Mean Max: 0.9253
Epoch 2/10, Train Loss: 0.3027 (value: 0.0023, weighted value: 0.1138, policy: 0.1888, weighted policy: 0.1888), Train Mean Max: 0.9261
Epoch 3/10, Train Loss: 0.2848 (value: 0.0020, weighted value: 0.1017, policy: 0.1831, weighted policy: 0.1831), Train Mean Max: 0.9265
Epoch 4/10, Train Loss: 0.2726 (value: 0.0019, weighted value: 0.0931, policy: 0.1795, weighted policy: 0.1795), Train Mean Max: 0.9269
Epoch 5/10, Train Loss: 0.2643 (value: 0.0018, weighted value: 0.0881, policy: 0.1761, weighted policy: 0.1761), Train Mean Max: 0.9273
Epoch 6/10, Train Loss: 0.2568 (value: 0.0016, weighted value: 0.0817, policy: 0.1751, weighted policy: 0.1751), Train Mean Max: 0.9274
Epoch 7/10, Train Loss: 0.2511 (value: 0.0016, weighted value: 0.0779, policy: 0.1732, weighted policy: 0.1732), Train Mean Max: 0.9278
Epoch 8/10, Train Loss: 0.2456 (value: 0.0015, weighted value: 0.0733, policy: 0.1723, weighted policy: 0.1723), Train Mean Max: 0.9279
Epoch 9/10, Train Loss: 0.2403 (value: 0.0014, weighted value: 0.0703, policy: 0.1700, weighted policy: 0.1700), Train Mean Max: 0.9283
Epoch 10/10, Train Loss: 0.2387 (value: 0.0014, weighted value: 0.0683, policy: 0.1704, weighted policy: 0.1704), Train Mean Max: 0.9285
..training done in 66.72 seconds
..evaluation done in 16.75 seconds
Old network+MCTS average reward: 0.67, min: 0.04, max: 1.54, stdev: 0.23
New network+MCTS average reward: 0.67, min: 0.04, max: 1.54, stdev: 0.23
Old bare network average reward: 0.62, min: 0.02, max: 1.43, stdev: 0.23
New bare network average reward: 0.61, min: -0.04, max: 1.28, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.30, max: 1.06, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: -0.06, max: 1.44, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.06, max: 1.67, stdev: 0.21
New network won 76 and tied 133 out of 300 games (47.50% wins where ties are half wins)
Reverting to the old network

Training iteration 152 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.57 seconds
Training examples lengths: [64545, 64636, 65016, 64683, 64697, 64891, 64806, 64591, 64917, 65068]
Total value: 431440.81
Training on 647850 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3612 (value: 0.0031, weighted value: 0.1548, policy: 0.2065, weighted policy: 0.2065), Train Mean Max: 0.9241
Epoch 2/10, Train Loss: 0.3206 (value: 0.0025, weighted value: 0.1271, policy: 0.1936, weighted policy: 0.1936), Train Mean Max: 0.9252
Epoch 3/10, Train Loss: 0.2996 (value: 0.0023, weighted value: 0.1129, policy: 0.1867, weighted policy: 0.1867), Train Mean Max: 0.9256
Epoch 4/10, Train Loss: 0.2854 (value: 0.0020, weighted value: 0.1023, policy: 0.1831, weighted policy: 0.1831), Train Mean Max: 0.9258
Epoch 5/10, Train Loss: 0.2756 (value: 0.0019, weighted value: 0.0957, policy: 0.1799, weighted policy: 0.1799), Train Mean Max: 0.9260
Epoch 6/10, Train Loss: 0.2688 (value: 0.0018, weighted value: 0.0904, policy: 0.1784, weighted policy: 0.1784), Train Mean Max: 0.9264
Epoch 7/10, Train Loss: 0.2606 (value: 0.0017, weighted value: 0.0843, policy: 0.1763, weighted policy: 0.1763), Train Mean Max: 0.9267
Epoch 8/10, Train Loss: 0.2525 (value: 0.0016, weighted value: 0.0788, policy: 0.1736, weighted policy: 0.1736), Train Mean Max: 0.9271
Epoch 9/10, Train Loss: 0.2493 (value: 0.0015, weighted value: 0.0758, policy: 0.1735, weighted policy: 0.1735), Train Mean Max: 0.9271
Epoch 10/10, Train Loss: 0.2457 (value: 0.0015, weighted value: 0.0730, policy: 0.1727, weighted policy: 0.1727), Train Mean Max: 0.9274
..training done in 67.65 seconds
..evaluation done in 17.14 seconds
Old network+MCTS average reward: 0.65, min: 0.02, max: 1.29, stdev: 0.21
New network+MCTS average reward: 0.65, min: 0.02, max: 1.28, stdev: 0.21
Old bare network average reward: 0.59, min: -0.05, max: 1.29, stdev: 0.22
New bare network average reward: 0.59, min: -0.05, max: 1.29, stdev: 0.22
External policy "random" average reward: 0.25, min: -0.32, max: 0.95, stdev: 0.21
External policy "individual greedy" average reward: 0.53, min: -0.17, max: 1.36, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: -0.07, max: 1.43, stdev: 0.22
New network won 85 and tied 134 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 153 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.10 seconds
Training examples lengths: [64636, 65016, 64683, 64697, 64891, 64806, 64591, 64917, 65068, 64973]
Total value: 432088.32
Training on 648278 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2692 (value: 0.0018, weighted value: 0.0884, policy: 0.1808, weighted policy: 0.1808), Train Mean Max: 0.9262
Epoch 2/10, Train Loss: 0.2597 (value: 0.0016, weighted value: 0.0815, policy: 0.1782, weighted policy: 0.1782), Train Mean Max: 0.9268
Epoch 3/10, Train Loss: 0.2509 (value: 0.0015, weighted value: 0.0759, policy: 0.1750, weighted policy: 0.1750), Train Mean Max: 0.9272
Epoch 4/10, Train Loss: 0.2452 (value: 0.0014, weighted value: 0.0719, policy: 0.1733, weighted policy: 0.1733), Train Mean Max: 0.9275
Epoch 5/10, Train Loss: 0.2408 (value: 0.0014, weighted value: 0.0679, policy: 0.1729, weighted policy: 0.1729), Train Mean Max: 0.9276
Epoch 6/10, Train Loss: 0.2380 (value: 0.0014, weighted value: 0.0677, policy: 0.1703, weighted policy: 0.1703), Train Mean Max: 0.9280
Epoch 7/10, Train Loss: 0.2336 (value: 0.0013, weighted value: 0.0635, policy: 0.1702, weighted policy: 0.1702), Train Mean Max: 0.9281
Epoch 8/10, Train Loss: 0.2288 (value: 0.0012, weighted value: 0.0595, policy: 0.1693, weighted policy: 0.1693), Train Mean Max: 0.9285
Epoch 9/10, Train Loss: 0.2293 (value: 0.0012, weighted value: 0.0607, policy: 0.1686, weighted policy: 0.1686), Train Mean Max: 0.9286
Epoch 10/10, Train Loss: 0.2253 (value: 0.0011, weighted value: 0.0570, policy: 0.1683, weighted policy: 0.1683), Train Mean Max: 0.9289
..training done in 71.06 seconds
..evaluation done in 15.94 seconds
Old network+MCTS average reward: 0.66, min: 0.00, max: 1.35, stdev: 0.23
New network+MCTS average reward: 0.65, min: 0.05, max: 1.35, stdev: 0.23
Old bare network average reward: 0.60, min: -0.01, max: 1.31, stdev: 0.23
New bare network average reward: 0.60, min: -0.04, max: 1.35, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.31, max: 1.01, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: 0.03, max: 1.23, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.01, max: 1.39, stdev: 0.22
New network won 76 and tied 148 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 154 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.99 seconds
Training examples lengths: [65016, 64683, 64697, 64891, 64806, 64591, 64917, 65068, 64973, 64572]
Total value: 432003.98
Training on 648214 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2505 (value: 0.0015, weighted value: 0.0735, policy: 0.1770, weighted policy: 0.1770), Train Mean Max: 0.9278
Epoch 2/10, Train Loss: 0.2430 (value: 0.0014, weighted value: 0.0695, policy: 0.1736, weighted policy: 0.1736), Train Mean Max: 0.9283
Epoch 3/10, Train Loss: 0.2357 (value: 0.0013, weighted value: 0.0647, policy: 0.1711, weighted policy: 0.1711), Train Mean Max: 0.9285
Epoch 4/10, Train Loss: 0.2301 (value: 0.0012, weighted value: 0.0611, policy: 0.1690, weighted policy: 0.1690), Train Mean Max: 0.9288
Epoch 5/10, Train Loss: 0.2270 (value: 0.0012, weighted value: 0.0587, policy: 0.1684, weighted policy: 0.1684), Train Mean Max: 0.9291
Epoch 6/10, Train Loss: 0.2252 (value: 0.0011, weighted value: 0.0570, policy: 0.1683, weighted policy: 0.1683), Train Mean Max: 0.9292
Epoch 7/10, Train Loss: 0.2215 (value: 0.0011, weighted value: 0.0549, policy: 0.1665, weighted policy: 0.1665), Train Mean Max: 0.9297
Epoch 8/10, Train Loss: 0.2199 (value: 0.0011, weighted value: 0.0538, policy: 0.1660, weighted policy: 0.1660), Train Mean Max: 0.9298
Epoch 9/10, Train Loss: 0.2183 (value: 0.0011, weighted value: 0.0532, policy: 0.1651, weighted policy: 0.1651), Train Mean Max: 0.9299
Epoch 10/10, Train Loss: 0.2163 (value: 0.0010, weighted value: 0.0512, policy: 0.1651, weighted policy: 0.1651), Train Mean Max: 0.9302
..training done in 65.78 seconds
..evaluation done in 16.10 seconds
Old network+MCTS average reward: 0.68, min: 0.14, max: 1.44, stdev: 0.25
New network+MCTS average reward: 0.67, min: 0.14, max: 1.43, stdev: 0.25
Old bare network average reward: 0.60, min: 0.00, max: 1.42, stdev: 0.26
New bare network average reward: 0.61, min: 0.00, max: 1.33, stdev: 0.26
External policy "random" average reward: 0.27, min: -0.55, max: 0.93, stdev: 0.25
External policy "individual greedy" average reward: 0.55, min: -0.11, max: 1.31, stdev: 0.27
External policy "total greedy" average reward: 0.67, min: 0.04, max: 1.59, stdev: 0.26
New network won 67 and tied 147 out of 300 games (46.83% wins where ties are half wins)
Reverting to the old network

Training iteration 155 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.91 seconds
Training examples lengths: [64683, 64697, 64891, 64806, 64591, 64917, 65068, 64973, 64572, 64727]
Total value: 431809.62
Training on 647925 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2766 (value: 0.0018, weighted value: 0.0921, policy: 0.1846, weighted policy: 0.1846), Train Mean Max: 0.9267
Epoch 2/10, Train Loss: 0.2613 (value: 0.0016, weighted value: 0.0818, policy: 0.1795, weighted policy: 0.1795), Train Mean Max: 0.9273
Epoch 3/10, Train Loss: 0.2513 (value: 0.0015, weighted value: 0.0761, policy: 0.1752, weighted policy: 0.1752), Train Mean Max: 0.9277
Epoch 4/10, Train Loss: 0.2426 (value: 0.0014, weighted value: 0.0706, policy: 0.1720, weighted policy: 0.1720), Train Mean Max: 0.9280
Epoch 5/10, Train Loss: 0.2383 (value: 0.0013, weighted value: 0.0667, policy: 0.1715, weighted policy: 0.1715), Train Mean Max: 0.9282
Epoch 6/10, Train Loss: 0.2339 (value: 0.0013, weighted value: 0.0643, policy: 0.1696, weighted policy: 0.1696), Train Mean Max: 0.9287
Epoch 7/10, Train Loss: 0.2308 (value: 0.0012, weighted value: 0.0613, policy: 0.1696, weighted policy: 0.1696), Train Mean Max: 0.9288
Epoch 8/10, Train Loss: 0.2291 (value: 0.0012, weighted value: 0.0600, policy: 0.1691, weighted policy: 0.1691), Train Mean Max: 0.9291
Epoch 9/10, Train Loss: 0.2247 (value: 0.0012, weighted value: 0.0577, policy: 0.1670, weighted policy: 0.1670), Train Mean Max: 0.9293
Epoch 10/10, Train Loss: 0.2222 (value: 0.0011, weighted value: 0.0556, policy: 0.1666, weighted policy: 0.1666), Train Mean Max: 0.9295
..training done in 65.97 seconds
..evaluation done in 16.75 seconds
Old network+MCTS average reward: 0.67, min: 0.05, max: 1.57, stdev: 0.24
New network+MCTS average reward: 0.68, min: 0.03, max: 1.57, stdev: 0.23
Old bare network average reward: 0.62, min: 0.00, max: 1.57, stdev: 0.24
New bare network average reward: 0.62, min: 0.01, max: 1.57, stdev: 0.24
External policy "random" average reward: 0.25, min: -0.38, max: 0.92, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.09, max: 1.27, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.08, max: 1.42, stdev: 0.23
New network won 81 and tied 138 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 156 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.21 seconds
Training examples lengths: [64697, 64891, 64806, 64591, 64917, 65068, 64973, 64572, 64727, 64912]
Total value: 432706.95
Training on 648154 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2501 (value: 0.0015, weighted value: 0.0748, policy: 0.1753, weighted policy: 0.1753), Train Mean Max: 0.9284
Epoch 2/10, Train Loss: 0.2390 (value: 0.0013, weighted value: 0.0667, policy: 0.1724, weighted policy: 0.1724), Train Mean Max: 0.9288
Epoch 3/10, Train Loss: 0.2323 (value: 0.0013, weighted value: 0.0638, policy: 0.1686, weighted policy: 0.1686), Train Mean Max: 0.9294
Epoch 4/10, Train Loss: 0.2282 (value: 0.0012, weighted value: 0.0607, policy: 0.1674, weighted policy: 0.1674), Train Mean Max: 0.9295
Epoch 5/10, Train Loss: 0.2248 (value: 0.0012, weighted value: 0.0583, policy: 0.1664, weighted policy: 0.1664), Train Mean Max: 0.9297
Epoch 6/10, Train Loss: 0.2227 (value: 0.0011, weighted value: 0.0569, policy: 0.1658, weighted policy: 0.1658), Train Mean Max: 0.9299
Epoch 7/10, Train Loss: 0.2202 (value: 0.0011, weighted value: 0.0543, policy: 0.1659, weighted policy: 0.1659), Train Mean Max: 0.9301
Epoch 8/10, Train Loss: 0.2179 (value: 0.0011, weighted value: 0.0532, policy: 0.1647, weighted policy: 0.1647), Train Mean Max: 0.9304
Epoch 9/10, Train Loss: 0.2147 (value: 0.0010, weighted value: 0.0505, policy: 0.1642, weighted policy: 0.1642), Train Mean Max: 0.9306
Epoch 10/10, Train Loss: 0.2136 (value: 0.0010, weighted value: 0.0505, policy: 0.1630, weighted policy: 0.1630), Train Mean Max: 0.9309
..training done in 71.91 seconds
..evaluation done in 16.26 seconds
Old network+MCTS average reward: 0.68, min: 0.05, max: 1.61, stdev: 0.23
New network+MCTS average reward: 0.68, min: -0.22, max: 1.57, stdev: 0.24
Old bare network average reward: 0.62, min: -0.02, max: 1.49, stdev: 0.24
New bare network average reward: 0.62, min: -0.15, max: 1.49, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.44, max: 1.06, stdev: 0.24
External policy "individual greedy" average reward: 0.55, min: -0.15, max: 1.31, stdev: 0.24
External policy "total greedy" average reward: 0.66, min: 0.11, max: 1.49, stdev: 0.23
New network won 79 and tied 146 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 157 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.54 seconds
Training examples lengths: [64891, 64806, 64591, 64917, 65068, 64973, 64572, 64727, 64912, 64876]
Total value: 434159.43
Training on 648333 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2428 (value: 0.0014, weighted value: 0.0693, policy: 0.1735, weighted policy: 0.1735), Train Mean Max: 0.9294
Epoch 2/10, Train Loss: 0.2330 (value: 0.0013, weighted value: 0.0639, policy: 0.1691, weighted policy: 0.1691), Train Mean Max: 0.9299
Epoch 3/10, Train Loss: 0.2257 (value: 0.0012, weighted value: 0.0589, policy: 0.1668, weighted policy: 0.1668), Train Mean Max: 0.9302
Epoch 4/10, Train Loss: 0.2227 (value: 0.0012, weighted value: 0.0578, policy: 0.1649, weighted policy: 0.1649), Train Mean Max: 0.9306
Epoch 5/10, Train Loss: 0.2187 (value: 0.0011, weighted value: 0.0549, policy: 0.1639, weighted policy: 0.1639), Train Mean Max: 0.9309
Epoch 6/10, Train Loss: 0.2152 (value: 0.0010, weighted value: 0.0519, policy: 0.1633, weighted policy: 0.1633), Train Mean Max: 0.9309
Epoch 7/10, Train Loss: 0.2142 (value: 0.0010, weighted value: 0.0516, policy: 0.1626, weighted policy: 0.1626), Train Mean Max: 0.9313
Epoch 8/10, Train Loss: 0.2124 (value: 0.0010, weighted value: 0.0494, policy: 0.1630, weighted policy: 0.1630), Train Mean Max: 0.9314
Epoch 9/10, Train Loss: 0.2114 (value: 0.0010, weighted value: 0.0491, policy: 0.1623, weighted policy: 0.1623), Train Mean Max: 0.9316
Epoch 10/10, Train Loss: 0.2077 (value: 0.0009, weighted value: 0.0462, policy: 0.1615, weighted policy: 0.1615), Train Mean Max: 0.9319
..training done in 71.45 seconds
..evaluation done in 16.86 seconds
Old network+MCTS average reward: 0.67, min: 0.06, max: 1.46, stdev: 0.23
New network+MCTS average reward: 0.67, min: 0.08, max: 1.49, stdev: 0.23
Old bare network average reward: 0.61, min: -0.14, max: 1.42, stdev: 0.24
New bare network average reward: 0.61, min: 0.08, max: 1.41, stdev: 0.24
External policy "random" average reward: 0.25, min: -0.31, max: 0.93, stdev: 0.24
External policy "individual greedy" average reward: 0.53, min: -0.01, max: 1.30, stdev: 0.24
External policy "total greedy" average reward: 0.65, min: 0.14, max: 1.43, stdev: 0.23
New network won 72 and tied 148 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 158 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.78 seconds
Training examples lengths: [64806, 64591, 64917, 65068, 64973, 64572, 64727, 64912, 64876, 64933]
Total value: 434683.51
Training on 648375 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2707 (value: 0.0018, weighted value: 0.0882, policy: 0.1825, weighted policy: 0.1825), Train Mean Max: 0.9284
Epoch 2/10, Train Loss: 0.2536 (value: 0.0016, weighted value: 0.0786, policy: 0.1750, weighted policy: 0.1750), Train Mean Max: 0.9292
Epoch 3/10, Train Loss: 0.2404 (value: 0.0014, weighted value: 0.0706, policy: 0.1699, weighted policy: 0.1699), Train Mean Max: 0.9297
Epoch 4/10, Train Loss: 0.2362 (value: 0.0014, weighted value: 0.0681, policy: 0.1681, weighted policy: 0.1681), Train Mean Max: 0.9299
Epoch 5/10, Train Loss: 0.2304 (value: 0.0013, weighted value: 0.0625, policy: 0.1679, weighted policy: 0.1679), Train Mean Max: 0.9301
Epoch 6/10, Train Loss: 0.2272 (value: 0.0012, weighted value: 0.0604, policy: 0.1668, weighted policy: 0.1668), Train Mean Max: 0.9305
Epoch 7/10, Train Loss: 0.2226 (value: 0.0012, weighted value: 0.0576, policy: 0.1651, weighted policy: 0.1651), Train Mean Max: 0.9306
Epoch 8/10, Train Loss: 0.2206 (value: 0.0011, weighted value: 0.0570, policy: 0.1636, weighted policy: 0.1636), Train Mean Max: 0.9310
Epoch 9/10, Train Loss: 0.2178 (value: 0.0011, weighted value: 0.0538, policy: 0.1640, weighted policy: 0.1640), Train Mean Max: 0.9310
Epoch 10/10, Train Loss: 0.2162 (value: 0.0011, weighted value: 0.0535, policy: 0.1627, weighted policy: 0.1627), Train Mean Max: 0.9313
..training done in 66.65 seconds
..evaluation done in 16.29 seconds
Old network+MCTS average reward: 0.67, min: 0.05, max: 1.38, stdev: 0.25
New network+MCTS average reward: 0.67, min: 0.03, max: 1.38, stdev: 0.24
Old bare network average reward: 0.61, min: 0.02, max: 1.32, stdev: 0.25
New bare network average reward: 0.61, min: -0.02, max: 1.27, stdev: 0.25
External policy "random" average reward: 0.26, min: -0.42, max: 1.12, stdev: 0.25
External policy "individual greedy" average reward: 0.54, min: -0.09, max: 1.35, stdev: 0.24
External policy "total greedy" average reward: 0.65, min: 0.04, max: 1.30, stdev: 0.24
New network won 77 and tied 147 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 159 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.30 seconds
Training examples lengths: [64591, 64917, 65068, 64973, 64572, 64727, 64912, 64876, 64933, 64854]
Total value: 434962.26
Training on 648423 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2442 (value: 0.0014, weighted value: 0.0719, policy: 0.1723, weighted policy: 0.1723), Train Mean Max: 0.9300
Epoch 2/10, Train Loss: 0.2327 (value: 0.0013, weighted value: 0.0645, policy: 0.1682, weighted policy: 0.1682), Train Mean Max: 0.9305
Epoch 3/10, Train Loss: 0.2285 (value: 0.0012, weighted value: 0.0618, policy: 0.1667, weighted policy: 0.1667), Train Mean Max: 0.9309
Epoch 4/10, Train Loss: 0.2215 (value: 0.0011, weighted value: 0.0565, policy: 0.1649, weighted policy: 0.1649), Train Mean Max: 0.9312
Epoch 5/10, Train Loss: 0.2195 (value: 0.0011, weighted value: 0.0562, policy: 0.1633, weighted policy: 0.1633), Train Mean Max: 0.9314
Epoch 6/10, Train Loss: 0.2163 (value: 0.0011, weighted value: 0.0534, policy: 0.1629, weighted policy: 0.1629), Train Mean Max: 0.9317
Epoch 7/10, Train Loss: 0.2156 (value: 0.0011, weighted value: 0.0534, policy: 0.1622, weighted policy: 0.1622), Train Mean Max: 0.9318
Epoch 8/10, Train Loss: 0.2135 (value: 0.0010, weighted value: 0.0514, policy: 0.1620, weighted policy: 0.1620), Train Mean Max: 0.9320
Epoch 9/10, Train Loss: 0.2105 (value: 0.0010, weighted value: 0.0492, policy: 0.1612, weighted policy: 0.1612), Train Mean Max: 0.9322
Epoch 10/10, Train Loss: 0.2081 (value: 0.0010, weighted value: 0.0476, policy: 0.1605, weighted policy: 0.1605), Train Mean Max: 0.9324
..training done in 66.42 seconds
..evaluation done in 16.41 seconds
Old network+MCTS average reward: 0.67, min: 0.08, max: 1.55, stdev: 0.21
New network+MCTS average reward: 0.67, min: 0.10, max: 1.55, stdev: 0.22
Old bare network average reward: 0.61, min: 0.10, max: 1.55, stdev: 0.22
New bare network average reward: 0.61, min: -0.01, max: 1.48, stdev: 0.22
External policy "random" average reward: 0.25, min: -0.25, max: 0.82, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: -0.10, max: 1.36, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.14, max: 1.38, stdev: 0.21
New network won 83 and tied 139 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 160 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.47 seconds
Training examples lengths: [64917, 65068, 64973, 64572, 64727, 64912, 64876, 64933, 64854, 65016]
Total value: 435629.49
Training on 648848 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2380 (value: 0.0013, weighted value: 0.0666, policy: 0.1714, weighted policy: 0.1714), Train Mean Max: 0.9310
Epoch 2/10, Train Loss: 0.2284 (value: 0.0012, weighted value: 0.0603, policy: 0.1680, weighted policy: 0.1680), Train Mean Max: 0.9315
Epoch 3/10, Train Loss: 0.2224 (value: 0.0012, weighted value: 0.0584, policy: 0.1641, weighted policy: 0.1641), Train Mean Max: 0.9320
Epoch 4/10, Train Loss: 0.2174 (value: 0.0011, weighted value: 0.0547, policy: 0.1626, weighted policy: 0.1626), Train Mean Max: 0.9322
Epoch 5/10, Train Loss: 0.2138 (value: 0.0011, weighted value: 0.0527, policy: 0.1612, weighted policy: 0.1612), Train Mean Max: 0.9326
Epoch 6/10, Train Loss: 0.2122 (value: 0.0010, weighted value: 0.0512, policy: 0.1611, weighted policy: 0.1611), Train Mean Max: 0.9326
Epoch 7/10, Train Loss: 0.2093 (value: 0.0010, weighted value: 0.0491, policy: 0.1602, weighted policy: 0.1602), Train Mean Max: 0.9329
Epoch 8/10, Train Loss: 0.2075 (value: 0.0010, weighted value: 0.0482, policy: 0.1592, weighted policy: 0.1592), Train Mean Max: 0.9331
Epoch 9/10, Train Loss: 0.2067 (value: 0.0009, weighted value: 0.0475, policy: 0.1592, weighted policy: 0.1592), Train Mean Max: 0.9332
Epoch 10/10, Train Loss: 0.2060 (value: 0.0009, weighted value: 0.0463, policy: 0.1597, weighted policy: 0.1597), Train Mean Max: 0.9332
..training done in 66.59 seconds
..evaluation done in 16.49 seconds
Old network+MCTS average reward: 0.65, min: 0.03, max: 1.28, stdev: 0.22
New network+MCTS average reward: 0.65, min: 0.07, max: 1.28, stdev: 0.22
Old bare network average reward: 0.59, min: 0.06, max: 1.33, stdev: 0.21
New bare network average reward: 0.59, min: -0.05, max: 1.33, stdev: 0.22
External policy "random" average reward: 0.25, min: -0.31, max: 0.85, stdev: 0.21
External policy "individual greedy" average reward: 0.51, min: -0.05, max: 1.08, stdev: 0.22
External policy "total greedy" average reward: 0.63, min: 0.10, max: 1.18, stdev: 0.21
New network won 71 and tied 152 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network
Saving intermediate checkpoint to zgaz_checkpoints/1757654225_iter_160

Training iteration 161 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.75 seconds
Training examples lengths: [65068, 64973, 64572, 64727, 64912, 64876, 64933, 64854, 65016, 64573]
Total value: 435550.84
Training on 648504 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2677 (value: 0.0018, weighted value: 0.0875, policy: 0.1802, weighted policy: 0.1802), Train Mean Max: 0.9298
Epoch 2/10, Train Loss: 0.2503 (value: 0.0015, weighted value: 0.0758, policy: 0.1744, weighted policy: 0.1744), Train Mean Max: 0.9305
Epoch 3/10, Train Loss: 0.2390 (value: 0.0014, weighted value: 0.0712, policy: 0.1677, weighted policy: 0.1677), Train Mean Max: 0.9309
Epoch 4/10, Train Loss: 0.2324 (value: 0.0013, weighted value: 0.0665, policy: 0.1659, weighted policy: 0.1659), Train Mean Max: 0.9312
Epoch 5/10, Train Loss: 0.2254 (value: 0.0012, weighted value: 0.0618, policy: 0.1636, weighted policy: 0.1636), Train Mean Max: 0.9315
Epoch 6/10, Train Loss: 0.2233 (value: 0.0012, weighted value: 0.0609, policy: 0.1624, weighted policy: 0.1624), Train Mean Max: 0.9318
Epoch 7/10, Train Loss: 0.2187 (value: 0.0011, weighted value: 0.0563, policy: 0.1624, weighted policy: 0.1624), Train Mean Max: 0.9319
Epoch 8/10, Train Loss: 0.2179 (value: 0.0011, weighted value: 0.0560, policy: 0.1619, weighted policy: 0.1619), Train Mean Max: 0.9321
Epoch 9/10, Train Loss: 0.2147 (value: 0.0011, weighted value: 0.0534, policy: 0.1613, weighted policy: 0.1613), Train Mean Max: 0.9323
Epoch 10/10, Train Loss: 0.2141 (value: 0.0010, weighted value: 0.0522, policy: 0.1619, weighted policy: 0.1619), Train Mean Max: 0.9325
..training done in 64.82 seconds
..evaluation done in 16.64 seconds
Old network+MCTS average reward: 0.65, min: -0.15, max: 1.39, stdev: 0.21
New network+MCTS average reward: 0.65, min: -0.19, max: 1.44, stdev: 0.22
Old bare network average reward: 0.59, min: -0.15, max: 1.34, stdev: 0.22
New bare network average reward: 0.59, min: -0.15, max: 1.42, stdev: 0.22
External policy "random" average reward: 0.25, min: -0.32, max: 0.94, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: -0.17, max: 1.21, stdev: 0.21
External policy "total greedy" average reward: 0.64, min: -0.01, max: 1.30, stdev: 0.21
New network won 81 and tied 147 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 162 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.24 seconds
Training examples lengths: [64973, 64572, 64727, 64912, 64876, 64933, 64854, 65016, 64573, 64763]
Total value: 436032.60
Training on 648199 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2403 (value: 0.0014, weighted value: 0.0702, policy: 0.1702, weighted policy: 0.1702), Train Mean Max: 0.9313
Epoch 2/10, Train Loss: 0.2324 (value: 0.0013, weighted value: 0.0655, policy: 0.1669, weighted policy: 0.1669), Train Mean Max: 0.9318
Epoch 3/10, Train Loss: 0.2227 (value: 0.0012, weighted value: 0.0598, policy: 0.1629, weighted policy: 0.1629), Train Mean Max: 0.9322
Epoch 4/10, Train Loss: 0.2185 (value: 0.0012, weighted value: 0.0576, policy: 0.1608, weighted policy: 0.1608), Train Mean Max: 0.9325
Epoch 5/10, Train Loss: 0.2163 (value: 0.0011, weighted value: 0.0550, policy: 0.1614, weighted policy: 0.1614), Train Mean Max: 0.9325
Epoch 6/10, Train Loss: 0.2158 (value: 0.0011, weighted value: 0.0552, policy: 0.1606, weighted policy: 0.1606), Train Mean Max: 0.9327
Epoch 7/10, Train Loss: 0.2124 (value: 0.0011, weighted value: 0.0528, policy: 0.1597, weighted policy: 0.1597), Train Mean Max: 0.9330
Epoch 8/10, Train Loss: 0.2092 (value: 0.0010, weighted value: 0.0492, policy: 0.1599, weighted policy: 0.1599), Train Mean Max: 0.9332
Epoch 9/10, Train Loss: 0.2060 (value: 0.0010, weighted value: 0.0481, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9336
Epoch 10/10, Train Loss: 0.2067 (value: 0.0010, weighted value: 0.0491, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9336
..training done in 66.01 seconds
..evaluation done in 16.08 seconds
Old network+MCTS average reward: 0.68, min: 0.13, max: 1.35, stdev: 0.23
New network+MCTS average reward: 0.68, min: 0.11, max: 1.38, stdev: 0.23
Old bare network average reward: 0.62, min: 0.00, max: 1.31, stdev: 0.25
New bare network average reward: 0.63, min: 0.00, max: 1.38, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.35, max: 0.86, stdev: 0.23
External policy "individual greedy" average reward: 0.56, min: -0.07, max: 1.32, stdev: 0.24
External policy "total greedy" average reward: 0.67, min: 0.03, max: 1.44, stdev: 0.23
New network won 83 and tied 142 out of 300 games (51.33% wins where ties are half wins)
Keeping the new network

Training iteration 163 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.37 seconds
Training examples lengths: [64572, 64727, 64912, 64876, 64933, 64854, 65016, 64573, 64763, 64958]
Total value: 435710.54
Training on 648184 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2368 (value: 0.0014, weighted value: 0.0678, policy: 0.1690, weighted policy: 0.1690), Train Mean Max: 0.9320
Epoch 2/10, Train Loss: 0.2256 (value: 0.0012, weighted value: 0.0608, policy: 0.1648, weighted policy: 0.1648), Train Mean Max: 0.9325
Epoch 3/10, Train Loss: 0.2207 (value: 0.0012, weighted value: 0.0586, policy: 0.1620, weighted policy: 0.1620), Train Mean Max: 0.9328
Epoch 4/10, Train Loss: 0.2151 (value: 0.0011, weighted value: 0.0543, policy: 0.1608, weighted policy: 0.1608), Train Mean Max: 0.9330
Epoch 5/10, Train Loss: 0.2131 (value: 0.0011, weighted value: 0.0534, policy: 0.1598, weighted policy: 0.1598), Train Mean Max: 0.9332
Epoch 6/10, Train Loss: 0.2106 (value: 0.0010, weighted value: 0.0522, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9334
Epoch 7/10, Train Loss: 0.2073 (value: 0.0010, weighted value: 0.0486, policy: 0.1587, weighted policy: 0.1587), Train Mean Max: 0.9335
Epoch 8/10, Train Loss: 0.2068 (value: 0.0010, weighted value: 0.0484, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9339
Epoch 9/10, Train Loss: 0.2045 (value: 0.0009, weighted value: 0.0474, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9339
Epoch 10/10, Train Loss: 0.2035 (value: 0.0009, weighted value: 0.0456, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9341
..training done in 73.11 seconds
..evaluation done in 16.29 seconds
Old network+MCTS average reward: 0.68, min: 0.01, max: 1.40, stdev: 0.22
New network+MCTS average reward: 0.68, min: -0.02, max: 1.40, stdev: 0.22
Old bare network average reward: 0.62, min: -0.03, max: 1.35, stdev: 0.23
New bare network average reward: 0.62, min: 0.02, max: 1.33, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.37, max: 1.22, stdev: 0.24
External policy "individual greedy" average reward: 0.55, min: -0.01, max: 1.48, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: 0.04, max: 1.56, stdev: 0.22
New network won 74 and tied 154 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 164 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.18 seconds
Training examples lengths: [64727, 64912, 64876, 64933, 64854, 65016, 64573, 64763, 64958, 64857]
Total value: 436230.10
Training on 648469 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2354 (value: 0.0013, weighted value: 0.0667, policy: 0.1687, weighted policy: 0.1687), Train Mean Max: 0.9324
Epoch 2/10, Train Loss: 0.2233 (value: 0.0012, weighted value: 0.0601, policy: 0.1632, weighted policy: 0.1632), Train Mean Max: 0.9331
Epoch 3/10, Train Loss: 0.2173 (value: 0.0011, weighted value: 0.0563, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9332
Epoch 4/10, Train Loss: 0.2130 (value: 0.0011, weighted value: 0.0532, policy: 0.1598, weighted policy: 0.1598), Train Mean Max: 0.9334
Epoch 5/10, Train Loss: 0.2111 (value: 0.0010, weighted value: 0.0513, policy: 0.1598, weighted policy: 0.1598), Train Mean Max: 0.9336
Epoch 6/10, Train Loss: 0.2110 (value: 0.0010, weighted value: 0.0514, policy: 0.1596, weighted policy: 0.1596), Train Mean Max: 0.9338
Epoch 7/10, Train Loss: 0.2058 (value: 0.0010, weighted value: 0.0482, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9341
Epoch 8/10, Train Loss: 0.2041 (value: 0.0009, weighted value: 0.0472, policy: 0.1569, weighted policy: 0.1569), Train Mean Max: 0.9342
Epoch 9/10, Train Loss: 0.2042 (value: 0.0009, weighted value: 0.0474, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9343
Epoch 10/10, Train Loss: 0.2014 (value: 0.0009, weighted value: 0.0449, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9345
..training done in 72.26 seconds
..evaluation done in 16.15 seconds
Old network+MCTS average reward: 0.66, min: 0.04, max: 1.43, stdev: 0.23
New network+MCTS average reward: 0.66, min: -0.10, max: 1.26, stdev: 0.23
Old bare network average reward: 0.60, min: -0.11, max: 1.34, stdev: 0.24
New bare network average reward: 0.60, min: 0.00, max: 1.26, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.37, max: 1.06, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: -0.05, max: 1.19, stdev: 0.21
External policy "total greedy" average reward: 0.65, min: 0.06, max: 1.18, stdev: 0.21
New network won 62 and tied 161 out of 300 games (47.50% wins where ties are half wins)
Reverting to the old network

Training iteration 165 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.50 seconds
Training examples lengths: [64912, 64876, 64933, 64854, 65016, 64573, 64763, 64958, 64857, 64579]
Total value: 436517.74
Training on 648321 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2627 (value: 0.0017, weighted value: 0.0850, policy: 0.1777, weighted policy: 0.1777), Train Mean Max: 0.9309
Epoch 2/10, Train Loss: 0.2455 (value: 0.0015, weighted value: 0.0748, policy: 0.1707, weighted policy: 0.1707), Train Mean Max: 0.9316
Epoch 3/10, Train Loss: 0.2345 (value: 0.0014, weighted value: 0.0688, policy: 0.1657, weighted policy: 0.1657), Train Mean Max: 0.9321
Epoch 4/10, Train Loss: 0.2275 (value: 0.0013, weighted value: 0.0633, policy: 0.1642, weighted policy: 0.1642), Train Mean Max: 0.9323
Epoch 5/10, Train Loss: 0.2223 (value: 0.0012, weighted value: 0.0606, policy: 0.1617, weighted policy: 0.1617), Train Mean Max: 0.9326
Epoch 6/10, Train Loss: 0.2206 (value: 0.0012, weighted value: 0.0599, policy: 0.1607, weighted policy: 0.1607), Train Mean Max: 0.9327
Epoch 7/10, Train Loss: 0.2147 (value: 0.0011, weighted value: 0.0545, policy: 0.1603, weighted policy: 0.1603), Train Mean Max: 0.9331
Epoch 8/10, Train Loss: 0.2131 (value: 0.0011, weighted value: 0.0539, policy: 0.1592, weighted policy: 0.1592), Train Mean Max: 0.9332
Epoch 9/10, Train Loss: 0.2119 (value: 0.0011, weighted value: 0.0533, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9333
Epoch 10/10, Train Loss: 0.2086 (value: 0.0010, weighted value: 0.0500, policy: 0.1587, weighted policy: 0.1587), Train Mean Max: 0.9336
..training done in 73.29 seconds
..evaluation done in 16.60 seconds
Old network+MCTS average reward: 0.67, min: 0.06, max: 1.33, stdev: 0.21
New network+MCTS average reward: 0.67, min: 0.06, max: 1.31, stdev: 0.21
Old bare network average reward: 0.62, min: -0.01, max: 1.31, stdev: 0.22
New bare network average reward: 0.62, min: 0.06, max: 1.31, stdev: 0.22
External policy "random" average reward: 0.26, min: -0.40, max: 0.97, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: -0.14, max: 1.40, stdev: 0.21
External policy "total greedy" average reward: 0.65, min: -0.06, max: 1.29, stdev: 0.21
New network won 82 and tied 134 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 166 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.84 seconds
Training examples lengths: [64876, 64933, 64854, 65016, 64573, 64763, 64958, 64857, 64579, 64831]
Total value: 436424.44
Training on 648240 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2915 (value: 0.0021, weighted value: 0.1051, policy: 0.1864, weighted policy: 0.1864), Train Mean Max: 0.9294
Epoch 2/10, Train Loss: 0.2643 (value: 0.0018, weighted value: 0.0889, policy: 0.1754, weighted policy: 0.1754), Train Mean Max: 0.9304
Epoch 3/10, Train Loss: 0.2528 (value: 0.0016, weighted value: 0.0820, policy: 0.1707, weighted policy: 0.1707), Train Mean Max: 0.9306
Epoch 4/10, Train Loss: 0.2427 (value: 0.0015, weighted value: 0.0751, policy: 0.1676, weighted policy: 0.1676), Train Mean Max: 0.9311
Epoch 5/10, Train Loss: 0.2367 (value: 0.0014, weighted value: 0.0708, policy: 0.1659, weighted policy: 0.1659), Train Mean Max: 0.9312
Epoch 6/10, Train Loss: 0.2305 (value: 0.0013, weighted value: 0.0669, policy: 0.1636, weighted policy: 0.1636), Train Mean Max: 0.9316
Epoch 7/10, Train Loss: 0.2267 (value: 0.0013, weighted value: 0.0635, policy: 0.1632, weighted policy: 0.1632), Train Mean Max: 0.9318
Epoch 8/10, Train Loss: 0.2224 (value: 0.0012, weighted value: 0.0605, policy: 0.1619, weighted policy: 0.1619), Train Mean Max: 0.9321
Epoch 9/10, Train Loss: 0.2204 (value: 0.0012, weighted value: 0.0594, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9322
Epoch 10/10, Train Loss: 0.2181 (value: 0.0011, weighted value: 0.0560, policy: 0.1621, weighted policy: 0.1621), Train Mean Max: 0.9326
..training done in 65.49 seconds
..evaluation done in 16.77 seconds
Old network+MCTS average reward: 0.68, min: 0.07, max: 1.32, stdev: 0.22
New network+MCTS average reward: 0.68, min: 0.07, max: 1.29, stdev: 0.22
Old bare network average reward: 0.62, min: -0.04, max: 1.28, stdev: 0.24
New bare network average reward: 0.63, min: -0.04, max: 1.32, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.31, max: 0.98, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: 0.00, max: 1.26, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.11, max: 1.23, stdev: 0.21
New network won 73 and tied 136 out of 300 games (47.00% wins where ties are half wins)
Reverting to the old network

Training iteration 167 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.88 seconds
Training examples lengths: [64933, 64854, 65016, 64573, 64763, 64958, 64857, 64579, 64831, 64970]
Total value: 435899.12
Training on 648334 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3189 (value: 0.0025, weighted value: 0.1240, policy: 0.1949, weighted policy: 0.1949), Train Mean Max: 0.9280
Epoch 2/10, Train Loss: 0.2837 (value: 0.0021, weighted value: 0.1026, policy: 0.1811, weighted policy: 0.1811), Train Mean Max: 0.9293
Epoch 3/10, Train Loss: 0.2653 (value: 0.0018, weighted value: 0.0906, policy: 0.1746, weighted policy: 0.1746), Train Mean Max: 0.9297
Epoch 4/10, Train Loss: 0.2555 (value: 0.0017, weighted value: 0.0845, policy: 0.1709, weighted policy: 0.1709), Train Mean Max: 0.9300
Epoch 5/10, Train Loss: 0.2478 (value: 0.0016, weighted value: 0.0793, policy: 0.1685, weighted policy: 0.1685), Train Mean Max: 0.9304
Epoch 6/10, Train Loss: 0.2411 (value: 0.0015, weighted value: 0.0746, policy: 0.1665, weighted policy: 0.1665), Train Mean Max: 0.9305
Epoch 7/10, Train Loss: 0.2339 (value: 0.0014, weighted value: 0.0681, policy: 0.1658, weighted policy: 0.1658), Train Mean Max: 0.9308
Epoch 8/10, Train Loss: 0.2324 (value: 0.0014, weighted value: 0.0675, policy: 0.1649, weighted policy: 0.1649), Train Mean Max: 0.9310
Epoch 9/10, Train Loss: 0.2274 (value: 0.0013, weighted value: 0.0634, policy: 0.1640, weighted policy: 0.1640), Train Mean Max: 0.9313
Epoch 10/10, Train Loss: 0.2256 (value: 0.0012, weighted value: 0.0623, policy: 0.1633, weighted policy: 0.1633), Train Mean Max: 0.9316
..training done in 71.34 seconds
..evaluation done in 17.33 seconds
Old network+MCTS average reward: 0.65, min: 0.14, max: 1.39, stdev: 0.22
New network+MCTS average reward: 0.66, min: 0.15, max: 1.39, stdev: 0.22
Old bare network average reward: 0.60, min: 0.02, max: 1.39, stdev: 0.22
New bare network average reward: 0.61, min: 0.01, max: 1.37, stdev: 0.22
External policy "random" average reward: 0.27, min: -0.31, max: 0.97, stdev: 0.21
External policy "individual greedy" average reward: 0.53, min: -0.01, max: 1.22, stdev: 0.21
External policy "total greedy" average reward: 0.64, min: 0.18, max: 1.24, stdev: 0.21
New network won 82 and tied 156 out of 300 games (53.33% wins where ties are half wins)
Keeping the new network

Training iteration 168 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.02 seconds
Training examples lengths: [64854, 65016, 64573, 64763, 64958, 64857, 64579, 64831, 64970, 64781]
Total value: 436018.61
Training on 648182 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2516 (value: 0.0016, weighted value: 0.0791, policy: 0.1724, weighted policy: 0.1724), Train Mean Max: 0.9301
Epoch 2/10, Train Loss: 0.2395 (value: 0.0014, weighted value: 0.0710, policy: 0.1685, weighted policy: 0.1685), Train Mean Max: 0.9306
Epoch 3/10, Train Loss: 0.2306 (value: 0.0013, weighted value: 0.0662, policy: 0.1644, weighted policy: 0.1644), Train Mean Max: 0.9313
Epoch 4/10, Train Loss: 0.2279 (value: 0.0013, weighted value: 0.0636, policy: 0.1642, weighted policy: 0.1642), Train Mean Max: 0.9311
Epoch 5/10, Train Loss: 0.2234 (value: 0.0012, weighted value: 0.0604, policy: 0.1630, weighted policy: 0.1630), Train Mean Max: 0.9316
Epoch 6/10, Train Loss: 0.2209 (value: 0.0012, weighted value: 0.0584, policy: 0.1625, weighted policy: 0.1625), Train Mean Max: 0.9318
Epoch 7/10, Train Loss: 0.2191 (value: 0.0011, weighted value: 0.0573, policy: 0.1617, weighted policy: 0.1617), Train Mean Max: 0.9320
Epoch 8/10, Train Loss: 0.2157 (value: 0.0011, weighted value: 0.0544, policy: 0.1613, weighted policy: 0.1613), Train Mean Max: 0.9322
Epoch 9/10, Train Loss: 0.2127 (value: 0.0011, weighted value: 0.0533, policy: 0.1594, weighted policy: 0.1594), Train Mean Max: 0.9326
Epoch 10/10, Train Loss: 0.2124 (value: 0.0011, weighted value: 0.0526, policy: 0.1598, weighted policy: 0.1598), Train Mean Max: 0.9327
..training done in 71.37 seconds
..evaluation done in 17.69 seconds
Old network+MCTS average reward: 0.68, min: 0.08, max: 1.35, stdev: 0.23
New network+MCTS average reward: 0.68, min: 0.09, max: 1.29, stdev: 0.22
Old bare network average reward: 0.62, min: 0.03, max: 1.21, stdev: 0.23
New bare network average reward: 0.63, min: 0.00, max: 1.26, stdev: 0.23
External policy "random" average reward: 0.28, min: -0.44, max: 0.96, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: 0.00, max: 1.16, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.13, max: 1.42, stdev: 0.21
New network won 65 and tied 156 out of 300 games (47.67% wins where ties are half wins)
Reverting to the old network

Training iteration 169 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.54 seconds
Training examples lengths: [65016, 64573, 64763, 64958, 64857, 64579, 64831, 64970, 64781, 64985]
Total value: 436822.52
Training on 648313 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2776 (value: 0.0019, weighted value: 0.0963, policy: 0.1813, weighted policy: 0.1813), Train Mean Max: 0.9288
Epoch 2/10, Train Loss: 0.2590 (value: 0.0017, weighted value: 0.0846, policy: 0.1743, weighted policy: 0.1743), Train Mean Max: 0.9295
Epoch 3/10, Train Loss: 0.2475 (value: 0.0016, weighted value: 0.0776, policy: 0.1699, weighted policy: 0.1699), Train Mean Max: 0.9299
Epoch 4/10, Train Loss: 0.2412 (value: 0.0015, weighted value: 0.0730, policy: 0.1682, weighted policy: 0.1682), Train Mean Max: 0.9302
Epoch 5/10, Train Loss: 0.2374 (value: 0.0014, weighted value: 0.0707, policy: 0.1667, weighted policy: 0.1667), Train Mean Max: 0.9305
Epoch 6/10, Train Loss: 0.2303 (value: 0.0013, weighted value: 0.0654, policy: 0.1649, weighted policy: 0.1649), Train Mean Max: 0.9309
Epoch 7/10, Train Loss: 0.2281 (value: 0.0013, weighted value: 0.0627, policy: 0.1654, weighted policy: 0.1654), Train Mean Max: 0.9311
Epoch 8/10, Train Loss: 0.2245 (value: 0.0012, weighted value: 0.0614, policy: 0.1631, weighted policy: 0.1631), Train Mean Max: 0.9313
Epoch 9/10, Train Loss: 0.2215 (value: 0.0012, weighted value: 0.0585, policy: 0.1629, weighted policy: 0.1629), Train Mean Max: 0.9315
Epoch 10/10, Train Loss: 0.2176 (value: 0.0011, weighted value: 0.0566, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9318
..training done in 65.41 seconds
..evaluation done in 16.45 seconds
Old network+MCTS average reward: 0.64, min: 0.04, max: 1.23, stdev: 0.22
New network+MCTS average reward: 0.64, min: 0.08, max: 1.21, stdev: 0.22
Old bare network average reward: 0.58, min: 0.05, max: 1.23, stdev: 0.23
New bare network average reward: 0.58, min: -0.09, max: 1.23, stdev: 0.23
External policy "random" average reward: 0.23, min: -0.37, max: 0.85, stdev: 0.23
External policy "individual greedy" average reward: 0.52, min: -0.06, max: 1.19, stdev: 0.22
External policy "total greedy" average reward: 0.62, min: 0.02, max: 1.31, stdev: 0.22
New network won 89 and tied 140 out of 300 games (53.00% wins where ties are half wins)
Keeping the new network

Training iteration 170 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.71 seconds
Training examples lengths: [64573, 64763, 64958, 64857, 64579, 64831, 64970, 64781, 64985, 64943]
Total value: 437224.31
Training on 648240 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2458 (value: 0.0015, weighted value: 0.0757, policy: 0.1701, weighted policy: 0.1701), Train Mean Max: 0.9306
Epoch 2/10, Train Loss: 0.2351 (value: 0.0014, weighted value: 0.0675, policy: 0.1676, weighted policy: 0.1676), Train Mean Max: 0.9310
Epoch 3/10, Train Loss: 0.2301 (value: 0.0013, weighted value: 0.0646, policy: 0.1655, weighted policy: 0.1655), Train Mean Max: 0.9313
Epoch 4/10, Train Loss: 0.2250 (value: 0.0012, weighted value: 0.0616, policy: 0.1634, weighted policy: 0.1634), Train Mean Max: 0.9316
Epoch 5/10, Train Loss: 0.2220 (value: 0.0012, weighted value: 0.0591, policy: 0.1629, weighted policy: 0.1629), Train Mean Max: 0.9319
Epoch 6/10, Train Loss: 0.2176 (value: 0.0011, weighted value: 0.0562, policy: 0.1614, weighted policy: 0.1614), Train Mean Max: 0.9320
Epoch 7/10, Train Loss: 0.2165 (value: 0.0011, weighted value: 0.0559, policy: 0.1606, weighted policy: 0.1606), Train Mean Max: 0.9322
Epoch 8/10, Train Loss: 0.2119 (value: 0.0010, weighted value: 0.0517, policy: 0.1602, weighted policy: 0.1602), Train Mean Max: 0.9324
Epoch 9/10, Train Loss: 0.2112 (value: 0.0010, weighted value: 0.0521, policy: 0.1591, weighted policy: 0.1591), Train Mean Max: 0.9328
Epoch 10/10, Train Loss: 0.2108 (value: 0.0010, weighted value: 0.0507, policy: 0.1601, weighted policy: 0.1601), Train Mean Max: 0.9329
..training done in 64.59 seconds
..evaluation done in 16.62 seconds
Old network+MCTS average reward: 0.65, min: 0.02, max: 1.43, stdev: 0.23
New network+MCTS average reward: 0.65, min: 0.03, max: 1.43, stdev: 0.23
Old bare network average reward: 0.60, min: 0.02, max: 1.37, stdev: 0.23
New bare network average reward: 0.60, min: -0.03, max: 1.43, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.43, max: 0.75, stdev: 0.21
External policy "individual greedy" average reward: 0.53, min: 0.02, max: 1.29, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.12, max: 1.42, stdev: 0.22
New network won 78 and tied 147 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 171 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.67 seconds
Training examples lengths: [64763, 64958, 64857, 64579, 64831, 64970, 64781, 64985, 64943, 64948]
Total value: 438032.03
Training on 648615 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2377 (value: 0.0014, weighted value: 0.0690, policy: 0.1687, weighted policy: 0.1687), Train Mean Max: 0.9315
Epoch 2/10, Train Loss: 0.2273 (value: 0.0012, weighted value: 0.0615, policy: 0.1658, weighted policy: 0.1658), Train Mean Max: 0.9319
Epoch 3/10, Train Loss: 0.2212 (value: 0.0012, weighted value: 0.0592, policy: 0.1620, weighted policy: 0.1620), Train Mean Max: 0.9324
Epoch 4/10, Train Loss: 0.2177 (value: 0.0011, weighted value: 0.0569, policy: 0.1608, weighted policy: 0.1608), Train Mean Max: 0.9326
Epoch 5/10, Train Loss: 0.2141 (value: 0.0011, weighted value: 0.0538, policy: 0.1603, weighted policy: 0.1603), Train Mean Max: 0.9328
Epoch 6/10, Train Loss: 0.2107 (value: 0.0010, weighted value: 0.0514, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9330
Epoch 7/10, Train Loss: 0.2086 (value: 0.0010, weighted value: 0.0498, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9333
Epoch 8/10, Train Loss: 0.2084 (value: 0.0010, weighted value: 0.0499, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9333
Epoch 9/10, Train Loss: 0.2059 (value: 0.0010, weighted value: 0.0477, policy: 0.1582, weighted policy: 0.1582), Train Mean Max: 0.9336
Epoch 10/10, Train Loss: 0.2043 (value: 0.0009, weighted value: 0.0475, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9339
..training done in 67.43 seconds
..evaluation done in 16.13 seconds
Old network+MCTS average reward: 0.66, min: 0.14, max: 1.34, stdev: 0.23
New network+MCTS average reward: 0.66, min: 0.18, max: 1.34, stdev: 0.23
Old bare network average reward: 0.60, min: 0.05, max: 1.32, stdev: 0.24
New bare network average reward: 0.60, min: 0.06, max: 1.27, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.44, max: 0.95, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: -0.06, max: 1.42, stdev: 0.24
External policy "total greedy" average reward: 0.65, min: 0.13, max: 1.47, stdev: 0.24
New network won 66 and tied 166 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 172 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.11 seconds
Training examples lengths: [64958, 64857, 64579, 64831, 64970, 64781, 64985, 64943, 64948, 64834]
Total value: 438826.96
Training on 648686 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2689 (value: 0.0018, weighted value: 0.0900, policy: 0.1789, weighted policy: 0.1789), Train Mean Max: 0.9299
Epoch 2/10, Train Loss: 0.2467 (value: 0.0015, weighted value: 0.0754, policy: 0.1714, weighted policy: 0.1714), Train Mean Max: 0.9309
Epoch 3/10, Train Loss: 0.2373 (value: 0.0014, weighted value: 0.0706, policy: 0.1667, weighted policy: 0.1667), Train Mean Max: 0.9314
Epoch 4/10, Train Loss: 0.2294 (value: 0.0013, weighted value: 0.0661, policy: 0.1633, weighted policy: 0.1633), Train Mean Max: 0.9318
Epoch 5/10, Train Loss: 0.2253 (value: 0.0012, weighted value: 0.0623, policy: 0.1630, weighted policy: 0.1630), Train Mean Max: 0.9318
Epoch 6/10, Train Loss: 0.2213 (value: 0.0012, weighted value: 0.0597, policy: 0.1615, weighted policy: 0.1615), Train Mean Max: 0.9321
Epoch 7/10, Train Loss: 0.2191 (value: 0.0012, weighted value: 0.0582, policy: 0.1609, weighted policy: 0.1609), Train Mean Max: 0.9323
Epoch 8/10, Train Loss: 0.2171 (value: 0.0011, weighted value: 0.0567, policy: 0.1604, weighted policy: 0.1604), Train Mean Max: 0.9324
Epoch 9/10, Train Loss: 0.2118 (value: 0.0011, weighted value: 0.0529, policy: 0.1589, weighted policy: 0.1589), Train Mean Max: 0.9328
Epoch 10/10, Train Loss: 0.2128 (value: 0.0011, weighted value: 0.0533, policy: 0.1595, weighted policy: 0.1595), Train Mean Max: 0.9330
..training done in 67.78 seconds
..evaluation done in 16.22 seconds
Old network+MCTS average reward: 0.65, min: -0.06, max: 1.40, stdev: 0.24
New network+MCTS average reward: 0.65, min: 0.01, max: 1.56, stdev: 0.23
Old bare network average reward: 0.60, min: -0.07, max: 1.28, stdev: 0.24
New bare network average reward: 0.60, min: -0.06, max: 1.40, stdev: 0.24
External policy "random" average reward: 0.23, min: -0.34, max: 0.94, stdev: 0.23
External policy "individual greedy" average reward: 0.51, min: -0.19, max: 1.26, stdev: 0.23
External policy "total greedy" average reward: 0.63, min: 0.11, max: 1.48, stdev: 0.22
New network won 86 and tied 142 out of 300 games (52.33% wins where ties are half wins)
Keeping the new network

Training iteration 173 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.07 seconds
Training examples lengths: [64857, 64579, 64831, 64970, 64781, 64985, 64943, 64948, 64834, 64778]
Total value: 438999.73
Training on 648506 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2371 (value: 0.0014, weighted value: 0.0686, policy: 0.1685, weighted policy: 0.1685), Train Mean Max: 0.9317
Epoch 2/10, Train Loss: 0.2271 (value: 0.0013, weighted value: 0.0632, policy: 0.1639, weighted policy: 0.1639), Train Mean Max: 0.9323
Epoch 3/10, Train Loss: 0.2222 (value: 0.0012, weighted value: 0.0600, policy: 0.1622, weighted policy: 0.1622), Train Mean Max: 0.9326
Epoch 4/10, Train Loss: 0.2166 (value: 0.0011, weighted value: 0.0566, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9330
Epoch 5/10, Train Loss: 0.2134 (value: 0.0011, weighted value: 0.0546, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9331
Epoch 6/10, Train Loss: 0.2119 (value: 0.0011, weighted value: 0.0534, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9334
Epoch 7/10, Train Loss: 0.2096 (value: 0.0010, weighted value: 0.0513, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9335
Epoch 8/10, Train Loss: 0.2071 (value: 0.0010, weighted value: 0.0502, policy: 0.1569, weighted policy: 0.1569), Train Mean Max: 0.9339
Epoch 9/10, Train Loss: 0.2047 (value: 0.0010, weighted value: 0.0477, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9339
Epoch 10/10, Train Loss: 0.2031 (value: 0.0009, weighted value: 0.0472, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9343
..training done in 71.82 seconds
..evaluation done in 17.14 seconds
Old network+MCTS average reward: 0.64, min: 0.04, max: 1.37, stdev: 0.23
New network+MCTS average reward: 0.64, min: 0.03, max: 1.37, stdev: 0.23
Old bare network average reward: 0.59, min: 0.06, max: 1.37, stdev: 0.23
New bare network average reward: 0.59, min: -0.03, max: 1.34, stdev: 0.24
External policy "random" average reward: 0.24, min: -0.40, max: 0.89, stdev: 0.22
External policy "individual greedy" average reward: 0.52, min: -0.10, max: 1.11, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: 0.00, max: 1.25, stdev: 0.22
New network won 72 and tied 140 out of 300 games (47.33% wins where ties are half wins)
Reverting to the old network

Training iteration 174 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.55 seconds
Training examples lengths: [64579, 64831, 64970, 64781, 64985, 64943, 64948, 64834, 64778, 64642]
Total value: 439234.51
Training on 648291 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2652 (value: 0.0018, weighted value: 0.0878, policy: 0.1774, weighted policy: 0.1774), Train Mean Max: 0.9303
Epoch 2/10, Train Loss: 0.2474 (value: 0.0015, weighted value: 0.0768, policy: 0.1706, weighted policy: 0.1706), Train Mean Max: 0.9312
Epoch 3/10, Train Loss: 0.2377 (value: 0.0014, weighted value: 0.0714, policy: 0.1663, weighted policy: 0.1663), Train Mean Max: 0.9317
Epoch 4/10, Train Loss: 0.2298 (value: 0.0013, weighted value: 0.0670, policy: 0.1628, weighted policy: 0.1628), Train Mean Max: 0.9321
Epoch 5/10, Train Loss: 0.2244 (value: 0.0012, weighted value: 0.0623, policy: 0.1621, weighted policy: 0.1621), Train Mean Max: 0.9322
Epoch 6/10, Train Loss: 0.2220 (value: 0.0012, weighted value: 0.0605, policy: 0.1615, weighted policy: 0.1615), Train Mean Max: 0.9324
Epoch 7/10, Train Loss: 0.2180 (value: 0.0012, weighted value: 0.0578, policy: 0.1602, weighted policy: 0.1602), Train Mean Max: 0.9327
Epoch 8/10, Train Loss: 0.2158 (value: 0.0011, weighted value: 0.0569, policy: 0.1589, weighted policy: 0.1589), Train Mean Max: 0.9330
Epoch 9/10, Train Loss: 0.2114 (value: 0.0011, weighted value: 0.0532, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9332
Epoch 10/10, Train Loss: 0.2103 (value: 0.0010, weighted value: 0.0518, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9334
..training done in 66.33 seconds
..evaluation done in 16.88 seconds
Old network+MCTS average reward: 0.69, min: 0.14, max: 1.41, stdev: 0.22
New network+MCTS average reward: 0.69, min: 0.06, max: 1.40, stdev: 0.22
Old bare network average reward: 0.64, min: 0.12, max: 1.41, stdev: 0.23
New bare network average reward: 0.64, min: 0.11, max: 1.40, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.39, max: 1.06, stdev: 0.22
External policy "individual greedy" average reward: 0.56, min: -0.01, max: 1.14, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.05, max: 1.40, stdev: 0.23
New network won 86 and tied 134 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 175 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.35 seconds
Training examples lengths: [64831, 64970, 64781, 64985, 64943, 64948, 64834, 64778, 64642, 64922]
Total value: 440086.47
Training on 648634 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2397 (value: 0.0014, weighted value: 0.0701, policy: 0.1695, weighted policy: 0.1695), Train Mean Max: 0.9318
Epoch 2/10, Train Loss: 0.2303 (value: 0.0013, weighted value: 0.0646, policy: 0.1657, weighted policy: 0.1657), Train Mean Max: 0.9323
Epoch 3/10, Train Loss: 0.2226 (value: 0.0012, weighted value: 0.0598, policy: 0.1628, weighted policy: 0.1628), Train Mean Max: 0.9327
Epoch 4/10, Train Loss: 0.2182 (value: 0.0012, weighted value: 0.0577, policy: 0.1605, weighted policy: 0.1605), Train Mean Max: 0.9330
Epoch 5/10, Train Loss: 0.2131 (value: 0.0011, weighted value: 0.0549, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9334
Epoch 6/10, Train Loss: 0.2111 (value: 0.0010, weighted value: 0.0524, policy: 0.1587, weighted policy: 0.1587), Train Mean Max: 0.9334
Epoch 7/10, Train Loss: 0.2093 (value: 0.0010, weighted value: 0.0519, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9337
Epoch 8/10, Train Loss: 0.2072 (value: 0.0010, weighted value: 0.0494, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9338
Epoch 9/10, Train Loss: 0.2060 (value: 0.0010, weighted value: 0.0489, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9340
Epoch 10/10, Train Loss: 0.2034 (value: 0.0009, weighted value: 0.0464, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9342
..training done in 73.11 seconds
..evaluation done in 16.83 seconds
Old network+MCTS average reward: 0.66, min: 0.08, max: 1.24, stdev: 0.22
New network+MCTS average reward: 0.67, min: 0.11, max: 1.24, stdev: 0.22
Old bare network average reward: 0.61, min: 0.06, max: 1.21, stdev: 0.23
New bare network average reward: 0.61, min: -0.01, max: 1.20, stdev: 0.23
External policy "random" average reward: 0.24, min: -0.25, max: 0.87, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: -0.06, max: 1.24, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.20, max: 1.31, stdev: 0.22
New network won 78 and tied 167 out of 300 games (53.83% wins where ties are half wins)
Keeping the new network

Training iteration 176 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.62 seconds
Training examples lengths: [64970, 64781, 64985, 64943, 64948, 64834, 64778, 64642, 64922, 64790]
Total value: 440262.81
Training on 648593 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2338 (value: 0.0013, weighted value: 0.0674, policy: 0.1664, weighted policy: 0.1664), Train Mean Max: 0.9327
Epoch 2/10, Train Loss: 0.2228 (value: 0.0012, weighted value: 0.0594, policy: 0.1633, weighted policy: 0.1633), Train Mean Max: 0.9332
Epoch 3/10, Train Loss: 0.2163 (value: 0.0011, weighted value: 0.0561, policy: 0.1602, weighted policy: 0.1602), Train Mean Max: 0.9336
Epoch 4/10, Train Loss: 0.2120 (value: 0.0011, weighted value: 0.0537, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9339
Epoch 5/10, Train Loss: 0.2111 (value: 0.0010, weighted value: 0.0523, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9338
Epoch 6/10, Train Loss: 0.2073 (value: 0.0010, weighted value: 0.0497, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9342
Epoch 7/10, Train Loss: 0.2055 (value: 0.0010, weighted value: 0.0489, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9343
Epoch 8/10, Train Loss: 0.2028 (value: 0.0009, weighted value: 0.0467, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9346
Epoch 9/10, Train Loss: 0.2023 (value: 0.0009, weighted value: 0.0462, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9346
Epoch 10/10, Train Loss: 0.1995 (value: 0.0009, weighted value: 0.0441, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9349
..training done in 65.50 seconds
..evaluation done in 16.67 seconds
Old network+MCTS average reward: 0.67, min: -0.04, max: 1.19, stdev: 0.21
New network+MCTS average reward: 0.67, min: 0.00, max: 1.20, stdev: 0.21
Old bare network average reward: 0.62, min: -0.07, max: 1.25, stdev: 0.23
New bare network average reward: 0.61, min: -0.08, max: 1.25, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.36, max: 0.78, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: -0.18, max: 1.19, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: -0.07, max: 1.28, stdev: 0.21
New network won 75 and tied 148 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 177 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.77 seconds
Training examples lengths: [64781, 64985, 64943, 64948, 64834, 64778, 64642, 64922, 64790, 64900]
Total value: 440696.76
Training on 648523 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2614 (value: 0.0017, weighted value: 0.0856, policy: 0.1757, weighted policy: 0.1757), Train Mean Max: 0.9311
Epoch 2/10, Train Loss: 0.2432 (value: 0.0015, weighted value: 0.0742, policy: 0.1690, weighted policy: 0.1690), Train Mean Max: 0.9319
Epoch 3/10, Train Loss: 0.2326 (value: 0.0014, weighted value: 0.0680, policy: 0.1646, weighted policy: 0.1646), Train Mean Max: 0.9324
Epoch 4/10, Train Loss: 0.2258 (value: 0.0013, weighted value: 0.0634, policy: 0.1624, weighted policy: 0.1624), Train Mean Max: 0.9326
Epoch 5/10, Train Loss: 0.2219 (value: 0.0012, weighted value: 0.0608, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9329
Epoch 6/10, Train Loss: 0.2177 (value: 0.0012, weighted value: 0.0577, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9332
Epoch 7/10, Train Loss: 0.2146 (value: 0.0011, weighted value: 0.0551, policy: 0.1595, weighted policy: 0.1595), Train Mean Max: 0.9334
Epoch 8/10, Train Loss: 0.2134 (value: 0.0011, weighted value: 0.0545, policy: 0.1589, weighted policy: 0.1589), Train Mean Max: 0.9336
Epoch 9/10, Train Loss: 0.2094 (value: 0.0010, weighted value: 0.0522, policy: 0.1572, weighted policy: 0.1572), Train Mean Max: 0.9339
Epoch 10/10, Train Loss: 0.2062 (value: 0.0010, weighted value: 0.0497, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9340
..training done in 65.64 seconds
..evaluation done in 16.09 seconds
Old network+MCTS average reward: 0.68, min: 0.03, max: 1.45, stdev: 0.22
New network+MCTS average reward: 0.68, min: 0.06, max: 1.50, stdev: 0.22
Old bare network average reward: 0.63, min: -0.05, max: 1.42, stdev: 0.23
New bare network average reward: 0.63, min: -0.06, max: 1.42, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.44, max: 0.89, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: -0.04, max: 1.40, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: 0.02, max: 1.42, stdev: 0.22
New network won 80 and tied 134 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 178 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.08 seconds
Training examples lengths: [64985, 64943, 64948, 64834, 64778, 64642, 64922, 64790, 64900, 64891]
Total value: 441151.33
Training on 648633 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2861 (value: 0.0021, weighted value: 0.1034, policy: 0.1827, weighted policy: 0.1827), Train Mean Max: 0.9299
Epoch 2/10, Train Loss: 0.2624 (value: 0.0018, weighted value: 0.0884, policy: 0.1740, weighted policy: 0.1740), Train Mean Max: 0.9308
Epoch 3/10, Train Loss: 0.2472 (value: 0.0016, weighted value: 0.0782, policy: 0.1690, weighted policy: 0.1690), Train Mean Max: 0.9313
Epoch 4/10, Train Loss: 0.2394 (value: 0.0015, weighted value: 0.0736, policy: 0.1659, weighted policy: 0.1659), Train Mean Max: 0.9316
Epoch 5/10, Train Loss: 0.2333 (value: 0.0014, weighted value: 0.0698, policy: 0.1635, weighted policy: 0.1635), Train Mean Max: 0.9320
Epoch 6/10, Train Loss: 0.2269 (value: 0.0013, weighted value: 0.0652, policy: 0.1617, weighted policy: 0.1617), Train Mean Max: 0.9323
Epoch 7/10, Train Loss: 0.2243 (value: 0.0013, weighted value: 0.0627, policy: 0.1616, weighted policy: 0.1616), Train Mean Max: 0.9323
Epoch 8/10, Train Loss: 0.2202 (value: 0.0012, weighted value: 0.0596, policy: 0.1606, weighted policy: 0.1606), Train Mean Max: 0.9327
Epoch 9/10, Train Loss: 0.2168 (value: 0.0011, weighted value: 0.0573, policy: 0.1594, weighted policy: 0.1594), Train Mean Max: 0.9329
Epoch 10/10, Train Loss: 0.2154 (value: 0.0011, weighted value: 0.0563, policy: 0.1590, weighted policy: 0.1590), Train Mean Max: 0.9331
..training done in 72.91 seconds
..evaluation done in 17.52 seconds
Old network+MCTS average reward: 0.67, min: 0.19, max: 1.44, stdev: 0.21
New network+MCTS average reward: 0.66, min: 0.19, max: 1.40, stdev: 0.21
Old bare network average reward: 0.61, min: 0.06, max: 1.44, stdev: 0.22
New bare network average reward: 0.61, min: 0.10, max: 1.35, stdev: 0.21
External policy "random" average reward: 0.26, min: -0.21, max: 1.06, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: 0.06, max: 1.25, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.14, max: 1.23, stdev: 0.21
New network won 78 and tied 143 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 179 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 48.91 seconds
Training examples lengths: [64943, 64948, 64834, 64778, 64642, 64922, 64790, 64900, 64891, 64880]
Total value: 440975.64
Training on 648528 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3132 (value: 0.0025, weighted value: 0.1239, policy: 0.1893, weighted policy: 0.1893), Train Mean Max: 0.9285
Epoch 2/10, Train Loss: 0.2796 (value: 0.0020, weighted value: 0.0995, policy: 0.1801, weighted policy: 0.1801), Train Mean Max: 0.9297
Epoch 3/10, Train Loss: 0.2630 (value: 0.0018, weighted value: 0.0904, policy: 0.1726, weighted policy: 0.1726), Train Mean Max: 0.9302
Epoch 4/10, Train Loss: 0.2531 (value: 0.0017, weighted value: 0.0832, policy: 0.1699, weighted policy: 0.1699), Train Mean Max: 0.9305
Epoch 5/10, Train Loss: 0.2441 (value: 0.0016, weighted value: 0.0776, policy: 0.1665, weighted policy: 0.1665), Train Mean Max: 0.9309
Epoch 6/10, Train Loss: 0.2374 (value: 0.0015, weighted value: 0.0725, policy: 0.1649, weighted policy: 0.1649), Train Mean Max: 0.9312
Epoch 7/10, Train Loss: 0.2316 (value: 0.0014, weighted value: 0.0685, policy: 0.1631, weighted policy: 0.1631), Train Mean Max: 0.9315
Epoch 8/10, Train Loss: 0.2275 (value: 0.0013, weighted value: 0.0656, policy: 0.1619, weighted policy: 0.1619), Train Mean Max: 0.9318
Epoch 9/10, Train Loss: 0.2251 (value: 0.0013, weighted value: 0.0629, policy: 0.1622, weighted policy: 0.1622), Train Mean Max: 0.9317
Epoch 10/10, Train Loss: 0.2202 (value: 0.0012, weighted value: 0.0591, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9323
..training done in 72.96 seconds
..evaluation done in 16.56 seconds
Old network+MCTS average reward: 0.65, min: 0.04, max: 1.37, stdev: 0.21
New network+MCTS average reward: 0.66, min: 0.06, max: 1.37, stdev: 0.21
Old bare network average reward: 0.61, min: 0.02, max: 1.28, stdev: 0.22
New bare network average reward: 0.59, min: -0.04, max: 1.37, stdev: 0.22
External policy "random" average reward: 0.25, min: -0.35, max: 1.07, stdev: 0.22
External policy "individual greedy" average reward: 0.52, min: -0.06, max: 1.31, stdev: 0.21
External policy "total greedy" average reward: 0.64, min: 0.04, max: 1.35, stdev: 0.21
New network won 92 and tied 139 out of 300 games (53.83% wins where ties are half wins)
Keeping the new network

Training iteration 180 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.88 seconds
Training examples lengths: [64948, 64834, 64778, 64642, 64922, 64790, 64900, 64891, 64880, 64758]
Total value: 440568.92
Training on 648343 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2462 (value: 0.0015, weighted value: 0.0757, policy: 0.1705, weighted policy: 0.1705), Train Mean Max: 0.9308
Epoch 2/10, Train Loss: 0.2361 (value: 0.0014, weighted value: 0.0702, policy: 0.1660, weighted policy: 0.1660), Train Mean Max: 0.9315
Epoch 3/10, Train Loss: 0.2287 (value: 0.0013, weighted value: 0.0655, policy: 0.1633, weighted policy: 0.1633), Train Mean Max: 0.9318
Epoch 4/10, Train Loss: 0.2246 (value: 0.0013, weighted value: 0.0625, policy: 0.1621, weighted policy: 0.1621), Train Mean Max: 0.9321
Epoch 5/10, Train Loss: 0.2206 (value: 0.0012, weighted value: 0.0598, policy: 0.1608, weighted policy: 0.1608), Train Mean Max: 0.9323
Epoch 6/10, Train Loss: 0.2160 (value: 0.0011, weighted value: 0.0558, policy: 0.1602, weighted policy: 0.1602), Train Mean Max: 0.9326
Epoch 7/10, Train Loss: 0.2150 (value: 0.0011, weighted value: 0.0555, policy: 0.1595, weighted policy: 0.1595), Train Mean Max: 0.9329
Epoch 8/10, Train Loss: 0.2120 (value: 0.0011, weighted value: 0.0539, policy: 0.1580, weighted policy: 0.1580), Train Mean Max: 0.9331
Epoch 9/10, Train Loss: 0.2095 (value: 0.0010, weighted value: 0.0521, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9333
Epoch 10/10, Train Loss: 0.2092 (value: 0.0010, weighted value: 0.0511, policy: 0.1581, weighted policy: 0.1581), Train Mean Max: 0.9334
..training done in 67.47 seconds
..evaluation done in 16.64 seconds
Old network+MCTS average reward: 0.67, min: 0.07, max: 1.38, stdev: 0.22
New network+MCTS average reward: 0.68, min: -0.01, max: 1.38, stdev: 0.22
Old bare network average reward: 0.63, min: 0.01, max: 1.37, stdev: 0.23
New bare network average reward: 0.63, min: -0.05, max: 1.38, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.28, max: 1.05, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: 0.04, max: 1.36, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.08, max: 1.40, stdev: 0.21
New network won 76 and tied 162 out of 300 games (52.33% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to zgaz_checkpoints/1757654225_iter_180

Training iteration 181 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.06 seconds
Training examples lengths: [64834, 64778, 64642, 64922, 64790, 64900, 64891, 64880, 64758, 65069]
Total value: 440854.25
Training on 648464 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2353 (value: 0.0014, weighted value: 0.0685, policy: 0.1668, weighted policy: 0.1668), Train Mean Max: 0.9322
Epoch 2/10, Train Loss: 0.2253 (value: 0.0012, weighted value: 0.0617, policy: 0.1635, weighted policy: 0.1635), Train Mean Max: 0.9327
Epoch 3/10, Train Loss: 0.2185 (value: 0.0012, weighted value: 0.0583, policy: 0.1602, weighted policy: 0.1602), Train Mean Max: 0.9331
Epoch 4/10, Train Loss: 0.2153 (value: 0.0011, weighted value: 0.0563, policy: 0.1590, weighted policy: 0.1590), Train Mean Max: 0.9334
Epoch 5/10, Train Loss: 0.2115 (value: 0.0011, weighted value: 0.0533, policy: 0.1581, weighted policy: 0.1581), Train Mean Max: 0.9335
Epoch 6/10, Train Loss: 0.2095 (value: 0.0010, weighted value: 0.0519, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9338
Epoch 7/10, Train Loss: 0.2073 (value: 0.0010, weighted value: 0.0510, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9339
Epoch 8/10, Train Loss: 0.2049 (value: 0.0010, weighted value: 0.0482, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9341
Epoch 9/10, Train Loss: 0.2046 (value: 0.0010, weighted value: 0.0484, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9344
Epoch 10/10, Train Loss: 0.2006 (value: 0.0009, weighted value: 0.0457, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9346
..training done in 65.97 seconds
..evaluation done in 17.19 seconds
Old network+MCTS average reward: 0.68, min: 0.07, max: 1.50, stdev: 0.23
New network+MCTS average reward: 0.68, min: 0.09, max: 1.41, stdev: 0.23
Old bare network average reward: 0.62, min: 0.02, max: 1.36, stdev: 0.24
New bare network average reward: 0.62, min: 0.02, max: 1.40, stdev: 0.24
External policy "random" average reward: 0.28, min: -0.25, max: 1.06, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: 0.03, max: 1.26, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: 0.18, max: 1.31, stdev: 0.22
New network won 78 and tied 145 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 182 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.67 seconds
Training examples lengths: [64778, 64642, 64922, 64790, 64900, 64891, 64880, 64758, 65069, 64594]
Total value: 440306.94
Training on 648224 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2296 (value: 0.0013, weighted value: 0.0644, policy: 0.1651, weighted policy: 0.1651), Train Mean Max: 0.9330
Epoch 2/10, Train Loss: 0.2215 (value: 0.0012, weighted value: 0.0593, policy: 0.1622, weighted policy: 0.1622), Train Mean Max: 0.9335
Epoch 3/10, Train Loss: 0.2138 (value: 0.0011, weighted value: 0.0549, policy: 0.1589, weighted policy: 0.1589), Train Mean Max: 0.9338
Epoch 4/10, Train Loss: 0.2091 (value: 0.0010, weighted value: 0.0518, policy: 0.1573, weighted policy: 0.1573), Train Mean Max: 0.9342
Epoch 5/10, Train Loss: 0.2073 (value: 0.0010, weighted value: 0.0510, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9344
Epoch 6/10, Train Loss: 0.2051 (value: 0.0010, weighted value: 0.0491, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9345
Epoch 7/10, Train Loss: 0.2028 (value: 0.0009, weighted value: 0.0474, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9347
Epoch 8/10, Train Loss: 0.2020 (value: 0.0009, weighted value: 0.0467, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9349
Epoch 9/10, Train Loss: 0.2001 (value: 0.0009, weighted value: 0.0459, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9350
Epoch 10/10, Train Loss: 0.1993 (value: 0.0009, weighted value: 0.0453, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9353
..training done in 65.31 seconds
..evaluation done in 16.44 seconds
Old network+MCTS average reward: 0.67, min: 0.12, max: 1.47, stdev: 0.23
New network+MCTS average reward: 0.66, min: 0.12, max: 1.47, stdev: 0.23
Old bare network average reward: 0.61, min: -0.16, max: 1.37, stdev: 0.23
New bare network average reward: 0.62, min: 0.03, max: 1.45, stdev: 0.24
External policy "random" average reward: 0.24, min: -0.41, max: 1.07, stdev: 0.23
External policy "individual greedy" average reward: 0.52, min: 0.04, max: 1.55, stdev: 0.24
External policy "total greedy" average reward: 0.63, min: 0.12, max: 1.46, stdev: 0.23
New network won 75 and tied 143 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 183 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.15 seconds
Training examples lengths: [64642, 64922, 64790, 64900, 64891, 64880, 64758, 65069, 64594, 64777]
Total value: 441102.65
Training on 648223 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2595 (value: 0.0017, weighted value: 0.0852, policy: 0.1742, weighted policy: 0.1742), Train Mean Max: 0.9316
Epoch 2/10, Train Loss: 0.2391 (value: 0.0014, weighted value: 0.0724, policy: 0.1667, weighted policy: 0.1667), Train Mean Max: 0.9324
Epoch 3/10, Train Loss: 0.2305 (value: 0.0013, weighted value: 0.0672, policy: 0.1633, weighted policy: 0.1633), Train Mean Max: 0.9328
Epoch 4/10, Train Loss: 0.2247 (value: 0.0013, weighted value: 0.0635, policy: 0.1612, weighted policy: 0.1612), Train Mean Max: 0.9330
Epoch 5/10, Train Loss: 0.2200 (value: 0.0012, weighted value: 0.0601, policy: 0.1598, weighted policy: 0.1598), Train Mean Max: 0.9332
Epoch 6/10, Train Loss: 0.2151 (value: 0.0011, weighted value: 0.0572, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9335
Epoch 7/10, Train Loss: 0.2121 (value: 0.0011, weighted value: 0.0546, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9338
Epoch 8/10, Train Loss: 0.2113 (value: 0.0011, weighted value: 0.0543, policy: 0.1569, weighted policy: 0.1569), Train Mean Max: 0.9339
Epoch 9/10, Train Loss: 0.2083 (value: 0.0010, weighted value: 0.0510, policy: 0.1573, weighted policy: 0.1573), Train Mean Max: 0.9341
Epoch 10/10, Train Loss: 0.2067 (value: 0.0010, weighted value: 0.0503, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9342
..training done in 65.56 seconds
..evaluation done in 17.76 seconds
Old network+MCTS average reward: 0.65, min: 0.08, max: 1.31, stdev: 0.24
New network+MCTS average reward: 0.66, min: 0.13, max: 1.33, stdev: 0.25
Old bare network average reward: 0.60, min: 0.06, max: 1.31, stdev: 0.24
New bare network average reward: 0.61, min: 0.01, max: 1.31, stdev: 0.24
External policy "random" average reward: 0.25, min: -0.31, max: 0.88, stdev: 0.23
External policy "individual greedy" average reward: 0.52, min: -0.03, max: 1.24, stdev: 0.24
External policy "total greedy" average reward: 0.63, min: 0.08, max: 1.34, stdev: 0.24
New network won 80 and tied 150 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 184 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 48.09 seconds
Training examples lengths: [64922, 64790, 64900, 64891, 64880, 64758, 65069, 64594, 64777, 64946]
Total value: 441955.73
Training on 648527 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2353 (value: 0.0014, weighted value: 0.0690, policy: 0.1663, weighted policy: 0.1663), Train Mean Max: 0.9328
Epoch 2/10, Train Loss: 0.2235 (value: 0.0012, weighted value: 0.0615, policy: 0.1619, weighted policy: 0.1619), Train Mean Max: 0.9333
Epoch 3/10, Train Loss: 0.2182 (value: 0.0012, weighted value: 0.0587, policy: 0.1595, weighted policy: 0.1595), Train Mean Max: 0.9335
Epoch 4/10, Train Loss: 0.2126 (value: 0.0011, weighted value: 0.0548, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9341
Epoch 5/10, Train Loss: 0.2124 (value: 0.0011, weighted value: 0.0547, policy: 0.1577, weighted policy: 0.1577), Train Mean Max: 0.9341
Epoch 6/10, Train Loss: 0.2066 (value: 0.0010, weighted value: 0.0503, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9343
Epoch 7/10, Train Loss: 0.2081 (value: 0.0010, weighted value: 0.0521, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9345
Epoch 8/10, Train Loss: 0.2040 (value: 0.0010, weighted value: 0.0481, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9347
Epoch 9/10, Train Loss: 0.2010 (value: 0.0009, weighted value: 0.0464, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9349
Epoch 10/10, Train Loss: 0.2008 (value: 0.0009, weighted value: 0.0468, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9352
..training done in 71.86 seconds
..evaluation done in 16.75 seconds
Old network+MCTS average reward: 0.66, min: 0.09, max: 1.49, stdev: 0.22
New network+MCTS average reward: 0.66, min: 0.08, max: 1.49, stdev: 0.23
Old bare network average reward: 0.61, min: 0.09, max: 1.49, stdev: 0.23
New bare network average reward: 0.61, min: 0.08, max: 1.49, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.23, max: 0.99, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: 0.00, max: 1.36, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.14, max: 1.41, stdev: 0.23
New network won 72 and tied 138 out of 300 games (47.00% wins where ties are half wins)
Reverting to the old network

Training iteration 185 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.75 seconds
Training examples lengths: [64790, 64900, 64891, 64880, 64758, 65069, 64594, 64777, 64946, 64975]
Total value: 441937.64
Training on 648580 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2619 (value: 0.0017, weighted value: 0.0874, policy: 0.1745, weighted policy: 0.1745), Train Mean Max: 0.9313
Epoch 2/10, Train Loss: 0.2444 (value: 0.0015, weighted value: 0.0752, policy: 0.1691, weighted policy: 0.1691), Train Mean Max: 0.9321
Epoch 3/10, Train Loss: 0.2340 (value: 0.0014, weighted value: 0.0702, policy: 0.1637, weighted policy: 0.1637), Train Mean Max: 0.9326
Epoch 4/10, Train Loss: 0.2255 (value: 0.0013, weighted value: 0.0649, policy: 0.1606, weighted policy: 0.1606), Train Mean Max: 0.9330
Epoch 5/10, Train Loss: 0.2220 (value: 0.0012, weighted value: 0.0624, policy: 0.1596, weighted policy: 0.1596), Train Mean Max: 0.9331
Epoch 6/10, Train Loss: 0.2176 (value: 0.0012, weighted value: 0.0592, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9334
Epoch 7/10, Train Loss: 0.2150 (value: 0.0011, weighted value: 0.0574, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9335
Epoch 8/10, Train Loss: 0.2120 (value: 0.0011, weighted value: 0.0542, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9338
Epoch 9/10, Train Loss: 0.2088 (value: 0.0010, weighted value: 0.0516, policy: 0.1572, weighted policy: 0.1572), Train Mean Max: 0.9340
Epoch 10/10, Train Loss: 0.2090 (value: 0.0010, weighted value: 0.0523, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9343
..training done in 67.29 seconds
..evaluation done in 16.79 seconds
Old network+MCTS average reward: 0.67, min: 0.08, max: 1.36, stdev: 0.24
New network+MCTS average reward: 0.67, min: 0.01, max: 1.36, stdev: 0.23
Old bare network average reward: 0.62, min: -0.10, max: 1.36, stdev: 0.25
New bare network average reward: 0.62, min: -0.10, max: 1.36, stdev: 0.24
External policy "random" average reward: 0.24, min: -0.43, max: 0.87, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.12, max: 1.25, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.06, max: 1.34, stdev: 0.23
New network won 68 and tied 151 out of 300 games (47.83% wins where ties are half wins)
Reverting to the old network

Training iteration 186 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.48 seconds
Training examples lengths: [64900, 64891, 64880, 64758, 65069, 64594, 64777, 64946, 64975, 64980]
Total value: 442448.45
Training on 648770 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2883 (value: 0.0021, weighted value: 0.1051, policy: 0.1832, weighted policy: 0.1832), Train Mean Max: 0.9300
Epoch 2/10, Train Loss: 0.2612 (value: 0.0018, weighted value: 0.0885, policy: 0.1727, weighted policy: 0.1727), Train Mean Max: 0.9310
Epoch 3/10, Train Loss: 0.2481 (value: 0.0016, weighted value: 0.0801, policy: 0.1680, weighted policy: 0.1680), Train Mean Max: 0.9315
Epoch 4/10, Train Loss: 0.2379 (value: 0.0015, weighted value: 0.0739, policy: 0.1640, weighted policy: 0.1640), Train Mean Max: 0.9321
Epoch 5/10, Train Loss: 0.2343 (value: 0.0014, weighted value: 0.0716, policy: 0.1627, weighted policy: 0.1627), Train Mean Max: 0.9321
Epoch 6/10, Train Loss: 0.2264 (value: 0.0013, weighted value: 0.0655, policy: 0.1609, weighted policy: 0.1609), Train Mean Max: 0.9325
Epoch 7/10, Train Loss: 0.2227 (value: 0.0012, weighted value: 0.0617, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9328
Epoch 8/10, Train Loss: 0.2204 (value: 0.0012, weighted value: 0.0610, policy: 0.1594, weighted policy: 0.1594), Train Mean Max: 0.9330
Epoch 9/10, Train Loss: 0.2183 (value: 0.0012, weighted value: 0.0595, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9331
Epoch 10/10, Train Loss: 0.2127 (value: 0.0011, weighted value: 0.0556, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9336
..training done in 66.10 seconds
..evaluation done in 17.18 seconds
Old network+MCTS average reward: 0.64, min: 0.01, max: 1.30, stdev: 0.22
New network+MCTS average reward: 0.65, min: 0.11, max: 1.29, stdev: 0.22
Old bare network average reward: 0.59, min: -0.12, max: 1.25, stdev: 0.22
New bare network average reward: 0.59, min: -0.12, max: 1.25, stdev: 0.22
External policy "random" average reward: 0.24, min: -0.38, max: 0.85, stdev: 0.22
External policy "individual greedy" average reward: 0.52, min: -0.13, max: 1.33, stdev: 0.21
External policy "total greedy" average reward: 0.63, min: 0.04, max: 1.37, stdev: 0.21
New network won 84 and tied 137 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 187 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.18 seconds
Training examples lengths: [64891, 64880, 64758, 65069, 64594, 64777, 64946, 64975, 64980, 64782]
Total value: 442899.54
Training on 648652 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2412 (value: 0.0015, weighted value: 0.0742, policy: 0.1670, weighted policy: 0.1670), Train Mean Max: 0.9320
Epoch 2/10, Train Loss: 0.2302 (value: 0.0013, weighted value: 0.0666, policy: 0.1636, weighted policy: 0.1636), Train Mean Max: 0.9326
Epoch 3/10, Train Loss: 0.2247 (value: 0.0013, weighted value: 0.0632, policy: 0.1615, weighted policy: 0.1615), Train Mean Max: 0.9329
Epoch 4/10, Train Loss: 0.2181 (value: 0.0012, weighted value: 0.0595, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9333
Epoch 5/10, Train Loss: 0.2168 (value: 0.0012, weighted value: 0.0583, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9334
Epoch 6/10, Train Loss: 0.2131 (value: 0.0011, weighted value: 0.0550, policy: 0.1581, weighted policy: 0.1581), Train Mean Max: 0.9337
Epoch 7/10, Train Loss: 0.2096 (value: 0.0011, weighted value: 0.0528, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9340
Epoch 8/10, Train Loss: 0.2069 (value: 0.0010, weighted value: 0.0502, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9342
Epoch 9/10, Train Loss: 0.2067 (value: 0.0010, weighted value: 0.0506, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9343
Epoch 10/10, Train Loss: 0.2044 (value: 0.0010, weighted value: 0.0494, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9346
..training done in 72.11 seconds
..evaluation done in 16.62 seconds
Old network+MCTS average reward: 0.68, min: 0.08, max: 1.34, stdev: 0.23
New network+MCTS average reward: 0.67, min: 0.06, max: 1.34, stdev: 0.23
Old bare network average reward: 0.63, min: 0.03, max: 1.34, stdev: 0.23
New bare network average reward: 0.62, min: 0.03, max: 1.34, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.34, max: 0.99, stdev: 0.24
External policy "individual greedy" average reward: 0.54, min: -0.06, max: 1.22, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.03, max: 1.29, stdev: 0.23
New network won 73 and tied 146 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 188 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.85 seconds
Training examples lengths: [64880, 64758, 65069, 64594, 64777, 64946, 64975, 64980, 64782, 64729]
Total value: 442855.31
Training on 648490 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2685 (value: 0.0018, weighted value: 0.0917, policy: 0.1768, weighted policy: 0.1768), Train Mean Max: 0.9306
Epoch 2/10, Train Loss: 0.2492 (value: 0.0016, weighted value: 0.0792, policy: 0.1700, weighted policy: 0.1700), Train Mean Max: 0.9314
Epoch 3/10, Train Loss: 0.2386 (value: 0.0015, weighted value: 0.0734, policy: 0.1651, weighted policy: 0.1651), Train Mean Max: 0.9320
Epoch 4/10, Train Loss: 0.2335 (value: 0.0014, weighted value: 0.0706, policy: 0.1629, weighted policy: 0.1629), Train Mean Max: 0.9323
Epoch 5/10, Train Loss: 0.2255 (value: 0.0013, weighted value: 0.0647, policy: 0.1608, weighted policy: 0.1608), Train Mean Max: 0.9326
Epoch 6/10, Train Loss: 0.2240 (value: 0.0013, weighted value: 0.0630, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9328
Epoch 7/10, Train Loss: 0.2179 (value: 0.0012, weighted value: 0.0589, policy: 0.1590, weighted policy: 0.1590), Train Mean Max: 0.9331
Epoch 8/10, Train Loss: 0.2158 (value: 0.0012, weighted value: 0.0575, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9334
Epoch 9/10, Train Loss: 0.2145 (value: 0.0011, weighted value: 0.0564, policy: 0.1582, weighted policy: 0.1582), Train Mean Max: 0.9335
Epoch 10/10, Train Loss: 0.2110 (value: 0.0011, weighted value: 0.0535, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9337
..training done in 71.49 seconds
..evaluation done in 16.13 seconds
Old network+MCTS average reward: 0.67, min: 0.06, max: 1.38, stdev: 0.21
New network+MCTS average reward: 0.67, min: 0.10, max: 1.38, stdev: 0.21
Old bare network average reward: 0.62, min: -0.03, max: 1.38, stdev: 0.22
New bare network average reward: 0.62, min: -0.04, max: 1.38, stdev: 0.22
External policy "random" average reward: 0.26, min: -0.38, max: 0.88, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: 0.02, max: 1.12, stdev: 0.21
External policy "total greedy" average reward: 0.65, min: 0.08, max: 1.30, stdev: 0.21
New network won 80 and tied 130 out of 300 games (48.33% wins where ties are half wins)
Reverting to the old network

Training iteration 189 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.73 seconds
Training examples lengths: [64758, 65069, 64594, 64777, 64946, 64975, 64980, 64782, 64729, 65052]
Total value: 443308.17
Training on 648662 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2950 (value: 0.0022, weighted value: 0.1089, policy: 0.1862, weighted policy: 0.1862), Train Mean Max: 0.9292
Epoch 2/10, Train Loss: 0.2684 (value: 0.0019, weighted value: 0.0936, policy: 0.1747, weighted policy: 0.1747), Train Mean Max: 0.9304
Epoch 3/10, Train Loss: 0.2539 (value: 0.0017, weighted value: 0.0845, policy: 0.1694, weighted policy: 0.1694), Train Mean Max: 0.9309
Epoch 4/10, Train Loss: 0.2443 (value: 0.0016, weighted value: 0.0786, policy: 0.1657, weighted policy: 0.1657), Train Mean Max: 0.9312
Epoch 5/10, Train Loss: 0.2370 (value: 0.0015, weighted value: 0.0735, policy: 0.1635, weighted policy: 0.1635), Train Mean Max: 0.9314
Epoch 6/10, Train Loss: 0.2319 (value: 0.0014, weighted value: 0.0692, policy: 0.1627, weighted policy: 0.1627), Train Mean Max: 0.9317
Epoch 7/10, Train Loss: 0.2275 (value: 0.0013, weighted value: 0.0662, policy: 0.1612, weighted policy: 0.1612), Train Mean Max: 0.9321
Epoch 8/10, Train Loss: 0.2248 (value: 0.0013, weighted value: 0.0642, policy: 0.1606, weighted policy: 0.1606), Train Mean Max: 0.9323
Epoch 9/10, Train Loss: 0.2191 (value: 0.0012, weighted value: 0.0592, policy: 0.1599, weighted policy: 0.1599), Train Mean Max: 0.9325
Epoch 10/10, Train Loss: 0.2174 (value: 0.0012, weighted value: 0.0586, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9328
..training done in 67.80 seconds
..evaluation done in 17.37 seconds
Old network+MCTS average reward: 0.67, min: 0.17, max: 1.44, stdev: 0.23
New network+MCTS average reward: 0.67, min: 0.20, max: 1.44, stdev: 0.23
Old bare network average reward: 0.62, min: 0.12, max: 1.36, stdev: 0.22
New bare network average reward: 0.62, min: 0.06, max: 1.36, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.28, max: 0.75, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: 0.00, max: 1.42, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.14, max: 1.51, stdev: 0.22
New network won 79 and tied 152 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 190 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.46 seconds
Training examples lengths: [65069, 64594, 64777, 64946, 64975, 64980, 64782, 64729, 65052, 64676]
Total value: 443668.55
Training on 648580 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2440 (value: 0.0015, weighted value: 0.0757, policy: 0.1683, weighted policy: 0.1683), Train Mean Max: 0.9313
Epoch 2/10, Train Loss: 0.2346 (value: 0.0014, weighted value: 0.0703, policy: 0.1643, weighted policy: 0.1643), Train Mean Max: 0.9319
Epoch 3/10, Train Loss: 0.2256 (value: 0.0013, weighted value: 0.0636, policy: 0.1620, weighted policy: 0.1620), Train Mean Max: 0.9323
Epoch 4/10, Train Loss: 0.2213 (value: 0.0012, weighted value: 0.0614, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9326
Epoch 5/10, Train Loss: 0.2178 (value: 0.0012, weighted value: 0.0583, policy: 0.1595, weighted policy: 0.1595), Train Mean Max: 0.9328
Epoch 6/10, Train Loss: 0.2143 (value: 0.0011, weighted value: 0.0558, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9330
Epoch 7/10, Train Loss: 0.2140 (value: 0.0011, weighted value: 0.0558, policy: 0.1581, weighted policy: 0.1581), Train Mean Max: 0.9332
Epoch 8/10, Train Loss: 0.2100 (value: 0.0010, weighted value: 0.0522, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9335
Epoch 9/10, Train Loss: 0.2085 (value: 0.0010, weighted value: 0.0514, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9338
Epoch 10/10, Train Loss: 0.2064 (value: 0.0010, weighted value: 0.0502, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9340
..training done in 71.75 seconds
..evaluation done in 16.96 seconds
Old network+MCTS average reward: 0.69, min: 0.12, max: 1.40, stdev: 0.23
New network+MCTS average reward: 0.68, min: 0.12, max: 1.40, stdev: 0.23
Old bare network average reward: 0.63, min: 0.10, max: 1.40, stdev: 0.23
New bare network average reward: 0.63, min: 0.06, max: 1.40, stdev: 0.22
External policy "random" average reward: 0.27, min: -0.23, max: 0.87, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.19, max: 1.13, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.06, max: 1.28, stdev: 0.22
New network won 73 and tied 154 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 191 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.34 seconds
Training examples lengths: [64594, 64777, 64946, 64975, 64980, 64782, 64729, 65052, 64676, 64470]
Total value: 443143.58
Training on 647981 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2345 (value: 0.0014, weighted value: 0.0676, policy: 0.1669, weighted policy: 0.1669), Train Mean Max: 0.9322
Epoch 2/10, Train Loss: 0.2239 (value: 0.0012, weighted value: 0.0614, policy: 0.1625, weighted policy: 0.1625), Train Mean Max: 0.9330
Epoch 3/10, Train Loss: 0.2188 (value: 0.0012, weighted value: 0.0582, policy: 0.1606, weighted policy: 0.1606), Train Mean Max: 0.9332
Epoch 4/10, Train Loss: 0.2127 (value: 0.0011, weighted value: 0.0546, policy: 0.1581, weighted policy: 0.1581), Train Mean Max: 0.9336
Epoch 5/10, Train Loss: 0.2121 (value: 0.0011, weighted value: 0.0535, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9337
Epoch 6/10, Train Loss: 0.2083 (value: 0.0010, weighted value: 0.0518, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9340
Epoch 7/10, Train Loss: 0.2059 (value: 0.0010, weighted value: 0.0498, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9340
Epoch 8/10, Train Loss: 0.2043 (value: 0.0010, weighted value: 0.0484, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9343
Epoch 9/10, Train Loss: 0.2042 (value: 0.0010, weighted value: 0.0483, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9343
Epoch 10/10, Train Loss: 0.2014 (value: 0.0009, weighted value: 0.0460, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9347
..training done in 71.85 seconds
..evaluation done in 16.53 seconds
Old network+MCTS average reward: 0.70, min: 0.04, max: 1.61, stdev: 0.24
New network+MCTS average reward: 0.70, min: 0.08, max: 1.56, stdev: 0.24
Old bare network average reward: 0.64, min: 0.02, max: 1.61, stdev: 0.25
New bare network average reward: 0.64, min: 0.01, max: 1.56, stdev: 0.25
External policy "random" average reward: 0.27, min: -0.31, max: 0.93, stdev: 0.23
External policy "individual greedy" average reward: 0.56, min: -0.05, max: 1.31, stdev: 0.23
External policy "total greedy" average reward: 0.68, min: 0.02, max: 1.51, stdev: 0.22
New network won 72 and tied 154 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 192 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.33 seconds
Training examples lengths: [64777, 64946, 64975, 64980, 64782, 64729, 65052, 64676, 64470, 64884]
Total value: 443933.25
Training on 648271 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2614 (value: 0.0017, weighted value: 0.0870, policy: 0.1745, weighted policy: 0.1745), Train Mean Max: 0.9309
Epoch 2/10, Train Loss: 0.2436 (value: 0.0015, weighted value: 0.0746, policy: 0.1689, weighted policy: 0.1689), Train Mean Max: 0.9317
Epoch 3/10, Train Loss: 0.2344 (value: 0.0014, weighted value: 0.0702, policy: 0.1642, weighted policy: 0.1642), Train Mean Max: 0.9321
Epoch 4/10, Train Loss: 0.2258 (value: 0.0013, weighted value: 0.0638, policy: 0.1620, weighted policy: 0.1620), Train Mean Max: 0.9325
Epoch 5/10, Train Loss: 0.2218 (value: 0.0012, weighted value: 0.0619, policy: 0.1599, weighted policy: 0.1599), Train Mean Max: 0.9327
Epoch 6/10, Train Loss: 0.2184 (value: 0.0012, weighted value: 0.0590, policy: 0.1594, weighted policy: 0.1594), Train Mean Max: 0.9330
Epoch 7/10, Train Loss: 0.2153 (value: 0.0011, weighted value: 0.0563, policy: 0.1590, weighted policy: 0.1590), Train Mean Max: 0.9332
Epoch 8/10, Train Loss: 0.2126 (value: 0.0011, weighted value: 0.0548, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9335
Epoch 9/10, Train Loss: 0.2101 (value: 0.0010, weighted value: 0.0525, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9337
Epoch 10/10, Train Loss: 0.2079 (value: 0.0010, weighted value: 0.0513, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9340
..training done in 69.78 seconds
..evaluation done in 16.65 seconds
Old network+MCTS average reward: 0.66, min: 0.10, max: 1.31, stdev: 0.23
New network+MCTS average reward: 0.67, min: 0.10, max: 1.31, stdev: 0.23
Old bare network average reward: 0.62, min: 0.03, max: 1.31, stdev: 0.23
New bare network average reward: 0.62, min: 0.08, max: 1.31, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.34, max: 1.06, stdev: 0.21
External policy "individual greedy" average reward: 0.53, min: 0.00, max: 1.24, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.21, max: 1.22, stdev: 0.21
New network won 81 and tied 145 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 193 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.56 seconds
Training examples lengths: [64946, 64975, 64980, 64782, 64729, 65052, 64676, 64470, 64884, 64592]
Total value: 443643.62
Training on 648086 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2355 (value: 0.0014, weighted value: 0.0695, policy: 0.1661, weighted policy: 0.1661), Train Mean Max: 0.9324
Epoch 2/10, Train Loss: 0.2257 (value: 0.0013, weighted value: 0.0629, policy: 0.1628, weighted policy: 0.1628), Train Mean Max: 0.9329
Epoch 3/10, Train Loss: 0.2180 (value: 0.0012, weighted value: 0.0585, policy: 0.1595, weighted policy: 0.1595), Train Mean Max: 0.9334
Epoch 4/10, Train Loss: 0.2154 (value: 0.0011, weighted value: 0.0562, policy: 0.1592, weighted policy: 0.1592), Train Mean Max: 0.9335
Epoch 5/10, Train Loss: 0.2125 (value: 0.0011, weighted value: 0.0554, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9338
Epoch 6/10, Train Loss: 0.2095 (value: 0.0011, weighted value: 0.0527, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9340
Epoch 7/10, Train Loss: 0.2066 (value: 0.0010, weighted value: 0.0501, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9342
Epoch 8/10, Train Loss: 0.2040 (value: 0.0010, weighted value: 0.0485, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9344
Epoch 9/10, Train Loss: 0.2038 (value: 0.0010, weighted value: 0.0478, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9345
Epoch 10/10, Train Loss: 0.2013 (value: 0.0009, weighted value: 0.0462, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9348
..training done in 71.35 seconds
..evaluation done in 15.80 seconds
Old network+MCTS average reward: 0.67, min: 0.07, max: 1.29, stdev: 0.21
New network+MCTS average reward: 0.67, min: 0.07, max: 1.29, stdev: 0.21
Old bare network average reward: 0.62, min: 0.06, max: 1.28, stdev: 0.22
New bare network average reward: 0.62, min: 0.02, max: 1.23, stdev: 0.22
External policy "random" average reward: 0.24, min: -0.36, max: 0.83, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: -0.03, max: 1.40, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: 0.06, max: 1.32, stdev: 0.21
New network won 79 and tied 147 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 194 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.05 seconds
Training examples lengths: [64975, 64980, 64782, 64729, 65052, 64676, 64470, 64884, 64592, 65093]
Total value: 444169.56
Training on 648233 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2291 (value: 0.0013, weighted value: 0.0640, policy: 0.1651, weighted policy: 0.1651), Train Mean Max: 0.9333
Epoch 2/10, Train Loss: 0.2204 (value: 0.0012, weighted value: 0.0586, policy: 0.1618, weighted policy: 0.1618), Train Mean Max: 0.9338
Epoch 3/10, Train Loss: 0.2158 (value: 0.0011, weighted value: 0.0564, policy: 0.1594, weighted policy: 0.1594), Train Mean Max: 0.9342
Epoch 4/10, Train Loss: 0.2088 (value: 0.0010, weighted value: 0.0521, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9345
Epoch 5/10, Train Loss: 0.2067 (value: 0.0010, weighted value: 0.0507, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9346
Epoch 6/10, Train Loss: 0.2043 (value: 0.0010, weighted value: 0.0488, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9348
Epoch 7/10, Train Loss: 0.2029 (value: 0.0010, weighted value: 0.0482, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9349
Epoch 8/10, Train Loss: 0.1996 (value: 0.0009, weighted value: 0.0453, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9352
Epoch 9/10, Train Loss: 0.2015 (value: 0.0009, weighted value: 0.0461, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9351
Epoch 10/10, Train Loss: 0.1978 (value: 0.0009, weighted value: 0.0438, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9355
..training done in 70.89 seconds
..evaluation done in 16.63 seconds
Old network+MCTS average reward: 0.68, min: 0.14, max: 1.28, stdev: 0.22
New network+MCTS average reward: 0.68, min: 0.17, max: 1.29, stdev: 0.22
Old bare network average reward: 0.62, min: 0.10, max: 1.28, stdev: 0.22
New bare network average reward: 0.62, min: 0.06, max: 1.27, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.35, max: 0.82, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: 0.03, max: 1.14, stdev: 0.21
External policy "total greedy" average reward: 0.64, min: 0.06, max: 1.15, stdev: 0.21
New network won 84 and tied 132 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 195 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.78 seconds
Training examples lengths: [64980, 64782, 64729, 65052, 64676, 64470, 64884, 64592, 65093, 64651]
Total value: 444243.81
Training on 647909 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2277 (value: 0.0012, weighted value: 0.0621, policy: 0.1656, weighted policy: 0.1656), Train Mean Max: 0.9336
Epoch 2/10, Train Loss: 0.2176 (value: 0.0011, weighted value: 0.0571, policy: 0.1605, weighted policy: 0.1605), Train Mean Max: 0.9343
Epoch 3/10, Train Loss: 0.2118 (value: 0.0011, weighted value: 0.0530, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9345
Epoch 4/10, Train Loss: 0.2064 (value: 0.0010, weighted value: 0.0507, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9350
Epoch 5/10, Train Loss: 0.2040 (value: 0.0010, weighted value: 0.0488, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9350
Epoch 6/10, Train Loss: 0.2044 (value: 0.0010, weighted value: 0.0491, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9352
Epoch 7/10, Train Loss: 0.1990 (value: 0.0009, weighted value: 0.0451, policy: 0.1539, weighted policy: 0.1539), Train Mean Max: 0.9355
Epoch 8/10, Train Loss: 0.1991 (value: 0.0009, weighted value: 0.0451, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9356
Epoch 9/10, Train Loss: 0.1961 (value: 0.0009, weighted value: 0.0429, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9357
Epoch 10/10, Train Loss: 0.1964 (value: 0.0009, weighted value: 0.0435, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9357
..training done in 71.12 seconds
..evaluation done in 16.49 seconds
Old network+MCTS average reward: 0.65, min: 0.13, max: 1.30, stdev: 0.22
New network+MCTS average reward: 0.66, min: 0.09, max: 1.30, stdev: 0.22
Old bare network average reward: 0.60, min: 0.06, max: 1.30, stdev: 0.23
New bare network average reward: 0.60, min: 0.08, max: 1.30, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.40, max: 0.89, stdev: 0.23
External policy "individual greedy" average reward: 0.52, min: -0.03, max: 1.22, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: 0.09, max: 1.40, stdev: 0.22
New network won 90 and tied 133 out of 300 games (52.17% wins where ties are half wins)
Keeping the new network

Training iteration 196 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.86 seconds
Training examples lengths: [64782, 64729, 65052, 64676, 64470, 64884, 64592, 65093, 64651, 64735]
Total value: 444301.39
Training on 647664 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2262 (value: 0.0012, weighted value: 0.0622, policy: 0.1641, weighted policy: 0.1641), Train Mean Max: 0.9340
Epoch 2/10, Train Loss: 0.2159 (value: 0.0011, weighted value: 0.0552, policy: 0.1607, weighted policy: 0.1607), Train Mean Max: 0.9345
Epoch 3/10, Train Loss: 0.2109 (value: 0.0011, weighted value: 0.0530, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9350
Epoch 4/10, Train Loss: 0.2068 (value: 0.0010, weighted value: 0.0499, policy: 0.1569, weighted policy: 0.1569), Train Mean Max: 0.9352
Epoch 5/10, Train Loss: 0.2034 (value: 0.0010, weighted value: 0.0486, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9354
Epoch 6/10, Train Loss: 0.2012 (value: 0.0009, weighted value: 0.0471, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9356
Epoch 7/10, Train Loss: 0.2001 (value: 0.0009, weighted value: 0.0455, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9355
Epoch 8/10, Train Loss: 0.1982 (value: 0.0009, weighted value: 0.0443, policy: 0.1539, weighted policy: 0.1539), Train Mean Max: 0.9358
Epoch 9/10, Train Loss: 0.1964 (value: 0.0009, weighted value: 0.0439, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9360
Epoch 10/10, Train Loss: 0.1946 (value: 0.0008, weighted value: 0.0415, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9361
..training done in 75.03 seconds
..evaluation done in 16.91 seconds
Old network+MCTS average reward: 0.68, min: 0.13, max: 1.39, stdev: 0.22
New network+MCTS average reward: 0.68, min: 0.13, max: 1.39, stdev: 0.22
Old bare network average reward: 0.62, min: 0.03, max: 1.39, stdev: 0.22
New bare network average reward: 0.62, min: 0.01, max: 1.39, stdev: 0.22
External policy "random" average reward: 0.25, min: -0.44, max: 1.04, stdev: 0.22
External policy "individual greedy" average reward: 0.52, min: -0.03, max: 1.08, stdev: 0.21
External policy "total greedy" average reward: 0.65, min: 0.03, max: 1.28, stdev: 0.21
New network won 76 and tied 147 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 197 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.44 seconds
Training examples lengths: [64729, 65052, 64676, 64470, 64884, 64592, 65093, 64651, 64735, 64740]
Total value: 443983.11
Training on 647622 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2539 (value: 0.0016, weighted value: 0.0796, policy: 0.1743, weighted policy: 0.1743), Train Mean Max: 0.9325
Epoch 2/10, Train Loss: 0.2377 (value: 0.0014, weighted value: 0.0708, policy: 0.1669, weighted policy: 0.1669), Train Mean Max: 0.9334
Epoch 3/10, Train Loss: 0.2279 (value: 0.0013, weighted value: 0.0650, policy: 0.1629, weighted policy: 0.1629), Train Mean Max: 0.9337
Epoch 4/10, Train Loss: 0.2209 (value: 0.0012, weighted value: 0.0604, policy: 0.1605, weighted policy: 0.1605), Train Mean Max: 0.9340
Epoch 5/10, Train Loss: 0.2150 (value: 0.0011, weighted value: 0.0565, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9342
Epoch 6/10, Train Loss: 0.2121 (value: 0.0011, weighted value: 0.0543, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9342
Epoch 7/10, Train Loss: 0.2092 (value: 0.0010, weighted value: 0.0524, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9345
Epoch 8/10, Train Loss: 0.2060 (value: 0.0010, weighted value: 0.0504, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9348
Epoch 9/10, Train Loss: 0.2040 (value: 0.0010, weighted value: 0.0486, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9348
Epoch 10/10, Train Loss: 0.2032 (value: 0.0010, weighted value: 0.0477, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9349
..training done in 72.65 seconds
..evaluation done in 17.10 seconds
Old network+MCTS average reward: 0.69, min: 0.05, max: 1.36, stdev: 0.22
New network+MCTS average reward: 0.69, min: 0.06, max: 1.31, stdev: 0.22
Old bare network average reward: 0.65, min: -0.06, max: 1.36, stdev: 0.23
New bare network average reward: 0.65, min: 0.00, max: 1.36, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.51, max: 0.92, stdev: 0.23
External policy "individual greedy" average reward: 0.57, min: -0.15, max: 1.29, stdev: 0.24
External policy "total greedy" average reward: 0.68, min: 0.02, max: 1.39, stdev: 0.22
New network won 80 and tied 132 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 198 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.09 seconds
Training examples lengths: [65052, 64676, 64470, 64884, 64592, 65093, 64651, 64735, 64740, 64729]
Total value: 444047.94
Training on 647622 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2833 (value: 0.0020, weighted value: 0.0995, policy: 0.1838, weighted policy: 0.1838), Train Mean Max: 0.9311
Epoch 2/10, Train Loss: 0.2563 (value: 0.0017, weighted value: 0.0829, policy: 0.1733, weighted policy: 0.1733), Train Mean Max: 0.9321
Epoch 3/10, Train Loss: 0.2422 (value: 0.0015, weighted value: 0.0747, policy: 0.1675, weighted policy: 0.1675), Train Mean Max: 0.9325
Epoch 4/10, Train Loss: 0.2336 (value: 0.0014, weighted value: 0.0697, policy: 0.1638, weighted policy: 0.1638), Train Mean Max: 0.9327
Epoch 5/10, Train Loss: 0.2273 (value: 0.0013, weighted value: 0.0653, policy: 0.1620, weighted policy: 0.1620), Train Mean Max: 0.9329
Epoch 6/10, Train Loss: 0.2237 (value: 0.0013, weighted value: 0.0625, policy: 0.1612, weighted policy: 0.1612), Train Mean Max: 0.9331
Epoch 7/10, Train Loss: 0.2187 (value: 0.0012, weighted value: 0.0599, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9335
Epoch 8/10, Train Loss: 0.2142 (value: 0.0011, weighted value: 0.0557, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9336
Epoch 9/10, Train Loss: 0.2139 (value: 0.0011, weighted value: 0.0550, policy: 0.1589, weighted policy: 0.1589), Train Mean Max: 0.9338
Epoch 10/10, Train Loss: 0.2102 (value: 0.0010, weighted value: 0.0524, policy: 0.1577, weighted policy: 0.1577), Train Mean Max: 0.9341
..training done in 73.43 seconds
..evaluation done in 16.51 seconds
Old network+MCTS average reward: 0.67, min: 0.03, max: 1.34, stdev: 0.23
New network+MCTS average reward: 0.67, min: 0.03, max: 1.34, stdev: 0.23
Old bare network average reward: 0.62, min: -0.06, max: 1.34, stdev: 0.24
New bare network average reward: 0.62, min: 0.00, max: 1.34, stdev: 0.24
External policy "random" average reward: 0.25, min: -0.29, max: 1.07, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: -0.06, max: 1.32, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.09, max: 1.34, stdev: 0.22
New network won 74 and tied 158 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 199 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.92 seconds
Training examples lengths: [64676, 64470, 64884, 64592, 65093, 64651, 64735, 64740, 64729, 64661]
Total value: 444134.07
Training on 647231 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2373 (value: 0.0014, weighted value: 0.0705, policy: 0.1667, weighted policy: 0.1667), Train Mean Max: 0.9325
Epoch 2/10, Train Loss: 0.2292 (value: 0.0013, weighted value: 0.0633, policy: 0.1659, weighted policy: 0.1659), Train Mean Max: 0.9330
Epoch 3/10, Train Loss: 0.2220 (value: 0.0012, weighted value: 0.0590, policy: 0.1630, weighted policy: 0.1630), Train Mean Max: 0.9335
Epoch 4/10, Train Loss: 0.2170 (value: 0.0011, weighted value: 0.0569, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9338
Epoch 5/10, Train Loss: 0.2135 (value: 0.0011, weighted value: 0.0546, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9339
Epoch 6/10, Train Loss: 0.2116 (value: 0.0010, weighted value: 0.0520, policy: 0.1596, weighted policy: 0.1596), Train Mean Max: 0.9340
Epoch 7/10, Train Loss: 0.2085 (value: 0.0010, weighted value: 0.0513, policy: 0.1572, weighted policy: 0.1572), Train Mean Max: 0.9344
Epoch 8/10, Train Loss: 0.2068 (value: 0.0010, weighted value: 0.0505, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9344
Epoch 9/10, Train Loss: 0.2037 (value: 0.0009, weighted value: 0.0470, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9345
Epoch 10/10, Train Loss: 0.2031 (value: 0.0009, weighted value: 0.0465, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9348
..training done in 69.97 seconds
..evaluation done in 17.28 seconds
Old network+MCTS average reward: 0.69, min: 0.15, max: 1.52, stdev: 0.23
New network+MCTS average reward: 0.69, min: 0.08, max: 1.54, stdev: 0.23
Old bare network average reward: 0.64, min: 0.06, max: 1.52, stdev: 0.24
New bare network average reward: 0.64, min: 0.01, max: 1.45, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.30, max: 1.15, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: 0.03, max: 1.19, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.08, max: 1.50, stdev: 0.24
New network won 85 and tied 134 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 200 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.50 seconds
Training examples lengths: [64470, 64884, 64592, 65093, 64651, 64735, 64740, 64729, 64661, 64813]
Total value: 444616.69
Training on 647368 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2353 (value: 0.0013, weighted value: 0.0655, policy: 0.1697, weighted policy: 0.1697), Train Mean Max: 0.9330
Epoch 2/10, Train Loss: 0.2208 (value: 0.0011, weighted value: 0.0572, policy: 0.1636, weighted policy: 0.1636), Train Mean Max: 0.9339
Epoch 3/10, Train Loss: 0.2170 (value: 0.0011, weighted value: 0.0553, policy: 0.1617, weighted policy: 0.1617), Train Mean Max: 0.9340
Epoch 4/10, Train Loss: 0.2128 (value: 0.0011, weighted value: 0.0530, policy: 0.1597, weighted policy: 0.1597), Train Mean Max: 0.9343
Epoch 5/10, Train Loss: 0.2084 (value: 0.0010, weighted value: 0.0512, policy: 0.1573, weighted policy: 0.1573), Train Mean Max: 0.9346
Epoch 6/10, Train Loss: 0.2062 (value: 0.0010, weighted value: 0.0486, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9344
Epoch 7/10, Train Loss: 0.2033 (value: 0.0009, weighted value: 0.0462, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9347
Epoch 8/10, Train Loss: 0.2037 (value: 0.0010, weighted value: 0.0476, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9350
Epoch 9/10, Train Loss: 0.1992 (value: 0.0009, weighted value: 0.0443, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9352
Epoch 10/10, Train Loss: 0.1989 (value: 0.0009, weighted value: 0.0442, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9352
..training done in 66.54 seconds
..evaluation done in 16.55 seconds
Old network+MCTS average reward: 0.67, min: 0.22, max: 1.56, stdev: 0.22
New network+MCTS average reward: 0.68, min: 0.23, max: 1.61, stdev: 0.22
Old bare network average reward: 0.63, min: 0.12, max: 1.45, stdev: 0.22
New bare network average reward: 0.63, min: 0.04, max: 1.46, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.31, max: 1.02, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: 0.06, max: 1.73, stdev: 0.24
External policy "total greedy" average reward: 0.66, min: 0.19, max: 1.82, stdev: 0.23
New network won 102 and tied 117 out of 300 games (53.50% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to zgaz_checkpoints/1757654225_iter_200

Training iteration 201 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 41.79 seconds
Training examples lengths: [64884, 64592, 65093, 64651, 64735, 64740, 64729, 64661, 64813, 64466]
Total value: 445202.83
Training on 647364 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2308 (value: 0.0013, weighted value: 0.0636, policy: 0.1673, weighted policy: 0.1673), Train Mean Max: 0.9334
Epoch 2/10, Train Loss: 0.2193 (value: 0.0011, weighted value: 0.0561, policy: 0.1631, weighted policy: 0.1631), Train Mean Max: 0.9341
Epoch 3/10, Train Loss: 0.2130 (value: 0.0011, weighted value: 0.0544, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9345
Epoch 4/10, Train Loss: 0.2087 (value: 0.0010, weighted value: 0.0508, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9344
Epoch 5/10, Train Loss: 0.2063 (value: 0.0010, weighted value: 0.0495, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9348
Epoch 6/10, Train Loss: 0.2045 (value: 0.0010, weighted value: 0.0483, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9349
Epoch 7/10, Train Loss: 0.2012 (value: 0.0009, weighted value: 0.0454, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9351
Epoch 8/10, Train Loss: 0.2009 (value: 0.0009, weighted value: 0.0447, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9352
Epoch 9/10, Train Loss: 0.1980 (value: 0.0009, weighted value: 0.0442, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9356
Epoch 10/10, Train Loss: 0.1967 (value: 0.0008, weighted value: 0.0422, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9355
..training done in 72.46 seconds
..evaluation done in 13.41 seconds
Old network+MCTS average reward: 0.67, min: 0.04, max: 1.30, stdev: 0.23
New network+MCTS average reward: 0.67, min: 0.03, max: 1.30, stdev: 0.23
Old bare network average reward: 0.62, min: 0.04, max: 1.30, stdev: 0.23
New bare network average reward: 0.62, min: 0.04, max: 1.30, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.33, max: 0.83, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: 0.01, max: 1.21, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.16, max: 1.42, stdev: 0.23
New network won 73 and tied 151 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 202 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.96 seconds
Training examples lengths: [64592, 65093, 64651, 64735, 64740, 64729, 64661, 64813, 64466, 64855]
Total value: 445781.74
Training on 647335 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2587 (value: 0.0016, weighted value: 0.0821, policy: 0.1766, weighted policy: 0.1766), Train Mean Max: 0.9318
Epoch 2/10, Train Loss: 0.2396 (value: 0.0014, weighted value: 0.0702, policy: 0.1694, weighted policy: 0.1694), Train Mean Max: 0.9326
Epoch 3/10, Train Loss: 0.2300 (value: 0.0013, weighted value: 0.0647, policy: 0.1653, weighted policy: 0.1653), Train Mean Max: 0.9330
Epoch 4/10, Train Loss: 0.2237 (value: 0.0012, weighted value: 0.0612, policy: 0.1625, weighted policy: 0.1625), Train Mean Max: 0.9332
Epoch 5/10, Train Loss: 0.2187 (value: 0.0012, weighted value: 0.0579, policy: 0.1608, weighted policy: 0.1608), Train Mean Max: 0.9335
Epoch 6/10, Train Loss: 0.2155 (value: 0.0011, weighted value: 0.0561, policy: 0.1594, weighted policy: 0.1594), Train Mean Max: 0.9337
Epoch 7/10, Train Loss: 0.2104 (value: 0.0010, weighted value: 0.0524, policy: 0.1580, weighted policy: 0.1580), Train Mean Max: 0.9339
Epoch 8/10, Train Loss: 0.2096 (value: 0.0010, weighted value: 0.0517, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9339
Epoch 9/10, Train Loss: 0.2059 (value: 0.0010, weighted value: 0.0491, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9342
Epoch 10/10, Train Loss: 0.2037 (value: 0.0009, weighted value: 0.0468, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9345
..training done in 64.16 seconds
..evaluation done in 13.63 seconds
Old network+MCTS average reward: 0.66, min: -0.06, max: 1.37, stdev: 0.23
New network+MCTS average reward: 0.67, min: -0.09, max: 1.37, stdev: 0.23
Old bare network average reward: 0.61, min: -0.12, max: 1.37, stdev: 0.23
New bare network average reward: 0.61, min: -0.07, max: 1.31, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.43, max: 0.94, stdev: 0.22
External policy "individual greedy" average reward: 0.52, min: -0.13, max: 1.20, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: -0.11, max: 1.33, stdev: 0.22
New network won 79 and tied 140 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 203 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.67 seconds
Training examples lengths: [65093, 64651, 64735, 64740, 64729, 64661, 64813, 64466, 64855, 65323]
Total value: 447156.55
Training on 648066 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2859 (value: 0.0020, weighted value: 0.0997, policy: 0.1862, weighted policy: 0.1862), Train Mean Max: 0.9304
Epoch 2/10, Train Loss: 0.2582 (value: 0.0017, weighted value: 0.0837, policy: 0.1745, weighted policy: 0.1745), Train Mean Max: 0.9314
Epoch 3/10, Train Loss: 0.2439 (value: 0.0015, weighted value: 0.0753, policy: 0.1686, weighted policy: 0.1686), Train Mean Max: 0.9320
Epoch 4/10, Train Loss: 0.2366 (value: 0.0014, weighted value: 0.0712, policy: 0.1654, weighted policy: 0.1654), Train Mean Max: 0.9321
Epoch 5/10, Train Loss: 0.2283 (value: 0.0013, weighted value: 0.0653, policy: 0.1630, weighted policy: 0.1630), Train Mean Max: 0.9324
Epoch 6/10, Train Loss: 0.2252 (value: 0.0013, weighted value: 0.0629, policy: 0.1623, weighted policy: 0.1623), Train Mean Max: 0.9325
Epoch 7/10, Train Loss: 0.2204 (value: 0.0012, weighted value: 0.0590, policy: 0.1614, weighted policy: 0.1614), Train Mean Max: 0.9327
Epoch 8/10, Train Loss: 0.2183 (value: 0.0012, weighted value: 0.0579, policy: 0.1605, weighted policy: 0.1605), Train Mean Max: 0.9329
Epoch 9/10, Train Loss: 0.2130 (value: 0.0011, weighted value: 0.0540, policy: 0.1591, weighted policy: 0.1591), Train Mean Max: 0.9334
Epoch 10/10, Train Loss: 0.2125 (value: 0.0011, weighted value: 0.0537, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9334
..training done in 65.10 seconds
..evaluation done in 13.04 seconds
Old network+MCTS average reward: 0.68, min: 0.07, max: 1.47, stdev: 0.21
New network+MCTS average reward: 0.68, min: 0.09, max: 1.54, stdev: 0.21
Old bare network average reward: 0.63, min: 0.04, max: 1.47, stdev: 0.22
New bare network average reward: 0.62, min: 0.06, max: 1.54, stdev: 0.22
External policy "random" average reward: 0.25, min: -0.28, max: 0.81, stdev: 0.21
External policy "individual greedy" average reward: 0.55, min: -0.04, max: 1.42, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.02, max: 1.50, stdev: 0.22
New network won 85 and tied 135 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 204 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.77 seconds
Training examples lengths: [64651, 64735, 64740, 64729, 64661, 64813, 64466, 64855, 65323, 64742]
Total value: 446934.70
Training on 647715 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2378 (value: 0.0014, weighted value: 0.0697, policy: 0.1681, weighted policy: 0.1681), Train Mean Max: 0.9319
Epoch 2/10, Train Loss: 0.2294 (value: 0.0013, weighted value: 0.0648, policy: 0.1646, weighted policy: 0.1646), Train Mean Max: 0.9325
Epoch 3/10, Train Loss: 0.2216 (value: 0.0012, weighted value: 0.0586, policy: 0.1630, weighted policy: 0.1630), Train Mean Max: 0.9326
Epoch 4/10, Train Loss: 0.2176 (value: 0.0011, weighted value: 0.0569, policy: 0.1606, weighted policy: 0.1606), Train Mean Max: 0.9330
Epoch 5/10, Train Loss: 0.2144 (value: 0.0011, weighted value: 0.0552, policy: 0.1592, weighted policy: 0.1592), Train Mean Max: 0.9333
Epoch 6/10, Train Loss: 0.2124 (value: 0.0011, weighted value: 0.0539, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9334
Epoch 7/10, Train Loss: 0.2079 (value: 0.0010, weighted value: 0.0501, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9337
Epoch 8/10, Train Loss: 0.2062 (value: 0.0010, weighted value: 0.0483, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9339
Epoch 9/10, Train Loss: 0.2044 (value: 0.0010, weighted value: 0.0480, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9340
Epoch 10/10, Train Loss: 0.2031 (value: 0.0009, weighted value: 0.0465, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9342
..training done in 69.39 seconds
..evaluation done in 13.56 seconds
Old network+MCTS average reward: 0.70, min: 0.10, max: 1.44, stdev: 0.22
New network+MCTS average reward: 0.69, min: 0.18, max: 1.41, stdev: 0.22
Old bare network average reward: 0.64, min: 0.05, max: 1.23, stdev: 0.22
New bare network average reward: 0.64, min: 0.15, max: 1.25, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.31, max: 0.83, stdev: 0.23
External policy "individual greedy" average reward: 0.56, min: -0.04, max: 1.21, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: -0.01, max: 1.31, stdev: 0.22
New network won 62 and tied 164 out of 300 games (48.00% wins where ties are half wins)
Reverting to the old network

Training iteration 205 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.40 seconds
Training examples lengths: [64735, 64740, 64729, 64661, 64813, 64466, 64855, 65323, 64742, 64864]
Total value: 447095.57
Training on 647928 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2658 (value: 0.0018, weighted value: 0.0880, policy: 0.1778, weighted policy: 0.1778), Train Mean Max: 0.9304
Epoch 2/10, Train Loss: 0.2461 (value: 0.0015, weighted value: 0.0754, policy: 0.1706, weighted policy: 0.1706), Train Mean Max: 0.9310
Epoch 3/10, Train Loss: 0.2364 (value: 0.0014, weighted value: 0.0702, policy: 0.1662, weighted policy: 0.1662), Train Mean Max: 0.9315
Epoch 4/10, Train Loss: 0.2298 (value: 0.0013, weighted value: 0.0664, policy: 0.1634, weighted policy: 0.1634), Train Mean Max: 0.9317
Epoch 5/10, Train Loss: 0.2249 (value: 0.0012, weighted value: 0.0621, policy: 0.1629, weighted policy: 0.1629), Train Mean Max: 0.9321
Epoch 6/10, Train Loss: 0.2212 (value: 0.0012, weighted value: 0.0595, policy: 0.1617, weighted policy: 0.1617), Train Mean Max: 0.9322
Epoch 7/10, Train Loss: 0.2182 (value: 0.0011, weighted value: 0.0573, policy: 0.1609, weighted policy: 0.1609), Train Mean Max: 0.9325
Epoch 8/10, Train Loss: 0.2151 (value: 0.0011, weighted value: 0.0554, policy: 0.1597, weighted policy: 0.1597), Train Mean Max: 0.9328
Epoch 9/10, Train Loss: 0.2125 (value: 0.0011, weighted value: 0.0534, policy: 0.1591, weighted policy: 0.1591), Train Mean Max: 0.9331
Epoch 10/10, Train Loss: 0.2092 (value: 0.0010, weighted value: 0.0511, policy: 0.1581, weighted policy: 0.1581), Train Mean Max: 0.9332
..training done in 68.43 seconds
..evaluation done in 13.57 seconds
Old network+MCTS average reward: 0.66, min: -0.16, max: 1.28, stdev: 0.23
New network+MCTS average reward: 0.66, min: -0.25, max: 1.22, stdev: 0.22
Old bare network average reward: 0.61, min: -0.16, max: 1.20, stdev: 0.23
New bare network average reward: 0.61, min: -0.29, max: 1.21, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.39, max: 0.97, stdev: 0.22
External policy "individual greedy" average reward: 0.52, min: -0.13, max: 1.26, stdev: 0.22
External policy "total greedy" average reward: 0.62, min: -0.03, max: 1.25, stdev: 0.20
New network won 75 and tied 156 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 206 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.32 seconds
Training examples lengths: [64740, 64729, 64661, 64813, 64466, 64855, 65323, 64742, 64864, 64807]
Total value: 447773.56
Training on 648000 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2377 (value: 0.0014, weighted value: 0.0695, policy: 0.1682, weighted policy: 0.1682), Train Mean Max: 0.9320
Epoch 2/10, Train Loss: 0.2244 (value: 0.0012, weighted value: 0.0614, policy: 0.1630, weighted policy: 0.1630), Train Mean Max: 0.9327
Epoch 3/10, Train Loss: 0.2183 (value: 0.0012, weighted value: 0.0577, policy: 0.1607, weighted policy: 0.1607), Train Mean Max: 0.9330
Epoch 4/10, Train Loss: 0.2148 (value: 0.0011, weighted value: 0.0554, policy: 0.1594, weighted policy: 0.1594), Train Mean Max: 0.9332
Epoch 5/10, Train Loss: 0.2127 (value: 0.0011, weighted value: 0.0542, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9335
Epoch 6/10, Train Loss: 0.2076 (value: 0.0010, weighted value: 0.0509, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9338
Epoch 7/10, Train Loss: 0.2081 (value: 0.0010, weighted value: 0.0517, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9340
Epoch 8/10, Train Loss: 0.2036 (value: 0.0010, weighted value: 0.0477, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9342
Epoch 9/10, Train Loss: 0.2026 (value: 0.0009, weighted value: 0.0464, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9342
Epoch 10/10, Train Loss: 0.2015 (value: 0.0009, weighted value: 0.0457, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9344
..training done in 67.22 seconds
..evaluation done in 13.36 seconds
Old network+MCTS average reward: 0.68, min: 0.18, max: 1.42, stdev: 0.23
New network+MCTS average reward: 0.67, min: 0.16, max: 1.42, stdev: 0.23
Old bare network average reward: 0.62, min: 0.13, max: 1.42, stdev: 0.24
New bare network average reward: 0.62, min: 0.13, max: 1.39, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.34, max: 0.90, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: -0.07, max: 1.23, stdev: 0.24
External policy "total greedy" average reward: 0.65, min: 0.09, max: 1.33, stdev: 0.23
New network won 61 and tied 159 out of 300 games (46.83% wins where ties are half wins)
Reverting to the old network

Training iteration 207 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.64 seconds
Training examples lengths: [64729, 64661, 64813, 64466, 64855, 65323, 64742, 64864, 64807, 64893]
Total value: 449073.15
Training on 648153 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2601 (value: 0.0017, weighted value: 0.0854, policy: 0.1746, weighted policy: 0.1746), Train Mean Max: 0.9309
Epoch 2/10, Train Loss: 0.2434 (value: 0.0015, weighted value: 0.0746, policy: 0.1688, weighted policy: 0.1688), Train Mean Max: 0.9317
Epoch 3/10, Train Loss: 0.2329 (value: 0.0014, weighted value: 0.0686, policy: 0.1643, weighted policy: 0.1643), Train Mean Max: 0.9323
Epoch 4/10, Train Loss: 0.2285 (value: 0.0013, weighted value: 0.0671, policy: 0.1613, weighted policy: 0.1613), Train Mean Max: 0.9324
Epoch 5/10, Train Loss: 0.2211 (value: 0.0012, weighted value: 0.0613, policy: 0.1598, weighted policy: 0.1598), Train Mean Max: 0.9328
Epoch 6/10, Train Loss: 0.2174 (value: 0.0012, weighted value: 0.0577, policy: 0.1597, weighted policy: 0.1597), Train Mean Max: 0.9330
Epoch 7/10, Train Loss: 0.2151 (value: 0.0011, weighted value: 0.0563, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9333
Epoch 8/10, Train Loss: 0.2109 (value: 0.0011, weighted value: 0.0531, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9336
Epoch 9/10, Train Loss: 0.2112 (value: 0.0011, weighted value: 0.0539, policy: 0.1572, weighted policy: 0.1572), Train Mean Max: 0.9338
Epoch 10/10, Train Loss: 0.2057 (value: 0.0010, weighted value: 0.0498, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9341
..training done in 72.70 seconds
..evaluation done in 13.82 seconds
Old network+MCTS average reward: 0.66, min: 0.12, max: 1.34, stdev: 0.21
New network+MCTS average reward: 0.66, min: 0.15, max: 1.37, stdev: 0.21
Old bare network average reward: 0.60, min: 0.07, max: 1.32, stdev: 0.22
New bare network average reward: 0.61, min: 0.07, max: 1.32, stdev: 0.21
External policy "random" average reward: 0.25, min: -0.40, max: 0.86, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: 0.05, max: 1.25, stdev: 0.22
External policy "total greedy" average reward: 0.63, min: 0.04, max: 1.39, stdev: 0.21
New network won 83 and tied 138 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 208 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 43.77 seconds
Training examples lengths: [64661, 64813, 64466, 64855, 65323, 64742, 64864, 64807, 64893, 64674]
Total value: 449754.92
Training on 648098 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2334 (value: 0.0014, weighted value: 0.0682, policy: 0.1652, weighted policy: 0.1652), Train Mean Max: 0.9328
Epoch 2/10, Train Loss: 0.2238 (value: 0.0012, weighted value: 0.0618, policy: 0.1621, weighted policy: 0.1621), Train Mean Max: 0.9332
Epoch 3/10, Train Loss: 0.2167 (value: 0.0012, weighted value: 0.0575, policy: 0.1592, weighted policy: 0.1592), Train Mean Max: 0.9337
Epoch 4/10, Train Loss: 0.2123 (value: 0.0011, weighted value: 0.0544, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9340
Epoch 5/10, Train Loss: 0.2085 (value: 0.0010, weighted value: 0.0523, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9342
Epoch 6/10, Train Loss: 0.2076 (value: 0.0010, weighted value: 0.0512, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9343
Epoch 7/10, Train Loss: 0.2044 (value: 0.0010, weighted value: 0.0494, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9347
Epoch 8/10, Train Loss: 0.2037 (value: 0.0010, weighted value: 0.0493, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9348
Epoch 9/10, Train Loss: 0.2001 (value: 0.0009, weighted value: 0.0464, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9349
Epoch 10/10, Train Loss: 0.1995 (value: 0.0009, weighted value: 0.0460, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9352
..training done in 71.40 seconds
..evaluation done in 13.97 seconds
Old network+MCTS average reward: 0.69, min: 0.12, max: 1.33, stdev: 0.23
New network+MCTS average reward: 0.69, min: 0.08, max: 1.28, stdev: 0.23
Old bare network average reward: 0.64, min: 0.05, max: 1.26, stdev: 0.23
New bare network average reward: 0.64, min: 0.08, max: 1.27, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.31, max: 0.87, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: -0.06, max: 1.24, stdev: 0.24
External policy "total greedy" average reward: 0.66, min: 0.15, max: 1.30, stdev: 0.23
New network won 78 and tied 150 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 209 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 43.75 seconds
Training examples lengths: [64813, 64466, 64855, 65323, 64742, 64864, 64807, 64893, 64674, 64635]
Total value: 449734.41
Training on 648072 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2272 (value: 0.0012, weighted value: 0.0622, policy: 0.1651, weighted policy: 0.1651), Train Mean Max: 0.9334
Epoch 2/10, Train Loss: 0.2192 (value: 0.0012, weighted value: 0.0581, policy: 0.1612, weighted policy: 0.1612), Train Mean Max: 0.9338
Epoch 3/10, Train Loss: 0.2114 (value: 0.0011, weighted value: 0.0539, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9344
Epoch 4/10, Train Loss: 0.2082 (value: 0.0010, weighted value: 0.0513, policy: 0.1569, weighted policy: 0.1569), Train Mean Max: 0.9345
Epoch 5/10, Train Loss: 0.2056 (value: 0.0010, weighted value: 0.0498, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9348
Epoch 6/10, Train Loss: 0.2037 (value: 0.0010, weighted value: 0.0483, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9349
Epoch 7/10, Train Loss: 0.1998 (value: 0.0009, weighted value: 0.0461, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9352
Epoch 8/10, Train Loss: 0.2004 (value: 0.0009, weighted value: 0.0466, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9353
Epoch 9/10, Train Loss: 0.1960 (value: 0.0009, weighted value: 0.0426, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9355
Epoch 10/10, Train Loss: 0.1963 (value: 0.0009, weighted value: 0.0438, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9357
..training done in 71.28 seconds
..evaluation done in 13.54 seconds
Old network+MCTS average reward: 0.71, min: 0.09, max: 1.32, stdev: 0.23
New network+MCTS average reward: 0.71, min: 0.07, max: 1.32, stdev: 0.23
Old bare network average reward: 0.66, min: 0.07, max: 1.29, stdev: 0.23
New bare network average reward: 0.66, min: 0.06, max: 1.32, stdev: 0.22
External policy "random" average reward: 0.28, min: -0.26, max: 0.99, stdev: 0.23
External policy "individual greedy" average reward: 0.57, min: -0.03, max: 1.16, stdev: 0.22
External policy "total greedy" average reward: 0.67, min: 0.10, max: 1.34, stdev: 0.22
New network won 77 and tied 139 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 210 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 43.73 seconds
Training examples lengths: [64466, 64855, 65323, 64742, 64864, 64807, 64893, 64674, 64635, 64840]
Total value: 449826.52
Training on 648099 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2558 (value: 0.0016, weighted value: 0.0812, policy: 0.1746, weighted policy: 0.1746), Train Mean Max: 0.9319
Epoch 2/10, Train Loss: 0.2375 (value: 0.0014, weighted value: 0.0707, policy: 0.1668, weighted policy: 0.1668), Train Mean Max: 0.9327
Epoch 3/10, Train Loss: 0.2269 (value: 0.0013, weighted value: 0.0646, policy: 0.1624, weighted policy: 0.1624), Train Mean Max: 0.9333
Epoch 4/10, Train Loss: 0.2216 (value: 0.0012, weighted value: 0.0610, policy: 0.1605, weighted policy: 0.1605), Train Mean Max: 0.9336
Epoch 5/10, Train Loss: 0.2166 (value: 0.0012, weighted value: 0.0581, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9337
Epoch 6/10, Train Loss: 0.2131 (value: 0.0011, weighted value: 0.0560, policy: 0.1572, weighted policy: 0.1572), Train Mean Max: 0.9341
Epoch 7/10, Train Loss: 0.2090 (value: 0.0011, weighted value: 0.0529, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9342
Epoch 8/10, Train Loss: 0.2086 (value: 0.0011, weighted value: 0.0533, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9344
Epoch 9/10, Train Loss: 0.2040 (value: 0.0010, weighted value: 0.0486, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9346
Epoch 10/10, Train Loss: 0.2024 (value: 0.0010, weighted value: 0.0479, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9349
..training done in 71.59 seconds
..evaluation done in 13.72 seconds
Old network+MCTS average reward: 0.69, min: 0.14, max: 1.43, stdev: 0.23
New network+MCTS average reward: 0.69, min: 0.13, max: 1.43, stdev: 0.23
Old bare network average reward: 0.64, min: 0.02, max: 1.42, stdev: 0.23
New bare network average reward: 0.64, min: 0.00, max: 1.43, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.29, max: 0.88, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.06, max: 1.41, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.12, max: 1.29, stdev: 0.22
New network won 69 and tied 150 out of 300 games (48.00% wins where ties are half wins)
Reverting to the old network

Training iteration 211 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.29 seconds
Training examples lengths: [64855, 65323, 64742, 64864, 64807, 64893, 64674, 64635, 64840, 64635]
Total value: 450255.53
Training on 648268 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2794 (value: 0.0019, weighted value: 0.0964, policy: 0.1829, weighted policy: 0.1829), Train Mean Max: 0.9307
Epoch 2/10, Train Loss: 0.2570 (value: 0.0017, weighted value: 0.0846, policy: 0.1724, weighted policy: 0.1724), Train Mean Max: 0.9317
Epoch 3/10, Train Loss: 0.2407 (value: 0.0015, weighted value: 0.0750, policy: 0.1657, weighted policy: 0.1657), Train Mean Max: 0.9325
Epoch 4/10, Train Loss: 0.2314 (value: 0.0014, weighted value: 0.0691, policy: 0.1623, weighted policy: 0.1623), Train Mean Max: 0.9327
Epoch 5/10, Train Loss: 0.2268 (value: 0.0013, weighted value: 0.0665, policy: 0.1604, weighted policy: 0.1604), Train Mean Max: 0.9328
Epoch 6/10, Train Loss: 0.2229 (value: 0.0013, weighted value: 0.0634, policy: 0.1595, weighted policy: 0.1595), Train Mean Max: 0.9332
Epoch 7/10, Train Loss: 0.2165 (value: 0.0012, weighted value: 0.0584, policy: 0.1582, weighted policy: 0.1582), Train Mean Max: 0.9334
Epoch 8/10, Train Loss: 0.2151 (value: 0.0011, weighted value: 0.0571, policy: 0.1580, weighted policy: 0.1580), Train Mean Max: 0.9335
Epoch 9/10, Train Loss: 0.2121 (value: 0.0011, weighted value: 0.0552, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9339
Epoch 10/10, Train Loss: 0.2088 (value: 0.0011, weighted value: 0.0527, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9342
..training done in 67.09 seconds
..evaluation done in 14.50 seconds
Old network+MCTS average reward: 0.69, min: 0.14, max: 1.45, stdev: 0.24
New network+MCTS average reward: 0.69, min: 0.13, max: 1.45, stdev: 0.24
Old bare network average reward: 0.64, min: 0.06, max: 1.45, stdev: 0.26
New bare network average reward: 0.64, min: 0.06, max: 1.45, stdev: 0.26
External policy "random" average reward: 0.26, min: -0.31, max: 1.01, stdev: 0.24
External policy "individual greedy" average reward: 0.54, min: -0.01, max: 1.45, stdev: 0.24
External policy "total greedy" average reward: 0.66, min: 0.10, max: 1.71, stdev: 0.24
New network won 75 and tied 160 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 212 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.70 seconds
Training examples lengths: [65323, 64742, 64864, 64807, 64893, 64674, 64635, 64840, 64635, 64841]
Total value: 449842.82
Training on 648254 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2337 (value: 0.0014, weighted value: 0.0694, policy: 0.1644, weighted policy: 0.1644), Train Mean Max: 0.9331
Epoch 2/10, Train Loss: 0.2245 (value: 0.0013, weighted value: 0.0628, policy: 0.1617, weighted policy: 0.1617), Train Mean Max: 0.9335
Epoch 3/10, Train Loss: 0.2164 (value: 0.0012, weighted value: 0.0584, policy: 0.1580, weighted policy: 0.1580), Train Mean Max: 0.9340
Epoch 4/10, Train Loss: 0.2130 (value: 0.0011, weighted value: 0.0564, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9342
Epoch 5/10, Train Loss: 0.2102 (value: 0.0011, weighted value: 0.0546, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9344
Epoch 6/10, Train Loss: 0.2065 (value: 0.0010, weighted value: 0.0509, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9346
Epoch 7/10, Train Loss: 0.2057 (value: 0.0010, weighted value: 0.0511, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9348
Epoch 8/10, Train Loss: 0.2028 (value: 0.0010, weighted value: 0.0485, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9350
Epoch 9/10, Train Loss: 0.2006 (value: 0.0009, weighted value: 0.0463, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9352
Epoch 10/10, Train Loss: 0.2004 (value: 0.0009, weighted value: 0.0475, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9356
..training done in 67.02 seconds
..evaluation done in 13.38 seconds
Old network+MCTS average reward: 0.68, min: 0.04, max: 1.31, stdev: 0.23
New network+MCTS average reward: 0.68, min: 0.06, max: 1.31, stdev: 0.23
Old bare network average reward: 0.63, min: -0.05, max: 1.31, stdev: 0.24
New bare network average reward: 0.63, min: -0.06, max: 1.31, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.32, max: 0.95, stdev: 0.23
External policy "individual greedy" average reward: 0.52, min: -0.07, max: 1.27, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.07, max: 1.30, stdev: 0.22
New network won 79 and tied 149 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 213 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.88 seconds
Training examples lengths: [64742, 64864, 64807, 64893, 64674, 64635, 64840, 64635, 64841, 65090]
Total value: 449633.36
Training on 648021 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2254 (value: 0.0012, weighted value: 0.0616, policy: 0.1639, weighted policy: 0.1639), Train Mean Max: 0.9338
Epoch 2/10, Train Loss: 0.2157 (value: 0.0011, weighted value: 0.0568, policy: 0.1589, weighted policy: 0.1589), Train Mean Max: 0.9346
Epoch 3/10, Train Loss: 0.2119 (value: 0.0011, weighted value: 0.0547, policy: 0.1572, weighted policy: 0.1572), Train Mean Max: 0.9346
Epoch 4/10, Train Loss: 0.2063 (value: 0.0010, weighted value: 0.0517, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9351
Epoch 5/10, Train Loss: 0.2044 (value: 0.0010, weighted value: 0.0499, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9352
Epoch 6/10, Train Loss: 0.2025 (value: 0.0010, weighted value: 0.0483, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9354
Epoch 7/10, Train Loss: 0.1994 (value: 0.0009, weighted value: 0.0465, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9357
Epoch 8/10, Train Loss: 0.1970 (value: 0.0009, weighted value: 0.0446, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9358
Epoch 9/10, Train Loss: 0.1970 (value: 0.0009, weighted value: 0.0447, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9358
Epoch 10/10, Train Loss: 0.1944 (value: 0.0009, weighted value: 0.0429, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9363
..training done in 70.98 seconds
..evaluation done in 13.88 seconds
Old network+MCTS average reward: 0.67, min: 0.12, max: 1.37, stdev: 0.24
New network+MCTS average reward: 0.67, min: 0.11, max: 1.37, stdev: 0.24
Old bare network average reward: 0.63, min: 0.10, max: 1.36, stdev: 0.24
New bare network average reward: 0.62, min: -0.03, max: 1.31, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.28, max: 1.12, stdev: 0.25
External policy "individual greedy" average reward: 0.53, min: -0.01, max: 1.29, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.07, max: 1.42, stdev: 0.23
New network won 70 and tied 163 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 214 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.45 seconds
Training examples lengths: [64864, 64807, 64893, 64674, 64635, 64840, 64635, 64841, 65090, 64781]
Total value: 449920.55
Training on 648060 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2221 (value: 0.0012, weighted value: 0.0600, policy: 0.1622, weighted policy: 0.1622), Train Mean Max: 0.9344
Epoch 2/10, Train Loss: 0.2142 (value: 0.0011, weighted value: 0.0557, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9351
Epoch 3/10, Train Loss: 0.2072 (value: 0.0010, weighted value: 0.0519, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9355
Epoch 4/10, Train Loss: 0.2031 (value: 0.0010, weighted value: 0.0494, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9357
Epoch 5/10, Train Loss: 0.1993 (value: 0.0009, weighted value: 0.0472, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9359
Epoch 6/10, Train Loss: 0.1986 (value: 0.0009, weighted value: 0.0459, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9361
Epoch 7/10, Train Loss: 0.1971 (value: 0.0009, weighted value: 0.0456, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9362
Epoch 8/10, Train Loss: 0.1940 (value: 0.0009, weighted value: 0.0428, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9365
Epoch 9/10, Train Loss: 0.1939 (value: 0.0008, weighted value: 0.0424, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9366
Epoch 10/10, Train Loss: 0.1931 (value: 0.0008, weighted value: 0.0417, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9366
..training done in 71.74 seconds
..evaluation done in 14.23 seconds
Old network+MCTS average reward: 0.67, min: -0.05, max: 1.51, stdev: 0.23
New network+MCTS average reward: 0.67, min: -0.03, max: 1.51, stdev: 0.23
Old bare network average reward: 0.63, min: -0.04, max: 1.51, stdev: 0.23
New bare network average reward: 0.62, min: -0.04, max: 1.51, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.20, max: 1.13, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: -0.09, max: 1.41, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: -0.05, max: 1.42, stdev: 0.21
New network won 71 and tied 160 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 215 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.56 seconds
Training examples lengths: [64807, 64893, 64674, 64635, 64840, 64635, 64841, 65090, 64781, 64735]
Total value: 450442.84
Training on 647931 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2202 (value: 0.0012, weighted value: 0.0593, policy: 0.1609, weighted policy: 0.1609), Train Mean Max: 0.9351
Epoch 2/10, Train Loss: 0.2122 (value: 0.0011, weighted value: 0.0539, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9356
Epoch 3/10, Train Loss: 0.2064 (value: 0.0010, weighted value: 0.0510, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9358
Epoch 4/10, Train Loss: 0.2025 (value: 0.0010, weighted value: 0.0492, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9361
Epoch 5/10, Train Loss: 0.1980 (value: 0.0009, weighted value: 0.0461, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9364
Epoch 6/10, Train Loss: 0.1958 (value: 0.0009, weighted value: 0.0450, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9365
Epoch 7/10, Train Loss: 0.1946 (value: 0.0009, weighted value: 0.0441, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9367
Epoch 8/10, Train Loss: 0.1946 (value: 0.0009, weighted value: 0.0430, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9366
Epoch 9/10, Train Loss: 0.1920 (value: 0.0008, weighted value: 0.0415, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9370
Epoch 10/10, Train Loss: 0.1920 (value: 0.0008, weighted value: 0.0408, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9370
..training done in 68.36 seconds
..evaluation done in 13.86 seconds
Old network+MCTS average reward: 0.70, min: 0.12, max: 1.71, stdev: 0.23
New network+MCTS average reward: 0.70, min: 0.10, max: 1.60, stdev: 0.23
Old bare network average reward: 0.65, min: 0.03, max: 1.71, stdev: 0.24
New bare network average reward: 0.65, min: 0.02, max: 1.71, stdev: 0.24
External policy "random" average reward: 0.28, min: -0.28, max: 0.99, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: -0.05, max: 1.43, stdev: 0.24
External policy "total greedy" average reward: 0.68, min: 0.07, max: 1.65, stdev: 0.23
New network won 83 and tied 144 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 216 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.43 seconds
Training examples lengths: [64893, 64674, 64635, 64840, 64635, 64841, 65090, 64781, 64735, 64943]
Total value: 450679.06
Training on 648067 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2218 (value: 0.0012, weighted value: 0.0586, policy: 0.1632, weighted policy: 0.1632), Train Mean Max: 0.9352
Epoch 2/10, Train Loss: 0.2102 (value: 0.0011, weighted value: 0.0528, policy: 0.1573, weighted policy: 0.1573), Train Mean Max: 0.9359
Epoch 3/10, Train Loss: 0.2041 (value: 0.0010, weighted value: 0.0494, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9362
Epoch 4/10, Train Loss: 0.2023 (value: 0.0010, weighted value: 0.0491, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9364
Epoch 5/10, Train Loss: 0.1982 (value: 0.0009, weighted value: 0.0462, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9366
Epoch 6/10, Train Loss: 0.1968 (value: 0.0009, weighted value: 0.0454, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9366
Epoch 7/10, Train Loss: 0.1937 (value: 0.0008, weighted value: 0.0423, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9367
Epoch 8/10, Train Loss: 0.1926 (value: 0.0008, weighted value: 0.0415, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9368
Epoch 9/10, Train Loss: 0.1912 (value: 0.0008, weighted value: 0.0411, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9372
Epoch 10/10, Train Loss: 0.1905 (value: 0.0008, weighted value: 0.0407, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9372
..training done in 65.93 seconds
..evaluation done in 14.15 seconds
Old network+MCTS average reward: 0.68, min: 0.11, max: 1.45, stdev: 0.24
New network+MCTS average reward: 0.68, min: 0.11, max: 1.45, stdev: 0.24
Old bare network average reward: 0.62, min: 0.04, max: 1.45, stdev: 0.24
New bare network average reward: 0.63, min: 0.00, max: 1.45, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.37, max: 0.99, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: -0.06, max: 1.32, stdev: 0.21
External policy "total greedy" average reward: 0.65, min: 0.09, max: 1.50, stdev: 0.23
New network won 83 and tied 147 out of 300 games (52.17% wins where ties are half wins)
Keeping the new network

Training iteration 217 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.76 seconds
Training examples lengths: [64674, 64635, 64840, 64635, 64841, 65090, 64781, 64735, 64943, 64746]
Total value: 450012.83
Training on 647920 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2170 (value: 0.0011, weighted value: 0.0572, policy: 0.1599, weighted policy: 0.1599), Train Mean Max: 0.9354
Epoch 2/10, Train Loss: 0.2099 (value: 0.0011, weighted value: 0.0526, policy: 0.1572, weighted policy: 0.1572), Train Mean Max: 0.9358
Epoch 3/10, Train Loss: 0.2033 (value: 0.0010, weighted value: 0.0499, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9365
Epoch 4/10, Train Loss: 0.2001 (value: 0.0010, weighted value: 0.0480, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9366
Epoch 5/10, Train Loss: 0.1980 (value: 0.0009, weighted value: 0.0458, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9367
Epoch 6/10, Train Loss: 0.1943 (value: 0.0009, weighted value: 0.0437, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1921 (value: 0.0008, weighted value: 0.0421, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9370
Epoch 8/10, Train Loss: 0.1936 (value: 0.0009, weighted value: 0.0429, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9371
Epoch 9/10, Train Loss: 0.1898 (value: 0.0008, weighted value: 0.0398, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9375
Epoch 10/10, Train Loss: 0.1891 (value: 0.0008, weighted value: 0.0390, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9375
..training done in 72.88 seconds
..evaluation done in 14.70 seconds
Old network+MCTS average reward: 0.69, min: 0.03, max: 1.51, stdev: 0.23
New network+MCTS average reward: 0.69, min: 0.02, max: 1.51, stdev: 0.24
Old bare network average reward: 0.65, min: 0.06, max: 1.47, stdev: 0.24
New bare network average reward: 0.65, min: 0.06, max: 1.47, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.44, max: 1.19, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: -0.05, max: 1.38, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.06, max: 1.53, stdev: 0.23
New network won 78 and tied 155 out of 300 games (51.83% wins where ties are half wins)
Keeping the new network

Training iteration 218 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.16 seconds
Training examples lengths: [64635, 64840, 64635, 64841, 65090, 64781, 64735, 64943, 64746, 64711]
Total value: 449794.16
Training on 647957 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2200 (value: 0.0012, weighted value: 0.0578, policy: 0.1621, weighted policy: 0.1621), Train Mean Max: 0.9356
Epoch 2/10, Train Loss: 0.2092 (value: 0.0010, weighted value: 0.0517, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9360
Epoch 3/10, Train Loss: 0.2018 (value: 0.0010, weighted value: 0.0489, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9368
Epoch 4/10, Train Loss: 0.1992 (value: 0.0010, weighted value: 0.0475, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9367
Epoch 5/10, Train Loss: 0.1958 (value: 0.0009, weighted value: 0.0444, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1939 (value: 0.0009, weighted value: 0.0433, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9369
Epoch 7/10, Train Loss: 0.1932 (value: 0.0009, weighted value: 0.0427, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9371
Epoch 8/10, Train Loss: 0.1916 (value: 0.0008, weighted value: 0.0408, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9372
Epoch 9/10, Train Loss: 0.1929 (value: 0.0008, weighted value: 0.0421, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9373
Epoch 10/10, Train Loss: 0.1889 (value: 0.0008, weighted value: 0.0391, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9376
..training done in 73.52 seconds
..evaluation done in 14.42 seconds
Old network+MCTS average reward: 0.69, min: 0.13, max: 1.32, stdev: 0.23
New network+MCTS average reward: 0.69, min: 0.13, max: 1.32, stdev: 0.22
Old bare network average reward: 0.64, min: 0.09, max: 1.25, stdev: 0.23
New bare network average reward: 0.65, min: 0.06, max: 1.27, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.26, max: 1.06, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: 0.00, max: 1.18, stdev: 0.21
External policy "total greedy" average reward: 0.65, min: 0.11, max: 1.44, stdev: 0.22
New network won 61 and tied 172 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 219 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.37 seconds
Training examples lengths: [64840, 64635, 64841, 65090, 64781, 64735, 64943, 64746, 64711, 64732]
Total value: 450027.43
Training on 648054 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2461 (value: 0.0015, weighted value: 0.0748, policy: 0.1714, weighted policy: 0.1714), Train Mean Max: 0.9339
Epoch 2/10, Train Loss: 0.2299 (value: 0.0013, weighted value: 0.0657, policy: 0.1641, weighted policy: 0.1641), Train Mean Max: 0.9348
Epoch 3/10, Train Loss: 0.2188 (value: 0.0012, weighted value: 0.0594, policy: 0.1594, weighted policy: 0.1594), Train Mean Max: 0.9353
Epoch 4/10, Train Loss: 0.2122 (value: 0.0011, weighted value: 0.0557, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9355
Epoch 5/10, Train Loss: 0.2093 (value: 0.0011, weighted value: 0.0545, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9357
Epoch 6/10, Train Loss: 0.2033 (value: 0.0010, weighted value: 0.0506, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9361
Epoch 7/10, Train Loss: 0.2017 (value: 0.0010, weighted value: 0.0490, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9360
Epoch 8/10, Train Loss: 0.1997 (value: 0.0009, weighted value: 0.0469, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9363
Epoch 9/10, Train Loss: 0.1989 (value: 0.0009, weighted value: 0.0465, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9364
Epoch 10/10, Train Loss: 0.1963 (value: 0.0009, weighted value: 0.0448, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9366
..training done in 68.02 seconds
..evaluation done in 13.99 seconds
Old network+MCTS average reward: 0.70, min: 0.10, max: 1.36, stdev: 0.22
New network+MCTS average reward: 0.70, min: 0.14, max: 1.36, stdev: 0.22
Old bare network average reward: 0.65, min: 0.10, max: 1.32, stdev: 0.22
New bare network average reward: 0.66, min: 0.09, max: 1.32, stdev: 0.22
External policy "random" average reward: 0.27, min: -0.28, max: 0.81, stdev: 0.21
External policy "individual greedy" average reward: 0.55, min: -0.05, max: 1.13, stdev: 0.22
External policy "total greedy" average reward: 0.67, min: 0.10, max: 1.24, stdev: 0.21
New network won 71 and tied 163 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 220 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.50 seconds
Training examples lengths: [64635, 64841, 65090, 64781, 64735, 64943, 64746, 64711, 64732, 64855]
Total value: 450244.88
Training on 648069 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2236 (value: 0.0012, weighted value: 0.0611, policy: 0.1625, weighted policy: 0.1625), Train Mean Max: 0.9349
Epoch 2/10, Train Loss: 0.2156 (value: 0.0011, weighted value: 0.0569, policy: 0.1587, weighted policy: 0.1587), Train Mean Max: 0.9354
Epoch 3/10, Train Loss: 0.2078 (value: 0.0011, weighted value: 0.0526, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9359
Epoch 4/10, Train Loss: 0.2043 (value: 0.0010, weighted value: 0.0500, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9360
Epoch 5/10, Train Loss: 0.2022 (value: 0.0010, weighted value: 0.0487, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9362
Epoch 6/10, Train Loss: 0.1989 (value: 0.0009, weighted value: 0.0471, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9364
Epoch 7/10, Train Loss: 0.1979 (value: 0.0009, weighted value: 0.0455, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9366
Epoch 8/10, Train Loss: 0.1948 (value: 0.0009, weighted value: 0.0431, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9366
Epoch 9/10, Train Loss: 0.1928 (value: 0.0008, weighted value: 0.0420, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9369
Epoch 10/10, Train Loss: 0.1931 (value: 0.0009, weighted value: 0.0427, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9370
..training done in 67.08 seconds
..evaluation done in 14.50 seconds
Old network+MCTS average reward: 0.67, min: -0.04, max: 1.22, stdev: 0.23
New network+MCTS average reward: 0.67, min: -0.19, max: 1.23, stdev: 0.24
Old bare network average reward: 0.62, min: -0.16, max: 1.20, stdev: 0.24
New bare network average reward: 0.63, min: -0.08, max: 1.17, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.28, max: 0.99, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: -0.02, max: 1.08, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.03, max: 1.25, stdev: 0.22
New network won 81 and tied 151 out of 300 games (52.17% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1757697740_iter_220

Training iteration 221 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.09 seconds
Training examples lengths: [64841, 65090, 64781, 64735, 64943, 64746, 64711, 64732, 64855, 65035]
Total value: 450992.28
Training on 648469 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2222 (value: 0.0012, weighted value: 0.0595, policy: 0.1627, weighted policy: 0.1627), Train Mean Max: 0.9350
Epoch 2/10, Train Loss: 0.2105 (value: 0.0011, weighted value: 0.0535, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9358
Epoch 3/10, Train Loss: 0.2035 (value: 0.0010, weighted value: 0.0494, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9362
Epoch 4/10, Train Loss: 0.2014 (value: 0.0010, weighted value: 0.0482, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9363
Epoch 5/10, Train Loss: 0.1983 (value: 0.0009, weighted value: 0.0465, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9365
Epoch 6/10, Train Loss: 0.1960 (value: 0.0009, weighted value: 0.0444, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9366
Epoch 7/10, Train Loss: 0.1961 (value: 0.0009, weighted value: 0.0447, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9368
Epoch 8/10, Train Loss: 0.1926 (value: 0.0008, weighted value: 0.0417, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9370
Epoch 9/10, Train Loss: 0.1915 (value: 0.0008, weighted value: 0.0408, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9370
Epoch 10/10, Train Loss: 0.1905 (value: 0.0008, weighted value: 0.0407, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9374
..training done in 67.26 seconds
..evaluation done in 14.30 seconds
Old network+MCTS average reward: 0.70, min: 0.12, max: 1.49, stdev: 0.24
New network+MCTS average reward: 0.70, min: 0.15, max: 1.49, stdev: 0.24
Old bare network average reward: 0.66, min: 0.04, max: 1.49, stdev: 0.24
New bare network average reward: 0.66, min: 0.04, max: 1.49, stdev: 0.24
External policy "random" average reward: 0.28, min: -0.29, max: 1.05, stdev: 0.24
External policy "individual greedy" average reward: 0.55, min: -0.17, max: 1.38, stdev: 0.25
External policy "total greedy" average reward: 0.67, min: 0.03, max: 1.47, stdev: 0.24
New network won 76 and tied 156 out of 300 games (51.33% wins where ties are half wins)
Keeping the new network

Training iteration 222 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.25 seconds
Training examples lengths: [65090, 64781, 64735, 64943, 64746, 64711, 64732, 64855, 65035, 64623]
Total value: 451476.26
Training on 648251 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2186 (value: 0.0011, weighted value: 0.0575, policy: 0.1611, weighted policy: 0.1611), Train Mean Max: 0.9353
Epoch 2/10, Train Loss: 0.2100 (value: 0.0011, weighted value: 0.0528, policy: 0.1572, weighted policy: 0.1572), Train Mean Max: 0.9360
Epoch 3/10, Train Loss: 0.2041 (value: 0.0010, weighted value: 0.0499, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9364
Epoch 4/10, Train Loss: 0.1999 (value: 0.0009, weighted value: 0.0469, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9364
Epoch 5/10, Train Loss: 0.1967 (value: 0.0009, weighted value: 0.0450, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9367
Epoch 6/10, Train Loss: 0.1962 (value: 0.0009, weighted value: 0.0451, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9369
Epoch 7/10, Train Loss: 0.1924 (value: 0.0008, weighted value: 0.0423, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9371
Epoch 8/10, Train Loss: 0.1922 (value: 0.0008, weighted value: 0.0421, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9371
Epoch 9/10, Train Loss: 0.1899 (value: 0.0008, weighted value: 0.0403, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9373
Epoch 10/10, Train Loss: 0.1892 (value: 0.0008, weighted value: 0.0391, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9374
..training done in 66.92 seconds
..evaluation done in 14.37 seconds
Old network+MCTS average reward: 0.69, min: 0.13, max: 1.47, stdev: 0.24
New network+MCTS average reward: 0.69, min: 0.04, max: 1.47, stdev: 0.24
Old bare network average reward: 0.64, min: 0.02, max: 1.45, stdev: 0.25
New bare network average reward: 0.63, min: -0.04, max: 1.39, stdev: 0.25
External policy "random" average reward: 0.26, min: -0.34, max: 1.12, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: -0.03, max: 1.60, stdev: 0.24
External policy "total greedy" average reward: 0.65, min: -0.06, max: 1.53, stdev: 0.24
New network won 67 and tied 162 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 223 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.60 seconds
Training examples lengths: [64781, 64735, 64943, 64746, 64711, 64732, 64855, 65035, 64623, 64660]
Total value: 450895.28
Training on 647821 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2470 (value: 0.0015, weighted value: 0.0768, policy: 0.1702, weighted policy: 0.1702), Train Mean Max: 0.9337
Epoch 2/10, Train Loss: 0.2285 (value: 0.0013, weighted value: 0.0653, policy: 0.1632, weighted policy: 0.1632), Train Mean Max: 0.9348
Epoch 3/10, Train Loss: 0.2211 (value: 0.0012, weighted value: 0.0621, policy: 0.1591, weighted policy: 0.1591), Train Mean Max: 0.9352
Epoch 4/10, Train Loss: 0.2118 (value: 0.0011, weighted value: 0.0558, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9354
Epoch 5/10, Train Loss: 0.2087 (value: 0.0011, weighted value: 0.0545, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9358
Epoch 6/10, Train Loss: 0.2054 (value: 0.0010, weighted value: 0.0517, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9358
Epoch 7/10, Train Loss: 0.2029 (value: 0.0010, weighted value: 0.0502, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9360
Epoch 8/10, Train Loss: 0.1992 (value: 0.0009, weighted value: 0.0471, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9363
Epoch 9/10, Train Loss: 0.2003 (value: 0.0010, weighted value: 0.0477, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9362
Epoch 10/10, Train Loss: 0.1947 (value: 0.0009, weighted value: 0.0439, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9367
..training done in 72.93 seconds
..evaluation done in 15.19 seconds
Old network+MCTS average reward: 0.70, min: 0.21, max: 1.39, stdev: 0.22
New network+MCTS average reward: 0.70, min: 0.19, max: 1.29, stdev: 0.22
Old bare network average reward: 0.64, min: -0.03, max: 1.34, stdev: 0.23
New bare network average reward: 0.65, min: -0.03, max: 1.34, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.40, max: 0.79, stdev: 0.24
External policy "individual greedy" average reward: 0.54, min: 0.00, max: 1.21, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.04, max: 1.38, stdev: 0.23
New network won 56 and tied 165 out of 300 games (46.17% wins where ties are half wins)
Reverting to the old network

Training iteration 224 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.47 seconds
Training examples lengths: [64735, 64943, 64746, 64711, 64732, 64855, 65035, 64623, 64660, 64455]
Total value: 450740.18
Training on 647495 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2717 (value: 0.0019, weighted value: 0.0929, policy: 0.1787, weighted policy: 0.1787), Train Mean Max: 0.9323
Epoch 2/10, Train Loss: 0.2479 (value: 0.0016, weighted value: 0.0782, policy: 0.1697, weighted policy: 0.1697), Train Mean Max: 0.9334
Epoch 3/10, Train Loss: 0.2330 (value: 0.0014, weighted value: 0.0711, policy: 0.1619, weighted policy: 0.1619), Train Mean Max: 0.9342
Epoch 4/10, Train Loss: 0.2248 (value: 0.0013, weighted value: 0.0665, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9343
Epoch 5/10, Train Loss: 0.2191 (value: 0.0012, weighted value: 0.0617, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9347
Epoch 6/10, Train Loss: 0.2143 (value: 0.0012, weighted value: 0.0585, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9350
Epoch 7/10, Train Loss: 0.2101 (value: 0.0011, weighted value: 0.0555, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9351
Epoch 8/10, Train Loss: 0.2081 (value: 0.0011, weighted value: 0.0538, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9354
Epoch 9/10, Train Loss: 0.2049 (value: 0.0010, weighted value: 0.0513, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9356
Epoch 10/10, Train Loss: 0.2024 (value: 0.0010, weighted value: 0.0489, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9357
..training done in 68.72 seconds
..evaluation done in 14.60 seconds
Old network+MCTS average reward: 0.67, min: 0.06, max: 1.48, stdev: 0.23
New network+MCTS average reward: 0.67, min: 0.02, max: 1.46, stdev: 0.23
Old bare network average reward: 0.62, min: -0.02, max: 1.34, stdev: 0.24
New bare network average reward: 0.62, min: -0.02, max: 1.37, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.36, max: 0.99, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: -0.06, max: 1.36, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.03, max: 1.44, stdev: 0.22
New network won 84 and tied 144 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network

Training iteration 225 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.21 seconds
Training examples lengths: [64943, 64746, 64711, 64732, 64855, 65035, 64623, 64660, 64455, 64800]
Total value: 451169.48
Training on 647560 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2294 (value: 0.0013, weighted value: 0.0670, policy: 0.1625, weighted policy: 0.1625), Train Mean Max: 0.9340
Epoch 2/10, Train Loss: 0.2162 (value: 0.0012, weighted value: 0.0579, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9347
Epoch 3/10, Train Loss: 0.2122 (value: 0.0011, weighted value: 0.0554, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9351
Epoch 4/10, Train Loss: 0.2088 (value: 0.0011, weighted value: 0.0538, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9353
Epoch 5/10, Train Loss: 0.2047 (value: 0.0010, weighted value: 0.0511, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9356
Epoch 6/10, Train Loss: 0.2022 (value: 0.0010, weighted value: 0.0485, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9357
Epoch 7/10, Train Loss: 0.2003 (value: 0.0009, weighted value: 0.0472, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9358
Epoch 8/10, Train Loss: 0.1975 (value: 0.0009, weighted value: 0.0455, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9363
Epoch 9/10, Train Loss: 0.1975 (value: 0.0009, weighted value: 0.0456, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9364
Epoch 10/10, Train Loss: 0.1944 (value: 0.0009, weighted value: 0.0436, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9366
..training done in 68.25 seconds
..evaluation done in 14.15 seconds
Old network+MCTS average reward: 0.69, min: 0.08, max: 1.30, stdev: 0.22
New network+MCTS average reward: 0.70, min: 0.09, max: 1.41, stdev: 0.23
Old bare network average reward: 0.65, min: 0.00, max: 1.29, stdev: 0.23
New bare network average reward: 0.65, min: 0.01, max: 1.34, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.37, max: 0.88, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.10, max: 1.51, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.12, max: 1.44, stdev: 0.23
New network won 74 and tied 154 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 226 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.89 seconds
Training examples lengths: [64746, 64711, 64732, 64855, 65035, 64623, 64660, 64455, 64800, 64978]
Total value: 451563.29
Training on 647595 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2232 (value: 0.0012, weighted value: 0.0602, policy: 0.1630, weighted policy: 0.1630), Train Mean Max: 0.9346
Epoch 2/10, Train Loss: 0.2134 (value: 0.0011, weighted value: 0.0549, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9353
Epoch 3/10, Train Loss: 0.2083 (value: 0.0011, weighted value: 0.0526, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9356
Epoch 4/10, Train Loss: 0.2032 (value: 0.0010, weighted value: 0.0495, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9360
Epoch 5/10, Train Loss: 0.1995 (value: 0.0009, weighted value: 0.0475, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9362
Epoch 6/10, Train Loss: 0.1980 (value: 0.0009, weighted value: 0.0454, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9362
Epoch 7/10, Train Loss: 0.1950 (value: 0.0009, weighted value: 0.0436, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9364
Epoch 8/10, Train Loss: 0.1947 (value: 0.0009, weighted value: 0.0431, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9365
Epoch 9/10, Train Loss: 0.1936 (value: 0.0008, weighted value: 0.0423, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9367
Epoch 10/10, Train Loss: 0.1913 (value: 0.0008, weighted value: 0.0400, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9368
..training done in 70.95 seconds
..evaluation done in 14.68 seconds
Old network+MCTS average reward: 0.69, min: 0.14, max: 1.56, stdev: 0.23
New network+MCTS average reward: 0.70, min: 0.11, max: 1.62, stdev: 0.23
Old bare network average reward: 0.64, min: 0.10, max: 1.46, stdev: 0.23
New bare network average reward: 0.65, min: 0.12, max: 1.35, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.39, max: 1.03, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.16, max: 1.40, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.10, max: 1.41, stdev: 0.22
New network won 75 and tied 159 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 227 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.70 seconds
Training examples lengths: [64711, 64732, 64855, 65035, 64623, 64660, 64455, 64800, 64978, 64826]
Total value: 451696.76
Training on 647675 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2232 (value: 0.0012, weighted value: 0.0620, policy: 0.1612, weighted policy: 0.1612), Train Mean Max: 0.9352
Epoch 2/10, Train Loss: 0.2100 (value: 0.0011, weighted value: 0.0530, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9359
Epoch 3/10, Train Loss: 0.2040 (value: 0.0010, weighted value: 0.0501, policy: 0.1539, weighted policy: 0.1539), Train Mean Max: 0.9361
Epoch 4/10, Train Loss: 0.2001 (value: 0.0009, weighted value: 0.0469, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9362
Epoch 5/10, Train Loss: 0.1993 (value: 0.0009, weighted value: 0.0468, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9365
Epoch 6/10, Train Loss: 0.1953 (value: 0.0009, weighted value: 0.0437, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9366
Epoch 7/10, Train Loss: 0.1962 (value: 0.0009, weighted value: 0.0451, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9368
Epoch 8/10, Train Loss: 0.1925 (value: 0.0008, weighted value: 0.0411, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9370
Epoch 9/10, Train Loss: 0.1917 (value: 0.0008, weighted value: 0.0415, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9371
Epoch 10/10, Train Loss: 0.1897 (value: 0.0008, weighted value: 0.0393, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9372
..training done in 68.88 seconds
..evaluation done in 15.13 seconds
Old network+MCTS average reward: 0.68, min: 0.13, max: 1.34, stdev: 0.21
New network+MCTS average reward: 0.68, min: 0.13, max: 1.34, stdev: 0.21
Old bare network average reward: 0.64, min: 0.11, max: 1.31, stdev: 0.22
New bare network average reward: 0.63, min: 0.11, max: 1.34, stdev: 0.22
External policy "random" average reward: 0.25, min: -0.30, max: 1.01, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: -0.02, max: 1.33, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.17, max: 1.32, stdev: 0.21
New network won 70 and tied 144 out of 300 games (47.33% wins where ties are half wins)
Reverting to the old network

Training iteration 228 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.27 seconds
Training examples lengths: [64732, 64855, 65035, 64623, 64660, 64455, 64800, 64978, 64826, 64668]
Total value: 451978.07
Training on 647632 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2493 (value: 0.0016, weighted value: 0.0778, policy: 0.1715, weighted policy: 0.1715), Train Mean Max: 0.9334
Epoch 2/10, Train Loss: 0.2314 (value: 0.0013, weighted value: 0.0671, policy: 0.1643, weighted policy: 0.1643), Train Mean Max: 0.9342
Epoch 3/10, Train Loss: 0.2201 (value: 0.0012, weighted value: 0.0611, policy: 0.1589, weighted policy: 0.1589), Train Mean Max: 0.9349
Epoch 4/10, Train Loss: 0.2136 (value: 0.0011, weighted value: 0.0567, policy: 0.1569, weighted policy: 0.1569), Train Mean Max: 0.9350
Epoch 5/10, Train Loss: 0.2106 (value: 0.0011, weighted value: 0.0545, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9352
Epoch 6/10, Train Loss: 0.2059 (value: 0.0010, weighted value: 0.0511, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9356
Epoch 7/10, Train Loss: 0.2034 (value: 0.0010, weighted value: 0.0499, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9357
Epoch 8/10, Train Loss: 0.2014 (value: 0.0010, weighted value: 0.0482, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9358
Epoch 9/10, Train Loss: 0.1999 (value: 0.0009, weighted value: 0.0472, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9359
Epoch 10/10, Train Loss: 0.1963 (value: 0.0009, weighted value: 0.0440, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9363
..training done in 68.55 seconds
..evaluation done in 14.41 seconds
Old network+MCTS average reward: 0.69, min: 0.17, max: 1.42, stdev: 0.22
New network+MCTS average reward: 0.69, min: 0.14, max: 1.42, stdev: 0.23
Old bare network average reward: 0.65, min: 0.09, max: 1.42, stdev: 0.23
New bare network average reward: 0.65, min: 0.09, max: 1.32, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.43, max: 0.95, stdev: 0.21
External policy "individual greedy" average reward: 0.53, min: 0.01, max: 1.20, stdev: 0.21
External policy "total greedy" average reward: 0.65, min: 0.17, max: 1.36, stdev: 0.21
New network won 71 and tied 150 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 229 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.40 seconds
Training examples lengths: [64855, 65035, 64623, 64660, 64455, 64800, 64978, 64826, 64668, 64598]
Total value: 451962.68
Training on 647498 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2759 (value: 0.0019, weighted value: 0.0961, policy: 0.1798, weighted policy: 0.1798), Train Mean Max: 0.9319
Epoch 2/10, Train Loss: 0.2503 (value: 0.0016, weighted value: 0.0788, policy: 0.1715, weighted policy: 0.1715), Train Mean Max: 0.9328
Epoch 3/10, Train Loss: 0.2345 (value: 0.0014, weighted value: 0.0711, policy: 0.1634, weighted policy: 0.1634), Train Mean Max: 0.9336
Epoch 4/10, Train Loss: 0.2262 (value: 0.0013, weighted value: 0.0659, policy: 0.1603, weighted policy: 0.1603), Train Mean Max: 0.9340
Epoch 5/10, Train Loss: 0.2214 (value: 0.0013, weighted value: 0.0631, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9340
Epoch 6/10, Train Loss: 0.2165 (value: 0.0012, weighted value: 0.0591, policy: 0.1573, weighted policy: 0.1573), Train Mean Max: 0.9343
Epoch 7/10, Train Loss: 0.2121 (value: 0.0011, weighted value: 0.0558, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9345
Epoch 8/10, Train Loss: 0.2095 (value: 0.0011, weighted value: 0.0534, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9348
Epoch 9/10, Train Loss: 0.2092 (value: 0.0011, weighted value: 0.0535, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9349
Epoch 10/10, Train Loss: 0.2051 (value: 0.0010, weighted value: 0.0504, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9351
..training done in 72.08 seconds
..evaluation done in 14.46 seconds
Old network+MCTS average reward: 0.69, min: 0.18, max: 1.32, stdev: 0.23
New network+MCTS average reward: 0.70, min: 0.16, max: 1.32, stdev: 0.22
Old bare network average reward: 0.65, min: 0.10, max: 1.32, stdev: 0.23
New bare network average reward: 0.65, min: 0.02, max: 1.22, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.38, max: 0.94, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: -0.01, max: 1.21, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.09, max: 1.38, stdev: 0.22
New network won 63 and tied 171 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 230 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.58 seconds
Training examples lengths: [65035, 64623, 64660, 64455, 64800, 64978, 64826, 64668, 64598, 64884]
Total value: 452672.56
Training on 647527 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3010 (value: 0.0023, weighted value: 0.1130, policy: 0.1880, weighted policy: 0.1880), Train Mean Max: 0.9306
Epoch 2/10, Train Loss: 0.2664 (value: 0.0018, weighted value: 0.0915, policy: 0.1749, weighted policy: 0.1749), Train Mean Max: 0.9319
Epoch 3/10, Train Loss: 0.2484 (value: 0.0016, weighted value: 0.0815, policy: 0.1669, weighted policy: 0.1669), Train Mean Max: 0.9324
Epoch 4/10, Train Loss: 0.2413 (value: 0.0015, weighted value: 0.0769, policy: 0.1643, weighted policy: 0.1643), Train Mean Max: 0.9328
Epoch 5/10, Train Loss: 0.2327 (value: 0.0014, weighted value: 0.0706, policy: 0.1621, weighted policy: 0.1621), Train Mean Max: 0.9329
Epoch 6/10, Train Loss: 0.2246 (value: 0.0013, weighted value: 0.0653, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9334
Epoch 7/10, Train Loss: 0.2218 (value: 0.0013, weighted value: 0.0634, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9335
Epoch 8/10, Train Loss: 0.2182 (value: 0.0012, weighted value: 0.0592, policy: 0.1589, weighted policy: 0.1589), Train Mean Max: 0.9337
Epoch 9/10, Train Loss: 0.2139 (value: 0.0011, weighted value: 0.0569, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9340
Epoch 10/10, Train Loss: 0.2116 (value: 0.0011, weighted value: 0.0550, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9342
..training done in 71.63 seconds
..evaluation done in 14.86 seconds
Old network+MCTS average reward: 0.67, min: 0.11, max: 1.37, stdev: 0.22
New network+MCTS average reward: 0.67, min: 0.14, max: 1.37, stdev: 0.22
Old bare network average reward: 0.62, min: -0.06, max: 1.37, stdev: 0.23
New bare network average reward: 0.62, min: -0.10, max: 1.37, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.34, max: 0.97, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.08, max: 1.33, stdev: 0.22
External policy "total greedy" average reward: 0.63, min: 0.05, max: 1.41, stdev: 0.21
New network won 72 and tied 145 out of 300 games (48.17% wins where ties are half wins)
Reverting to the old network

Training iteration 231 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.96 seconds
Training examples lengths: [64623, 64660, 64455, 64800, 64978, 64826, 64668, 64598, 64884, 64756]
Total value: 452375.84
Training on 647248 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3239 (value: 0.0026, weighted value: 0.1294, policy: 0.1945, weighted policy: 0.1945), Train Mean Max: 0.9294
Epoch 2/10, Train Loss: 0.2867 (value: 0.0021, weighted value: 0.1035, policy: 0.1833, weighted policy: 0.1833), Train Mean Max: 0.9304
Epoch 3/10, Train Loss: 0.2638 (value: 0.0018, weighted value: 0.0907, policy: 0.1731, weighted policy: 0.1731), Train Mean Max: 0.9312
Epoch 4/10, Train Loss: 0.2543 (value: 0.0017, weighted value: 0.0840, policy: 0.1703, weighted policy: 0.1703), Train Mean Max: 0.9315
Epoch 5/10, Train Loss: 0.2436 (value: 0.0016, weighted value: 0.0787, policy: 0.1649, weighted policy: 0.1649), Train Mean Max: 0.9319
Epoch 6/10, Train Loss: 0.2362 (value: 0.0015, weighted value: 0.0726, policy: 0.1636, weighted policy: 0.1636), Train Mean Max: 0.9320
Epoch 7/10, Train Loss: 0.2315 (value: 0.0014, weighted value: 0.0677, policy: 0.1638, weighted policy: 0.1638), Train Mean Max: 0.9323
Epoch 8/10, Train Loss: 0.2254 (value: 0.0013, weighted value: 0.0652, policy: 0.1602, weighted policy: 0.1602), Train Mean Max: 0.9328
Epoch 9/10, Train Loss: 0.2242 (value: 0.0013, weighted value: 0.0636, policy: 0.1606, weighted policy: 0.1606), Train Mean Max: 0.9328
Epoch 10/10, Train Loss: 0.2182 (value: 0.0012, weighted value: 0.0583, policy: 0.1599, weighted policy: 0.1599), Train Mean Max: 0.9330
..training done in 67.44 seconds
..evaluation done in 14.98 seconds
Old network+MCTS average reward: 0.72, min: 0.12, max: 1.48, stdev: 0.23
New network+MCTS average reward: 0.73, min: 0.19, max: 1.38, stdev: 0.23
Old bare network average reward: 0.68, min: 0.14, max: 1.48, stdev: 0.24
New bare network average reward: 0.68, min: 0.13, max: 1.48, stdev: 0.24
External policy "random" average reward: 0.28, min: -0.27, max: 1.10, stdev: 0.23
External policy "individual greedy" average reward: 0.56, min: -0.01, max: 1.46, stdev: 0.23
External policy "total greedy" average reward: 0.68, min: 0.15, max: 1.50, stdev: 0.23
New network won 96 and tied 138 out of 300 games (55.00% wins where ties are half wins)
Keeping the new network

Training iteration 232 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.97 seconds
Training examples lengths: [64660, 64455, 64800, 64978, 64826, 64668, 64598, 64884, 64756, 64865]
Total value: 452883.38
Training on 647490 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2418 (value: 0.0014, weighted value: 0.0724, policy: 0.1694, weighted policy: 0.1694), Train Mean Max: 0.9319
Epoch 2/10, Train Loss: 0.2313 (value: 0.0013, weighted value: 0.0675, policy: 0.1639, weighted policy: 0.1639), Train Mean Max: 0.9326
Epoch 3/10, Train Loss: 0.2232 (value: 0.0012, weighted value: 0.0623, policy: 0.1608, weighted policy: 0.1608), Train Mean Max: 0.9330
Epoch 4/10, Train Loss: 0.2199 (value: 0.0012, weighted value: 0.0606, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9332
Epoch 5/10, Train Loss: 0.2137 (value: 0.0011, weighted value: 0.0554, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9335
Epoch 6/10, Train Loss: 0.2119 (value: 0.0011, weighted value: 0.0548, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9337
Epoch 7/10, Train Loss: 0.2099 (value: 0.0011, weighted value: 0.0532, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9339
Epoch 8/10, Train Loss: 0.2072 (value: 0.0010, weighted value: 0.0513, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9341
Epoch 9/10, Train Loss: 0.2044 (value: 0.0010, weighted value: 0.0489, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9345
Epoch 10/10, Train Loss: 0.2022 (value: 0.0009, weighted value: 0.0475, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9347
..training done in 69.78 seconds
..evaluation done in 15.09 seconds
Old network+MCTS average reward: 0.72, min: -0.07, max: 1.37, stdev: 0.22
New network+MCTS average reward: 0.71, min: -0.10, max: 1.37, stdev: 0.22
Old bare network average reward: 0.66, min: -0.21, max: 1.37, stdev: 0.23
New bare network average reward: 0.67, min: -0.11, max: 1.37, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.31, max: 0.92, stdev: 0.22
External policy "individual greedy" average reward: 0.57, min: -0.07, max: 1.29, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: 0.01, max: 1.30, stdev: 0.22
New network won 75 and tied 148 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 233 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.89 seconds
Training examples lengths: [64455, 64800, 64978, 64826, 64668, 64598, 64884, 64756, 64865, 65150]
Total value: 453407.04
Training on 647980 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2686 (value: 0.0018, weighted value: 0.0907, policy: 0.1779, weighted policy: 0.1779), Train Mean Max: 0.9303
Epoch 2/10, Train Loss: 0.2485 (value: 0.0016, weighted value: 0.0784, policy: 0.1701, weighted policy: 0.1701), Train Mean Max: 0.9313
Epoch 3/10, Train Loss: 0.2367 (value: 0.0014, weighted value: 0.0722, policy: 0.1645, weighted policy: 0.1645), Train Mean Max: 0.9319
Epoch 4/10, Train Loss: 0.2285 (value: 0.0013, weighted value: 0.0662, policy: 0.1623, weighted policy: 0.1623), Train Mean Max: 0.9321
Epoch 5/10, Train Loss: 0.2261 (value: 0.0013, weighted value: 0.0643, policy: 0.1618, weighted policy: 0.1618), Train Mean Max: 0.9324
Epoch 6/10, Train Loss: 0.2216 (value: 0.0012, weighted value: 0.0612, policy: 0.1604, weighted policy: 0.1604), Train Mean Max: 0.9326
Epoch 7/10, Train Loss: 0.2169 (value: 0.0012, weighted value: 0.0581, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9330
Epoch 8/10, Train Loss: 0.2139 (value: 0.0011, weighted value: 0.0560, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9331
Epoch 9/10, Train Loss: 0.2108 (value: 0.0011, weighted value: 0.0530, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9334
Epoch 10/10, Train Loss: 0.2106 (value: 0.0011, weighted value: 0.0530, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9337
..training done in 68.77 seconds
..evaluation done in 14.72 seconds
Old network+MCTS average reward: 0.69, min: 0.05, max: 1.48, stdev: 0.24
New network+MCTS average reward: 0.69, min: 0.04, max: 1.48, stdev: 0.24
Old bare network average reward: 0.64, min: -0.07, max: 1.48, stdev: 0.25
New bare network average reward: 0.64, min: -0.02, max: 1.48, stdev: 0.25
External policy "random" average reward: 0.27, min: -0.40, max: 1.01, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.17, max: 1.36, stdev: 0.24
External policy "total greedy" average reward: 0.66, min: -0.06, max: 1.51, stdev: 0.23
New network won 74 and tied 143 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 234 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.57 seconds
Training examples lengths: [64800, 64978, 64826, 64668, 64598, 64884, 64756, 64865, 65150, 64868]
Total value: 453687.56
Training on 648393 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2907 (value: 0.0021, weighted value: 0.1047, policy: 0.1859, weighted policy: 0.1859), Train Mean Max: 0.9292
Epoch 2/10, Train Loss: 0.2651 (value: 0.0018, weighted value: 0.0899, policy: 0.1751, weighted policy: 0.1751), Train Mean Max: 0.9303
Epoch 3/10, Train Loss: 0.2519 (value: 0.0016, weighted value: 0.0819, policy: 0.1699, weighted policy: 0.1699), Train Mean Max: 0.9306
Epoch 4/10, Train Loss: 0.2416 (value: 0.0015, weighted value: 0.0749, policy: 0.1667, weighted policy: 0.1667), Train Mean Max: 0.9310
Epoch 5/10, Train Loss: 0.2355 (value: 0.0014, weighted value: 0.0708, policy: 0.1647, weighted policy: 0.1647), Train Mean Max: 0.9313
Epoch 6/10, Train Loss: 0.2313 (value: 0.0014, weighted value: 0.0686, policy: 0.1627, weighted policy: 0.1627), Train Mean Max: 0.9317
Epoch 7/10, Train Loss: 0.2242 (value: 0.0013, weighted value: 0.0632, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9319
Epoch 8/10, Train Loss: 0.2218 (value: 0.0012, weighted value: 0.0610, policy: 0.1608, weighted policy: 0.1608), Train Mean Max: 0.9321
Epoch 9/10, Train Loss: 0.2170 (value: 0.0011, weighted value: 0.0575, policy: 0.1595, weighted policy: 0.1595), Train Mean Max: 0.9325
Epoch 10/10, Train Loss: 0.2143 (value: 0.0011, weighted value: 0.0554, policy: 0.1589, weighted policy: 0.1589), Train Mean Max: 0.9328
..training done in 67.71 seconds
..evaluation done in 15.35 seconds
Old network+MCTS average reward: 0.71, min: 0.07, max: 1.57, stdev: 0.21
New network+MCTS average reward: 0.71, min: 0.19, max: 1.50, stdev: 0.21
Old bare network average reward: 0.65, min: 0.09, max: 1.41, stdev: 0.22
New bare network average reward: 0.66, min: 0.01, max: 1.42, stdev: 0.22
External policy "random" average reward: 0.28, min: -0.34, max: 1.06, stdev: 0.23
External policy "individual greedy" average reward: 0.56, min: 0.04, max: 1.23, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: 0.19, max: 1.56, stdev: 0.22
New network won 76 and tied 144 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 235 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.82 seconds
Training examples lengths: [64978, 64826, 64668, 64598, 64884, 64756, 64865, 65150, 64868, 64872]
Total value: 453570.80
Training on 648465 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3152 (value: 0.0024, weighted value: 0.1210, policy: 0.1942, weighted policy: 0.1942), Train Mean Max: 0.9281
Epoch 2/10, Train Loss: 0.2798 (value: 0.0020, weighted value: 0.1000, policy: 0.1798, weighted policy: 0.1798), Train Mean Max: 0.9292
Epoch 3/10, Train Loss: 0.2630 (value: 0.0018, weighted value: 0.0906, policy: 0.1724, weighted policy: 0.1724), Train Mean Max: 0.9298
Epoch 4/10, Train Loss: 0.2536 (value: 0.0017, weighted value: 0.0841, policy: 0.1695, weighted policy: 0.1695), Train Mean Max: 0.9300
Epoch 5/10, Train Loss: 0.2436 (value: 0.0015, weighted value: 0.0766, policy: 0.1669, weighted policy: 0.1669), Train Mean Max: 0.9306
Epoch 6/10, Train Loss: 0.2373 (value: 0.0015, weighted value: 0.0727, policy: 0.1646, weighted policy: 0.1646), Train Mean Max: 0.9307
Epoch 7/10, Train Loss: 0.2328 (value: 0.0014, weighted value: 0.0689, policy: 0.1639, weighted policy: 0.1639), Train Mean Max: 0.9311
Epoch 8/10, Train Loss: 0.2281 (value: 0.0013, weighted value: 0.0656, policy: 0.1625, weighted policy: 0.1625), Train Mean Max: 0.9314
Epoch 9/10, Train Loss: 0.2238 (value: 0.0012, weighted value: 0.0623, policy: 0.1615, weighted policy: 0.1615), Train Mean Max: 0.9317
Epoch 10/10, Train Loss: 0.2206 (value: 0.0012, weighted value: 0.0597, policy: 0.1609, weighted policy: 0.1609), Train Mean Max: 0.9320
..training done in 68.26 seconds
..evaluation done in 14.74 seconds
Old network+MCTS average reward: 0.67, min: 0.04, max: 1.43, stdev: 0.22
New network+MCTS average reward: 0.67, min: 0.02, max: 1.43, stdev: 0.22
Old bare network average reward: 0.62, min: 0.04, max: 1.50, stdev: 0.23
New bare network average reward: 0.62, min: 0.02, max: 1.27, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.34, max: 1.06, stdev: 0.23
External policy "individual greedy" average reward: 0.51, min: -0.03, max: 1.27, stdev: 0.24
External policy "total greedy" average reward: 0.63, min: 0.06, max: 1.40, stdev: 0.23
New network won 78 and tied 136 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 236 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.33 seconds
Training examples lengths: [64826, 64668, 64598, 64884, 64756, 64865, 65150, 64868, 64872, 64748]
Total value: 453717.34
Training on 648235 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3341 (value: 0.0027, weighted value: 0.1350, policy: 0.1990, weighted policy: 0.1990), Train Mean Max: 0.9273
Epoch 2/10, Train Loss: 0.2937 (value: 0.0022, weighted value: 0.1121, policy: 0.1816, weighted policy: 0.1816), Train Mean Max: 0.9285
Epoch 3/10, Train Loss: 0.2731 (value: 0.0020, weighted value: 0.0987, policy: 0.1744, weighted policy: 0.1744), Train Mean Max: 0.9290
Epoch 4/10, Train Loss: 0.2612 (value: 0.0018, weighted value: 0.0894, policy: 0.1718, weighted policy: 0.1718), Train Mean Max: 0.9294
Epoch 5/10, Train Loss: 0.2538 (value: 0.0017, weighted value: 0.0845, policy: 0.1693, weighted policy: 0.1693), Train Mean Max: 0.9296
Epoch 6/10, Train Loss: 0.2456 (value: 0.0016, weighted value: 0.0787, policy: 0.1669, weighted policy: 0.1669), Train Mean Max: 0.9301
Epoch 7/10, Train Loss: 0.2383 (value: 0.0015, weighted value: 0.0735, policy: 0.1649, weighted policy: 0.1649), Train Mean Max: 0.9305
Epoch 8/10, Train Loss: 0.2339 (value: 0.0014, weighted value: 0.0695, policy: 0.1644, weighted policy: 0.1644), Train Mean Max: 0.9308
Epoch 9/10, Train Loss: 0.2290 (value: 0.0013, weighted value: 0.0659, policy: 0.1631, weighted policy: 0.1631), Train Mean Max: 0.9310
Epoch 10/10, Train Loss: 0.2252 (value: 0.0013, weighted value: 0.0635, policy: 0.1617, weighted policy: 0.1617), Train Mean Max: 0.9315
..training done in 71.27 seconds
..evaluation done in 15.17 seconds
Old network+MCTS average reward: 0.66, min: -0.02, max: 1.57, stdev: 0.22
New network+MCTS average reward: 0.67, min: 0.00, max: 1.59, stdev: 0.23
Old bare network average reward: 0.62, min: 0.00, max: 1.57, stdev: 0.24
New bare network average reward: 0.62, min: -0.06, max: 1.49, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.35, max: 1.15, stdev: 0.23
External policy "individual greedy" average reward: 0.52, min: -0.01, max: 1.33, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: 0.13, max: 1.41, stdev: 0.22
New network won 80 and tied 157 out of 300 games (52.83% wins where ties are half wins)
Keeping the new network

Training iteration 237 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.12 seconds
Training examples lengths: [64668, 64598, 64884, 64756, 64865, 65150, 64868, 64872, 64748, 64716]
Total value: 454005.26
Training on 648125 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2437 (value: 0.0015, weighted value: 0.0749, policy: 0.1688, weighted policy: 0.1688), Train Mean Max: 0.9306
Epoch 2/10, Train Loss: 0.2342 (value: 0.0014, weighted value: 0.0684, policy: 0.1658, weighted policy: 0.1658), Train Mean Max: 0.9311
Epoch 3/10, Train Loss: 0.2295 (value: 0.0013, weighted value: 0.0668, policy: 0.1628, weighted policy: 0.1628), Train Mean Max: 0.9316
Epoch 4/10, Train Loss: 0.2211 (value: 0.0012, weighted value: 0.0602, policy: 0.1609, weighted policy: 0.1609), Train Mean Max: 0.9320
Epoch 5/10, Train Loss: 0.2197 (value: 0.0012, weighted value: 0.0597, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9323
Epoch 6/10, Train Loss: 0.2154 (value: 0.0011, weighted value: 0.0567, policy: 0.1587, weighted policy: 0.1587), Train Mean Max: 0.9325
Epoch 7/10, Train Loss: 0.2124 (value: 0.0011, weighted value: 0.0542, policy: 0.1582, weighted policy: 0.1582), Train Mean Max: 0.9327
Epoch 8/10, Train Loss: 0.2117 (value: 0.0011, weighted value: 0.0535, policy: 0.1582, weighted policy: 0.1582), Train Mean Max: 0.9329
Epoch 9/10, Train Loss: 0.2078 (value: 0.0010, weighted value: 0.0511, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9335
Epoch 10/10, Train Loss: 0.2049 (value: 0.0010, weighted value: 0.0488, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9337
..training done in 65.35 seconds
..evaluation done in 14.53 seconds
Old network+MCTS average reward: 0.72, min: 0.19, max: 1.48, stdev: 0.25
New network+MCTS average reward: 0.71, min: 0.19, max: 1.48, stdev: 0.24
Old bare network average reward: 0.66, min: 0.03, max: 1.51, stdev: 0.26
New bare network average reward: 0.67, min: 0.07, max: 1.54, stdev: 0.26
External policy "random" average reward: 0.29, min: -0.25, max: 1.06, stdev: 0.22
External policy "individual greedy" average reward: 0.58, min: 0.05, max: 1.51, stdev: 0.24
External policy "total greedy" average reward: 0.68, min: 0.00, max: 1.44, stdev: 0.24
New network won 76 and tied 142 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 238 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.51 seconds
Training examples lengths: [64598, 64884, 64756, 64865, 65150, 64868, 64872, 64748, 64716, 64934]
Total value: 455394.85
Training on 648391 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2649 (value: 0.0018, weighted value: 0.0901, policy: 0.1749, weighted policy: 0.1749), Train Mean Max: 0.9298
Epoch 2/10, Train Loss: 0.2474 (value: 0.0016, weighted value: 0.0795, policy: 0.1679, weighted policy: 0.1679), Train Mean Max: 0.9306
Epoch 3/10, Train Loss: 0.2387 (value: 0.0015, weighted value: 0.0733, policy: 0.1654, weighted policy: 0.1654), Train Mean Max: 0.9311
Epoch 4/10, Train Loss: 0.2313 (value: 0.0014, weighted value: 0.0682, policy: 0.1631, weighted policy: 0.1631), Train Mean Max: 0.9316
Epoch 5/10, Train Loss: 0.2250 (value: 0.0013, weighted value: 0.0638, policy: 0.1613, weighted policy: 0.1613), Train Mean Max: 0.9320
Epoch 6/10, Train Loss: 0.2234 (value: 0.0013, weighted value: 0.0628, policy: 0.1606, weighted policy: 0.1606), Train Mean Max: 0.9320
Epoch 7/10, Train Loss: 0.2158 (value: 0.0012, weighted value: 0.0577, policy: 0.1581, weighted policy: 0.1581), Train Mean Max: 0.9328
Epoch 8/10, Train Loss: 0.2146 (value: 0.0011, weighted value: 0.0571, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9328
Epoch 9/10, Train Loss: 0.2115 (value: 0.0011, weighted value: 0.0544, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9332
Epoch 10/10, Train Loss: 0.2096 (value: 0.0011, weighted value: 0.0528, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9334
..training done in 72.74 seconds
..evaluation done in 15.02 seconds
Old network+MCTS average reward: 0.71, min: -0.04, max: 1.38, stdev: 0.23
New network+MCTS average reward: 0.71, min: -0.04, max: 1.34, stdev: 0.23
Old bare network average reward: 0.66, min: -0.04, max: 1.29, stdev: 0.23
New bare network average reward: 0.67, min: -0.04, max: 1.34, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.28, max: 0.91, stdev: 0.23
External policy "individual greedy" average reward: 0.56, min: 0.04, max: 1.24, stdev: 0.22
External policy "total greedy" average reward: 0.67, min: 0.00, max: 1.43, stdev: 0.22
New network won 83 and tied 149 out of 300 games (52.50% wins where ties are half wins)
Keeping the new network

Training iteration 239 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.43 seconds
Training examples lengths: [64884, 64756, 64865, 65150, 64868, 64872, 64748, 64716, 64934, 64911]
Total value: 456614.77
Training on 648704 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2331 (value: 0.0014, weighted value: 0.0679, policy: 0.1653, weighted policy: 0.1653), Train Mean Max: 0.9322
Epoch 2/10, Train Loss: 0.2206 (value: 0.0012, weighted value: 0.0601, policy: 0.1605, weighted policy: 0.1605), Train Mean Max: 0.9331
Epoch 3/10, Train Loss: 0.2150 (value: 0.0011, weighted value: 0.0569, policy: 0.1581, weighted policy: 0.1581), Train Mean Max: 0.9334
Epoch 4/10, Train Loss: 0.2113 (value: 0.0011, weighted value: 0.0548, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9337
Epoch 5/10, Train Loss: 0.2097 (value: 0.0011, weighted value: 0.0535, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9340
Epoch 6/10, Train Loss: 0.2059 (value: 0.0010, weighted value: 0.0505, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9342
Epoch 7/10, Train Loss: 0.2035 (value: 0.0010, weighted value: 0.0493, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9345
Epoch 8/10, Train Loss: 0.2018 (value: 0.0010, weighted value: 0.0478, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9346
Epoch 9/10, Train Loss: 0.2000 (value: 0.0009, weighted value: 0.0462, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9350
Epoch 10/10, Train Loss: 0.1985 (value: 0.0009, weighted value: 0.0451, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9352
..training done in 67.71 seconds
..evaluation done in 14.55 seconds
Old network+MCTS average reward: 0.68, min: 0.14, max: 1.41, stdev: 0.22
New network+MCTS average reward: 0.69, min: 0.14, max: 1.41, stdev: 0.22
Old bare network average reward: 0.64, min: 0.06, max: 1.31, stdev: 0.22
New bare network average reward: 0.64, min: 0.02, max: 1.31, stdev: 0.22
External policy "random" average reward: 0.26, min: -0.38, max: 0.83, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.02, max: 1.15, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.16, max: 1.30, stdev: 0.21
New network won 75 and tied 152 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 240 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.08 seconds
Training examples lengths: [64756, 64865, 65150, 64868, 64872, 64748, 64716, 64934, 64911, 64743]
Total value: 456568.47
Training on 648563 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2220 (value: 0.0012, weighted value: 0.0604, policy: 0.1616, weighted policy: 0.1616), Train Mean Max: 0.9340
Epoch 2/10, Train Loss: 0.2140 (value: 0.0011, weighted value: 0.0554, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9344
Epoch 3/10, Train Loss: 0.2074 (value: 0.0011, weighted value: 0.0525, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9350
Epoch 4/10, Train Loss: 0.2055 (value: 0.0010, weighted value: 0.0510, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9351
Epoch 5/10, Train Loss: 0.2013 (value: 0.0010, weighted value: 0.0480, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9354
Epoch 6/10, Train Loss: 0.1992 (value: 0.0009, weighted value: 0.0463, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9356
Epoch 7/10, Train Loss: 0.1971 (value: 0.0009, weighted value: 0.0449, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9358
Epoch 8/10, Train Loss: 0.1951 (value: 0.0009, weighted value: 0.0440, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9361
Epoch 9/10, Train Loss: 0.1940 (value: 0.0008, weighted value: 0.0422, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9362
Epoch 10/10, Train Loss: 0.1939 (value: 0.0009, weighted value: 0.0427, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9363
..training done in 71.45 seconds
..evaluation done in 15.52 seconds
Old network+MCTS average reward: 0.69, min: 0.19, max: 1.35, stdev: 0.21
New network+MCTS average reward: 0.69, min: 0.19, max: 1.38, stdev: 0.21
Old bare network average reward: 0.64, min: 0.19, max: 1.26, stdev: 0.21
New bare network average reward: 0.64, min: 0.19, max: 1.26, stdev: 0.21
External policy "random" average reward: 0.27, min: -0.25, max: 1.03, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: 0.04, max: 1.17, stdev: 0.21
External policy "total greedy" average reward: 0.65, min: 0.08, max: 1.17, stdev: 0.20
New network won 69 and tied 146 out of 300 games (47.33% wins where ties are half wins)
Reverting to the old network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1757697740_iter_240

Training iteration 241 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.01 seconds
Training examples lengths: [64865, 65150, 64868, 64872, 64748, 64716, 64934, 64911, 64743, 64329]
Total value: 455989.68
Training on 648136 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2473 (value: 0.0015, weighted value: 0.0764, policy: 0.1709, weighted policy: 0.1709), Train Mean Max: 0.9325
Epoch 2/10, Train Loss: 0.2322 (value: 0.0014, weighted value: 0.0683, policy: 0.1639, weighted policy: 0.1639), Train Mean Max: 0.9332
Epoch 3/10, Train Loss: 0.2215 (value: 0.0013, weighted value: 0.0628, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9340
Epoch 4/10, Train Loss: 0.2139 (value: 0.0011, weighted value: 0.0569, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9343
Epoch 5/10, Train Loss: 0.2129 (value: 0.0011, weighted value: 0.0569, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9344
Epoch 6/10, Train Loss: 0.2069 (value: 0.0010, weighted value: 0.0524, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9348
Epoch 7/10, Train Loss: 0.2061 (value: 0.0011, weighted value: 0.0529, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9351
Epoch 8/10, Train Loss: 0.2024 (value: 0.0010, weighted value: 0.0491, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9352
Epoch 9/10, Train Loss: 0.1993 (value: 0.0009, weighted value: 0.0469, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9355
Epoch 10/10, Train Loss: 0.1976 (value: 0.0009, weighted value: 0.0459, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9358
..training done in 65.60 seconds
..evaluation done in 15.15 seconds
Old network+MCTS average reward: 0.69, min: 0.19, max: 1.40, stdev: 0.20
New network+MCTS average reward: 0.68, min: 0.03, max: 1.40, stdev: 0.20
Old bare network average reward: 0.63, min: 0.11, max: 1.40, stdev: 0.21
New bare network average reward: 0.64, min: 0.03, max: 1.40, stdev: 0.21
External policy "random" average reward: 0.26, min: -0.29, max: 0.93, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: -0.06, max: 1.30, stdev: 0.21
External policy "total greedy" average reward: 0.65, min: 0.12, max: 1.33, stdev: 0.20
New network won 58 and tied 150 out of 300 games (44.33% wins where ties are half wins)
Reverting to the old network

Training iteration 242 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.34 seconds
Training examples lengths: [65150, 64868, 64872, 64748, 64716, 64934, 64911, 64743, 64329, 64550]
Total value: 455705.28
Training on 647821 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2692 (value: 0.0018, weighted value: 0.0912, policy: 0.1780, weighted policy: 0.1780), Train Mean Max: 0.9314
Epoch 2/10, Train Loss: 0.2470 (value: 0.0016, weighted value: 0.0789, policy: 0.1680, weighted policy: 0.1680), Train Mean Max: 0.9326
Epoch 3/10, Train Loss: 0.2351 (value: 0.0014, weighted value: 0.0711, policy: 0.1640, weighted policy: 0.1640), Train Mean Max: 0.9330
Epoch 4/10, Train Loss: 0.2250 (value: 0.0013, weighted value: 0.0657, policy: 0.1594, weighted policy: 0.1594), Train Mean Max: 0.9334
Epoch 5/10, Train Loss: 0.2205 (value: 0.0013, weighted value: 0.0626, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9337
Epoch 6/10, Train Loss: 0.2169 (value: 0.0012, weighted value: 0.0601, policy: 0.1569, weighted policy: 0.1569), Train Mean Max: 0.9340
Epoch 7/10, Train Loss: 0.2120 (value: 0.0011, weighted value: 0.0569, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9343
Epoch 8/10, Train Loss: 0.2094 (value: 0.0011, weighted value: 0.0545, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9345
Epoch 9/10, Train Loss: 0.2076 (value: 0.0011, weighted value: 0.0525, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9348
Epoch 10/10, Train Loss: 0.2037 (value: 0.0010, weighted value: 0.0500, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9351
..training done in 73.82 seconds
..evaluation done in 14.93 seconds
Old network+MCTS average reward: 0.69, min: 0.00, max: 1.53, stdev: 0.24
New network+MCTS average reward: 0.68, min: -0.09, max: 1.47, stdev: 0.24
Old bare network average reward: 0.63, min: -0.05, max: 1.47, stdev: 0.24
New bare network average reward: 0.63, min: -0.06, max: 1.44, stdev: 0.25
External policy "random" average reward: 0.26, min: -0.31, max: 0.99, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: -0.09, max: 1.31, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: -0.13, max: 1.52, stdev: 0.23
New network won 71 and tied 144 out of 300 games (47.67% wins where ties are half wins)
Reverting to the old network

Training iteration 243 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.05 seconds
Training examples lengths: [64868, 64872, 64748, 64716, 64934, 64911, 64743, 64329, 64550, 64887]
Total value: 456109.08
Training on 647558 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2932 (value: 0.0022, weighted value: 0.1087, policy: 0.1845, weighted policy: 0.1845), Train Mean Max: 0.9304
Epoch 2/10, Train Loss: 0.2614 (value: 0.0018, weighted value: 0.0887, policy: 0.1728, weighted policy: 0.1728), Train Mean Max: 0.9316
Epoch 3/10, Train Loss: 0.2473 (value: 0.0016, weighted value: 0.0804, policy: 0.1669, weighted policy: 0.1669), Train Mean Max: 0.9322
Epoch 4/10, Train Loss: 0.2365 (value: 0.0015, weighted value: 0.0743, policy: 0.1622, weighted policy: 0.1622), Train Mean Max: 0.9327
Epoch 5/10, Train Loss: 0.2296 (value: 0.0014, weighted value: 0.0689, policy: 0.1606, weighted policy: 0.1606), Train Mean Max: 0.9330
Epoch 6/10, Train Loss: 0.2242 (value: 0.0013, weighted value: 0.0662, policy: 0.1580, weighted policy: 0.1580), Train Mean Max: 0.9334
Epoch 7/10, Train Loss: 0.2182 (value: 0.0012, weighted value: 0.0610, policy: 0.1572, weighted policy: 0.1572), Train Mean Max: 0.9337
Epoch 8/10, Train Loss: 0.2163 (value: 0.0012, weighted value: 0.0595, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9340
Epoch 9/10, Train Loss: 0.2131 (value: 0.0011, weighted value: 0.0569, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9341
Epoch 10/10, Train Loss: 0.2090 (value: 0.0011, weighted value: 0.0538, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9345
..training done in 74.63 seconds
..evaluation done in 15.52 seconds
Old network+MCTS average reward: 0.72, min: 0.13, max: 1.61, stdev: 0.24
New network+MCTS average reward: 0.72, min: 0.12, max: 1.61, stdev: 0.24
Old bare network average reward: 0.67, min: 0.05, max: 1.61, stdev: 0.25
New bare network average reward: 0.67, min: 0.05, max: 1.61, stdev: 0.25
External policy "random" average reward: 0.28, min: -0.23, max: 1.13, stdev: 0.23
External policy "individual greedy" average reward: 0.56, min: -0.03, max: 1.35, stdev: 0.23
External policy "total greedy" average reward: 0.68, min: 0.19, max: 1.53, stdev: 0.23
New network won 80 and tied 137 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 244 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.31 seconds
Training examples lengths: [64872, 64748, 64716, 64934, 64911, 64743, 64329, 64550, 64887, 64975]
Total value: 455830.96
Training on 647665 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3126 (value: 0.0024, weighted value: 0.1207, policy: 0.1919, weighted policy: 0.1919), Train Mean Max: 0.9292
Epoch 2/10, Train Loss: 0.2757 (value: 0.0020, weighted value: 0.0993, policy: 0.1764, weighted policy: 0.1764), Train Mean Max: 0.9307
Epoch 3/10, Train Loss: 0.2576 (value: 0.0018, weighted value: 0.0883, policy: 0.1692, weighted policy: 0.1692), Train Mean Max: 0.9314
Epoch 4/10, Train Loss: 0.2472 (value: 0.0016, weighted value: 0.0811, policy: 0.1661, weighted policy: 0.1661), Train Mean Max: 0.9317
Epoch 5/10, Train Loss: 0.2384 (value: 0.0015, weighted value: 0.0748, policy: 0.1635, weighted policy: 0.1635), Train Mean Max: 0.9320
Epoch 6/10, Train Loss: 0.2315 (value: 0.0014, weighted value: 0.0708, policy: 0.1607, weighted policy: 0.1607), Train Mean Max: 0.9324
Epoch 7/10, Train Loss: 0.2249 (value: 0.0013, weighted value: 0.0661, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9329
Epoch 8/10, Train Loss: 0.2220 (value: 0.0013, weighted value: 0.0630, policy: 0.1589, weighted policy: 0.1589), Train Mean Max: 0.9331
Epoch 9/10, Train Loss: 0.2181 (value: 0.0012, weighted value: 0.0602, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9333
Epoch 10/10, Train Loss: 0.2148 (value: 0.0011, weighted value: 0.0574, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9335
..training done in 68.47 seconds
..evaluation done in 16.03 seconds
Old network+MCTS average reward: 0.68, min: 0.09, max: 1.49, stdev: 0.24
New network+MCTS average reward: 0.69, min: 0.07, max: 1.43, stdev: 0.24
Old bare network average reward: 0.64, min: 0.06, max: 1.49, stdev: 0.25
New bare network average reward: 0.64, min: 0.05, max: 1.43, stdev: 0.25
External policy "random" average reward: 0.24, min: -0.30, max: 0.85, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: -0.13, max: 1.39, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.00, max: 1.44, stdev: 0.23
New network won 83 and tied 143 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 245 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.54 seconds
Training examples lengths: [64748, 64716, 64934, 64911, 64743, 64329, 64550, 64887, 64975, 65129]
Total value: 456365.16
Training on 647922 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2375 (value: 0.0014, weighted value: 0.0720, policy: 0.1655, weighted policy: 0.1655), Train Mean Max: 0.9325
Epoch 2/10, Train Loss: 0.2248 (value: 0.0013, weighted value: 0.0635, policy: 0.1613, weighted policy: 0.1613), Train Mean Max: 0.9330
Epoch 3/10, Train Loss: 0.2195 (value: 0.0012, weighted value: 0.0607, policy: 0.1589, weighted policy: 0.1589), Train Mean Max: 0.9334
Epoch 4/10, Train Loss: 0.2147 (value: 0.0011, weighted value: 0.0575, policy: 0.1572, weighted policy: 0.1572), Train Mean Max: 0.9338
Epoch 5/10, Train Loss: 0.2120 (value: 0.0011, weighted value: 0.0554, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9341
Epoch 6/10, Train Loss: 0.2074 (value: 0.0011, weighted value: 0.0527, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9343
Epoch 7/10, Train Loss: 0.2057 (value: 0.0010, weighted value: 0.0509, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9346
Epoch 8/10, Train Loss: 0.2049 (value: 0.0010, weighted value: 0.0505, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9348
Epoch 9/10, Train Loss: 0.2017 (value: 0.0010, weighted value: 0.0480, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9351
Epoch 10/10, Train Loss: 0.1989 (value: 0.0009, weighted value: 0.0462, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9354
..training done in 68.32 seconds
..evaluation done in 15.66 seconds
Old network+MCTS average reward: 0.71, min: 0.19, max: 1.45, stdev: 0.22
New network+MCTS average reward: 0.71, min: 0.19, max: 1.39, stdev: 0.22
Old bare network average reward: 0.66, min: 0.11, max: 1.41, stdev: 0.22
New bare network average reward: 0.66, min: 0.06, max: 1.39, stdev: 0.22
External policy "random" average reward: 0.26, min: -0.33, max: 0.94, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: 0.05, max: 1.33, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.03, max: 1.41, stdev: 0.21
New network won 86 and tied 147 out of 300 games (53.17% wins where ties are half wins)
Keeping the new network

Training iteration 246 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.57 seconds
Training examples lengths: [64716, 64934, 64911, 64743, 64329, 64550, 64887, 64975, 65129, 64698]
Total value: 456546.02
Training on 647872 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2250 (value: 0.0012, weighted value: 0.0618, policy: 0.1632, weighted policy: 0.1632), Train Mean Max: 0.9339
Epoch 2/10, Train Loss: 0.2141 (value: 0.0011, weighted value: 0.0559, policy: 0.1582, weighted policy: 0.1582), Train Mean Max: 0.9346
Epoch 3/10, Train Loss: 0.2099 (value: 0.0011, weighted value: 0.0545, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9349
Epoch 4/10, Train Loss: 0.2055 (value: 0.0010, weighted value: 0.0509, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9351
Epoch 5/10, Train Loss: 0.2018 (value: 0.0010, weighted value: 0.0485, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9353
Epoch 6/10, Train Loss: 0.2008 (value: 0.0010, weighted value: 0.0478, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9356
Epoch 7/10, Train Loss: 0.1973 (value: 0.0009, weighted value: 0.0455, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9358
Epoch 8/10, Train Loss: 0.1977 (value: 0.0009, weighted value: 0.0454, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9359
Epoch 9/10, Train Loss: 0.1943 (value: 0.0009, weighted value: 0.0433, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9362
Epoch 10/10, Train Loss: 0.1934 (value: 0.0008, weighted value: 0.0422, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9363
..training done in 71.31 seconds
..evaluation done in 14.83 seconds
Old network+MCTS average reward: 0.70, min: 0.20, max: 1.19, stdev: 0.20
New network+MCTS average reward: 0.70, min: 0.19, max: 1.19, stdev: 0.20
Old bare network average reward: 0.65, min: 0.09, max: 1.19, stdev: 0.21
New bare network average reward: 0.65, min: 0.13, max: 1.19, stdev: 0.21
External policy "random" average reward: 0.27, min: -0.22, max: 0.81, stdev: 0.21
External policy "individual greedy" average reward: 0.56, min: -0.04, max: 1.28, stdev: 0.22
External policy "total greedy" average reward: 0.67, min: 0.16, max: 1.34, stdev: 0.21
New network won 75 and tied 151 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 247 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.79 seconds
Training examples lengths: [64934, 64911, 64743, 64329, 64550, 64887, 64975, 65129, 64698, 64632]
Total value: 456740.41
Training on 647788 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2206 (value: 0.0012, weighted value: 0.0593, policy: 0.1613, weighted policy: 0.1613), Train Mean Max: 0.9348
Epoch 2/10, Train Loss: 0.2130 (value: 0.0011, weighted value: 0.0547, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9351
Epoch 3/10, Train Loss: 0.2037 (value: 0.0010, weighted value: 0.0487, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9356
Epoch 4/10, Train Loss: 0.2019 (value: 0.0010, weighted value: 0.0488, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9359
Epoch 5/10, Train Loss: 0.1982 (value: 0.0009, weighted value: 0.0458, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9361
Epoch 6/10, Train Loss: 0.1978 (value: 0.0009, weighted value: 0.0455, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9361
Epoch 7/10, Train Loss: 0.1945 (value: 0.0009, weighted value: 0.0435, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9365
Epoch 8/10, Train Loss: 0.1928 (value: 0.0008, weighted value: 0.0418, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9365
Epoch 9/10, Train Loss: 0.1916 (value: 0.0008, weighted value: 0.0412, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9368
Epoch 10/10, Train Loss: 0.1908 (value: 0.0008, weighted value: 0.0408, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9369
..training done in 70.92 seconds
..evaluation done in 15.61 seconds
Old network+MCTS average reward: 0.67, min: 0.09, max: 1.42, stdev: 0.23
New network+MCTS average reward: 0.67, min: 0.11, max: 1.42, stdev: 0.23
Old bare network average reward: 0.62, min: 0.09, max: 1.37, stdev: 0.24
New bare network average reward: 0.62, min: 0.06, max: 1.37, stdev: 0.24
External policy "random" average reward: 0.25, min: -0.28, max: 1.04, stdev: 0.24
External policy "individual greedy" average reward: 0.53, min: 0.04, max: 1.30, stdev: 0.23
External policy "total greedy" average reward: 0.63, min: 0.11, max: 1.39, stdev: 0.23
New network won 72 and tied 149 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 248 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.80 seconds
Training examples lengths: [64911, 64743, 64329, 64550, 64887, 64975, 65129, 64698, 64632, 64780]
Total value: 456214.56
Training on 647634 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2474 (value: 0.0015, weighted value: 0.0763, policy: 0.1711, weighted policy: 0.1711), Train Mean Max: 0.9331
Epoch 2/10, Train Loss: 0.2301 (value: 0.0013, weighted value: 0.0660, policy: 0.1641, weighted policy: 0.1641), Train Mean Max: 0.9338
Epoch 3/10, Train Loss: 0.2220 (value: 0.0012, weighted value: 0.0620, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9342
Epoch 4/10, Train Loss: 0.2136 (value: 0.0011, weighted value: 0.0565, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9347
Epoch 5/10, Train Loss: 0.2104 (value: 0.0011, weighted value: 0.0545, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9348
Epoch 6/10, Train Loss: 0.2074 (value: 0.0010, weighted value: 0.0523, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9351
Epoch 7/10, Train Loss: 0.2046 (value: 0.0010, weighted value: 0.0512, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9353
Epoch 8/10, Train Loss: 0.2018 (value: 0.0010, weighted value: 0.0479, policy: 0.1539, weighted policy: 0.1539), Train Mean Max: 0.9354
Epoch 9/10, Train Loss: 0.1995 (value: 0.0009, weighted value: 0.0463, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9356
Epoch 10/10, Train Loss: 0.1988 (value: 0.0009, weighted value: 0.0467, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9359
..training done in 68.98 seconds
..evaluation done in 14.88 seconds
Old network+MCTS average reward: 0.69, min: 0.14, max: 1.60, stdev: 0.22
New network+MCTS average reward: 0.69, min: 0.10, max: 1.60, stdev: 0.22
Old bare network average reward: 0.64, min: 0.06, max: 1.60, stdev: 0.22
New bare network average reward: 0.64, min: 0.02, max: 1.60, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.22, max: 1.04, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: 0.05, max: 1.37, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.18, max: 1.53, stdev: 0.22
New network won 69 and tied 145 out of 300 games (47.17% wins where ties are half wins)
Reverting to the old network

Training iteration 249 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.09 seconds
Training examples lengths: [64743, 64329, 64550, 64887, 64975, 65129, 64698, 64632, 64780, 64767]
Total value: 455515.41
Training on 647490 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2758 (value: 0.0019, weighted value: 0.0956, policy: 0.1802, weighted policy: 0.1802), Train Mean Max: 0.9316
Epoch 2/10, Train Loss: 0.2471 (value: 0.0016, weighted value: 0.0778, policy: 0.1693, weighted policy: 0.1693), Train Mean Max: 0.9326
Epoch 3/10, Train Loss: 0.2361 (value: 0.0014, weighted value: 0.0719, policy: 0.1642, weighted policy: 0.1642), Train Mean Max: 0.9331
Epoch 4/10, Train Loss: 0.2275 (value: 0.0013, weighted value: 0.0671, policy: 0.1604, weighted policy: 0.1604), Train Mean Max: 0.9336
Epoch 5/10, Train Loss: 0.2208 (value: 0.0012, weighted value: 0.0624, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9338
Epoch 6/10, Train Loss: 0.2162 (value: 0.0012, weighted value: 0.0590, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9341
Epoch 7/10, Train Loss: 0.2133 (value: 0.0011, weighted value: 0.0568, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9343
Epoch 8/10, Train Loss: 0.2087 (value: 0.0011, weighted value: 0.0530, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9344
Epoch 9/10, Train Loss: 0.2066 (value: 0.0010, weighted value: 0.0518, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9349
Epoch 10/10, Train Loss: 0.2051 (value: 0.0010, weighted value: 0.0504, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9349
..training done in 67.77 seconds
..evaluation done in 15.08 seconds
Old network+MCTS average reward: 0.70, min: 0.09, max: 1.36, stdev: 0.23
New network+MCTS average reward: 0.70, min: 0.09, max: 1.36, stdev: 0.23
Old bare network average reward: 0.65, min: -0.04, max: 1.36, stdev: 0.24
New bare network average reward: 0.65, min: -0.21, max: 1.36, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.44, max: 1.06, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: 0.03, max: 1.26, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.17, max: 1.37, stdev: 0.21
New network won 85 and tied 140 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 250 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.04 seconds
Training examples lengths: [64329, 64550, 64887, 64975, 65129, 64698, 64632, 64780, 64767, 64639]
Total value: 455570.65
Training on 647386 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2283 (value: 0.0013, weighted value: 0.0653, policy: 0.1630, weighted policy: 0.1630), Train Mean Max: 0.9336
Epoch 2/10, Train Loss: 0.2175 (value: 0.0012, weighted value: 0.0585, policy: 0.1591, weighted policy: 0.1591), Train Mean Max: 0.9344
Epoch 3/10, Train Loss: 0.2155 (value: 0.0012, weighted value: 0.0577, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9346
Epoch 4/10, Train Loss: 0.2076 (value: 0.0010, weighted value: 0.0518, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9350
Epoch 5/10, Train Loss: 0.2059 (value: 0.0010, weighted value: 0.0509, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9352
Epoch 6/10, Train Loss: 0.2048 (value: 0.0010, weighted value: 0.0501, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9353
Epoch 7/10, Train Loss: 0.2001 (value: 0.0009, weighted value: 0.0469, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9355
Epoch 8/10, Train Loss: 0.1986 (value: 0.0009, weighted value: 0.0459, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9358
Epoch 9/10, Train Loss: 0.1982 (value: 0.0009, weighted value: 0.0453, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9360
Epoch 10/10, Train Loss: 0.1956 (value: 0.0009, weighted value: 0.0431, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9361
..training done in 65.96 seconds
..evaluation done in 15.19 seconds
Old network+MCTS average reward: 0.70, min: 0.04, max: 1.54, stdev: 0.24
New network+MCTS average reward: 0.70, min: 0.07, max: 1.54, stdev: 0.23
Old bare network average reward: 0.66, min: 0.00, max: 1.54, stdev: 0.24
New bare network average reward: 0.66, min: 0.06, max: 1.54, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.26, max: 1.05, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: -0.05, max: 1.32, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.16, max: 1.38, stdev: 0.22
New network won 85 and tied 133 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 251 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.98 seconds
Training examples lengths: [64550, 64887, 64975, 65129, 64698, 64632, 64780, 64767, 64639, 64861]
Total value: 456716.24
Training on 647918 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2235 (value: 0.0012, weighted value: 0.0607, policy: 0.1627, weighted policy: 0.1627), Train Mean Max: 0.9346
Epoch 2/10, Train Loss: 0.2127 (value: 0.0011, weighted value: 0.0542, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9351
Epoch 3/10, Train Loss: 0.2077 (value: 0.0010, weighted value: 0.0513, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9354
Epoch 4/10, Train Loss: 0.2041 (value: 0.0010, weighted value: 0.0494, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9358
Epoch 5/10, Train Loss: 0.2008 (value: 0.0009, weighted value: 0.0468, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9359
Epoch 6/10, Train Loss: 0.1979 (value: 0.0009, weighted value: 0.0454, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9361
Epoch 7/10, Train Loss: 0.1969 (value: 0.0009, weighted value: 0.0442, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9361
Epoch 8/10, Train Loss: 0.1952 (value: 0.0009, weighted value: 0.0435, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9364
Epoch 9/10, Train Loss: 0.1943 (value: 0.0009, weighted value: 0.0427, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9365
Epoch 10/10, Train Loss: 0.1917 (value: 0.0008, weighted value: 0.0408, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9367
..training done in 68.06 seconds
..evaluation done in 15.26 seconds
Old network+MCTS average reward: 0.71, min: 0.01, max: 1.41, stdev: 0.23
New network+MCTS average reward: 0.71, min: 0.09, max: 1.56, stdev: 0.22
Old bare network average reward: 0.66, min: 0.00, max: 1.47, stdev: 0.23
New bare network average reward: 0.67, min: 0.07, max: 1.50, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.28, max: 0.99, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: -0.06, max: 1.21, stdev: 0.22
External policy "total greedy" average reward: 0.67, min: -0.06, max: 1.35, stdev: 0.22
New network won 93 and tied 133 out of 300 games (53.17% wins where ties are half wins)
Keeping the new network

Training iteration 252 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.02 seconds
Training examples lengths: [64887, 64975, 65129, 64698, 64632, 64780, 64767, 64639, 64861, 64916]
Total value: 456831.74
Training on 648284 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2200 (value: 0.0012, weighted value: 0.0581, policy: 0.1618, weighted policy: 0.1618), Train Mean Max: 0.9349
Epoch 2/10, Train Loss: 0.2088 (value: 0.0010, weighted value: 0.0504, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9354
Epoch 3/10, Train Loss: 0.2047 (value: 0.0010, weighted value: 0.0496, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9358
Epoch 4/10, Train Loss: 0.2002 (value: 0.0009, weighted value: 0.0460, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9361
Epoch 5/10, Train Loss: 0.1983 (value: 0.0009, weighted value: 0.0451, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9363
Epoch 6/10, Train Loss: 0.1967 (value: 0.0009, weighted value: 0.0445, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9363
Epoch 7/10, Train Loss: 0.1934 (value: 0.0008, weighted value: 0.0415, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9365
Epoch 8/10, Train Loss: 0.1937 (value: 0.0008, weighted value: 0.0419, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9367
Epoch 9/10, Train Loss: 0.1919 (value: 0.0008, weighted value: 0.0408, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9368
Epoch 10/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0390, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9370
..training done in 65.07 seconds
..evaluation done in 15.07 seconds
Old network+MCTS average reward: 0.68, min: 0.11, max: 1.36, stdev: 0.22
New network+MCTS average reward: 0.68, min: 0.14, max: 1.26, stdev: 0.22
Old bare network average reward: 0.64, min: 0.05, max: 1.35, stdev: 0.22
New bare network average reward: 0.64, min: 0.12, max: 1.26, stdev: 0.21
External policy "random" average reward: 0.25, min: -0.31, max: 0.84, stdev: 0.21
External policy "individual greedy" average reward: 0.53, min: 0.06, max: 1.37, stdev: 0.21
External policy "total greedy" average reward: 0.65, min: 0.08, max: 1.44, stdev: 0.21
New network won 79 and tied 146 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 253 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.32 seconds
Training examples lengths: [64975, 65129, 64698, 64632, 64780, 64767, 64639, 64861, 64916, 64681]
Total value: 456657.25
Training on 648078 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2160 (value: 0.0011, weighted value: 0.0538, policy: 0.1621, weighted policy: 0.1621), Train Mean Max: 0.9350
Epoch 2/10, Train Loss: 0.2073 (value: 0.0010, weighted value: 0.0500, policy: 0.1573, weighted policy: 0.1573), Train Mean Max: 0.9358
Epoch 3/10, Train Loss: 0.2019 (value: 0.0009, weighted value: 0.0471, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9361
Epoch 4/10, Train Loss: 0.1988 (value: 0.0009, weighted value: 0.0458, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9362
Epoch 5/10, Train Loss: 0.1944 (value: 0.0009, weighted value: 0.0425, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9365
Epoch 6/10, Train Loss: 0.1943 (value: 0.0008, weighted value: 0.0419, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9365
Epoch 7/10, Train Loss: 0.1923 (value: 0.0008, weighted value: 0.0409, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9367
Epoch 8/10, Train Loss: 0.1912 (value: 0.0008, weighted value: 0.0405, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9368
Epoch 9/10, Train Loss: 0.1889 (value: 0.0008, weighted value: 0.0381, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9370
Epoch 10/10, Train Loss: 0.1881 (value: 0.0008, weighted value: 0.0382, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9372
..training done in 65.52 seconds
..evaluation done in 15.02 seconds
Old network+MCTS average reward: 0.69, min: 0.08, max: 1.41, stdev: 0.22
New network+MCTS average reward: 0.69, min: 0.08, max: 1.44, stdev: 0.22
Old bare network average reward: 0.64, min: -0.02, max: 1.43, stdev: 0.23
New bare network average reward: 0.63, min: 0.06, max: 1.44, stdev: 0.22
External policy "random" average reward: 0.26, min: -0.26, max: 0.84, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: -0.01, max: 1.31, stdev: 0.21
External policy "total greedy" average reward: 0.66, min: 0.17, max: 1.41, stdev: 0.21
New network won 73 and tied 150 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 254 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.48 seconds
Training examples lengths: [65129, 64698, 64632, 64780, 64767, 64639, 64861, 64916, 64681, 65164]
Total value: 457722.66
Training on 648267 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2415 (value: 0.0014, weighted value: 0.0715, policy: 0.1700, weighted policy: 0.1700), Train Mean Max: 0.9334
Epoch 2/10, Train Loss: 0.2235 (value: 0.0012, weighted value: 0.0608, policy: 0.1628, weighted policy: 0.1628), Train Mean Max: 0.9344
Epoch 3/10, Train Loss: 0.2150 (value: 0.0011, weighted value: 0.0563, policy: 0.1587, weighted policy: 0.1587), Train Mean Max: 0.9350
Epoch 4/10, Train Loss: 0.2110 (value: 0.0011, weighted value: 0.0553, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9352
Epoch 5/10, Train Loss: 0.2053 (value: 0.0010, weighted value: 0.0502, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9354
Epoch 6/10, Train Loss: 0.2045 (value: 0.0010, weighted value: 0.0505, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9356
Epoch 7/10, Train Loss: 0.1993 (value: 0.0009, weighted value: 0.0465, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9359
Epoch 8/10, Train Loss: 0.1986 (value: 0.0009, weighted value: 0.0460, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9359
Epoch 9/10, Train Loss: 0.1971 (value: 0.0009, weighted value: 0.0450, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9360
Epoch 10/10, Train Loss: 0.1937 (value: 0.0008, weighted value: 0.0421, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9363
..training done in 65.02 seconds
..evaluation done in 15.57 seconds
Old network+MCTS average reward: 0.70, min: 0.03, max: 1.60, stdev: 0.24
New network+MCTS average reward: 0.70, min: 0.07, max: 1.62, stdev: 0.23
Old bare network average reward: 0.65, min: -0.01, max: 1.60, stdev: 0.24
New bare network average reward: 0.65, min: 0.07, max: 1.60, stdev: 0.24
External policy "random" average reward: 0.28, min: -0.35, max: 0.93, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.03, max: 1.38, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.14, max: 1.47, stdev: 0.23
New network won 66 and tied 155 out of 300 games (47.83% wins where ties are half wins)
Reverting to the old network

Training iteration 255 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.67 seconds
Training examples lengths: [64698, 64632, 64780, 64767, 64639, 64861, 64916, 64681, 65164, 64925]
Total value: 457661.32
Training on 648063 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2673 (value: 0.0018, weighted value: 0.0884, policy: 0.1789, weighted policy: 0.1789), Train Mean Max: 0.9321
Epoch 2/10, Train Loss: 0.2422 (value: 0.0015, weighted value: 0.0741, policy: 0.1681, weighted policy: 0.1681), Train Mean Max: 0.9333
Epoch 3/10, Train Loss: 0.2316 (value: 0.0014, weighted value: 0.0693, policy: 0.1624, weighted policy: 0.1624), Train Mean Max: 0.9337
Epoch 4/10, Train Loss: 0.2206 (value: 0.0012, weighted value: 0.0621, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9341
Epoch 5/10, Train Loss: 0.2170 (value: 0.0012, weighted value: 0.0600, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9343
Epoch 6/10, Train Loss: 0.2114 (value: 0.0011, weighted value: 0.0556, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9346
Epoch 7/10, Train Loss: 0.2114 (value: 0.0011, weighted value: 0.0556, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9347
Epoch 8/10, Train Loss: 0.2052 (value: 0.0010, weighted value: 0.0512, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9351
Epoch 9/10, Train Loss: 0.2040 (value: 0.0010, weighted value: 0.0500, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9352
Epoch 10/10, Train Loss: 0.2002 (value: 0.0009, weighted value: 0.0470, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9355
..training done in 65.85 seconds
..evaluation done in 15.03 seconds
Old network+MCTS average reward: 0.72, min: 0.12, max: 1.64, stdev: 0.25
New network+MCTS average reward: 0.72, min: 0.12, max: 1.64, stdev: 0.25
Old bare network average reward: 0.66, min: -0.08, max: 1.64, stdev: 0.26
New bare network average reward: 0.66, min: -0.08, max: 1.64, stdev: 0.26
External policy "random" average reward: 0.29, min: -0.40, max: 1.07, stdev: 0.24
External policy "individual greedy" average reward: 0.55, min: -0.25, max: 1.34, stdev: 0.25
External policy "total greedy" average reward: 0.67, min: -0.06, max: 1.65, stdev: 0.25
New network won 70 and tied 152 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 256 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.91 seconds
Training examples lengths: [64632, 64780, 64767, 64639, 64861, 64916, 64681, 65164, 64925, 64974]
Total value: 458014.91
Training on 648339 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2901 (value: 0.0021, weighted value: 0.1044, policy: 0.1856, weighted policy: 0.1856), Train Mean Max: 0.9306
Epoch 2/10, Train Loss: 0.2597 (value: 0.0017, weighted value: 0.0863, policy: 0.1734, weighted policy: 0.1734), Train Mean Max: 0.9320
Epoch 3/10, Train Loss: 0.2431 (value: 0.0016, weighted value: 0.0776, policy: 0.1654, weighted policy: 0.1654), Train Mean Max: 0.9328
Epoch 4/10, Train Loss: 0.2330 (value: 0.0014, weighted value: 0.0709, policy: 0.1621, weighted policy: 0.1621), Train Mean Max: 0.9330
Epoch 5/10, Train Loss: 0.2273 (value: 0.0013, weighted value: 0.0675, policy: 0.1598, weighted policy: 0.1598), Train Mean Max: 0.9333
Epoch 6/10, Train Loss: 0.2222 (value: 0.0013, weighted value: 0.0629, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9334
Epoch 7/10, Train Loss: 0.2173 (value: 0.0012, weighted value: 0.0594, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9338
Epoch 8/10, Train Loss: 0.2145 (value: 0.0012, weighted value: 0.0577, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9340
Epoch 9/10, Train Loss: 0.2101 (value: 0.0011, weighted value: 0.0553, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9344
Epoch 10/10, Train Loss: 0.2062 (value: 0.0010, weighted value: 0.0511, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9346
..training done in 72.88 seconds
..evaluation done in 15.37 seconds
Old network+MCTS average reward: 0.70, min: -0.11, max: 1.44, stdev: 0.22
New network+MCTS average reward: 0.70, min: -0.11, max: 1.42, stdev: 0.22
Old bare network average reward: 0.65, min: -0.11, max: 1.24, stdev: 0.22
New bare network average reward: 0.66, min: -0.11, max: 1.42, stdev: 0.22
External policy "random" average reward: 0.26, min: -0.44, max: 0.81, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: -0.25, max: 1.30, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: -0.14, max: 1.55, stdev: 0.22
New network won 74 and tied 146 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 257 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.84 seconds
Training examples lengths: [64780, 64767, 64639, 64861, 64916, 64681, 65164, 64925, 64974, 64791]
Total value: 458216.81
Training on 648498 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3131 (value: 0.0024, weighted value: 0.1197, policy: 0.1934, weighted policy: 0.1934), Train Mean Max: 0.9297
Epoch 2/10, Train Loss: 0.2735 (value: 0.0019, weighted value: 0.0963, policy: 0.1772, weighted policy: 0.1772), Train Mean Max: 0.9311
Epoch 3/10, Train Loss: 0.2557 (value: 0.0017, weighted value: 0.0865, policy: 0.1692, weighted policy: 0.1692), Train Mean Max: 0.9317
Epoch 4/10, Train Loss: 0.2443 (value: 0.0016, weighted value: 0.0797, policy: 0.1646, weighted policy: 0.1646), Train Mean Max: 0.9321
Epoch 5/10, Train Loss: 0.2350 (value: 0.0015, weighted value: 0.0730, policy: 0.1620, weighted policy: 0.1620), Train Mean Max: 0.9325
Epoch 6/10, Train Loss: 0.2279 (value: 0.0014, weighted value: 0.0683, policy: 0.1597, weighted policy: 0.1597), Train Mean Max: 0.9328
Epoch 7/10, Train Loss: 0.2245 (value: 0.0013, weighted value: 0.0649, policy: 0.1596, weighted policy: 0.1596), Train Mean Max: 0.9330
Epoch 8/10, Train Loss: 0.2195 (value: 0.0012, weighted value: 0.0611, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9333
Epoch 9/10, Train Loss: 0.2158 (value: 0.0012, weighted value: 0.0585, policy: 0.1573, weighted policy: 0.1573), Train Mean Max: 0.9337
Epoch 10/10, Train Loss: 0.2119 (value: 0.0011, weighted value: 0.0555, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9339
..training done in 70.72 seconds
..evaluation done in 14.73 seconds
Old network+MCTS average reward: 0.70, min: -0.09, max: 1.45, stdev: 0.24
New network+MCTS average reward: 0.70, min: -0.03, max: 1.36, stdev: 0.23
Old bare network average reward: 0.65, min: -0.04, max: 1.38, stdev: 0.24
New bare network average reward: 0.66, min: -0.02, max: 1.38, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.39, max: 0.99, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: -0.07, max: 1.23, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: -0.07, max: 1.27, stdev: 0.22
New network won 87 and tied 132 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 258 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.56 seconds
Training examples lengths: [64767, 64639, 64861, 64916, 64681, 65164, 64925, 64974, 64791, 64827]
Total value: 458328.59
Training on 648545 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2351 (value: 0.0014, weighted value: 0.0701, policy: 0.1651, weighted policy: 0.1651), Train Mean Max: 0.9326
Epoch 2/10, Train Loss: 0.2252 (value: 0.0013, weighted value: 0.0632, policy: 0.1620, weighted policy: 0.1620), Train Mean Max: 0.9330
Epoch 3/10, Train Loss: 0.2201 (value: 0.0012, weighted value: 0.0605, policy: 0.1596, weighted policy: 0.1596), Train Mean Max: 0.9335
Epoch 4/10, Train Loss: 0.2145 (value: 0.0011, weighted value: 0.0570, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9338
Epoch 5/10, Train Loss: 0.2100 (value: 0.0011, weighted value: 0.0547, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9342
Epoch 6/10, Train Loss: 0.2074 (value: 0.0010, weighted value: 0.0520, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9341
Epoch 7/10, Train Loss: 0.2050 (value: 0.0010, weighted value: 0.0503, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9346
Epoch 8/10, Train Loss: 0.2037 (value: 0.0010, weighted value: 0.0491, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9348
Epoch 9/10, Train Loss: 0.2007 (value: 0.0009, weighted value: 0.0467, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9350
Epoch 10/10, Train Loss: 0.1996 (value: 0.0009, weighted value: 0.0465, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9354
..training done in 69.75 seconds
..evaluation done in 14.75 seconds
Old network+MCTS average reward: 0.68, min: 0.11, max: 1.38, stdev: 0.24
New network+MCTS average reward: 0.68, min: 0.11, max: 1.38, stdev: 0.23
Old bare network average reward: 0.63, min: 0.00, max: 1.32, stdev: 0.24
New bare network average reward: 0.63, min: 0.01, max: 1.32, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.33, max: 0.94, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: -0.08, max: 1.23, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.02, max: 1.34, stdev: 0.22
New network won 88 and tied 142 out of 300 games (53.00% wins where ties are half wins)
Keeping the new network

Training iteration 259 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.65 seconds
Training examples lengths: [64639, 64861, 64916, 64681, 65164, 64925, 64974, 64791, 64827, 64489]
Total value: 458945.83
Training on 648267 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2252 (value: 0.0013, weighted value: 0.0629, policy: 0.1624, weighted policy: 0.1624), Train Mean Max: 0.9337
Epoch 2/10, Train Loss: 0.2144 (value: 0.0011, weighted value: 0.0556, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9343
Epoch 3/10, Train Loss: 0.2089 (value: 0.0011, weighted value: 0.0537, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9348
Epoch 4/10, Train Loss: 0.2043 (value: 0.0010, weighted value: 0.0497, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9350
Epoch 5/10, Train Loss: 0.2023 (value: 0.0010, weighted value: 0.0489, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9352
Epoch 6/10, Train Loss: 0.1998 (value: 0.0009, weighted value: 0.0469, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9354
Epoch 7/10, Train Loss: 0.1980 (value: 0.0009, weighted value: 0.0460, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9357
Epoch 8/10, Train Loss: 0.1962 (value: 0.0009, weighted value: 0.0440, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9358
Epoch 9/10, Train Loss: 0.1939 (value: 0.0008, weighted value: 0.0422, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9361
Epoch 10/10, Train Loss: 0.1951 (value: 0.0009, weighted value: 0.0434, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9362
..training done in 69.80 seconds
..evaluation done in 15.09 seconds
Old network+MCTS average reward: 0.71, min: 0.00, max: 1.42, stdev: 0.24
New network+MCTS average reward: 0.71, min: 0.04, max: 1.42, stdev: 0.24
Old bare network average reward: 0.67, min: -0.10, max: 1.42, stdev: 0.24
New bare network average reward: 0.66, min: -0.03, max: 1.42, stdev: 0.25
External policy "random" average reward: 0.27, min: -0.43, max: 1.07, stdev: 0.24
External policy "individual greedy" average reward: 0.55, min: -0.07, max: 1.36, stdev: 0.24
External policy "total greedy" average reward: 0.67, min: 0.00, max: 1.28, stdev: 0.23
New network won 65 and tied 152 out of 300 games (47.00% wins where ties are half wins)
Reverting to the old network

Training iteration 260 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.87 seconds
Training examples lengths: [64861, 64916, 64681, 65164, 64925, 64974, 64791, 64827, 64489, 65189]
Total value: 459253.04
Training on 648817 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2483 (value: 0.0015, weighted value: 0.0773, policy: 0.1711, weighted policy: 0.1711), Train Mean Max: 0.9321
Epoch 2/10, Train Loss: 0.2306 (value: 0.0013, weighted value: 0.0673, policy: 0.1632, weighted policy: 0.1632), Train Mean Max: 0.9332
Epoch 3/10, Train Loss: 0.2223 (value: 0.0012, weighted value: 0.0620, policy: 0.1603, weighted policy: 0.1603), Train Mean Max: 0.9336
Epoch 4/10, Train Loss: 0.2169 (value: 0.0012, weighted value: 0.0588, policy: 0.1581, weighted policy: 0.1581), Train Mean Max: 0.9339
Epoch 5/10, Train Loss: 0.2121 (value: 0.0011, weighted value: 0.0560, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9342
Epoch 6/10, Train Loss: 0.2087 (value: 0.0011, weighted value: 0.0535, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9344
Epoch 7/10, Train Loss: 0.2061 (value: 0.0010, weighted value: 0.0517, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9347
Epoch 8/10, Train Loss: 0.2023 (value: 0.0010, weighted value: 0.0491, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9349
Epoch 9/10, Train Loss: 0.2014 (value: 0.0010, weighted value: 0.0487, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9351
Epoch 10/10, Train Loss: 0.1988 (value: 0.0009, weighted value: 0.0460, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9354
..training done in 68.34 seconds
..evaluation done in 15.29 seconds
Old network+MCTS average reward: 0.71, min: 0.06, max: 1.57, stdev: 0.25
New network+MCTS average reward: 0.71, min: 0.07, max: 1.59, stdev: 0.24
Old bare network average reward: 0.67, min: 0.04, max: 1.57, stdev: 0.25
New bare network average reward: 0.67, min: 0.04, max: 1.51, stdev: 0.25
External policy "random" average reward: 0.26, min: -0.41, max: 1.21, stdev: 0.25
External policy "individual greedy" average reward: 0.55, min: -0.16, max: 1.45, stdev: 0.25
External policy "total greedy" average reward: 0.66, min: 0.13, max: 1.53, stdev: 0.24
New network won 75 and tied 152 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1757697740_iter_260

Training iteration 261 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.70 seconds
Training examples lengths: [64916, 64681, 65164, 64925, 64974, 64791, 64827, 64489, 65189, 64795]
Total value: 459319.55
Training on 648751 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2223 (value: 0.0012, weighted value: 0.0606, policy: 0.1617, weighted policy: 0.1617), Train Mean Max: 0.9340
Epoch 2/10, Train Loss: 0.2152 (value: 0.0011, weighted value: 0.0559, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9345
Epoch 3/10, Train Loss: 0.2089 (value: 0.0011, weighted value: 0.0537, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9351
Epoch 4/10, Train Loss: 0.2047 (value: 0.0010, weighted value: 0.0512, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9354
Epoch 5/10, Train Loss: 0.2012 (value: 0.0010, weighted value: 0.0484, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9355
Epoch 6/10, Train Loss: 0.1997 (value: 0.0009, weighted value: 0.0463, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9355
Epoch 7/10, Train Loss: 0.1987 (value: 0.0009, weighted value: 0.0469, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9359
Epoch 8/10, Train Loss: 0.1940 (value: 0.0009, weighted value: 0.0434, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9362
Epoch 9/10, Train Loss: 0.1944 (value: 0.0009, weighted value: 0.0431, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9362
Epoch 10/10, Train Loss: 0.1910 (value: 0.0008, weighted value: 0.0406, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9367
..training done in 67.34 seconds
..evaluation done in 15.49 seconds
Old network+MCTS average reward: 0.68, min: -0.03, max: 1.28, stdev: 0.22
New network+MCTS average reward: 0.68, min: -0.03, max: 1.27, stdev: 0.22
Old bare network average reward: 0.63, min: -0.03, max: 1.31, stdev: 0.23
New bare network average reward: 0.63, min: -0.01, max: 1.30, stdev: 0.22
External policy "random" average reward: 0.24, min: -0.31, max: 0.93, stdev: 0.23
External policy "individual greedy" average reward: 0.51, min: -0.02, max: 1.19, stdev: 0.23
External policy "total greedy" average reward: 0.63, min: -0.02, max: 1.20, stdev: 0.21
New network won 65 and tied 160 out of 300 games (48.33% wins where ties are half wins)
Reverting to the old network

Training iteration 262 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.54 seconds
Training examples lengths: [64681, 65164, 64925, 64974, 64791, 64827, 64489, 65189, 64795, 64713]
Total value: 459481.82
Training on 648548 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2482 (value: 0.0016, weighted value: 0.0777, policy: 0.1705, weighted policy: 0.1705), Train Mean Max: 0.9325
Epoch 2/10, Train Loss: 0.2316 (value: 0.0014, weighted value: 0.0678, policy: 0.1639, weighted policy: 0.1639), Train Mean Max: 0.9335
Epoch 3/10, Train Loss: 0.2240 (value: 0.0013, weighted value: 0.0643, policy: 0.1597, weighted policy: 0.1597), Train Mean Max: 0.9340
Epoch 4/10, Train Loss: 0.2141 (value: 0.0012, weighted value: 0.0583, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9346
Epoch 5/10, Train Loss: 0.2122 (value: 0.0011, weighted value: 0.0572, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9347
Epoch 6/10, Train Loss: 0.2072 (value: 0.0011, weighted value: 0.0536, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9349
Epoch 7/10, Train Loss: 0.2044 (value: 0.0010, weighted value: 0.0512, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9351
Epoch 8/10, Train Loss: 0.2029 (value: 0.0010, weighted value: 0.0500, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9353
Epoch 9/10, Train Loss: 0.2006 (value: 0.0010, weighted value: 0.0475, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9355
Epoch 10/10, Train Loss: 0.1987 (value: 0.0009, weighted value: 0.0460, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9357
..training done in 68.07 seconds
..evaluation done in 15.39 seconds
Old network+MCTS average reward: 0.70, min: 0.13, max: 1.38, stdev: 0.22
New network+MCTS average reward: 0.70, min: 0.06, max: 1.37, stdev: 0.23
Old bare network average reward: 0.65, min: 0.05, max: 1.32, stdev: 0.23
New bare network average reward: 0.65, min: 0.05, max: 1.37, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.26, max: 0.82, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.12, max: 1.16, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.13, max: 1.31, stdev: 0.22
New network won 68 and tied 157 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 263 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.03 seconds
Training examples lengths: [65164, 64925, 64974, 64791, 64827, 64489, 65189, 64795, 64713, 64800]
Total value: 459953.21
Training on 648667 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2718 (value: 0.0019, weighted value: 0.0929, policy: 0.1790, weighted policy: 0.1790), Train Mean Max: 0.9312
Epoch 2/10, Train Loss: 0.2463 (value: 0.0016, weighted value: 0.0779, policy: 0.1684, weighted policy: 0.1684), Train Mean Max: 0.9325
Epoch 3/10, Train Loss: 0.2380 (value: 0.0015, weighted value: 0.0740, policy: 0.1640, weighted policy: 0.1640), Train Mean Max: 0.9328
Epoch 4/10, Train Loss: 0.2269 (value: 0.0013, weighted value: 0.0659, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9334
Epoch 5/10, Train Loss: 0.2199 (value: 0.0012, weighted value: 0.0623, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9338
Epoch 6/10, Train Loss: 0.2174 (value: 0.0012, weighted value: 0.0607, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9339
Epoch 7/10, Train Loss: 0.2127 (value: 0.0011, weighted value: 0.0561, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9343
Epoch 8/10, Train Loss: 0.2090 (value: 0.0011, weighted value: 0.0540, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9345
Epoch 9/10, Train Loss: 0.2064 (value: 0.0011, weighted value: 0.0525, policy: 0.1539, weighted policy: 0.1539), Train Mean Max: 0.9348
Epoch 10/10, Train Loss: 0.2049 (value: 0.0010, weighted value: 0.0506, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9349
..training done in 67.01 seconds
..evaluation done in 15.84 seconds
Old network+MCTS average reward: 0.67, min: 0.17, max: 1.40, stdev: 0.24
New network+MCTS average reward: 0.67, min: 0.17, max: 1.40, stdev: 0.24
Old bare network average reward: 0.63, min: 0.09, max: 1.40, stdev: 0.24
New bare network average reward: 0.63, min: 0.12, max: 1.40, stdev: 0.24
External policy "random" average reward: 0.24, min: -0.30, max: 0.93, stdev: 0.23
External policy "individual greedy" average reward: 0.51, min: -0.07, max: 1.24, stdev: 0.22
External policy "total greedy" average reward: 0.63, min: 0.08, max: 1.31, stdev: 0.21
New network won 73 and tied 162 out of 300 games (51.33% wins where ties are half wins)
Keeping the new network

Training iteration 264 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.62 seconds
Training examples lengths: [64925, 64974, 64791, 64827, 64489, 65189, 64795, 64713, 64800, 64652]
Total value: 459983.50
Training on 648155 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2258 (value: 0.0013, weighted value: 0.0639, policy: 0.1618, weighted policy: 0.1618), Train Mean Max: 0.9338
Epoch 2/10, Train Loss: 0.2172 (value: 0.0012, weighted value: 0.0586, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9342
Epoch 3/10, Train Loss: 0.2132 (value: 0.0011, weighted value: 0.0556, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9345
Epoch 4/10, Train Loss: 0.2073 (value: 0.0010, weighted value: 0.0523, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9350
Epoch 5/10, Train Loss: 0.2049 (value: 0.0010, weighted value: 0.0513, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9353
Epoch 6/10, Train Loss: 0.2011 (value: 0.0009, weighted value: 0.0474, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9353
Epoch 7/10, Train Loss: 0.2016 (value: 0.0010, weighted value: 0.0489, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9356
Epoch 8/10, Train Loss: 0.1964 (value: 0.0009, weighted value: 0.0443, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9357
Epoch 9/10, Train Loss: 0.1969 (value: 0.0009, weighted value: 0.0449, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9360
Epoch 10/10, Train Loss: 0.1942 (value: 0.0009, weighted value: 0.0431, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9362
..training done in 66.67 seconds
..evaluation done in 15.69 seconds
Old network+MCTS average reward: 0.70, min: 0.12, max: 1.48, stdev: 0.24
New network+MCTS average reward: 0.70, min: 0.11, max: 1.48, stdev: 0.24
Old bare network average reward: 0.65, min: 0.13, max: 1.48, stdev: 0.24
New bare network average reward: 0.65, min: 0.13, max: 1.48, stdev: 0.24
External policy "random" average reward: 0.25, min: -0.47, max: 1.02, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: -0.19, max: 1.30, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.01, max: 1.36, stdev: 0.22
New network won 73 and tied 163 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 265 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.23 seconds
Training examples lengths: [64974, 64791, 64827, 64489, 65189, 64795, 64713, 64800, 64652, 64620]
Total value: 460449.75
Training on 647850 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2214 (value: 0.0012, weighted value: 0.0593, policy: 0.1620, weighted policy: 0.1620), Train Mean Max: 0.9345
Epoch 2/10, Train Loss: 0.2103 (value: 0.0011, weighted value: 0.0529, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9351
Epoch 3/10, Train Loss: 0.2044 (value: 0.0010, weighted value: 0.0499, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9356
Epoch 4/10, Train Loss: 0.2018 (value: 0.0010, weighted value: 0.0478, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9357
Epoch 5/10, Train Loss: 0.1991 (value: 0.0009, weighted value: 0.0461, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9359
Epoch 6/10, Train Loss: 0.1959 (value: 0.0009, weighted value: 0.0439, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9362
Epoch 7/10, Train Loss: 0.1958 (value: 0.0009, weighted value: 0.0445, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9364
Epoch 8/10, Train Loss: 0.1934 (value: 0.0008, weighted value: 0.0421, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9365
Epoch 9/10, Train Loss: 0.1916 (value: 0.0008, weighted value: 0.0411, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9366
Epoch 10/10, Train Loss: 0.1904 (value: 0.0008, weighted value: 0.0398, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9368
..training done in 67.16 seconds
..evaluation done in 15.70 seconds
Old network+MCTS average reward: 0.72, min: 0.13, max: 1.32, stdev: 0.23
New network+MCTS average reward: 0.72, min: 0.18, max: 1.35, stdev: 0.23
Old bare network average reward: 0.67, min: 0.12, max: 1.32, stdev: 0.23
New bare network average reward: 0.67, min: 0.12, max: 1.32, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.52, max: 1.07, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: -0.14, max: 1.32, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.05, max: 1.33, stdev: 0.22
New network won 86 and tied 137 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 266 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.08 seconds
Training examples lengths: [64791, 64827, 64489, 65189, 64795, 64713, 64800, 64652, 64620, 64665]
Total value: 459672.80
Training on 647541 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2171 (value: 0.0011, weighted value: 0.0564, policy: 0.1606, weighted policy: 0.1606), Train Mean Max: 0.9351
Epoch 2/10, Train Loss: 0.2095 (value: 0.0010, weighted value: 0.0513, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9356
Epoch 3/10, Train Loss: 0.2034 (value: 0.0009, weighted value: 0.0474, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9357
Epoch 4/10, Train Loss: 0.1991 (value: 0.0009, weighted value: 0.0453, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9361
Epoch 5/10, Train Loss: 0.1958 (value: 0.0009, weighted value: 0.0440, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9365
Epoch 6/10, Train Loss: 0.1954 (value: 0.0009, weighted value: 0.0435, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9365
Epoch 7/10, Train Loss: 0.1920 (value: 0.0008, weighted value: 0.0409, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9368
Epoch 8/10, Train Loss: 0.1911 (value: 0.0008, weighted value: 0.0403, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9368
Epoch 9/10, Train Loss: 0.1912 (value: 0.0008, weighted value: 0.0400, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9369
Epoch 10/10, Train Loss: 0.1878 (value: 0.0008, weighted value: 0.0377, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9373
..training done in 67.10 seconds
..evaluation done in 15.04 seconds
Old network+MCTS average reward: 0.68, min: 0.16, max: 1.45, stdev: 0.23
New network+MCTS average reward: 0.68, min: 0.11, max: 1.45, stdev: 0.24
Old bare network average reward: 0.64, min: 0.10, max: 1.45, stdev: 0.24
New bare network average reward: 0.64, min: 0.01, max: 1.45, stdev: 0.24
External policy "random" average reward: 0.25, min: -0.35, max: 0.87, stdev: 0.23
External policy "individual greedy" average reward: 0.52, min: -0.12, max: 1.29, stdev: 0.22
External policy "total greedy" average reward: 0.63, min: 0.08, max: 1.35, stdev: 0.22
New network won 67 and tied 159 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 267 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.11 seconds
Training examples lengths: [64827, 64489, 65189, 64795, 64713, 64800, 64652, 64620, 64665, 65130]
Total value: 459759.66
Training on 647880 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2443 (value: 0.0014, weighted value: 0.0723, policy: 0.1720, weighted policy: 0.1720), Train Mean Max: 0.9332
Epoch 2/10, Train Loss: 0.2275 (value: 0.0013, weighted value: 0.0632, policy: 0.1644, weighted policy: 0.1644), Train Mean Max: 0.9339
Epoch 3/10, Train Loss: 0.2176 (value: 0.0012, weighted value: 0.0584, policy: 0.1591, weighted policy: 0.1591), Train Mean Max: 0.9346
Epoch 4/10, Train Loss: 0.2117 (value: 0.0011, weighted value: 0.0549, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9350
Epoch 5/10, Train Loss: 0.2082 (value: 0.0010, weighted value: 0.0522, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9351
Epoch 6/10, Train Loss: 0.2047 (value: 0.0010, weighted value: 0.0498, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9351
Epoch 7/10, Train Loss: 0.2011 (value: 0.0010, weighted value: 0.0476, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9355
Epoch 8/10, Train Loss: 0.1998 (value: 0.0009, weighted value: 0.0456, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9354
Epoch 9/10, Train Loss: 0.1974 (value: 0.0009, weighted value: 0.0449, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9358
Epoch 10/10, Train Loss: 0.1965 (value: 0.0009, weighted value: 0.0436, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9359
..training done in 73.31 seconds
..evaluation done in 15.18 seconds
Old network+MCTS average reward: 0.69, min: 0.11, max: 1.28, stdev: 0.22
New network+MCTS average reward: 0.69, min: 0.08, max: 1.30, stdev: 0.22
Old bare network average reward: 0.64, min: 0.08, max: 1.28, stdev: 0.23
New bare network average reward: 0.64, min: 0.08, max: 1.24, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.26, max: 0.91, stdev: 0.21
External policy "individual greedy" average reward: 0.52, min: 0.04, max: 1.16, stdev: 0.22
External policy "total greedy" average reward: 0.63, min: 0.14, max: 1.27, stdev: 0.22
New network won 72 and tied 168 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network

Training iteration 268 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.27 seconds
Training examples lengths: [64489, 65189, 64795, 64713, 64800, 64652, 64620, 64665, 65130, 64749]
Total value: 459513.91
Training on 647802 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2208 (value: 0.0012, weighted value: 0.0590, policy: 0.1618, weighted policy: 0.1618), Train Mean Max: 0.9346
Epoch 2/10, Train Loss: 0.2114 (value: 0.0011, weighted value: 0.0535, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9352
Epoch 3/10, Train Loss: 0.2066 (value: 0.0010, weighted value: 0.0514, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9355
Epoch 4/10, Train Loss: 0.2016 (value: 0.0010, weighted value: 0.0476, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9357
Epoch 5/10, Train Loss: 0.1991 (value: 0.0009, weighted value: 0.0462, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9359
Epoch 6/10, Train Loss: 0.1984 (value: 0.0009, weighted value: 0.0462, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9362
Epoch 7/10, Train Loss: 0.1948 (value: 0.0009, weighted value: 0.0429, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9363
Epoch 8/10, Train Loss: 0.1936 (value: 0.0008, weighted value: 0.0424, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9365
Epoch 9/10, Train Loss: 0.1915 (value: 0.0008, weighted value: 0.0409, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9368
Epoch 10/10, Train Loss: 0.1913 (value: 0.0008, weighted value: 0.0398, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9366
..training done in 65.72 seconds
..evaluation done in 15.09 seconds
Old network+MCTS average reward: 0.71, min: 0.08, max: 1.81, stdev: 0.24
New network+MCTS average reward: 0.71, min: 0.20, max: 1.79, stdev: 0.23
Old bare network average reward: 0.66, min: 0.11, max: 1.81, stdev: 0.24
New bare network average reward: 0.66, min: 0.15, max: 1.79, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.39, max: 1.31, stdev: 0.24
External policy "individual greedy" average reward: 0.56, min: 0.02, max: 1.41, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.14, max: 1.59, stdev: 0.23
New network won 87 and tied 140 out of 300 games (52.33% wins where ties are half wins)
Keeping the new network

Training iteration 269 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.48 seconds
Training examples lengths: [65189, 64795, 64713, 64800, 64652, 64620, 64665, 65130, 64749, 64803]
Total value: 459483.11
Training on 648116 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2167 (value: 0.0011, weighted value: 0.0560, policy: 0.1607, weighted policy: 0.1607), Train Mean Max: 0.9353
Epoch 2/10, Train Loss: 0.2083 (value: 0.0010, weighted value: 0.0507, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9356
Epoch 3/10, Train Loss: 0.2015 (value: 0.0009, weighted value: 0.0471, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9361
Epoch 4/10, Train Loss: 0.1994 (value: 0.0009, weighted value: 0.0461, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9364
Epoch 5/10, Train Loss: 0.1963 (value: 0.0009, weighted value: 0.0441, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9365
Epoch 6/10, Train Loss: 0.1942 (value: 0.0009, weighted value: 0.0430, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9366
Epoch 7/10, Train Loss: 0.1931 (value: 0.0008, weighted value: 0.0416, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9367
Epoch 8/10, Train Loss: 0.1906 (value: 0.0008, weighted value: 0.0403, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9371
Epoch 9/10, Train Loss: 0.1893 (value: 0.0008, weighted value: 0.0388, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9371
Epoch 10/10, Train Loss: 0.1892 (value: 0.0008, weighted value: 0.0388, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9373
..training done in 73.07 seconds
..evaluation done in 14.68 seconds
Old network+MCTS average reward: 0.69, min: 0.01, max: 1.36, stdev: 0.23
New network+MCTS average reward: 0.68, min: 0.06, max: 1.36, stdev: 0.23
Old bare network average reward: 0.65, min: -0.02, max: 1.42, stdev: 0.24
New bare network average reward: 0.64, min: 0.02, max: 1.42, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.46, max: 0.85, stdev: 0.23
External policy "individual greedy" average reward: 0.52, min: -0.11, max: 1.17, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.08, max: 1.25, stdev: 0.22
New network won 68 and tied 156 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 270 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.24 seconds
Training examples lengths: [64795, 64713, 64800, 64652, 64620, 64665, 65130, 64749, 64803, 64788]
Total value: 459975.94
Training on 647715 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2447 (value: 0.0015, weighted value: 0.0728, policy: 0.1719, weighted policy: 0.1719), Train Mean Max: 0.9335
Epoch 2/10, Train Loss: 0.2276 (value: 0.0013, weighted value: 0.0631, policy: 0.1645, weighted policy: 0.1645), Train Mean Max: 0.9344
Epoch 3/10, Train Loss: 0.2162 (value: 0.0012, weighted value: 0.0577, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9350
Epoch 4/10, Train Loss: 0.2111 (value: 0.0011, weighted value: 0.0550, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9352
Epoch 5/10, Train Loss: 0.2068 (value: 0.0010, weighted value: 0.0520, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9353
Epoch 6/10, Train Loss: 0.2047 (value: 0.0010, weighted value: 0.0506, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9355
Epoch 7/10, Train Loss: 0.1995 (value: 0.0009, weighted value: 0.0463, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9357
Epoch 8/10, Train Loss: 0.1992 (value: 0.0009, weighted value: 0.0459, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9359
Epoch 9/10, Train Loss: 0.1975 (value: 0.0009, weighted value: 0.0453, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9361
Epoch 10/10, Train Loss: 0.1947 (value: 0.0009, weighted value: 0.0427, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9363
..training done in 66.39 seconds
..evaluation done in 15.36 seconds
Old network+MCTS average reward: 0.72, min: 0.09, max: 1.40, stdev: 0.24
New network+MCTS average reward: 0.73, min: 0.08, max: 1.36, stdev: 0.24
Old bare network average reward: 0.68, min: 0.08, max: 1.40, stdev: 0.24
New bare network average reward: 0.68, min: 0.11, max: 1.29, stdev: 0.24
External policy "random" average reward: 0.28, min: -0.30, max: 0.91, stdev: 0.25
External policy "individual greedy" average reward: 0.55, min: 0.01, max: 1.09, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: 0.07, max: 1.29, stdev: 0.23
New network won 90 and tied 148 out of 300 games (54.67% wins where ties are half wins)
Keeping the new network

Training iteration 271 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.98 seconds
Training examples lengths: [64713, 64800, 64652, 64620, 64665, 65130, 64749, 64803, 64788, 64537]
Total value: 459782.27
Training on 647457 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2203 (value: 0.0012, weighted value: 0.0584, policy: 0.1619, weighted policy: 0.1619), Train Mean Max: 0.9348
Epoch 2/10, Train Loss: 0.2118 (value: 0.0011, weighted value: 0.0525, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9352
Epoch 3/10, Train Loss: 0.2062 (value: 0.0010, weighted value: 0.0505, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9356
Epoch 4/10, Train Loss: 0.2021 (value: 0.0010, weighted value: 0.0482, policy: 0.1539, weighted policy: 0.1539), Train Mean Max: 0.9358
Epoch 5/10, Train Loss: 0.1994 (value: 0.0009, weighted value: 0.0472, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9360
Epoch 6/10, Train Loss: 0.1968 (value: 0.0009, weighted value: 0.0442, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9362
Epoch 7/10, Train Loss: 0.1948 (value: 0.0009, weighted value: 0.0427, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9363
Epoch 8/10, Train Loss: 0.1921 (value: 0.0008, weighted value: 0.0413, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9366
Epoch 9/10, Train Loss: 0.1927 (value: 0.0008, weighted value: 0.0421, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9368
Epoch 10/10, Train Loss: 0.1907 (value: 0.0008, weighted value: 0.0393, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9368
..training done in 73.43 seconds
..evaluation done in 14.95 seconds
Old network+MCTS average reward: 0.68, min: 0.13, max: 1.43, stdev: 0.22
New network+MCTS average reward: 0.67, min: 0.13, max: 1.40, stdev: 0.21
Old bare network average reward: 0.63, min: 0.03, max: 1.43, stdev: 0.22
New bare network average reward: 0.63, min: 0.03, max: 1.43, stdev: 0.22
External policy "random" average reward: 0.23, min: -0.34, max: 0.97, stdev: 0.22
External policy "individual greedy" average reward: 0.50, min: 0.01, max: 1.32, stdev: 0.21
External policy "total greedy" average reward: 0.63, min: 0.14, max: 1.42, stdev: 0.21
New network won 76 and tied 148 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 272 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.17 seconds
Training examples lengths: [64800, 64652, 64620, 64665, 65130, 64749, 64803, 64788, 64537, 64759]
Total value: 460507.75
Training on 647503 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2183 (value: 0.0011, weighted value: 0.0563, policy: 0.1620, weighted policy: 0.1620), Train Mean Max: 0.9352
Epoch 2/10, Train Loss: 0.2099 (value: 0.0010, weighted value: 0.0520, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9358
Epoch 3/10, Train Loss: 0.2026 (value: 0.0009, weighted value: 0.0470, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9361
Epoch 4/10, Train Loss: 0.1991 (value: 0.0009, weighted value: 0.0462, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9364
Epoch 5/10, Train Loss: 0.1963 (value: 0.0009, weighted value: 0.0437, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9364
Epoch 6/10, Train Loss: 0.1941 (value: 0.0009, weighted value: 0.0427, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9368
Epoch 7/10, Train Loss: 0.1925 (value: 0.0008, weighted value: 0.0411, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9367
Epoch 8/10, Train Loss: 0.1914 (value: 0.0008, weighted value: 0.0410, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9369
Epoch 9/10, Train Loss: 0.1893 (value: 0.0008, weighted value: 0.0388, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9371
Epoch 10/10, Train Loss: 0.1881 (value: 0.0008, weighted value: 0.0385, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9372
..training done in 74.58 seconds
..evaluation done in 15.34 seconds
Old network+MCTS average reward: 0.71, min: 0.11, max: 1.38, stdev: 0.22
New network+MCTS average reward: 0.70, min: 0.07, max: 1.38, stdev: 0.22
Old bare network average reward: 0.66, min: 0.06, max: 1.38, stdev: 0.23
New bare network average reward: 0.66, min: 0.07, max: 1.29, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.30, max: 0.96, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: 0.01, max: 1.14, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: 0.15, max: 1.31, stdev: 0.21
New network won 70 and tied 152 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 273 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.93 seconds
Training examples lengths: [64652, 64620, 64665, 65130, 64749, 64803, 64788, 64537, 64759, 65086]
Total value: 460724.96
Training on 647789 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2470 (value: 0.0015, weighted value: 0.0752, policy: 0.1718, weighted policy: 0.1718), Train Mean Max: 0.9336
Epoch 2/10, Train Loss: 0.2265 (value: 0.0013, weighted value: 0.0626, policy: 0.1639, weighted policy: 0.1639), Train Mean Max: 0.9344
Epoch 3/10, Train Loss: 0.2170 (value: 0.0012, weighted value: 0.0579, policy: 0.1592, weighted policy: 0.1592), Train Mean Max: 0.9349
Epoch 4/10, Train Loss: 0.2109 (value: 0.0011, weighted value: 0.0546, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9351
Epoch 5/10, Train Loss: 0.2072 (value: 0.0010, weighted value: 0.0521, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9354
Epoch 6/10, Train Loss: 0.2038 (value: 0.0010, weighted value: 0.0498, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9355
Epoch 7/10, Train Loss: 0.2020 (value: 0.0010, weighted value: 0.0479, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9356
Epoch 8/10, Train Loss: 0.1989 (value: 0.0009, weighted value: 0.0462, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9358
Epoch 9/10, Train Loss: 0.1972 (value: 0.0009, weighted value: 0.0438, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9361
Epoch 10/10, Train Loss: 0.1955 (value: 0.0009, weighted value: 0.0439, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9362
..training done in 72.11 seconds
..evaluation done in 15.72 seconds
Old network+MCTS average reward: 0.71, min: 0.13, max: 1.40, stdev: 0.21
New network+MCTS average reward: 0.71, min: 0.10, max: 1.40, stdev: 0.21
Old bare network average reward: 0.66, min: -0.02, max: 1.36, stdev: 0.21
New bare network average reward: 0.66, min: 0.05, max: 1.40, stdev: 0.22
External policy "random" average reward: 0.26, min: -0.34, max: 0.94, stdev: 0.21
External policy "individual greedy" average reward: 0.53, min: -0.06, max: 1.39, stdev: 0.21
External policy "total greedy" average reward: 0.65, min: 0.19, max: 1.30, stdev: 0.19
New network won 86 and tied 132 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 274 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.98 seconds
Training examples lengths: [64620, 64665, 65130, 64749, 64803, 64788, 64537, 64759, 65086, 64835]
Total value: 460933.64
Training on 647972 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2207 (value: 0.0011, weighted value: 0.0573, policy: 0.1634, weighted policy: 0.1634), Train Mean Max: 0.9344
Epoch 2/10, Train Loss: 0.2114 (value: 0.0010, weighted value: 0.0524, policy: 0.1590, weighted policy: 0.1590), Train Mean Max: 0.9351
Epoch 3/10, Train Loss: 0.2061 (value: 0.0010, weighted value: 0.0499, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9353
Epoch 4/10, Train Loss: 0.2022 (value: 0.0010, weighted value: 0.0477, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9356
Epoch 5/10, Train Loss: 0.1987 (value: 0.0009, weighted value: 0.0456, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9358
Epoch 6/10, Train Loss: 0.1964 (value: 0.0009, weighted value: 0.0441, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9360
Epoch 7/10, Train Loss: 0.1955 (value: 0.0008, weighted value: 0.0424, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9360
Epoch 8/10, Train Loss: 0.1940 (value: 0.0008, weighted value: 0.0412, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9362
Epoch 9/10, Train Loss: 0.1912 (value: 0.0008, weighted value: 0.0403, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9365
Epoch 10/10, Train Loss: 0.1913 (value: 0.0008, weighted value: 0.0402, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9366
..training done in 65.64 seconds
..evaluation done in 14.84 seconds
Old network+MCTS average reward: 0.69, min: 0.11, max: 1.44, stdev: 0.24
New network+MCTS average reward: 0.70, min: 0.17, max: 1.48, stdev: 0.23
Old bare network average reward: 0.65, min: 0.02, max: 1.44, stdev: 0.24
New bare network average reward: 0.65, min: 0.08, max: 1.44, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.29, max: 1.02, stdev: 0.24
External policy "individual greedy" average reward: 0.53, min: -0.10, max: 1.44, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.11, max: 1.54, stdev: 0.22
New network won 73 and tied 166 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network

Training iteration 275 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.57 seconds
Training examples lengths: [64665, 65130, 64749, 64803, 64788, 64537, 64759, 65086, 64835, 64725]
Total value: 460660.58
Training on 648077 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2185 (value: 0.0011, weighted value: 0.0556, policy: 0.1629, weighted policy: 0.1629), Train Mean Max: 0.9349
Epoch 2/10, Train Loss: 0.2079 (value: 0.0010, weighted value: 0.0498, policy: 0.1581, weighted policy: 0.1581), Train Mean Max: 0.9356
Epoch 3/10, Train Loss: 0.2019 (value: 0.0009, weighted value: 0.0467, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9358
Epoch 4/10, Train Loss: 0.1973 (value: 0.0009, weighted value: 0.0442, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9362
Epoch 5/10, Train Loss: 0.1956 (value: 0.0009, weighted value: 0.0430, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9362
Epoch 6/10, Train Loss: 0.1943 (value: 0.0009, weighted value: 0.0426, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9364
Epoch 7/10, Train Loss: 0.1921 (value: 0.0008, weighted value: 0.0414, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9367
Epoch 8/10, Train Loss: 0.1901 (value: 0.0008, weighted value: 0.0392, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9366
Epoch 9/10, Train Loss: 0.1917 (value: 0.0008, weighted value: 0.0394, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9367
Epoch 10/10, Train Loss: 0.1878 (value: 0.0008, weighted value: 0.0377, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9371
..training done in 73.12 seconds
..evaluation done in 14.86 seconds
Old network+MCTS average reward: 0.70, min: 0.05, max: 1.38, stdev: 0.22
New network+MCTS average reward: 0.70, min: 0.12, max: 1.46, stdev: 0.23
Old bare network average reward: 0.65, min: 0.03, max: 1.38, stdev: 0.23
New bare network average reward: 0.66, min: 0.03, max: 1.38, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.34, max: 0.81, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: 0.02, max: 1.18, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.00, max: 1.37, stdev: 0.22
New network won 69 and tied 153 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 276 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.08 seconds
Training examples lengths: [65130, 64749, 64803, 64788, 64537, 64759, 65086, 64835, 64725, 64972]
Total value: 461977.92
Training on 648384 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2430 (value: 0.0014, weighted value: 0.0714, policy: 0.1716, weighted policy: 0.1716), Train Mean Max: 0.9334
Epoch 2/10, Train Loss: 0.2264 (value: 0.0013, weighted value: 0.0630, policy: 0.1634, weighted policy: 0.1634), Train Mean Max: 0.9343
Epoch 3/10, Train Loss: 0.2154 (value: 0.0011, weighted value: 0.0571, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9350
Epoch 4/10, Train Loss: 0.2090 (value: 0.0011, weighted value: 0.0530, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9352
Epoch 5/10, Train Loss: 0.2066 (value: 0.0010, weighted value: 0.0517, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9353
Epoch 6/10, Train Loss: 0.2026 (value: 0.0010, weighted value: 0.0500, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9356
Epoch 7/10, Train Loss: 0.1993 (value: 0.0009, weighted value: 0.0463, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9357
Epoch 8/10, Train Loss: 0.1963 (value: 0.0009, weighted value: 0.0438, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9360
Epoch 9/10, Train Loss: 0.1966 (value: 0.0009, weighted value: 0.0445, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9361
Epoch 10/10, Train Loss: 0.1949 (value: 0.0009, weighted value: 0.0430, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9364
..training done in 68.22 seconds
..evaluation done in 15.05 seconds
Old network+MCTS average reward: 0.70, min: 0.14, max: 1.54, stdev: 0.22
New network+MCTS average reward: 0.69, min: 0.10, max: 1.50, stdev: 0.23
Old bare network average reward: 0.65, min: 0.07, max: 1.49, stdev: 0.23
New bare network average reward: 0.65, min: 0.00, max: 1.49, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.26, max: 1.19, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: 0.05, max: 1.52, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.14, max: 1.56, stdev: 0.22
New network won 67 and tied 158 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 277 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.05 seconds
Training examples lengths: [64749, 64803, 64788, 64537, 64759, 65086, 64835, 64725, 64972, 65019]
Total value: 462695.23
Training on 648273 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2671 (value: 0.0017, weighted value: 0.0865, policy: 0.1805, weighted policy: 0.1805), Train Mean Max: 0.9322
Epoch 2/10, Train Loss: 0.2420 (value: 0.0015, weighted value: 0.0733, policy: 0.1686, weighted policy: 0.1686), Train Mean Max: 0.9334
Epoch 3/10, Train Loss: 0.2292 (value: 0.0013, weighted value: 0.0674, policy: 0.1619, weighted policy: 0.1619), Train Mean Max: 0.9338
Epoch 4/10, Train Loss: 0.2204 (value: 0.0012, weighted value: 0.0613, policy: 0.1591, weighted policy: 0.1591), Train Mean Max: 0.9343
Epoch 5/10, Train Loss: 0.2166 (value: 0.0012, weighted value: 0.0591, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9344
Epoch 6/10, Train Loss: 0.2106 (value: 0.0011, weighted value: 0.0550, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9348
Epoch 7/10, Train Loss: 0.2077 (value: 0.0010, weighted value: 0.0520, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9348
Epoch 8/10, Train Loss: 0.2050 (value: 0.0010, weighted value: 0.0508, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9352
Epoch 9/10, Train Loss: 0.2028 (value: 0.0010, weighted value: 0.0486, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9353
Epoch 10/10, Train Loss: 0.2003 (value: 0.0009, weighted value: 0.0473, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9356
..training done in 71.23 seconds
..evaluation done in 15.16 seconds
Old network+MCTS average reward: 0.72, min: 0.03, max: 1.44, stdev: 0.24
New network+MCTS average reward: 0.72, min: 0.03, max: 1.44, stdev: 0.24
Old bare network average reward: 0.67, min: 0.03, max: 1.44, stdev: 0.24
New bare network average reward: 0.67, min: 0.03, max: 1.44, stdev: 0.25
External policy "random" average reward: 0.26, min: -0.41, max: 1.09, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.07, max: 1.25, stdev: 0.24
External policy "total greedy" average reward: 0.66, min: 0.10, max: 1.33, stdev: 0.24
New network won 77 and tied 153 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 278 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.72 seconds
Training examples lengths: [64803, 64788, 64537, 64759, 65086, 64835, 64725, 64972, 65019, 64934]
Total value: 463512.81
Training on 648458 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2252 (value: 0.0013, weighted value: 0.0634, policy: 0.1618, weighted policy: 0.1618), Train Mean Max: 0.9340
Epoch 2/10, Train Loss: 0.2152 (value: 0.0011, weighted value: 0.0567, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9345
Epoch 3/10, Train Loss: 0.2083 (value: 0.0010, weighted value: 0.0524, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9350
Epoch 4/10, Train Loss: 0.2044 (value: 0.0010, weighted value: 0.0496, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9352
Epoch 5/10, Train Loss: 0.2016 (value: 0.0010, weighted value: 0.0492, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9355
Epoch 6/10, Train Loss: 0.1991 (value: 0.0009, weighted value: 0.0464, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9355
Epoch 7/10, Train Loss: 0.1967 (value: 0.0009, weighted value: 0.0445, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9359
Epoch 8/10, Train Loss: 0.1961 (value: 0.0009, weighted value: 0.0444, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9361
Epoch 9/10, Train Loss: 0.1930 (value: 0.0008, weighted value: 0.0419, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9363
Epoch 10/10, Train Loss: 0.1933 (value: 0.0008, weighted value: 0.0420, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9364
..training done in 70.67 seconds
..evaluation done in 15.73 seconds
Old network+MCTS average reward: 0.71, min: -0.05, max: 1.29, stdev: 0.23
New network+MCTS average reward: 0.71, min: -0.05, max: 1.29, stdev: 0.23
Old bare network average reward: 0.67, min: -0.12, max: 1.29, stdev: 0.24
New bare network average reward: 0.67, min: -0.12, max: 1.29, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.44, max: 0.86, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: 0.00, max: 1.19, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.01, max: 1.18, stdev: 0.21
New network won 74 and tied 166 out of 300 games (52.33% wins where ties are half wins)
Keeping the new network

Training iteration 279 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.73 seconds
Training examples lengths: [64788, 64537, 64759, 65086, 64835, 64725, 64972, 65019, 64934, 64522]
Total value: 463399.28
Training on 648177 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2179 (value: 0.0011, weighted value: 0.0573, policy: 0.1606, weighted policy: 0.1606), Train Mean Max: 0.9347
Epoch 2/10, Train Loss: 0.2099 (value: 0.0011, weighted value: 0.0527, policy: 0.1572, weighted policy: 0.1572), Train Mean Max: 0.9354
Epoch 3/10, Train Loss: 0.2025 (value: 0.0010, weighted value: 0.0482, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9357
Epoch 4/10, Train Loss: 0.1992 (value: 0.0009, weighted value: 0.0468, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9360
Epoch 5/10, Train Loss: 0.1966 (value: 0.0009, weighted value: 0.0444, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9361
Epoch 6/10, Train Loss: 0.1948 (value: 0.0009, weighted value: 0.0435, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9363
Epoch 7/10, Train Loss: 0.1928 (value: 0.0008, weighted value: 0.0422, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9365
Epoch 8/10, Train Loss: 0.1939 (value: 0.0008, weighted value: 0.0424, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9365
Epoch 9/10, Train Loss: 0.1902 (value: 0.0008, weighted value: 0.0397, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9368
Epoch 10/10, Train Loss: 0.1881 (value: 0.0008, weighted value: 0.0384, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9371
..training done in 68.24 seconds
..evaluation done in 15.40 seconds
Old network+MCTS average reward: 0.71, min: 0.16, max: 1.45, stdev: 0.23
New network+MCTS average reward: 0.71, min: 0.12, max: 1.45, stdev: 0.23
Old bare network average reward: 0.66, min: 0.04, max: 1.45, stdev: 0.24
New bare network average reward: 0.66, min: 0.11, max: 1.45, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.31, max: 0.97, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: -0.01, max: 1.38, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.07, max: 1.23, stdev: 0.22
New network won 74 and tied 153 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 280 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.01 seconds
Training examples lengths: [64537, 64759, 65086, 64835, 64725, 64972, 65019, 64934, 64522, 65106]
Total value: 463672.94
Training on 648495 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2170 (value: 0.0011, weighted value: 0.0563, policy: 0.1606, weighted policy: 0.1606), Train Mean Max: 0.9353
Epoch 2/10, Train Loss: 0.2065 (value: 0.0010, weighted value: 0.0500, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9360
Epoch 3/10, Train Loss: 0.2000 (value: 0.0009, weighted value: 0.0469, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9364
Epoch 4/10, Train Loss: 0.1979 (value: 0.0009, weighted value: 0.0454, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9365
Epoch 5/10, Train Loss: 0.1942 (value: 0.0009, weighted value: 0.0430, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9367
Epoch 6/10, Train Loss: 0.1930 (value: 0.0009, weighted value: 0.0425, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9369
Epoch 7/10, Train Loss: 0.1911 (value: 0.0008, weighted value: 0.0407, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9370
Epoch 8/10, Train Loss: 0.1897 (value: 0.0008, weighted value: 0.0394, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9371
Epoch 9/10, Train Loss: 0.1880 (value: 0.0008, weighted value: 0.0383, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9372
Epoch 10/10, Train Loss: 0.1858 (value: 0.0007, weighted value: 0.0367, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9376
..training done in 66.22 seconds
..evaluation done in 15.07 seconds
Old network+MCTS average reward: 0.71, min: 0.15, max: 1.47, stdev: 0.22
New network+MCTS average reward: 0.71, min: 0.15, max: 1.47, stdev: 0.22
Old bare network average reward: 0.67, min: 0.09, max: 1.47, stdev: 0.22
New bare network average reward: 0.66, min: 0.15, max: 1.44, stdev: 0.22
External policy "random" average reward: 0.28, min: -0.28, max: 0.92, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: 0.04, max: 1.12, stdev: 0.21
External policy "total greedy" average reward: 0.65, min: 0.15, max: 1.36, stdev: 0.21
New network won 60 and tied 170 out of 300 games (48.33% wins where ties are half wins)
Reverting to the old network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1757697740_iter_280

Training iteration 281 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.24 seconds
Training examples lengths: [64759, 65086, 64835, 64725, 64972, 65019, 64934, 64522, 65106, 65121]
Total value: 464127.11
Training on 649079 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2415 (value: 0.0014, weighted value: 0.0721, policy: 0.1694, weighted policy: 0.1694), Train Mean Max: 0.9338
Epoch 2/10, Train Loss: 0.2222 (value: 0.0012, weighted value: 0.0602, policy: 0.1619, weighted policy: 0.1619), Train Mean Max: 0.9348
Epoch 3/10, Train Loss: 0.2139 (value: 0.0011, weighted value: 0.0570, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9352
Epoch 4/10, Train Loss: 0.2091 (value: 0.0011, weighted value: 0.0542, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9354
Epoch 5/10, Train Loss: 0.2043 (value: 0.0010, weighted value: 0.0501, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9357
Epoch 6/10, Train Loss: 0.2022 (value: 0.0010, weighted value: 0.0489, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9358
Epoch 7/10, Train Loss: 0.1968 (value: 0.0009, weighted value: 0.0454, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9363
Epoch 8/10, Train Loss: 0.1972 (value: 0.0009, weighted value: 0.0453, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9363
Epoch 9/10, Train Loss: 0.1946 (value: 0.0009, weighted value: 0.0440, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9365
Epoch 10/10, Train Loss: 0.1910 (value: 0.0008, weighted value: 0.0410, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9367
..training done in 72.10 seconds
..evaluation done in 15.38 seconds
Old network+MCTS average reward: 0.69, min: 0.12, max: 1.47, stdev: 0.22
New network+MCTS average reward: 0.70, min: 0.16, max: 1.47, stdev: 0.22
Old bare network average reward: 0.65, min: 0.06, max: 1.47, stdev: 0.23
New bare network average reward: 0.65, min: 0.06, max: 1.47, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.43, max: 0.92, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: -0.08, max: 1.33, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.11, max: 1.40, stdev: 0.21
New network won 79 and tied 169 out of 300 games (54.50% wins where ties are half wins)
Keeping the new network

Training iteration 282 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.68 seconds
Training examples lengths: [65086, 64835, 64725, 64972, 65019, 64934, 64522, 65106, 65121, 64679]
Total value: 463640.06
Training on 648999 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2181 (value: 0.0012, weighted value: 0.0586, policy: 0.1595, weighted policy: 0.1595), Train Mean Max: 0.9352
Epoch 2/10, Train Loss: 0.2080 (value: 0.0011, weighted value: 0.0526, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9360
Epoch 3/10, Train Loss: 0.2016 (value: 0.0010, weighted value: 0.0481, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9363
Epoch 4/10, Train Loss: 0.2000 (value: 0.0010, weighted value: 0.0480, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9365
Epoch 5/10, Train Loss: 0.1964 (value: 0.0009, weighted value: 0.0458, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9367
Epoch 6/10, Train Loss: 0.1927 (value: 0.0009, weighted value: 0.0427, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9370
Epoch 7/10, Train Loss: 0.1911 (value: 0.0008, weighted value: 0.0415, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9370
Epoch 8/10, Train Loss: 0.1913 (value: 0.0008, weighted value: 0.0412, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9372
Epoch 9/10, Train Loss: 0.1898 (value: 0.0008, weighted value: 0.0407, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9374
Epoch 10/10, Train Loss: 0.1882 (value: 0.0008, weighted value: 0.0394, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9376
..training done in 66.37 seconds
..evaluation done in 15.65 seconds
Old network+MCTS average reward: 0.68, min: -0.07, max: 1.69, stdev: 0.23
New network+MCTS average reward: 0.68, min: -0.07, max: 1.64, stdev: 0.23
Old bare network average reward: 0.64, min: -0.11, max: 1.65, stdev: 0.23
New bare network average reward: 0.64, min: -0.15, max: 1.64, stdev: 0.23
External policy "random" average reward: 0.23, min: -0.32, max: 1.08, stdev: 0.22
External policy "individual greedy" average reward: 0.51, min: -0.32, max: 1.55, stdev: 0.23
External policy "total greedy" average reward: 0.62, min: 0.01, max: 1.56, stdev: 0.22
New network won 66 and tied 165 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 283 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.32 seconds
Training examples lengths: [64835, 64725, 64972, 65019, 64934, 64522, 65106, 65121, 64679, 64527]
Total value: 463208.63
Training on 648440 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2442 (value: 0.0015, weighted value: 0.0760, policy: 0.1682, weighted policy: 0.1682), Train Mean Max: 0.9338
Epoch 2/10, Train Loss: 0.2264 (value: 0.0013, weighted value: 0.0649, policy: 0.1614, weighted policy: 0.1614), Train Mean Max: 0.9348
Epoch 3/10, Train Loss: 0.2165 (value: 0.0012, weighted value: 0.0600, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9353
Epoch 4/10, Train Loss: 0.2091 (value: 0.0011, weighted value: 0.0545, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9355
Epoch 5/10, Train Loss: 0.2063 (value: 0.0011, weighted value: 0.0536, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9358
Epoch 6/10, Train Loss: 0.2016 (value: 0.0010, weighted value: 0.0496, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9361
Epoch 7/10, Train Loss: 0.2021 (value: 0.0010, weighted value: 0.0501, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9362
Epoch 8/10, Train Loss: 0.1980 (value: 0.0009, weighted value: 0.0462, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9365
Epoch 9/10, Train Loss: 0.1959 (value: 0.0009, weighted value: 0.0452, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9366
Epoch 10/10, Train Loss: 0.1933 (value: 0.0009, weighted value: 0.0432, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9369
..training done in 71.94 seconds
..evaluation done in 16.05 seconds
Old network+MCTS average reward: 0.71, min: 0.17, max: 1.51, stdev: 0.23
New network+MCTS average reward: 0.72, min: 0.23, max: 1.50, stdev: 0.22
Old bare network average reward: 0.68, min: 0.13, max: 1.51, stdev: 0.22
New bare network average reward: 0.68, min: 0.12, max: 1.51, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.34, max: 0.95, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: 0.06, max: 1.29, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.01, max: 1.36, stdev: 0.22
New network won 73 and tied 169 out of 300 games (52.50% wins where ties are half wins)
Keeping the new network

Training iteration 284 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.19 seconds
Training examples lengths: [64725, 64972, 65019, 64934, 64522, 65106, 65121, 64679, 64527, 64783]
Total value: 463362.35
Training on 648388 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2199 (value: 0.0012, weighted value: 0.0597, policy: 0.1602, weighted policy: 0.1602), Train Mean Max: 0.9354
Epoch 2/10, Train Loss: 0.2096 (value: 0.0011, weighted value: 0.0538, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9361
Epoch 3/10, Train Loss: 0.2037 (value: 0.0010, weighted value: 0.0503, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9362
Epoch 4/10, Train Loss: 0.2011 (value: 0.0010, weighted value: 0.0494, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9365
Epoch 5/10, Train Loss: 0.1960 (value: 0.0009, weighted value: 0.0455, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9367
Epoch 6/10, Train Loss: 0.1948 (value: 0.0009, weighted value: 0.0441, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9368
Epoch 7/10, Train Loss: 0.1940 (value: 0.0009, weighted value: 0.0437, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9370
Epoch 8/10, Train Loss: 0.1918 (value: 0.0009, weighted value: 0.0428, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9373
Epoch 9/10, Train Loss: 0.1892 (value: 0.0008, weighted value: 0.0408, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9375
Epoch 10/10, Train Loss: 0.1887 (value: 0.0008, weighted value: 0.0397, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9376
..training done in 72.63 seconds
..evaluation done in 15.24 seconds
Old network+MCTS average reward: 0.72, min: 0.06, max: 1.48, stdev: 0.24
New network+MCTS average reward: 0.72, min: 0.06, max: 1.45, stdev: 0.24
Old bare network average reward: 0.68, min: -0.04, max: 1.48, stdev: 0.24
New bare network average reward: 0.68, min: -0.06, max: 1.45, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.38, max: 1.20, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: -0.17, max: 1.56, stdev: 0.24
External policy "total greedy" average reward: 0.67, min: -0.02, max: 1.56, stdev: 0.23
New network won 72 and tied 150 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 285 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.85 seconds
Training examples lengths: [64972, 65019, 64934, 64522, 65106, 65121, 64679, 64527, 64783, 64892]
Total value: 463894.69
Training on 648555 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2442 (value: 0.0015, weighted value: 0.0753, policy: 0.1689, weighted policy: 0.1689), Train Mean Max: 0.9337
Epoch 2/10, Train Loss: 0.2278 (value: 0.0013, weighted value: 0.0660, policy: 0.1617, weighted policy: 0.1617), Train Mean Max: 0.9347
Epoch 3/10, Train Loss: 0.2167 (value: 0.0012, weighted value: 0.0594, policy: 0.1572, weighted policy: 0.1572), Train Mean Max: 0.9351
Epoch 4/10, Train Loss: 0.2115 (value: 0.0011, weighted value: 0.0568, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9354
Epoch 5/10, Train Loss: 0.2070 (value: 0.0011, weighted value: 0.0541, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9357
Epoch 6/10, Train Loss: 0.2041 (value: 0.0010, weighted value: 0.0515, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9358
Epoch 7/10, Train Loss: 0.1998 (value: 0.0010, weighted value: 0.0482, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9361
Epoch 8/10, Train Loss: 0.1993 (value: 0.0010, weighted value: 0.0477, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9363
Epoch 9/10, Train Loss: 0.1968 (value: 0.0009, weighted value: 0.0454, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9365
Epoch 10/10, Train Loss: 0.1965 (value: 0.0009, weighted value: 0.0450, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9365
..training done in 65.99 seconds
..evaluation done in 15.24 seconds
Old network+MCTS average reward: 0.71, min: 0.14, max: 1.51, stdev: 0.22
New network+MCTS average reward: 0.71, min: 0.19, max: 1.41, stdev: 0.22
Old bare network average reward: 0.67, min: 0.10, max: 1.41, stdev: 0.22
New bare network average reward: 0.67, min: 0.10, max: 1.41, stdev: 0.22
External policy "random" average reward: 0.27, min: -0.32, max: 0.84, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: 0.00, max: 1.35, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.11, max: 1.34, stdev: 0.22
New network won 77 and tied 148 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 286 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.87 seconds
Training examples lengths: [65019, 64934, 64522, 65106, 65121, 64679, 64527, 64783, 64892, 64786]
Total value: 463220.73
Training on 648369 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2187 (value: 0.0012, weighted value: 0.0587, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9352
Epoch 2/10, Train Loss: 0.2104 (value: 0.0011, weighted value: 0.0545, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9358
Epoch 3/10, Train Loss: 0.2032 (value: 0.0010, weighted value: 0.0503, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9360
Epoch 4/10, Train Loss: 0.2004 (value: 0.0010, weighted value: 0.0481, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9364
Epoch 5/10, Train Loss: 0.1976 (value: 0.0009, weighted value: 0.0458, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9365
Epoch 6/10, Train Loss: 0.1955 (value: 0.0009, weighted value: 0.0455, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9368
Epoch 7/10, Train Loss: 0.1920 (value: 0.0008, weighted value: 0.0421, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9370
Epoch 8/10, Train Loss: 0.1916 (value: 0.0008, weighted value: 0.0421, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9371
Epoch 9/10, Train Loss: 0.1904 (value: 0.0008, weighted value: 0.0412, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9373
Epoch 10/10, Train Loss: 0.1901 (value: 0.0008, weighted value: 0.0403, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9374
..training done in 71.22 seconds
..evaluation done in 15.50 seconds
Old network+MCTS average reward: 0.70, min: 0.17, max: 1.31, stdev: 0.21
New network+MCTS average reward: 0.70, min: 0.12, max: 1.51, stdev: 0.22
Old bare network average reward: 0.65, min: 0.08, max: 1.31, stdev: 0.22
New bare network average reward: 0.66, min: 0.00, max: 1.27, stdev: 0.22
External policy "random" average reward: 0.25, min: -0.26, max: 0.89, stdev: 0.21
External policy "individual greedy" average reward: 0.53, min: -0.03, max: 1.33, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.06, max: 1.32, stdev: 0.22
New network won 73 and tied 163 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 287 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.08 seconds
Training examples lengths: [64934, 64522, 65106, 65121, 64679, 64527, 64783, 64892, 64786, 64517]
Total value: 462585.16
Training on 647867 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2141 (value: 0.0011, weighted value: 0.0553, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9358
Epoch 2/10, Train Loss: 0.2059 (value: 0.0010, weighted value: 0.0504, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9364
Epoch 3/10, Train Loss: 0.1989 (value: 0.0009, weighted value: 0.0464, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9368
Epoch 4/10, Train Loss: 0.1964 (value: 0.0009, weighted value: 0.0454, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9369
Epoch 5/10, Train Loss: 0.1947 (value: 0.0009, weighted value: 0.0444, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9371
Epoch 6/10, Train Loss: 0.1901 (value: 0.0008, weighted value: 0.0408, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9375
Epoch 7/10, Train Loss: 0.1913 (value: 0.0008, weighted value: 0.0417, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1895 (value: 0.0008, weighted value: 0.0406, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1866 (value: 0.0008, weighted value: 0.0380, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9378
Epoch 10/10, Train Loss: 0.1865 (value: 0.0008, weighted value: 0.0375, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9378
..training done in 68.24 seconds
..evaluation done in 15.45 seconds
Old network+MCTS average reward: 0.71, min: 0.06, max: 1.38, stdev: 0.25
New network+MCTS average reward: 0.71, min: 0.10, max: 1.39, stdev: 0.25
Old bare network average reward: 0.66, min: 0.04, max: 1.40, stdev: 0.25
New bare network average reward: 0.67, min: 0.01, max: 1.39, stdev: 0.25
External policy "random" average reward: 0.26, min: -0.34, max: 0.89, stdev: 0.24
External policy "individual greedy" average reward: 0.55, min: 0.02, max: 1.44, stdev: 0.24
External policy "total greedy" average reward: 0.66, min: 0.09, max: 1.40, stdev: 0.23
New network won 75 and tied 160 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 288 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.57 seconds
Training examples lengths: [64522, 65106, 65121, 64679, 64527, 64783, 64892, 64786, 64517, 64820]
Total value: 462289.94
Training on 647753 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2127 (value: 0.0011, weighted value: 0.0544, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9361
Epoch 2/10, Train Loss: 0.2050 (value: 0.0010, weighted value: 0.0501, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9365
Epoch 3/10, Train Loss: 0.1986 (value: 0.0009, weighted value: 0.0461, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9368
Epoch 4/10, Train Loss: 0.1946 (value: 0.0009, weighted value: 0.0436, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9371
Epoch 5/10, Train Loss: 0.1924 (value: 0.0009, weighted value: 0.0428, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9374
Epoch 6/10, Train Loss: 0.1911 (value: 0.0008, weighted value: 0.0409, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9373
Epoch 7/10, Train Loss: 0.1891 (value: 0.0008, weighted value: 0.0398, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9376
Epoch 8/10, Train Loss: 0.1878 (value: 0.0008, weighted value: 0.0385, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9376
Epoch 9/10, Train Loss: 0.1872 (value: 0.0008, weighted value: 0.0381, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1849 (value: 0.0007, weighted value: 0.0365, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9378
..training done in 69.33 seconds
..evaluation done in 15.84 seconds
Old network+MCTS average reward: 0.72, min: 0.13, max: 1.53, stdev: 0.24
New network+MCTS average reward: 0.72, min: 0.08, max: 1.55, stdev: 0.24
Old bare network average reward: 0.68, min: 0.07, max: 1.53, stdev: 0.24
New bare network average reward: 0.67, min: 0.08, max: 1.55, stdev: 0.24
External policy "random" average reward: 0.28, min: -0.31, max: 1.20, stdev: 0.23
External policy "individual greedy" average reward: 0.56, min: 0.00, max: 1.46, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: -0.04, max: 1.59, stdev: 0.24
New network won 68 and tied 158 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 289 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.63 seconds
Training examples lengths: [65106, 65121, 64679, 64527, 64783, 64892, 64786, 64517, 64820, 64603]
Total value: 462333.44
Training on 647834 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2413 (value: 0.0014, weighted value: 0.0717, policy: 0.1696, weighted policy: 0.1696), Train Mean Max: 0.9341
Epoch 2/10, Train Loss: 0.2215 (value: 0.0012, weighted value: 0.0604, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9352
Epoch 3/10, Train Loss: 0.2129 (value: 0.0011, weighted value: 0.0566, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9356
Epoch 4/10, Train Loss: 0.2058 (value: 0.0010, weighted value: 0.0523, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9359
Epoch 5/10, Train Loss: 0.2021 (value: 0.0010, weighted value: 0.0492, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9360
Epoch 6/10, Train Loss: 0.2000 (value: 0.0009, weighted value: 0.0475, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9362
Epoch 7/10, Train Loss: 0.1984 (value: 0.0009, weighted value: 0.0472, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9364
Epoch 8/10, Train Loss: 0.1952 (value: 0.0009, weighted value: 0.0439, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9365
Epoch 9/10, Train Loss: 0.1927 (value: 0.0008, weighted value: 0.0419, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9368
Epoch 10/10, Train Loss: 0.1929 (value: 0.0008, weighted value: 0.0421, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9369
..training done in 67.78 seconds
..evaluation done in 15.39 seconds
Old network+MCTS average reward: 0.71, min: 0.14, max: 1.43, stdev: 0.22
New network+MCTS average reward: 0.71, min: 0.13, max: 1.36, stdev: 0.22
Old bare network average reward: 0.66, min: 0.03, max: 1.33, stdev: 0.22
New bare network average reward: 0.66, min: 0.03, max: 1.33, stdev: 0.22
External policy "random" average reward: 0.27, min: -0.37, max: 0.86, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: -0.06, max: 1.18, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.06, max: 1.31, stdev: 0.21
New network won 73 and tied 153 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 290 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.89 seconds
Training examples lengths: [65121, 64679, 64527, 64783, 64892, 64786, 64517, 64820, 64603, 64803]
Total value: 461867.25
Training on 647531 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2647 (value: 0.0017, weighted value: 0.0870, policy: 0.1777, weighted policy: 0.1777), Train Mean Max: 0.9326
Epoch 2/10, Train Loss: 0.2383 (value: 0.0014, weighted value: 0.0712, policy: 0.1671, weighted policy: 0.1671), Train Mean Max: 0.9338
Epoch 3/10, Train Loss: 0.2265 (value: 0.0013, weighted value: 0.0656, policy: 0.1609, weighted policy: 0.1609), Train Mean Max: 0.9343
Epoch 4/10, Train Loss: 0.2183 (value: 0.0012, weighted value: 0.0599, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9346
Epoch 5/10, Train Loss: 0.2126 (value: 0.0011, weighted value: 0.0566, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9349
Epoch 6/10, Train Loss: 0.2098 (value: 0.0011, weighted value: 0.0541, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9351
Epoch 7/10, Train Loss: 0.2064 (value: 0.0010, weighted value: 0.0520, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9353
Epoch 8/10, Train Loss: 0.2006 (value: 0.0010, weighted value: 0.0478, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9355
Epoch 9/10, Train Loss: 0.2008 (value: 0.0010, weighted value: 0.0485, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9358
Epoch 10/10, Train Loss: 0.1969 (value: 0.0009, weighted value: 0.0445, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9359
..training done in 70.41 seconds
..evaluation done in 15.46 seconds
Old network+MCTS average reward: 0.71, min: 0.12, max: 1.26, stdev: 0.21
New network+MCTS average reward: 0.71, min: 0.19, max: 1.26, stdev: 0.20
Old bare network average reward: 0.67, min: 0.17, max: 1.26, stdev: 0.21
New bare network average reward: 0.66, min: 0.07, max: 1.26, stdev: 0.21
External policy "random" average reward: 0.25, min: -0.23, max: 0.84, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: 0.00, max: 1.19, stdev: 0.21
External policy "total greedy" average reward: 0.64, min: 0.13, max: 1.16, stdev: 0.20
New network won 75 and tied 152 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 291 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.78 seconds
Training examples lengths: [64679, 64527, 64783, 64892, 64786, 64517, 64820, 64603, 64803, 65049]
Total value: 462504.58
Training on 647459 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2243 (value: 0.0012, weighted value: 0.0614, policy: 0.1629, weighted policy: 0.1629), Train Mean Max: 0.9344
Epoch 2/10, Train Loss: 0.2124 (value: 0.0011, weighted value: 0.0542, policy: 0.1582, weighted policy: 0.1582), Train Mean Max: 0.9351
Epoch 3/10, Train Loss: 0.2055 (value: 0.0010, weighted value: 0.0505, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9356
Epoch 4/10, Train Loss: 0.2026 (value: 0.0010, weighted value: 0.0489, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9357
Epoch 5/10, Train Loss: 0.2005 (value: 0.0010, weighted value: 0.0476, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9360
Epoch 6/10, Train Loss: 0.1971 (value: 0.0009, weighted value: 0.0448, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9361
Epoch 7/10, Train Loss: 0.1965 (value: 0.0009, weighted value: 0.0447, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9364
Epoch 8/10, Train Loss: 0.1921 (value: 0.0008, weighted value: 0.0415, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9366
Epoch 9/10, Train Loss: 0.1934 (value: 0.0009, weighted value: 0.0430, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9367
Epoch 10/10, Train Loss: 0.1904 (value: 0.0008, weighted value: 0.0398, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9367
..training done in 66.66 seconds
..evaluation done in 15.56 seconds
Old network+MCTS average reward: 0.72, min: 0.06, max: 1.41, stdev: 0.24
New network+MCTS average reward: 0.72, min: 0.06, max: 1.41, stdev: 0.24
Old bare network average reward: 0.68, min: 0.01, max: 1.36, stdev: 0.24
New bare network average reward: 0.67, min: 0.01, max: 1.41, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.26, max: 1.11, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.11, max: 1.20, stdev: 0.24
External policy "total greedy" average reward: 0.66, min: 0.13, max: 1.37, stdev: 0.23
New network won 75 and tied 138 out of 300 games (48.00% wins where ties are half wins)
Reverting to the old network

Training iteration 292 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.66 seconds
Training examples lengths: [64527, 64783, 64892, 64786, 64517, 64820, 64603, 64803, 65049, 64817]
Total value: 463230.34
Training on 647597 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2496 (value: 0.0016, weighted value: 0.0777, policy: 0.1719, weighted policy: 0.1719), Train Mean Max: 0.9329
Epoch 2/10, Train Loss: 0.2280 (value: 0.0013, weighted value: 0.0646, policy: 0.1634, weighted policy: 0.1634), Train Mean Max: 0.9340
Epoch 3/10, Train Loss: 0.2195 (value: 0.0012, weighted value: 0.0605, policy: 0.1590, weighted policy: 0.1590), Train Mean Max: 0.9345
Epoch 4/10, Train Loss: 0.2120 (value: 0.0011, weighted value: 0.0559, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9347
Epoch 5/10, Train Loss: 0.2102 (value: 0.0011, weighted value: 0.0547, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9348
Epoch 6/10, Train Loss: 0.2055 (value: 0.0010, weighted value: 0.0518, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9354
Epoch 7/10, Train Loss: 0.2010 (value: 0.0010, weighted value: 0.0479, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9356
Epoch 8/10, Train Loss: 0.2004 (value: 0.0010, weighted value: 0.0481, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9357
Epoch 9/10, Train Loss: 0.1973 (value: 0.0009, weighted value: 0.0452, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9360
Epoch 10/10, Train Loss: 0.1963 (value: 0.0009, weighted value: 0.0445, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9362
..training done in 68.28 seconds
..evaluation done in 15.14 seconds
Old network+MCTS average reward: 0.70, min: 0.11, max: 1.58, stdev: 0.24
New network+MCTS average reward: 0.70, min: 0.11, max: 1.58, stdev: 0.24
Old bare network average reward: 0.65, min: 0.06, max: 1.58, stdev: 0.25
New bare network average reward: 0.65, min: 0.07, max: 1.58, stdev: 0.25
External policy "random" average reward: 0.25, min: -0.38, max: 0.88, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: 0.00, max: 1.36, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.07, max: 1.49, stdev: 0.23
New network won 73 and tied 163 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 293 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.45 seconds
Training examples lengths: [64783, 64892, 64786, 64517, 64820, 64603, 64803, 65049, 64817, 64942]
Total value: 464489.31
Training on 648012 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2206 (value: 0.0012, weighted value: 0.0594, policy: 0.1612, weighted policy: 0.1612), Train Mean Max: 0.9347
Epoch 2/10, Train Loss: 0.2099 (value: 0.0011, weighted value: 0.0533, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9355
Epoch 3/10, Train Loss: 0.2048 (value: 0.0010, weighted value: 0.0505, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9357
Epoch 4/10, Train Loss: 0.2006 (value: 0.0010, weighted value: 0.0480, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9359
Epoch 5/10, Train Loss: 0.1977 (value: 0.0009, weighted value: 0.0461, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9362
Epoch 6/10, Train Loss: 0.1962 (value: 0.0009, weighted value: 0.0452, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9364
Epoch 7/10, Train Loss: 0.1929 (value: 0.0008, weighted value: 0.0424, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9365
Epoch 8/10, Train Loss: 0.1916 (value: 0.0008, weighted value: 0.0416, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9368
Epoch 9/10, Train Loss: 0.1908 (value: 0.0008, weighted value: 0.0407, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9370
Epoch 10/10, Train Loss: 0.1886 (value: 0.0008, weighted value: 0.0394, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9372
..training done in 66.87 seconds
..evaluation done in 15.64 seconds
Old network+MCTS average reward: 0.69, min: 0.10, max: 1.35, stdev: 0.22
New network+MCTS average reward: 0.69, min: 0.16, max: 1.39, stdev: 0.22
Old bare network average reward: 0.64, min: 0.04, max: 1.27, stdev: 0.22
New bare network average reward: 0.65, min: 0.08, max: 1.29, stdev: 0.22
External policy "random" average reward: 0.24, min: -0.22, max: 0.85, stdev: 0.21
External policy "individual greedy" average reward: 0.53, min: 0.05, max: 1.34, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: 0.13, max: 1.25, stdev: 0.21
New network won 84 and tied 143 out of 300 games (51.83% wins where ties are half wins)
Keeping the new network

Training iteration 294 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.47 seconds
Training examples lengths: [64892, 64786, 64517, 64820, 64603, 64803, 65049, 64817, 64942, 64920]
Total value: 464720.29
Training on 648149 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2157 (value: 0.0011, weighted value: 0.0552, policy: 0.1605, weighted policy: 0.1605), Train Mean Max: 0.9354
Epoch 2/10, Train Loss: 0.2052 (value: 0.0010, weighted value: 0.0493, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9362
Epoch 3/10, Train Loss: 0.1998 (value: 0.0010, weighted value: 0.0475, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9366
Epoch 4/10, Train Loss: 0.1967 (value: 0.0009, weighted value: 0.0450, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9366
Epoch 5/10, Train Loss: 0.1936 (value: 0.0009, weighted value: 0.0433, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1909 (value: 0.0008, weighted value: 0.0411, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1911 (value: 0.0008, weighted value: 0.0410, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9371
Epoch 8/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0389, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1873 (value: 0.0008, weighted value: 0.0391, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1860 (value: 0.0007, weighted value: 0.0373, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9377
..training done in 66.50 seconds
..evaluation done in 15.33 seconds
Old network+MCTS average reward: 0.69, min: -0.03, max: 1.39, stdev: 0.25
New network+MCTS average reward: 0.69, min: -0.03, max: 1.41, stdev: 0.25
Old bare network average reward: 0.65, min: 0.00, max: 1.37, stdev: 0.25
New bare network average reward: 0.64, min: -0.14, max: 1.41, stdev: 0.26
External policy "random" average reward: 0.26, min: -0.30, max: 0.84, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: 0.00, max: 1.19, stdev: 0.24
External policy "total greedy" average reward: 0.66, min: 0.13, max: 1.27, stdev: 0.23
New network won 76 and tied 150 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 295 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.37 seconds
Training examples lengths: [64786, 64517, 64820, 64603, 64803, 65049, 64817, 64942, 64920, 65115]
Total value: 464741.12
Training on 648372 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2139 (value: 0.0011, weighted value: 0.0533, policy: 0.1606, weighted policy: 0.1606), Train Mean Max: 0.9357
Epoch 2/10, Train Loss: 0.2013 (value: 0.0009, weighted value: 0.0471, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9368
Epoch 3/10, Train Loss: 0.1966 (value: 0.0009, weighted value: 0.0453, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9370
Epoch 4/10, Train Loss: 0.1944 (value: 0.0009, weighted value: 0.0435, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9370
Epoch 5/10, Train Loss: 0.1909 (value: 0.0008, weighted value: 0.0412, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9374
Epoch 6/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0400, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9374
Epoch 7/10, Train Loss: 0.1891 (value: 0.0008, weighted value: 0.0405, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9377
Epoch 8/10, Train Loss: 0.1866 (value: 0.0008, weighted value: 0.0380, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9377
Epoch 9/10, Train Loss: 0.1844 (value: 0.0007, weighted value: 0.0364, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9380
Epoch 10/10, Train Loss: 0.1834 (value: 0.0007, weighted value: 0.0357, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9381
..training done in 71.22 seconds
..evaluation done in 15.05 seconds
Old network+MCTS average reward: 0.71, min: 0.12, max: 1.44, stdev: 0.22
New network+MCTS average reward: 0.71, min: 0.13, max: 1.45, stdev: 0.21
Old bare network average reward: 0.67, min: 0.03, max: 1.45, stdev: 0.22
New bare network average reward: 0.67, min: 0.06, max: 1.45, stdev: 0.22
External policy "random" average reward: 0.27, min: -0.32, max: 0.84, stdev: 0.21
External policy "individual greedy" average reward: 0.55, min: -0.05, max: 1.38, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.03, max: 1.40, stdev: 0.21
New network won 72 and tied 168 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network

Training iteration 296 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.21 seconds
Training examples lengths: [64517, 64820, 64603, 64803, 65049, 64817, 64942, 64920, 65115, 64496]
Total value: 464526.38
Training on 648082 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2108 (value: 0.0010, weighted value: 0.0523, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9362
Epoch 2/10, Train Loss: 0.2009 (value: 0.0009, weighted value: 0.0471, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9370
Epoch 3/10, Train Loss: 0.1952 (value: 0.0009, weighted value: 0.0437, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9372
Epoch 4/10, Train Loss: 0.1934 (value: 0.0009, weighted value: 0.0432, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9375
Epoch 5/10, Train Loss: 0.1897 (value: 0.0008, weighted value: 0.0406, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1874 (value: 0.0008, weighted value: 0.0388, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9378
Epoch 7/10, Train Loss: 0.1866 (value: 0.0008, weighted value: 0.0385, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1859 (value: 0.0007, weighted value: 0.0375, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9380
Epoch 9/10, Train Loss: 0.1827 (value: 0.0007, weighted value: 0.0354, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9383
Epoch 10/10, Train Loss: 0.1831 (value: 0.0007, weighted value: 0.0358, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9384
..training done in 69.11 seconds
..evaluation done in 15.74 seconds
Old network+MCTS average reward: 0.71, min: 0.10, max: 1.25, stdev: 0.22
New network+MCTS average reward: 0.71, min: 0.08, max: 1.28, stdev: 0.22
Old bare network average reward: 0.66, min: 0.06, max: 1.22, stdev: 0.23
New bare network average reward: 0.66, min: 0.08, max: 1.20, stdev: 0.22
External policy "random" average reward: 0.25, min: -0.39, max: 0.89, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: -0.01, max: 1.15, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.05, max: 1.25, stdev: 0.21
New network won 73 and tied 154 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 297 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.05 seconds
Training examples lengths: [64820, 64603, 64803, 65049, 64817, 64942, 64920, 65115, 64496, 64595]
Total value: 464758.38
Training on 648160 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2082 (value: 0.0010, weighted value: 0.0500, policy: 0.1582, weighted policy: 0.1582), Train Mean Max: 0.9365
Epoch 2/10, Train Loss: 0.1982 (value: 0.0009, weighted value: 0.0453, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9374
Epoch 3/10, Train Loss: 0.1950 (value: 0.0009, weighted value: 0.0437, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9375
Epoch 4/10, Train Loss: 0.1907 (value: 0.0008, weighted value: 0.0411, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9377
Epoch 5/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0401, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9379
Epoch 6/10, Train Loss: 0.1849 (value: 0.0007, weighted value: 0.0373, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9381
Epoch 7/10, Train Loss: 0.1846 (value: 0.0007, weighted value: 0.0369, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9382
Epoch 8/10, Train Loss: 0.1841 (value: 0.0007, weighted value: 0.0366, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1827 (value: 0.0007, weighted value: 0.0358, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9385
Epoch 10/10, Train Loss: 0.1819 (value: 0.0007, weighted value: 0.0345, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9386
..training done in 71.84 seconds
..evaluation done in 15.39 seconds
Old network+MCTS average reward: 0.70, min: 0.04, max: 1.45, stdev: 0.24
New network+MCTS average reward: 0.70, min: 0.07, max: 1.45, stdev: 0.24
Old bare network average reward: 0.66, min: 0.04, max: 1.45, stdev: 0.24
New bare network average reward: 0.66, min: 0.04, max: 1.45, stdev: 0.25
External policy "random" average reward: 0.25, min: -0.31, max: 0.93, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: -0.13, max: 1.30, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.09, max: 1.25, stdev: 0.22
New network won 70 and tied 164 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 298 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.48 seconds
Training examples lengths: [64603, 64803, 65049, 64817, 64942, 64920, 65115, 64496, 64595, 64852]
Total value: 465507.87
Training on 648192 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2079 (value: 0.0010, weighted value: 0.0505, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9369
Epoch 2/10, Train Loss: 0.1988 (value: 0.0009, weighted value: 0.0453, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9375
Epoch 3/10, Train Loss: 0.1925 (value: 0.0008, weighted value: 0.0421, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9380
Epoch 4/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0407, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9383
Epoch 5/10, Train Loss: 0.1868 (value: 0.0008, weighted value: 0.0394, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9384
Epoch 6/10, Train Loss: 0.1865 (value: 0.0008, weighted value: 0.0384, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9384
Epoch 7/10, Train Loss: 0.1833 (value: 0.0007, weighted value: 0.0367, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9387
Epoch 8/10, Train Loss: 0.1832 (value: 0.0007, weighted value: 0.0358, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9386
Epoch 9/10, Train Loss: 0.1816 (value: 0.0007, weighted value: 0.0353, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9390
Epoch 10/10, Train Loss: 0.1802 (value: 0.0007, weighted value: 0.0342, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9391
..training done in 67.75 seconds
..evaluation done in 15.26 seconds
Old network+MCTS average reward: 0.73, min: 0.15, max: 1.44, stdev: 0.23
New network+MCTS average reward: 0.73, min: 0.15, max: 1.44, stdev: 0.23
Old bare network average reward: 0.69, min: 0.15, max: 1.39, stdev: 0.23
New bare network average reward: 0.69, min: 0.15, max: 1.39, stdev: 0.23
External policy "random" average reward: 0.28, min: -0.30, max: 1.00, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: -0.01, max: 1.30, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: 0.17, max: 1.41, stdev: 0.22
New network won 53 and tied 168 out of 300 games (45.67% wins where ties are half wins)
Reverting to the old network

Training iteration 299 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.75 seconds
Training examples lengths: [64803, 65049, 64817, 64942, 64920, 65115, 64496, 64595, 64852, 64900]
Total value: 466718.56
Training on 648489 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2323 (value: 0.0013, weighted value: 0.0661, policy: 0.1662, weighted policy: 0.1662), Train Mean Max: 0.9356
Epoch 2/10, Train Loss: 0.2168 (value: 0.0012, weighted value: 0.0576, policy: 0.1592, weighted policy: 0.1592), Train Mean Max: 0.9365
Epoch 3/10, Train Loss: 0.2057 (value: 0.0010, weighted value: 0.0523, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9371
Epoch 4/10, Train Loss: 0.1997 (value: 0.0010, weighted value: 0.0489, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9373
Epoch 5/10, Train Loss: 0.1963 (value: 0.0009, weighted value: 0.0466, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9374
Epoch 6/10, Train Loss: 0.1934 (value: 0.0009, weighted value: 0.0444, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9376
Epoch 7/10, Train Loss: 0.1913 (value: 0.0009, weighted value: 0.0433, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1895 (value: 0.0008, weighted value: 0.0412, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1876 (value: 0.0008, weighted value: 0.0400, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9382
Epoch 10/10, Train Loss: 0.1879 (value: 0.0008, weighted value: 0.0407, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9383
..training done in 70.25 seconds
..evaluation done in 15.68 seconds
Old network+MCTS average reward: 0.74, min: 0.13, max: 1.43, stdev: 0.24
New network+MCTS average reward: 0.74, min: 0.03, max: 1.47, stdev: 0.24
Old bare network average reward: 0.70, min: 0.01, max: 1.44, stdev: 0.24
New bare network average reward: 0.70, min: 0.08, max: 1.47, stdev: 0.24
External policy "random" average reward: 0.28, min: -0.33, max: 0.99, stdev: 0.24
External policy "individual greedy" average reward: 0.56, min: -0.09, max: 1.26, stdev: 0.23
External policy "total greedy" average reward: 0.68, min: 0.09, max: 1.29, stdev: 0.23
New network won 63 and tied 167 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 300 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.10 seconds
Training examples lengths: [65049, 64817, 64942, 64920, 65115, 64496, 64595, 64852, 64900, 64511]
Total value: 466347.04
Training on 648197 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2579 (value: 0.0016, weighted value: 0.0823, policy: 0.1756, weighted policy: 0.1756), Train Mean Max: 0.9338
Epoch 2/10, Train Loss: 0.2312 (value: 0.0014, weighted value: 0.0680, policy: 0.1632, weighted policy: 0.1632), Train Mean Max: 0.9352
Epoch 3/10, Train Loss: 0.2187 (value: 0.0012, weighted value: 0.0617, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9358
Epoch 4/10, Train Loss: 0.2119 (value: 0.0012, weighted value: 0.0579, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9363
Epoch 5/10, Train Loss: 0.2063 (value: 0.0011, weighted value: 0.0538, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9362
Epoch 6/10, Train Loss: 0.2015 (value: 0.0010, weighted value: 0.0500, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9367
Epoch 7/10, Train Loss: 0.2001 (value: 0.0010, weighted value: 0.0498, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9370
Epoch 8/10, Train Loss: 0.1950 (value: 0.0009, weighted value: 0.0455, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9372
Epoch 9/10, Train Loss: 0.1949 (value: 0.0009, weighted value: 0.0459, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9375
Epoch 10/10, Train Loss: 0.1918 (value: 0.0009, weighted value: 0.0433, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9377
..training done in 67.32 seconds
..evaluation done in 15.76 seconds
Old network+MCTS average reward: 0.71, min: 0.01, max: 1.42, stdev: 0.25
New network+MCTS average reward: 0.71, min: 0.12, max: 1.47, stdev: 0.25
Old bare network average reward: 0.66, min: 0.00, max: 1.42, stdev: 0.26
New bare network average reward: 0.66, min: -0.08, max: 1.47, stdev: 0.26
External policy "random" average reward: 0.27, min: -0.31, max: 1.03, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: 0.01, max: 1.19, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.11, max: 1.33, stdev: 0.23
New network won 88 and tied 147 out of 300 games (53.83% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1757697740_iter_300

Training iteration 301 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.29 seconds
Training examples lengths: [64817, 64942, 64920, 65115, 64496, 64595, 64852, 64900, 64511, 64903]
Total value: 466416.19
Training on 648051 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2163 (value: 0.0012, weighted value: 0.0578, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9359
Epoch 2/10, Train Loss: 0.2074 (value: 0.0011, weighted value: 0.0525, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9363
Epoch 3/10, Train Loss: 0.2016 (value: 0.0010, weighted value: 0.0493, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9367
Epoch 4/10, Train Loss: 0.1986 (value: 0.0010, weighted value: 0.0476, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9369
Epoch 5/10, Train Loss: 0.1959 (value: 0.0009, weighted value: 0.0449, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9371
Epoch 6/10, Train Loss: 0.1924 (value: 0.0009, weighted value: 0.0430, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9375
Epoch 7/10, Train Loss: 0.1904 (value: 0.0008, weighted value: 0.0424, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9376
Epoch 8/10, Train Loss: 0.1897 (value: 0.0008, weighted value: 0.0408, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9377
Epoch 9/10, Train Loss: 0.1876 (value: 0.0008, weighted value: 0.0392, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1875 (value: 0.0008, weighted value: 0.0393, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9380
..training done in 72.12 seconds
..evaluation done in 15.26 seconds
Old network+MCTS average reward: 0.69, min: 0.09, max: 1.33, stdev: 0.21
New network+MCTS average reward: 0.69, min: 0.08, max: 1.33, stdev: 0.21
Old bare network average reward: 0.65, min: 0.05, max: 1.33, stdev: 0.22
New bare network average reward: 0.65, min: 0.08, max: 1.29, stdev: 0.22
External policy "random" average reward: 0.25, min: -0.33, max: 0.82, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: -0.02, max: 1.27, stdev: 0.21
External policy "total greedy" average reward: 0.64, min: 0.10, max: 1.28, stdev: 0.21
New network won 70 and tied 159 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 302 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.35 seconds
Training examples lengths: [64942, 64920, 65115, 64496, 64595, 64852, 64900, 64511, 64903, 64806]
Total value: 466205.97
Training on 648040 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2429 (value: 0.0015, weighted value: 0.0744, policy: 0.1684, weighted policy: 0.1684), Train Mean Max: 0.9340
Epoch 2/10, Train Loss: 0.2254 (value: 0.0013, weighted value: 0.0635, policy: 0.1619, weighted policy: 0.1619), Train Mean Max: 0.9350
Epoch 3/10, Train Loss: 0.2162 (value: 0.0012, weighted value: 0.0594, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9354
Epoch 4/10, Train Loss: 0.2082 (value: 0.0011, weighted value: 0.0542, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9357
Epoch 5/10, Train Loss: 0.2070 (value: 0.0011, weighted value: 0.0533, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9358
Epoch 6/10, Train Loss: 0.2018 (value: 0.0010, weighted value: 0.0496, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9361
Epoch 7/10, Train Loss: 0.1998 (value: 0.0010, weighted value: 0.0479, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9363
Epoch 8/10, Train Loss: 0.1970 (value: 0.0009, weighted value: 0.0457, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9364
Epoch 9/10, Train Loss: 0.1937 (value: 0.0009, weighted value: 0.0443, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9370
Epoch 10/10, Train Loss: 0.1934 (value: 0.0009, weighted value: 0.0429, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9368
..training done in 67.59 seconds
..evaluation done in 15.60 seconds
Old network+MCTS average reward: 0.71, min: 0.05, max: 1.81, stdev: 0.23
New network+MCTS average reward: 0.71, min: 0.05, max: 1.69, stdev: 0.22
Old bare network average reward: 0.67, min: 0.02, max: 1.69, stdev: 0.23
New bare network average reward: 0.67, min: 0.02, max: 1.69, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.34, max: 0.83, stdev: 0.21
External policy "individual greedy" average reward: 0.56, min: 0.03, max: 1.21, stdev: 0.22
External policy "total greedy" average reward: 0.67, min: 0.12, max: 1.38, stdev: 0.21
New network won 69 and tied 153 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 303 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.10 seconds
Training examples lengths: [64920, 65115, 64496, 64595, 64852, 64900, 64511, 64903, 64806, 64796]
Total value: 465543.20
Training on 647894 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2645 (value: 0.0018, weighted value: 0.0878, policy: 0.1767, weighted policy: 0.1767), Train Mean Max: 0.9324
Epoch 2/10, Train Loss: 0.2428 (value: 0.0015, weighted value: 0.0754, policy: 0.1673, weighted policy: 0.1673), Train Mean Max: 0.9335
Epoch 3/10, Train Loss: 0.2287 (value: 0.0014, weighted value: 0.0683, policy: 0.1604, weighted policy: 0.1604), Train Mean Max: 0.9341
Epoch 4/10, Train Loss: 0.2193 (value: 0.0012, weighted value: 0.0616, policy: 0.1577, weighted policy: 0.1577), Train Mean Max: 0.9343
Epoch 5/10, Train Loss: 0.2148 (value: 0.0012, weighted value: 0.0588, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9347
Epoch 6/10, Train Loss: 0.2102 (value: 0.0011, weighted value: 0.0558, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9350
Epoch 7/10, Train Loss: 0.2084 (value: 0.0011, weighted value: 0.0538, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9352
Epoch 8/10, Train Loss: 0.2048 (value: 0.0010, weighted value: 0.0514, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9353
Epoch 9/10, Train Loss: 0.1999 (value: 0.0010, weighted value: 0.0477, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9357
Epoch 10/10, Train Loss: 0.1996 (value: 0.0010, weighted value: 0.0477, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9359
..training done in 70.73 seconds
..evaluation done in 15.23 seconds
Old network+MCTS average reward: 0.72, min: 0.13, max: 1.39, stdev: 0.22
New network+MCTS average reward: 0.72, min: 0.11, max: 1.36, stdev: 0.22
Old bare network average reward: 0.68, min: 0.00, max: 1.37, stdev: 0.23
New bare network average reward: 0.68, min: 0.00, max: 1.36, stdev: 0.22
External policy "random" average reward: 0.26, min: -0.31, max: 0.95, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: -0.04, max: 1.17, stdev: 0.21
External policy "total greedy" average reward: 0.67, min: 0.10, max: 1.31, stdev: 0.20
New network won 78 and tied 160 out of 300 games (52.67% wins where ties are half wins)
Keeping the new network

Training iteration 304 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.09 seconds
Training examples lengths: [65115, 64496, 64595, 64852, 64900, 64511, 64903, 64806, 64796, 64787]
Total value: 465596.19
Training on 647761 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2218 (value: 0.0012, weighted value: 0.0617, policy: 0.1601, weighted policy: 0.1601), Train Mean Max: 0.9344
Epoch 2/10, Train Loss: 0.2116 (value: 0.0011, weighted value: 0.0544, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9351
Epoch 3/10, Train Loss: 0.2058 (value: 0.0010, weighted value: 0.0516, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9355
Epoch 4/10, Train Loss: 0.2027 (value: 0.0010, weighted value: 0.0506, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9357
Epoch 5/10, Train Loss: 0.2002 (value: 0.0010, weighted value: 0.0482, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9360
Epoch 6/10, Train Loss: 0.1976 (value: 0.0009, weighted value: 0.0452, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9361
Epoch 7/10, Train Loss: 0.1942 (value: 0.0009, weighted value: 0.0439, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9366
Epoch 8/10, Train Loss: 0.1938 (value: 0.0009, weighted value: 0.0433, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9366
Epoch 9/10, Train Loss: 0.1922 (value: 0.0008, weighted value: 0.0419, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9367
Epoch 10/10, Train Loss: 0.1897 (value: 0.0008, weighted value: 0.0403, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9370
..training done in 67.51 seconds
..evaluation done in 15.83 seconds
Old network+MCTS average reward: 0.72, min: 0.16, max: 1.45, stdev: 0.23
New network+MCTS average reward: 0.72, min: 0.21, max: 1.45, stdev: 0.23
Old bare network average reward: 0.68, min: 0.09, max: 1.45, stdev: 0.23
New bare network average reward: 0.68, min: 0.12, max: 1.45, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.37, max: 0.99, stdev: 0.21
External policy "individual greedy" average reward: 0.55, min: 0.07, max: 1.28, stdev: 0.21
External policy "total greedy" average reward: 0.66, min: 0.12, max: 1.32, stdev: 0.21
New network won 83 and tied 156 out of 300 games (53.67% wins where ties are half wins)
Keeping the new network

Training iteration 305 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.42 seconds
Training examples lengths: [64496, 64595, 64852, 64900, 64511, 64903, 64806, 64796, 64787, 64939]
Total value: 465803.11
Training on 647585 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2147 (value: 0.0011, weighted value: 0.0554, policy: 0.1592, weighted policy: 0.1592), Train Mean Max: 0.9355
Epoch 2/10, Train Loss: 0.2062 (value: 0.0010, weighted value: 0.0504, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9360
Epoch 3/10, Train Loss: 0.2008 (value: 0.0010, weighted value: 0.0478, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9365
Epoch 4/10, Train Loss: 0.1957 (value: 0.0009, weighted value: 0.0452, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9368
Epoch 5/10, Train Loss: 0.1935 (value: 0.0009, weighted value: 0.0430, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1926 (value: 0.0009, weighted value: 0.0428, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9370
Epoch 7/10, Train Loss: 0.1910 (value: 0.0008, weighted value: 0.0410, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9372
Epoch 8/10, Train Loss: 0.1879 (value: 0.0008, weighted value: 0.0394, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9376
Epoch 9/10, Train Loss: 0.1871 (value: 0.0008, weighted value: 0.0386, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9375
Epoch 10/10, Train Loss: 0.1878 (value: 0.0008, weighted value: 0.0388, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9376
..training done in 67.91 seconds
..evaluation done in 15.63 seconds
Old network+MCTS average reward: 0.72, min: 0.19, max: 1.39, stdev: 0.22
New network+MCTS average reward: 0.73, min: 0.23, max: 1.46, stdev: 0.22
Old bare network average reward: 0.69, min: 0.19, max: 1.34, stdev: 0.22
New bare network average reward: 0.68, min: 0.17, max: 1.30, stdev: 0.22
External policy "random" average reward: 0.27, min: -0.24, max: 0.83, stdev: 0.20
External policy "individual greedy" average reward: 0.55, min: 0.02, max: 1.13, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.04, max: 1.40, stdev: 0.21
New network won 85 and tied 147 out of 300 games (52.83% wins where ties are half wins)
Keeping the new network

Training iteration 306 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.83 seconds
Training examples lengths: [64595, 64852, 64900, 64511, 64903, 64806, 64796, 64787, 64939, 64779]
Total value: 466619.30
Training on 647868 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2108 (value: 0.0010, weighted value: 0.0514, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9361
Epoch 2/10, Train Loss: 0.2028 (value: 0.0010, weighted value: 0.0479, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9369
Epoch 3/10, Train Loss: 0.1957 (value: 0.0009, weighted value: 0.0443, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9371
Epoch 4/10, Train Loss: 0.1926 (value: 0.0009, weighted value: 0.0430, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9373
Epoch 5/10, Train Loss: 0.1901 (value: 0.0008, weighted value: 0.0409, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9375
Epoch 6/10, Train Loss: 0.1894 (value: 0.0008, weighted value: 0.0404, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9376
Epoch 7/10, Train Loss: 0.1877 (value: 0.0008, weighted value: 0.0393, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9377
Epoch 8/10, Train Loss: 0.1842 (value: 0.0007, weighted value: 0.0365, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9380
Epoch 9/10, Train Loss: 0.1841 (value: 0.0007, weighted value: 0.0367, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9382
Epoch 10/10, Train Loss: 0.1828 (value: 0.0007, weighted value: 0.0351, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9382
..training done in 72.28 seconds
..evaluation done in 15.82 seconds
Old network+MCTS average reward: 0.70, min: 0.09, max: 1.51, stdev: 0.24
New network+MCTS average reward: 0.69, min: 0.11, max: 1.51, stdev: 0.24
Old bare network average reward: 0.65, min: 0.06, max: 1.51, stdev: 0.25
New bare network average reward: 0.65, min: 0.03, max: 1.51, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.28, max: 0.90, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: -0.12, max: 1.26, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.02, max: 1.35, stdev: 0.24
New network won 60 and tied 156 out of 300 games (46.00% wins where ties are half wins)
Reverting to the old network

Training iteration 307 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.95 seconds
Training examples lengths: [64852, 64900, 64511, 64903, 64806, 64796, 64787, 64939, 64779, 64645]
Total value: 466886.86
Training on 647918 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2362 (value: 0.0013, weighted value: 0.0675, policy: 0.1688, weighted policy: 0.1688), Train Mean Max: 0.9345
Epoch 2/10, Train Loss: 0.2186 (value: 0.0012, weighted value: 0.0580, policy: 0.1606, weighted policy: 0.1606), Train Mean Max: 0.9357
Epoch 3/10, Train Loss: 0.2107 (value: 0.0011, weighted value: 0.0548, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9358
Epoch 4/10, Train Loss: 0.2039 (value: 0.0010, weighted value: 0.0509, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9362
Epoch 5/10, Train Loss: 0.1996 (value: 0.0010, weighted value: 0.0480, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9365
Epoch 6/10, Train Loss: 0.1972 (value: 0.0009, weighted value: 0.0465, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9367
Epoch 7/10, Train Loss: 0.1945 (value: 0.0009, weighted value: 0.0450, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9370
Epoch 8/10, Train Loss: 0.1927 (value: 0.0009, weighted value: 0.0426, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9369
Epoch 9/10, Train Loss: 0.1898 (value: 0.0008, weighted value: 0.0414, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9373
Epoch 10/10, Train Loss: 0.1886 (value: 0.0008, weighted value: 0.0395, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9374
..training done in 71.85 seconds
..evaluation done in 15.27 seconds
Old network+MCTS average reward: 0.70, min: -0.06, max: 1.47, stdev: 0.23
New network+MCTS average reward: 0.70, min: -0.06, max: 1.36, stdev: 0.23
Old bare network average reward: 0.65, min: -0.06, max: 1.34, stdev: 0.24
New bare network average reward: 0.66, min: 0.06, max: 1.31, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.30, max: 0.93, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: 0.02, max: 1.24, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: -0.02, max: 1.33, stdev: 0.22
New network won 72 and tied 154 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 308 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.90 seconds
Training examples lengths: [64900, 64511, 64903, 64806, 64796, 64787, 64939, 64779, 64645, 64796]
Total value: 466821.02
Training on 647862 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2590 (value: 0.0016, weighted value: 0.0822, policy: 0.1768, weighted policy: 0.1768), Train Mean Max: 0.9329
Epoch 2/10, Train Loss: 0.2348 (value: 0.0014, weighted value: 0.0691, policy: 0.1657, weighted policy: 0.1657), Train Mean Max: 0.9342
Epoch 3/10, Train Loss: 0.2230 (value: 0.0013, weighted value: 0.0630, policy: 0.1599, weighted policy: 0.1599), Train Mean Max: 0.9348
Epoch 4/10, Train Loss: 0.2143 (value: 0.0012, weighted value: 0.0576, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9352
Epoch 5/10, Train Loss: 0.2103 (value: 0.0011, weighted value: 0.0550, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9353
Epoch 6/10, Train Loss: 0.2051 (value: 0.0010, weighted value: 0.0515, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9358
Epoch 7/10, Train Loss: 0.2028 (value: 0.0010, weighted value: 0.0504, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9358
Epoch 8/10, Train Loss: 0.1988 (value: 0.0009, weighted value: 0.0470, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9361
Epoch 9/10, Train Loss: 0.1983 (value: 0.0009, weighted value: 0.0467, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9362
Epoch 10/10, Train Loss: 0.1945 (value: 0.0009, weighted value: 0.0433, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9365
..training done in 66.82 seconds
..evaluation done in 15.24 seconds
Old network+MCTS average reward: 0.72, min: 0.19, max: 1.49, stdev: 0.23
New network+MCTS average reward: 0.71, min: 0.13, max: 1.49, stdev: 0.23
Old bare network average reward: 0.67, min: 0.06, max: 1.39, stdev: 0.23
New bare network average reward: 0.68, min: 0.04, max: 1.35, stdev: 0.23
External policy "random" average reward: 0.24, min: -0.31, max: 0.92, stdev: 0.21
External policy "individual greedy" average reward: 0.53, min: -0.05, max: 1.27, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.12, max: 1.43, stdev: 0.22
New network won 72 and tied 150 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 309 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.80 seconds
Training examples lengths: [64511, 64903, 64806, 64796, 64787, 64939, 64779, 64645, 64796, 64808]
Total value: 466539.49
Training on 647770 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2814 (value: 0.0019, weighted value: 0.0956, policy: 0.1858, weighted policy: 0.1858), Train Mean Max: 0.9316
Epoch 2/10, Train Loss: 0.2508 (value: 0.0016, weighted value: 0.0806, policy: 0.1702, weighted policy: 0.1702), Train Mean Max: 0.9330
Epoch 3/10, Train Loss: 0.2344 (value: 0.0014, weighted value: 0.0712, policy: 0.1632, weighted policy: 0.1632), Train Mean Max: 0.9335
Epoch 4/10, Train Loss: 0.2246 (value: 0.0013, weighted value: 0.0648, policy: 0.1598, weighted policy: 0.1598), Train Mean Max: 0.9339
Epoch 5/10, Train Loss: 0.2192 (value: 0.0012, weighted value: 0.0622, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9343
Epoch 6/10, Train Loss: 0.2136 (value: 0.0012, weighted value: 0.0583, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9344
Epoch 7/10, Train Loss: 0.2091 (value: 0.0011, weighted value: 0.0541, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9346
Epoch 8/10, Train Loss: 0.2072 (value: 0.0011, weighted value: 0.0529, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9350
Epoch 9/10, Train Loss: 0.2030 (value: 0.0010, weighted value: 0.0496, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9355
Epoch 10/10, Train Loss: 0.2000 (value: 0.0009, weighted value: 0.0469, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9355
..training done in 67.60 seconds
..evaluation done in 15.38 seconds
Old network+MCTS average reward: 0.73, min: -0.04, max: 1.46, stdev: 0.23
New network+MCTS average reward: 0.74, min: -0.04, max: 1.46, stdev: 0.23
Old bare network average reward: 0.69, min: -0.06, max: 1.44, stdev: 0.23
New bare network average reward: 0.69, min: -0.04, max: 1.44, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.55, max: 0.93, stdev: 0.22
External policy "individual greedy" average reward: 0.56, min: -0.16, max: 1.23, stdev: 0.23
External policy "total greedy" average reward: 0.69, min: 0.03, max: 1.36, stdev: 0.22
New network won 84 and tied 151 out of 300 games (53.17% wins where ties are half wins)
Keeping the new network

Training iteration 310 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.76 seconds
Training examples lengths: [64903, 64806, 64796, 64787, 64939, 64779, 64645, 64796, 64808, 64669]
Total value: 467392.31
Training on 647928 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2238 (value: 0.0012, weighted value: 0.0616, policy: 0.1622, weighted policy: 0.1622), Train Mean Max: 0.9341
Epoch 2/10, Train Loss: 0.2135 (value: 0.0011, weighted value: 0.0551, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9348
Epoch 3/10, Train Loss: 0.2079 (value: 0.0010, weighted value: 0.0522, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9352
Epoch 4/10, Train Loss: 0.2028 (value: 0.0010, weighted value: 0.0490, policy: 0.1539, weighted policy: 0.1539), Train Mean Max: 0.9355
Epoch 5/10, Train Loss: 0.2015 (value: 0.0010, weighted value: 0.0483, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9356
Epoch 6/10, Train Loss: 0.1974 (value: 0.0009, weighted value: 0.0450, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9359
Epoch 7/10, Train Loss: 0.1955 (value: 0.0009, weighted value: 0.0449, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9361
Epoch 8/10, Train Loss: 0.1952 (value: 0.0009, weighted value: 0.0433, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9362
Epoch 9/10, Train Loss: 0.1918 (value: 0.0008, weighted value: 0.0409, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9365
Epoch 10/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0398, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9368
..training done in 67.00 seconds
..evaluation done in 15.66 seconds
Old network+MCTS average reward: 0.72, min: 0.01, max: 1.35, stdev: 0.24
New network+MCTS average reward: 0.72, min: 0.01, max: 1.31, stdev: 0.24
Old bare network average reward: 0.67, min: -0.02, max: 1.30, stdev: 0.25
New bare network average reward: 0.68, min: 0.02, max: 1.27, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.38, max: 0.95, stdev: 0.22
External policy "individual greedy" average reward: 0.56, min: -0.10, max: 1.26, stdev: 0.24
External policy "total greedy" average reward: 0.66, min: -0.04, max: 1.29, stdev: 0.24
New network won 70 and tied 161 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 311 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.38 seconds
Training examples lengths: [64806, 64796, 64787, 64939, 64779, 64645, 64796, 64808, 64669, 64864]
Total value: 467188.94
Training on 647889 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2177 (value: 0.0012, weighted value: 0.0581, policy: 0.1596, weighted policy: 0.1596), Train Mean Max: 0.9351
Epoch 2/10, Train Loss: 0.2045 (value: 0.0010, weighted value: 0.0491, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9360
Epoch 3/10, Train Loss: 0.2000 (value: 0.0009, weighted value: 0.0466, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9362
Epoch 4/10, Train Loss: 0.1960 (value: 0.0009, weighted value: 0.0448, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9368
Epoch 5/10, Train Loss: 0.1935 (value: 0.0009, weighted value: 0.0430, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9368
Epoch 6/10, Train Loss: 0.1935 (value: 0.0009, weighted value: 0.0431, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9369
Epoch 7/10, Train Loss: 0.1914 (value: 0.0008, weighted value: 0.0415, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9370
Epoch 8/10, Train Loss: 0.1877 (value: 0.0008, weighted value: 0.0385, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1869 (value: 0.0008, weighted value: 0.0379, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9375
Epoch 10/10, Train Loss: 0.1856 (value: 0.0008, weighted value: 0.0379, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9378
..training done in 66.09 seconds
..evaluation done in 15.60 seconds
Old network+MCTS average reward: 0.71, min: 0.12, max: 1.44, stdev: 0.24
New network+MCTS average reward: 0.71, min: 0.08, max: 1.44, stdev: 0.24
Old bare network average reward: 0.66, min: -0.04, max: 1.36, stdev: 0.24
New bare network average reward: 0.66, min: -0.04, max: 1.38, stdev: 0.25
External policy "random" average reward: 0.25, min: -0.37, max: 0.94, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.09, max: 1.20, stdev: 0.24
External policy "total greedy" average reward: 0.65, min: 0.03, max: 1.40, stdev: 0.24
New network won 69 and tied 178 out of 300 games (52.67% wins where ties are half wins)
Keeping the new network

Training iteration 312 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.61 seconds
Training examples lengths: [64796, 64787, 64939, 64779, 64645, 64796, 64808, 64669, 64864, 64754]
Total value: 466667.38
Training on 647837 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2127 (value: 0.0011, weighted value: 0.0533, policy: 0.1594, weighted policy: 0.1594), Train Mean Max: 0.9360
Epoch 2/10, Train Loss: 0.2019 (value: 0.0009, weighted value: 0.0472, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9369
Epoch 3/10, Train Loss: 0.1968 (value: 0.0009, weighted value: 0.0446, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9371
Epoch 4/10, Train Loss: 0.1948 (value: 0.0009, weighted value: 0.0450, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9373
Epoch 5/10, Train Loss: 0.1899 (value: 0.0008, weighted value: 0.0411, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9375
Epoch 6/10, Train Loss: 0.1891 (value: 0.0008, weighted value: 0.0398, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9375
Epoch 7/10, Train Loss: 0.1869 (value: 0.0008, weighted value: 0.0386, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9377
Epoch 8/10, Train Loss: 0.1862 (value: 0.0008, weighted value: 0.0384, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9379
Epoch 9/10, Train Loss: 0.1840 (value: 0.0007, weighted value: 0.0362, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9381
Epoch 10/10, Train Loss: 0.1842 (value: 0.0007, weighted value: 0.0361, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9382
..training done in 66.59 seconds
..evaluation done in 15.74 seconds
Old network+MCTS average reward: 0.72, min: 0.13, max: 1.47, stdev: 0.25
New network+MCTS average reward: 0.73, min: 0.18, max: 1.48, stdev: 0.25
Old bare network average reward: 0.69, min: 0.11, max: 1.47, stdev: 0.25
New bare network average reward: 0.69, min: 0.11, max: 1.47, stdev: 0.25
External policy "random" average reward: 0.28, min: -0.29, max: 1.09, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: -0.03, max: 1.31, stdev: 0.25
External policy "total greedy" average reward: 0.66, min: 0.03, max: 1.44, stdev: 0.24
New network won 67 and tied 157 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 313 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.29 seconds
Training examples lengths: [64787, 64939, 64779, 64645, 64796, 64808, 64669, 64864, 64754, 64934]
Total value: 467168.88
Training on 647975 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2362 (value: 0.0014, weighted value: 0.0692, policy: 0.1671, weighted policy: 0.1671), Train Mean Max: 0.9346
Epoch 2/10, Train Loss: 0.2201 (value: 0.0012, weighted value: 0.0599, policy: 0.1602, weighted policy: 0.1602), Train Mean Max: 0.9357
Epoch 3/10, Train Loss: 0.2100 (value: 0.0011, weighted value: 0.0549, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9362
Epoch 4/10, Train Loss: 0.2032 (value: 0.0010, weighted value: 0.0510, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9365
Epoch 5/10, Train Loss: 0.1997 (value: 0.0010, weighted value: 0.0484, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9365
Epoch 6/10, Train Loss: 0.1973 (value: 0.0009, weighted value: 0.0465, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9368
Epoch 7/10, Train Loss: 0.1945 (value: 0.0009, weighted value: 0.0447, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9370
Epoch 8/10, Train Loss: 0.1910 (value: 0.0008, weighted value: 0.0420, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1919 (value: 0.0009, weighted value: 0.0431, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9375
Epoch 10/10, Train Loss: 0.1893 (value: 0.0008, weighted value: 0.0407, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9377
..training done in 67.79 seconds
..evaluation done in 15.40 seconds
Old network+MCTS average reward: 0.71, min: 0.13, max: 1.34, stdev: 0.22
New network+MCTS average reward: 0.70, min: 0.12, max: 1.33, stdev: 0.22
Old bare network average reward: 0.66, min: 0.11, max: 1.34, stdev: 0.23
New bare network average reward: 0.66, min: 0.06, max: 1.33, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.29, max: 0.81, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: 0.00, max: 1.15, stdev: 0.21
External policy "total greedy" average reward: 0.66, min: 0.14, max: 1.26, stdev: 0.21
New network won 64 and tied 157 out of 300 games (47.50% wins where ties are half wins)
Reverting to the old network

Training iteration 314 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.87 seconds
Training examples lengths: [64939, 64779, 64645, 64796, 64808, 64669, 64864, 64754, 64934, 64653]
Total value: 467364.57
Training on 647841 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2594 (value: 0.0017, weighted value: 0.0837, policy: 0.1757, weighted policy: 0.1757), Train Mean Max: 0.9330
Epoch 2/10, Train Loss: 0.2349 (value: 0.0014, weighted value: 0.0698, policy: 0.1651, weighted policy: 0.1651), Train Mean Max: 0.9343
Epoch 3/10, Train Loss: 0.2216 (value: 0.0013, weighted value: 0.0630, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9350
Epoch 4/10, Train Loss: 0.2155 (value: 0.0012, weighted value: 0.0595, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9353
Epoch 5/10, Train Loss: 0.2079 (value: 0.0011, weighted value: 0.0543, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9357
Epoch 6/10, Train Loss: 0.2051 (value: 0.0011, weighted value: 0.0527, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9359
Epoch 7/10, Train Loss: 0.2010 (value: 0.0010, weighted value: 0.0493, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9360
Epoch 8/10, Train Loss: 0.1986 (value: 0.0009, weighted value: 0.0475, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9363
Epoch 9/10, Train Loss: 0.1961 (value: 0.0009, weighted value: 0.0454, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9365
Epoch 10/10, Train Loss: 0.1963 (value: 0.0009, weighted value: 0.0453, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9367
..training done in 67.03 seconds
..evaluation done in 15.53 seconds
Old network+MCTS average reward: 0.71, min: 0.02, max: 1.53, stdev: 0.23
New network+MCTS average reward: 0.71, min: 0.01, max: 1.48, stdev: 0.23
Old bare network average reward: 0.67, min: -0.15, max: 1.47, stdev: 0.23
New bare network average reward: 0.68, min: -0.15, max: 1.47, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.32, max: 1.04, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: -0.11, max: 1.21, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.06, max: 1.31, stdev: 0.23
New network won 75 and tied 151 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 315 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.10 seconds
Training examples lengths: [64779, 64645, 64796, 64808, 64669, 64864, 64754, 64934, 64653, 64742]
Total value: 467294.35
Training on 647644 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2184 (value: 0.0012, weighted value: 0.0588, policy: 0.1596, weighted policy: 0.1596), Train Mean Max: 0.9352
Epoch 2/10, Train Loss: 0.2080 (value: 0.0011, weighted value: 0.0533, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9359
Epoch 3/10, Train Loss: 0.2024 (value: 0.0010, weighted value: 0.0494, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9362
Epoch 4/10, Train Loss: 0.1987 (value: 0.0010, weighted value: 0.0478, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9365
Epoch 5/10, Train Loss: 0.1965 (value: 0.0009, weighted value: 0.0457, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9367
Epoch 6/10, Train Loss: 0.1944 (value: 0.0009, weighted value: 0.0437, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9369
Epoch 7/10, Train Loss: 0.1919 (value: 0.0008, weighted value: 0.0424, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9371
Epoch 8/10, Train Loss: 0.1905 (value: 0.0008, weighted value: 0.0412, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1897 (value: 0.0008, weighted value: 0.0410, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9375
Epoch 10/10, Train Loss: 0.1866 (value: 0.0008, weighted value: 0.0392, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9378
..training done in 67.79 seconds
..evaluation done in 15.18 seconds
Old network+MCTS average reward: 0.69, min: 0.04, max: 1.56, stdev: 0.23
New network+MCTS average reward: 0.69, min: 0.06, max: 1.51, stdev: 0.23
Old bare network average reward: 0.65, min: 0.03, max: 1.50, stdev: 0.24
New bare network average reward: 0.65, min: 0.06, max: 1.51, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.51, max: 0.98, stdev: 0.23
External policy "individual greedy" average reward: 0.52, min: -0.10, max: 1.33, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: -0.06, max: 1.38, stdev: 0.22
New network won 78 and tied 159 out of 300 games (52.50% wins where ties are half wins)
Keeping the new network

Training iteration 316 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.39 seconds
Training examples lengths: [64645, 64796, 64808, 64669, 64864, 64754, 64934, 64653, 64742, 65040]
Total value: 467633.28
Training on 647905 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2134 (value: 0.0011, weighted value: 0.0536, policy: 0.1598, weighted policy: 0.1598), Train Mean Max: 0.9358
Epoch 2/10, Train Loss: 0.2045 (value: 0.0010, weighted value: 0.0491, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9366
Epoch 3/10, Train Loss: 0.1990 (value: 0.0009, weighted value: 0.0466, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9367
Epoch 4/10, Train Loss: 0.1941 (value: 0.0009, weighted value: 0.0433, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9371
Epoch 5/10, Train Loss: 0.1922 (value: 0.0008, weighted value: 0.0422, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9373
Epoch 6/10, Train Loss: 0.1901 (value: 0.0008, weighted value: 0.0410, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9375
Epoch 7/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0396, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9375
Epoch 8/10, Train Loss: 0.1870 (value: 0.0008, weighted value: 0.0387, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1865 (value: 0.0008, weighted value: 0.0382, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9380
Epoch 10/10, Train Loss: 0.1838 (value: 0.0007, weighted value: 0.0359, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9380
..training done in 67.21 seconds
..evaluation done in 15.88 seconds
Old network+MCTS average reward: 0.73, min: 0.24, max: 1.60, stdev: 0.23
New network+MCTS average reward: 0.73, min: 0.22, max: 1.51, stdev: 0.24
Old bare network average reward: 0.69, min: 0.18, max: 1.53, stdev: 0.24
New bare network average reward: 0.70, min: 0.18, max: 1.53, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.56, max: 0.94, stdev: 0.23
External policy "individual greedy" average reward: 0.56, min: 0.03, max: 1.45, stdev: 0.24
External policy "total greedy" average reward: 0.66, min: 0.16, max: 1.49, stdev: 0.24
New network won 67 and tied 161 out of 300 games (49.17% wins where ties are half wins)
Reverting to the old network

Training iteration 317 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.24 seconds
Training examples lengths: [64796, 64808, 64669, 64864, 64754, 64934, 64653, 64742, 65040, 64802]
Total value: 467644.62
Training on 648062 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2382 (value: 0.0014, weighted value: 0.0694, policy: 0.1688, weighted policy: 0.1688), Train Mean Max: 0.9343
Epoch 2/10, Train Loss: 0.2204 (value: 0.0012, weighted value: 0.0598, policy: 0.1606, weighted policy: 0.1606), Train Mean Max: 0.9353
Epoch 3/10, Train Loss: 0.2113 (value: 0.0011, weighted value: 0.0549, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9357
Epoch 4/10, Train Loss: 0.2063 (value: 0.0010, weighted value: 0.0521, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9359
Epoch 5/10, Train Loss: 0.2019 (value: 0.0010, weighted value: 0.0493, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9362
Epoch 6/10, Train Loss: 0.1979 (value: 0.0009, weighted value: 0.0464, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9364
Epoch 7/10, Train Loss: 0.1960 (value: 0.0009, weighted value: 0.0452, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9366
Epoch 8/10, Train Loss: 0.1949 (value: 0.0009, weighted value: 0.0441, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9368
Epoch 9/10, Train Loss: 0.1915 (value: 0.0008, weighted value: 0.0414, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9369
Epoch 10/10, Train Loss: 0.1903 (value: 0.0008, weighted value: 0.0407, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9371
..training done in 68.06 seconds
..evaluation done in 16.15 seconds
Old network+MCTS average reward: 0.69, min: 0.05, max: 1.44, stdev: 0.25
New network+MCTS average reward: 0.69, min: 0.05, max: 1.55, stdev: 0.25
Old bare network average reward: 0.64, min: -0.04, max: 1.42, stdev: 0.25
New bare network average reward: 0.65, min: 0.02, max: 1.55, stdev: 0.25
External policy "random" average reward: 0.25, min: -0.42, max: 1.28, stdev: 0.25
External policy "individual greedy" average reward: 0.52, min: -0.02, max: 1.30, stdev: 0.24
External policy "total greedy" average reward: 0.64, min: 0.08, max: 1.37, stdev: 0.23
New network won 74 and tied 162 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 318 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.55 seconds
Training examples lengths: [64808, 64669, 64864, 64754, 64934, 64653, 64742, 65040, 64802, 64708]
Total value: 467588.10
Training on 647974 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2133 (value: 0.0011, weighted value: 0.0546, policy: 0.1587, weighted policy: 0.1587), Train Mean Max: 0.9355
Epoch 2/10, Train Loss: 0.2046 (value: 0.0010, weighted value: 0.0497, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9362
Epoch 3/10, Train Loss: 0.2001 (value: 0.0009, weighted value: 0.0472, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9366
Epoch 4/10, Train Loss: 0.1952 (value: 0.0009, weighted value: 0.0438, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9368
Epoch 5/10, Train Loss: 0.1932 (value: 0.0009, weighted value: 0.0429, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1919 (value: 0.0008, weighted value: 0.0417, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1894 (value: 0.0008, weighted value: 0.0394, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1892 (value: 0.0008, weighted value: 0.0399, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1862 (value: 0.0008, weighted value: 0.0375, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1846 (value: 0.0007, weighted value: 0.0362, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9378
..training done in 71.38 seconds
..evaluation done in 15.70 seconds
Old network+MCTS average reward: 0.70, min: 0.14, max: 1.38, stdev: 0.24
New network+MCTS average reward: 0.70, min: 0.16, max: 1.43, stdev: 0.24
Old bare network average reward: 0.66, min: 0.09, max: 1.36, stdev: 0.24
New bare network average reward: 0.66, min: 0.11, max: 1.36, stdev: 0.24
External policy "random" average reward: 0.24, min: -0.31, max: 1.06, stdev: 0.23
External policy "individual greedy" average reward: 0.51, min: -0.06, max: 1.22, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.07, max: 1.31, stdev: 0.22
New network won 58 and tied 177 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 319 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.39 seconds
Training examples lengths: [64669, 64864, 64754, 64934, 64653, 64742, 65040, 64802, 64708, 64835]
Total value: 467763.51
Training on 648001 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2407 (value: 0.0014, weighted value: 0.0718, policy: 0.1688, weighted policy: 0.1688), Train Mean Max: 0.9338
Epoch 2/10, Train Loss: 0.2209 (value: 0.0012, weighted value: 0.0601, policy: 0.1608, weighted policy: 0.1608), Train Mean Max: 0.9350
Epoch 3/10, Train Loss: 0.2123 (value: 0.0011, weighted value: 0.0561, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9354
Epoch 4/10, Train Loss: 0.2074 (value: 0.0011, weighted value: 0.0531, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9356
Epoch 5/10, Train Loss: 0.2028 (value: 0.0010, weighted value: 0.0500, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9358
Epoch 6/10, Train Loss: 0.1986 (value: 0.0009, weighted value: 0.0472, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9362
Epoch 7/10, Train Loss: 0.1976 (value: 0.0009, weighted value: 0.0455, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9362
Epoch 8/10, Train Loss: 0.1953 (value: 0.0009, weighted value: 0.0443, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9365
Epoch 9/10, Train Loss: 0.1938 (value: 0.0009, weighted value: 0.0441, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9368
Epoch 10/10, Train Loss: 0.1910 (value: 0.0008, weighted value: 0.0411, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9369
..training done in 66.34 seconds
..evaluation done in 15.62 seconds
Old network+MCTS average reward: 0.71, min: 0.17, max: 1.77, stdev: 0.24
New network+MCTS average reward: 0.71, min: 0.17, max: 1.77, stdev: 0.24
Old bare network average reward: 0.67, min: 0.16, max: 1.69, stdev: 0.24
New bare network average reward: 0.67, min: 0.07, max: 1.74, stdev: 0.25
External policy "random" average reward: 0.27, min: -0.27, max: 0.92, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: 0.07, max: 1.31, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.09, max: 1.54, stdev: 0.22
New network won 66 and tied 162 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 320 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.06 seconds
Training examples lengths: [64864, 64754, 64934, 64653, 64742, 65040, 64802, 64708, 64835, 64998]
Total value: 468209.87
Training on 648330 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2621 (value: 0.0017, weighted value: 0.0852, policy: 0.1769, weighted policy: 0.1769), Train Mean Max: 0.9323
Epoch 2/10, Train Loss: 0.2378 (value: 0.0014, weighted value: 0.0719, policy: 0.1659, weighted policy: 0.1659), Train Mean Max: 0.9337
Epoch 3/10, Train Loss: 0.2242 (value: 0.0013, weighted value: 0.0648, policy: 0.1594, weighted policy: 0.1594), Train Mean Max: 0.9343
Epoch 4/10, Train Loss: 0.2178 (value: 0.0012, weighted value: 0.0608, policy: 0.1569, weighted policy: 0.1569), Train Mean Max: 0.9345
Epoch 5/10, Train Loss: 0.2112 (value: 0.0011, weighted value: 0.0561, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9348
Epoch 6/10, Train Loss: 0.2065 (value: 0.0010, weighted value: 0.0524, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9351
Epoch 7/10, Train Loss: 0.2038 (value: 0.0010, weighted value: 0.0504, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9353
Epoch 8/10, Train Loss: 0.2003 (value: 0.0010, weighted value: 0.0484, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9357
Epoch 9/10, Train Loss: 0.2000 (value: 0.0010, weighted value: 0.0480, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9357
Epoch 10/10, Train Loss: 0.1950 (value: 0.0009, weighted value: 0.0438, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9359
..training done in 67.48 seconds
..evaluation done in 15.92 seconds
Old network+MCTS average reward: 0.71, min: 0.07, max: 1.36, stdev: 0.24
New network+MCTS average reward: 0.71, min: 0.12, max: 1.36, stdev: 0.24
Old bare network average reward: 0.67, min: -0.02, max: 1.36, stdev: 0.25
New bare network average reward: 0.67, min: -0.01, max: 1.36, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.37, max: 1.10, stdev: 0.25
External policy "individual greedy" average reward: 0.54, min: -0.19, max: 1.24, stdev: 0.24
External policy "total greedy" average reward: 0.66, min: 0.07, max: 1.31, stdev: 0.23
New network won 74 and tied 158 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1757697740_iter_320

Training iteration 321 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.01 seconds
Training examples lengths: [64754, 64934, 64653, 64742, 65040, 64802, 64708, 64835, 64998, 64646]
Total value: 468240.02
Training on 648112 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2197 (value: 0.0012, weighted value: 0.0598, policy: 0.1598, weighted policy: 0.1598), Train Mean Max: 0.9347
Epoch 2/10, Train Loss: 0.2105 (value: 0.0011, weighted value: 0.0541, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9352
Epoch 3/10, Train Loss: 0.2051 (value: 0.0010, weighted value: 0.0502, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9355
Epoch 4/10, Train Loss: 0.1997 (value: 0.0009, weighted value: 0.0465, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9358
Epoch 5/10, Train Loss: 0.1973 (value: 0.0009, weighted value: 0.0458, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9361
Epoch 6/10, Train Loss: 0.1957 (value: 0.0009, weighted value: 0.0441, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9363
Epoch 7/10, Train Loss: 0.1930 (value: 0.0009, weighted value: 0.0427, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9365
Epoch 8/10, Train Loss: 0.1919 (value: 0.0008, weighted value: 0.0420, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9367
Epoch 9/10, Train Loss: 0.1895 (value: 0.0008, weighted value: 0.0395, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9368
Epoch 10/10, Train Loss: 0.1880 (value: 0.0008, weighted value: 0.0390, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9371
..training done in 71.64 seconds
..evaluation done in 15.74 seconds
Old network+MCTS average reward: 0.70, min: 0.17, max: 1.56, stdev: 0.23
New network+MCTS average reward: 0.70, min: 0.13, max: 1.59, stdev: 0.23
Old bare network average reward: 0.66, min: 0.01, max: 1.54, stdev: 0.24
New bare network average reward: 0.66, min: 0.00, max: 1.59, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.27, max: 0.87, stdev: 0.21
External policy "individual greedy" average reward: 0.53, min: 0.01, max: 1.22, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.08, max: 1.28, stdev: 0.22
New network won 71 and tied 162 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 322 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.79 seconds
Training examples lengths: [64934, 64653, 64742, 65040, 64802, 64708, 64835, 64998, 64646, 64620]
Total value: 468946.81
Training on 647978 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2133 (value: 0.0011, weighted value: 0.0541, policy: 0.1592, weighted policy: 0.1592), Train Mean Max: 0.9356
Epoch 2/10, Train Loss: 0.2032 (value: 0.0010, weighted value: 0.0482, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9361
Epoch 3/10, Train Loss: 0.1989 (value: 0.0009, weighted value: 0.0461, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9364
Epoch 4/10, Train Loss: 0.1948 (value: 0.0009, weighted value: 0.0434, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9367
Epoch 5/10, Train Loss: 0.1921 (value: 0.0008, weighted value: 0.0418, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9368
Epoch 6/10, Train Loss: 0.1916 (value: 0.0008, weighted value: 0.0417, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9370
Epoch 7/10, Train Loss: 0.1891 (value: 0.0008, weighted value: 0.0387, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9371
Epoch 8/10, Train Loss: 0.1874 (value: 0.0008, weighted value: 0.0384, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9373
Epoch 9/10, Train Loss: 0.1858 (value: 0.0007, weighted value: 0.0368, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9375
Epoch 10/10, Train Loss: 0.1863 (value: 0.0007, weighted value: 0.0374, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9375
..training done in 66.86 seconds
..evaluation done in 16.24 seconds
Old network+MCTS average reward: 0.71, min: 0.13, max: 1.37, stdev: 0.23
New network+MCTS average reward: 0.71, min: 0.13, max: 1.33, stdev: 0.23
Old bare network average reward: 0.66, min: 0.15, max: 1.31, stdev: 0.23
New bare network average reward: 0.66, min: 0.11, max: 1.31, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.36, max: 0.92, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.02, max: 1.24, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: -0.03, max: 1.48, stdev: 0.23
New network won 78 and tied 153 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 323 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.04 seconds
Training examples lengths: [64653, 64742, 65040, 64802, 64708, 64835, 64998, 64646, 64620, 64558]
Total value: 468905.89
Training on 647602 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2098 (value: 0.0010, weighted value: 0.0512, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9360
Epoch 2/10, Train Loss: 0.2008 (value: 0.0009, weighted value: 0.0460, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9368
Epoch 3/10, Train Loss: 0.1950 (value: 0.0009, weighted value: 0.0432, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9369
Epoch 4/10, Train Loss: 0.1927 (value: 0.0008, weighted value: 0.0419, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9372
Epoch 5/10, Train Loss: 0.1895 (value: 0.0008, weighted value: 0.0402, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9374
Epoch 6/10, Train Loss: 0.1867 (value: 0.0008, weighted value: 0.0379, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9375
Epoch 7/10, Train Loss: 0.1879 (value: 0.0008, weighted value: 0.0390, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9376
Epoch 8/10, Train Loss: 0.1852 (value: 0.0007, weighted value: 0.0366, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1837 (value: 0.0007, weighted value: 0.0350, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9378
Epoch 10/10, Train Loss: 0.1828 (value: 0.0007, weighted value: 0.0351, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9382
..training done in 67.41 seconds
..evaluation done in 16.33 seconds
Old network+MCTS average reward: 0.73, min: 0.15, max: 1.35, stdev: 0.23
New network+MCTS average reward: 0.73, min: 0.16, max: 1.29, stdev: 0.23
Old bare network average reward: 0.69, min: -0.01, max: 1.35, stdev: 0.23
New bare network average reward: 0.69, min: 0.08, max: 1.29, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.29, max: 0.83, stdev: 0.21
External policy "individual greedy" average reward: 0.55, min: -0.11, max: 1.31, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.09, max: 1.36, stdev: 0.21
New network won 69 and tied 158 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 324 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.70 seconds
Training examples lengths: [64742, 65040, 64802, 64708, 64835, 64998, 64646, 64620, 64558, 64882]
Total value: 469152.20
Training on 647831 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2337 (value: 0.0013, weighted value: 0.0668, policy: 0.1669, weighted policy: 0.1669), Train Mean Max: 0.9345
Epoch 2/10, Train Loss: 0.2175 (value: 0.0011, weighted value: 0.0571, policy: 0.1604, weighted policy: 0.1604), Train Mean Max: 0.9354
Epoch 3/10, Train Loss: 0.2080 (value: 0.0010, weighted value: 0.0523, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9360
Epoch 4/10, Train Loss: 0.2026 (value: 0.0010, weighted value: 0.0491, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9362
Epoch 5/10, Train Loss: 0.2005 (value: 0.0010, weighted value: 0.0484, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9364
Epoch 6/10, Train Loss: 0.1947 (value: 0.0009, weighted value: 0.0441, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9366
Epoch 7/10, Train Loss: 0.1937 (value: 0.0009, weighted value: 0.0435, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9367
Epoch 8/10, Train Loss: 0.1910 (value: 0.0008, weighted value: 0.0412, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9368
Epoch 9/10, Train Loss: 0.1894 (value: 0.0008, weighted value: 0.0400, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9371
Epoch 10/10, Train Loss: 0.1894 (value: 0.0008, weighted value: 0.0406, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9374
..training done in 67.59 seconds
..evaluation done in 16.11 seconds
Old network+MCTS average reward: 0.73, min: 0.06, max: 1.46, stdev: 0.23
New network+MCTS average reward: 0.74, min: 0.05, max: 1.46, stdev: 0.23
Old bare network average reward: 0.69, min: 0.05, max: 1.42, stdev: 0.23
New bare network average reward: 0.70, min: -0.05, max: 1.35, stdev: 0.23
External policy "random" average reward: 0.28, min: -0.33, max: 0.83, stdev: 0.22
External policy "individual greedy" average reward: 0.56, min: -0.08, max: 1.26, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: 0.14, max: 1.36, stdev: 0.23
New network won 80 and tied 153 out of 300 games (52.17% wins where ties are half wins)
Keeping the new network

Training iteration 325 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.46 seconds
Training examples lengths: [65040, 64802, 64708, 64835, 64998, 64646, 64620, 64558, 64882, 64696]
Total value: 469514.18
Training on 647785 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2124 (value: 0.0011, weighted value: 0.0529, policy: 0.1595, weighted policy: 0.1595), Train Mean Max: 0.9356
Epoch 2/10, Train Loss: 0.2041 (value: 0.0010, weighted value: 0.0479, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9362
Epoch 3/10, Train Loss: 0.2009 (value: 0.0009, weighted value: 0.0472, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9364
Epoch 4/10, Train Loss: 0.1944 (value: 0.0009, weighted value: 0.0425, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9368
Epoch 5/10, Train Loss: 0.1917 (value: 0.0008, weighted value: 0.0415, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9372
Epoch 6/10, Train Loss: 0.1897 (value: 0.0008, weighted value: 0.0405, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1894 (value: 0.0008, weighted value: 0.0399, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9372
Epoch 8/10, Train Loss: 0.1874 (value: 0.0008, weighted value: 0.0379, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9373
Epoch 9/10, Train Loss: 0.1857 (value: 0.0007, weighted value: 0.0365, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9375
Epoch 10/10, Train Loss: 0.1856 (value: 0.0007, weighted value: 0.0367, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9378
..training done in 67.19 seconds
..evaluation done in 15.61 seconds
Old network+MCTS average reward: 0.73, min: 0.14, max: 1.57, stdev: 0.23
New network+MCTS average reward: 0.72, min: 0.21, max: 1.54, stdev: 0.23
Old bare network average reward: 0.69, min: 0.10, max: 1.52, stdev: 0.23
New bare network average reward: 0.69, min: 0.05, max: 1.54, stdev: 0.23
External policy "random" average reward: 0.28, min: -0.35, max: 0.97, stdev: 0.21
External policy "individual greedy" average reward: 0.55, min: -0.06, max: 1.55, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: 0.17, max: 1.67, stdev: 0.22
New network won 56 and tied 175 out of 300 games (47.83% wins where ties are half wins)
Reverting to the old network

Training iteration 326 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.95 seconds
Training examples lengths: [64802, 64708, 64835, 64998, 64646, 64620, 64558, 64882, 64696, 64881]
Total value: 469412.28
Training on 647626 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2378 (value: 0.0014, weighted value: 0.0700, policy: 0.1678, weighted policy: 0.1678), Train Mean Max: 0.9341
Epoch 2/10, Train Loss: 0.2198 (value: 0.0012, weighted value: 0.0582, policy: 0.1617, weighted policy: 0.1617), Train Mean Max: 0.9351
Epoch 3/10, Train Loss: 0.2115 (value: 0.0011, weighted value: 0.0547, policy: 0.1569, weighted policy: 0.1569), Train Mean Max: 0.9356
Epoch 4/10, Train Loss: 0.2062 (value: 0.0010, weighted value: 0.0519, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9358
Epoch 5/10, Train Loss: 0.2024 (value: 0.0010, weighted value: 0.0498, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9361
Epoch 6/10, Train Loss: 0.1975 (value: 0.0009, weighted value: 0.0459, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9364
Epoch 7/10, Train Loss: 0.1959 (value: 0.0009, weighted value: 0.0445, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9364
Epoch 8/10, Train Loss: 0.1947 (value: 0.0009, weighted value: 0.0434, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9366
Epoch 9/10, Train Loss: 0.1917 (value: 0.0008, weighted value: 0.0409, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9368
Epoch 10/10, Train Loss: 0.1909 (value: 0.0008, weighted value: 0.0409, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9370
..training done in 73.62 seconds
..evaluation done in 15.85 seconds
Old network+MCTS average reward: 0.71, min: 0.11, max: 1.35, stdev: 0.23
New network+MCTS average reward: 0.71, min: 0.11, max: 1.33, stdev: 0.23
Old bare network average reward: 0.67, min: 0.05, max: 1.32, stdev: 0.23
New bare network average reward: 0.67, min: 0.05, max: 1.35, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.49, max: 0.95, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: -0.02, max: 1.21, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.06, max: 1.29, stdev: 0.21
New network won 80 and tied 156 out of 300 games (52.67% wins where ties are half wins)
Keeping the new network

Training iteration 327 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.66 seconds
Training examples lengths: [64708, 64835, 64998, 64646, 64620, 64558, 64882, 64696, 64881, 64757]
Total value: 469820.85
Training on 647581 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2145 (value: 0.0011, weighted value: 0.0549, policy: 0.1596, weighted policy: 0.1596), Train Mean Max: 0.9355
Epoch 2/10, Train Loss: 0.2046 (value: 0.0010, weighted value: 0.0492, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9363
Epoch 3/10, Train Loss: 0.1992 (value: 0.0009, weighted value: 0.0460, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9365
Epoch 4/10, Train Loss: 0.1967 (value: 0.0009, weighted value: 0.0453, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9367
Epoch 5/10, Train Loss: 0.1928 (value: 0.0008, weighted value: 0.0421, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1915 (value: 0.0008, weighted value: 0.0416, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9372
Epoch 7/10, Train Loss: 0.1891 (value: 0.0008, weighted value: 0.0401, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1873 (value: 0.0008, weighted value: 0.0380, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1872 (value: 0.0008, weighted value: 0.0383, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9376
Epoch 10/10, Train Loss: 0.1868 (value: 0.0007, weighted value: 0.0372, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9377
..training done in 72.79 seconds
..evaluation done in 15.86 seconds
Old network+MCTS average reward: 0.72, min: 0.19, max: 1.27, stdev: 0.21
New network+MCTS average reward: 0.72, min: 0.19, max: 1.27, stdev: 0.21
Old bare network average reward: 0.68, min: 0.12, max: 1.27, stdev: 0.21
New bare network average reward: 0.67, min: 0.12, max: 1.20, stdev: 0.21
External policy "random" average reward: 0.26, min: -0.26, max: 0.72, stdev: 0.21
External policy "individual greedy" average reward: 0.55, min: -0.11, max: 1.09, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.18, max: 1.23, stdev: 0.20
New network won 73 and tied 159 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 328 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 44.66 seconds
Training examples lengths: [64835, 64998, 64646, 64620, 64558, 64882, 64696, 64881, 64757, 64933]
Total value: 470030.82
Training on 647806 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2096 (value: 0.0010, weighted value: 0.0504, policy: 0.1592, weighted policy: 0.1592), Train Mean Max: 0.9360
Epoch 2/10, Train Loss: 0.2018 (value: 0.0009, weighted value: 0.0458, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9364
Epoch 3/10, Train Loss: 0.1959 (value: 0.0009, weighted value: 0.0441, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9370
Epoch 4/10, Train Loss: 0.1918 (value: 0.0008, weighted value: 0.0418, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9371
Epoch 5/10, Train Loss: 0.1901 (value: 0.0008, weighted value: 0.0404, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9373
Epoch 6/10, Train Loss: 0.1882 (value: 0.0008, weighted value: 0.0388, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9375
Epoch 7/10, Train Loss: 0.1876 (value: 0.0008, weighted value: 0.0386, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9375
Epoch 8/10, Train Loss: 0.1835 (value: 0.0007, weighted value: 0.0354, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1852 (value: 0.0007, weighted value: 0.0361, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9378
Epoch 10/10, Train Loss: 0.1832 (value: 0.0007, weighted value: 0.0348, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9380
..training done in 68.95 seconds
..evaluation done in 16.25 seconds
Old network+MCTS average reward: 0.73, min: -0.01, max: 1.48, stdev: 0.26
New network+MCTS average reward: 0.73, min: -0.01, max: 1.48, stdev: 0.26
Old bare network average reward: 0.68, min: -0.15, max: 1.41, stdev: 0.26
New bare network average reward: 0.69, min: -0.04, max: 1.48, stdev: 0.26
External policy "random" average reward: 0.27, min: -0.37, max: 0.92, stdev: 0.24
External policy "individual greedy" average reward: 0.56, min: -0.11, max: 1.21, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: 0.02, max: 1.34, stdev: 0.23
New network won 76 and tied 161 out of 300 games (52.17% wins where ties are half wins)
Keeping the new network

Training iteration 329 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.35 seconds
Training examples lengths: [64998, 64646, 64620, 64558, 64882, 64696, 64881, 64757, 64933, 64736]
Total value: 470226.00
Training on 647707 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2085 (value: 0.0010, weighted value: 0.0508, policy: 0.1577, weighted policy: 0.1577), Train Mean Max: 0.9364
Epoch 2/10, Train Loss: 0.1985 (value: 0.0009, weighted value: 0.0448, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9371
Epoch 3/10, Train Loss: 0.1939 (value: 0.0009, weighted value: 0.0428, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9373
Epoch 4/10, Train Loss: 0.1913 (value: 0.0008, weighted value: 0.0407, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9375
Epoch 5/10, Train Loss: 0.1885 (value: 0.0008, weighted value: 0.0391, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9377
Epoch 6/10, Train Loss: 0.1865 (value: 0.0008, weighted value: 0.0376, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9378
Epoch 7/10, Train Loss: 0.1845 (value: 0.0007, weighted value: 0.0363, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9381
Epoch 8/10, Train Loss: 0.1833 (value: 0.0007, weighted value: 0.0351, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9380
Epoch 9/10, Train Loss: 0.1836 (value: 0.0007, weighted value: 0.0359, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9382
Epoch 10/10, Train Loss: 0.1812 (value: 0.0007, weighted value: 0.0335, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9382
..training done in 71.74 seconds
..evaluation done in 17.58 seconds
Old network+MCTS average reward: 0.73, min: 0.09, max: 1.49, stdev: 0.24
New network+MCTS average reward: 0.73, min: 0.07, max: 1.49, stdev: 0.24
Old bare network average reward: 0.69, min: 0.06, max: 1.42, stdev: 0.25
New bare network average reward: 0.68, min: 0.02, max: 1.47, stdev: 0.25
External policy "random" average reward: 0.28, min: -0.31, max: 0.96, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: -0.07, max: 1.31, stdev: 0.24
External policy "total greedy" average reward: 0.66, min: 0.10, max: 1.40, stdev: 0.23
New network won 67 and tied 160 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 330 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.17 seconds
Training examples lengths: [64646, 64620, 64558, 64882, 64696, 64881, 64757, 64933, 64736, 64816]
Total value: 469804.07
Training on 647525 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2330 (value: 0.0013, weighted value: 0.0656, policy: 0.1674, weighted policy: 0.1674), Train Mean Max: 0.9346
Epoch 2/10, Train Loss: 0.2162 (value: 0.0011, weighted value: 0.0561, policy: 0.1601, weighted policy: 0.1601), Train Mean Max: 0.9358
Epoch 3/10, Train Loss: 0.2071 (value: 0.0010, weighted value: 0.0516, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9365
Epoch 4/10, Train Loss: 0.2018 (value: 0.0010, weighted value: 0.0488, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9366
Epoch 5/10, Train Loss: 0.1974 (value: 0.0009, weighted value: 0.0466, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1936 (value: 0.0009, weighted value: 0.0435, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9369
Epoch 7/10, Train Loss: 0.1917 (value: 0.0008, weighted value: 0.0419, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9370
Epoch 8/10, Train Loss: 0.1906 (value: 0.0008, weighted value: 0.0408, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9372
Epoch 9/10, Train Loss: 0.1887 (value: 0.0008, weighted value: 0.0390, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9373
Epoch 10/10, Train Loss: 0.1878 (value: 0.0008, weighted value: 0.0385, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9376
..training done in 66.72 seconds
..evaluation done in 15.63 seconds
Old network+MCTS average reward: 0.72, min: 0.19, max: 1.35, stdev: 0.23
New network+MCTS average reward: 0.71, min: 0.15, max: 1.31, stdev: 0.24
Old bare network average reward: 0.67, min: -0.01, max: 1.30, stdev: 0.25
New bare network average reward: 0.67, min: 0.01, max: 1.30, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.28, max: 0.92, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: 0.03, max: 1.20, stdev: 0.21
External policy "total greedy" average reward: 0.65, min: 0.09, max: 1.36, stdev: 0.22
New network won 64 and tied 160 out of 300 games (48.00% wins where ties are half wins)
Reverting to the old network

Training iteration 331 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.76 seconds
Training examples lengths: [64620, 64558, 64882, 64696, 64881, 64757, 64933, 64736, 64816, 64853]
Total value: 470556.98
Training on 647732 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2552 (value: 0.0016, weighted value: 0.0800, policy: 0.1752, weighted policy: 0.1752), Train Mean Max: 0.9331
Epoch 2/10, Train Loss: 0.2322 (value: 0.0014, weighted value: 0.0677, policy: 0.1645, weighted policy: 0.1645), Train Mean Max: 0.9346
Epoch 3/10, Train Loss: 0.2199 (value: 0.0012, weighted value: 0.0622, policy: 0.1577, weighted policy: 0.1577), Train Mean Max: 0.9351
Epoch 4/10, Train Loss: 0.2113 (value: 0.0011, weighted value: 0.0553, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9355
Epoch 5/10, Train Loss: 0.2065 (value: 0.0011, weighted value: 0.0533, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9358
Epoch 6/10, Train Loss: 0.2016 (value: 0.0010, weighted value: 0.0490, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9359
Epoch 7/10, Train Loss: 0.1995 (value: 0.0010, weighted value: 0.0484, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9363
Epoch 8/10, Train Loss: 0.1966 (value: 0.0009, weighted value: 0.0453, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9363
Epoch 9/10, Train Loss: 0.1942 (value: 0.0009, weighted value: 0.0436, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9366
Epoch 10/10, Train Loss: 0.1930 (value: 0.0009, weighted value: 0.0428, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9368
..training done in 65.99 seconds
..evaluation done in 16.45 seconds
Old network+MCTS average reward: 0.73, min: 0.07, max: 1.51, stdev: 0.23
New network+MCTS average reward: 0.73, min: 0.08, max: 1.51, stdev: 0.23
Old bare network average reward: 0.68, min: 0.06, max: 1.40, stdev: 0.23
New bare network average reward: 0.68, min: 0.06, max: 1.51, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.32, max: 0.91, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: 0.00, max: 1.35, stdev: 0.24
External policy "total greedy" average reward: 0.66, min: 0.08, max: 1.30, stdev: 0.23
New network won 80 and tied 150 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 332 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.01 seconds
Training examples lengths: [64558, 64882, 64696, 64881, 64757, 64933, 64736, 64816, 64853, 64860]
Total value: 470858.05
Training on 647972 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2150 (value: 0.0011, weighted value: 0.0561, policy: 0.1589, weighted policy: 0.1589), Train Mean Max: 0.9356
Epoch 2/10, Train Loss: 0.2063 (value: 0.0010, weighted value: 0.0509, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9359
Epoch 3/10, Train Loss: 0.2004 (value: 0.0010, weighted value: 0.0475, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9363
Epoch 4/10, Train Loss: 0.1978 (value: 0.0009, weighted value: 0.0462, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9364
Epoch 5/10, Train Loss: 0.1937 (value: 0.0009, weighted value: 0.0439, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1919 (value: 0.0009, weighted value: 0.0426, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1885 (value: 0.0008, weighted value: 0.0397, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1888 (value: 0.0008, weighted value: 0.0396, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9373
Epoch 9/10, Train Loss: 0.1889 (value: 0.0008, weighted value: 0.0396, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9375
Epoch 10/10, Train Loss: 0.1849 (value: 0.0007, weighted value: 0.0368, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9378
..training done in 65.98 seconds
..evaluation done in 15.41 seconds
Old network+MCTS average reward: 0.69, min: 0.01, max: 1.55, stdev: 0.24
New network+MCTS average reward: 0.69, min: 0.06, max: 1.55, stdev: 0.24
Old bare network average reward: 0.65, min: -0.08, max: 1.55, stdev: 0.25
New bare network average reward: 0.65, min: -0.06, max: 1.55, stdev: 0.25
External policy "random" average reward: 0.23, min: -0.35, max: 0.94, stdev: 0.23
External policy "individual greedy" average reward: 0.51, min: -0.12, max: 1.40, stdev: 0.23
External policy "total greedy" average reward: 0.63, min: 0.17, max: 1.41, stdev: 0.22
New network won 72 and tied 156 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 333 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.41 seconds
Training examples lengths: [64882, 64696, 64881, 64757, 64933, 64736, 64816, 64853, 64860, 64707]
Total value: 471398.58
Training on 648121 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2094 (value: 0.0010, weighted value: 0.0513, policy: 0.1582, weighted policy: 0.1582), Train Mean Max: 0.9361
Epoch 2/10, Train Loss: 0.2002 (value: 0.0009, weighted value: 0.0464, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9368
Epoch 3/10, Train Loss: 0.1963 (value: 0.0009, weighted value: 0.0449, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9369
Epoch 4/10, Train Loss: 0.1922 (value: 0.0008, weighted value: 0.0417, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9373
Epoch 5/10, Train Loss: 0.1899 (value: 0.0008, weighted value: 0.0414, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1869 (value: 0.0008, weighted value: 0.0385, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9377
Epoch 7/10, Train Loss: 0.1860 (value: 0.0008, weighted value: 0.0380, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1843 (value: 0.0007, weighted value: 0.0366, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9380
Epoch 9/10, Train Loss: 0.1835 (value: 0.0007, weighted value: 0.0361, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9381
Epoch 10/10, Train Loss: 0.1809 (value: 0.0007, weighted value: 0.0343, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9384
..training done in 71.48 seconds
..evaluation done in 15.91 seconds
Old network+MCTS average reward: 0.73, min: 0.21, max: 1.64, stdev: 0.23
New network+MCTS average reward: 0.73, min: 0.19, max: 1.64, stdev: 0.23
Old bare network average reward: 0.69, min: 0.17, max: 1.61, stdev: 0.23
New bare network average reward: 0.68, min: 0.13, max: 1.56, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.24, max: 1.12, stdev: 0.24
External policy "individual greedy" average reward: 0.55, min: 0.05, max: 1.42, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: 0.19, max: 1.61, stdev: 0.22
New network won 67 and tied 157 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 334 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.43 seconds
Training examples lengths: [64696, 64881, 64757, 64933, 64736, 64816, 64853, 64860, 64707, 65015]
Total value: 472133.29
Training on 648254 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2347 (value: 0.0014, weighted value: 0.0681, policy: 0.1666, weighted policy: 0.1666), Train Mean Max: 0.9345
Epoch 2/10, Train Loss: 0.2165 (value: 0.0011, weighted value: 0.0574, policy: 0.1590, weighted policy: 0.1590), Train Mean Max: 0.9356
Epoch 3/10, Train Loss: 0.2081 (value: 0.0011, weighted value: 0.0529, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9361
Epoch 4/10, Train Loss: 0.2025 (value: 0.0010, weighted value: 0.0496, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9363
Epoch 5/10, Train Loss: 0.1978 (value: 0.0009, weighted value: 0.0467, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9367
Epoch 6/10, Train Loss: 0.1957 (value: 0.0009, weighted value: 0.0459, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9368
Epoch 7/10, Train Loss: 0.1920 (value: 0.0008, weighted value: 0.0420, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9370
Epoch 8/10, Train Loss: 0.1920 (value: 0.0009, weighted value: 0.0432, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9371
Epoch 9/10, Train Loss: 0.1891 (value: 0.0008, weighted value: 0.0396, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9373
Epoch 10/10, Train Loss: 0.1885 (value: 0.0008, weighted value: 0.0399, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9376
..training done in 66.16 seconds
..evaluation done in 16.30 seconds
Old network+MCTS average reward: 0.75, min: 0.09, max: 1.49, stdev: 0.25
New network+MCTS average reward: 0.74, min: 0.16, max: 1.49, stdev: 0.25
Old bare network average reward: 0.70, min: 0.02, max: 1.41, stdev: 0.25
New bare network average reward: 0.70, min: -0.01, max: 1.41, stdev: 0.25
External policy "random" average reward: 0.30, min: -0.42, max: 1.01, stdev: 0.23
External policy "individual greedy" average reward: 0.57, min: 0.02, max: 1.29, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: 0.17, max: 1.31, stdev: 0.22
New network won 52 and tied 180 out of 300 games (47.33% wins where ties are half wins)
Reverting to the old network

Training iteration 335 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.36 seconds
Training examples lengths: [64881, 64757, 64933, 64736, 64816, 64853, 64860, 64707, 65015, 64743]
Total value: 472224.88
Training on 648301 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2550 (value: 0.0016, weighted value: 0.0812, policy: 0.1738, weighted policy: 0.1738), Train Mean Max: 0.9333
Epoch 2/10, Train Loss: 0.2314 (value: 0.0014, weighted value: 0.0680, policy: 0.1635, weighted policy: 0.1635), Train Mean Max: 0.9348
Epoch 3/10, Train Loss: 0.2175 (value: 0.0012, weighted value: 0.0606, policy: 0.1569, weighted policy: 0.1569), Train Mean Max: 0.9353
Epoch 4/10, Train Loss: 0.2120 (value: 0.0011, weighted value: 0.0566, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9355
Epoch 5/10, Train Loss: 0.2055 (value: 0.0011, weighted value: 0.0533, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9360
Epoch 6/10, Train Loss: 0.2022 (value: 0.0010, weighted value: 0.0502, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9362
Epoch 7/10, Train Loss: 0.1979 (value: 0.0010, weighted value: 0.0480, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9365
Epoch 8/10, Train Loss: 0.1972 (value: 0.0009, weighted value: 0.0470, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9366
Epoch 9/10, Train Loss: 0.1930 (value: 0.0009, weighted value: 0.0438, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9369
Epoch 10/10, Train Loss: 0.1919 (value: 0.0009, weighted value: 0.0430, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9371
..training done in 66.86 seconds
..evaluation done in 15.92 seconds
Old network+MCTS average reward: 0.72, min: 0.06, max: 1.31, stdev: 0.23
New network+MCTS average reward: 0.72, min: 0.09, max: 1.40, stdev: 0.23
Old bare network average reward: 0.68, min: 0.03, max: 1.28, stdev: 0.23
New bare network average reward: 0.68, min: 0.02, max: 1.27, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.50, max: 0.94, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: 0.04, max: 1.19, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.05, max: 1.46, stdev: 0.22
New network won 79 and tied 161 out of 300 games (53.17% wins where ties are half wins)
Keeping the new network

Training iteration 336 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.50 seconds
Training examples lengths: [64757, 64933, 64736, 64816, 64853, 64860, 64707, 65015, 64743, 65007]
Total value: 472946.83
Training on 648427 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2155 (value: 0.0011, weighted value: 0.0569, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9355
Epoch 2/10, Train Loss: 0.2049 (value: 0.0010, weighted value: 0.0500, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9363
Epoch 3/10, Train Loss: 0.1995 (value: 0.0010, weighted value: 0.0481, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9368
Epoch 4/10, Train Loss: 0.1957 (value: 0.0009, weighted value: 0.0461, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9371
Epoch 5/10, Train Loss: 0.1926 (value: 0.0009, weighted value: 0.0439, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9373
Epoch 6/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0414, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9374
Epoch 7/10, Train Loss: 0.1893 (value: 0.0008, weighted value: 0.0415, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9377
Epoch 8/10, Train Loss: 0.1889 (value: 0.0008, weighted value: 0.0404, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9376
Epoch 9/10, Train Loss: 0.1857 (value: 0.0008, weighted value: 0.0384, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9380
Epoch 10/10, Train Loss: 0.1843 (value: 0.0008, weighted value: 0.0378, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9381
..training done in 71.90 seconds
..evaluation done in 15.59 seconds
Old network+MCTS average reward: 0.71, min: 0.06, max: 1.39, stdev: 0.23
New network+MCTS average reward: 0.71, min: 0.16, max: 1.28, stdev: 0.23
Old bare network average reward: 0.66, min: 0.06, max: 1.27, stdev: 0.23
New bare network average reward: 0.66, min: 0.06, max: 1.39, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.27, max: 0.99, stdev: 0.22
External policy "individual greedy" average reward: 0.52, min: -0.08, max: 1.16, stdev: 0.22
External policy "total greedy" average reward: 0.63, min: -0.01, max: 1.31, stdev: 0.22
New network won 60 and tied 171 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 337 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.33 seconds
Training examples lengths: [64933, 64736, 64816, 64853, 64860, 64707, 65015, 64743, 65007, 64975]
Total value: 473363.31
Training on 648645 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2387 (value: 0.0014, weighted value: 0.0706, policy: 0.1681, weighted policy: 0.1681), Train Mean Max: 0.9338
Epoch 2/10, Train Loss: 0.2208 (value: 0.0012, weighted value: 0.0620, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9351
Epoch 3/10, Train Loss: 0.2106 (value: 0.0011, weighted value: 0.0552, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9356
Epoch 4/10, Train Loss: 0.2069 (value: 0.0011, weighted value: 0.0538, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9359
Epoch 5/10, Train Loss: 0.2021 (value: 0.0010, weighted value: 0.0500, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9361
Epoch 6/10, Train Loss: 0.1972 (value: 0.0009, weighted value: 0.0474, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9365
Epoch 7/10, Train Loss: 0.1971 (value: 0.0009, weighted value: 0.0471, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9366
Epoch 8/10, Train Loss: 0.1934 (value: 0.0009, weighted value: 0.0442, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9370
Epoch 9/10, Train Loss: 0.1913 (value: 0.0008, weighted value: 0.0423, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9371
Epoch 10/10, Train Loss: 0.1904 (value: 0.0008, weighted value: 0.0416, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9373
..training done in 67.59 seconds
..evaluation done in 15.79 seconds
Old network+MCTS average reward: 0.72, min: 0.16, max: 1.43, stdev: 0.23
New network+MCTS average reward: 0.72, min: 0.12, max: 1.44, stdev: 0.23
Old bare network average reward: 0.68, min: 0.04, max: 1.31, stdev: 0.23
New bare network average reward: 0.68, min: 0.12, max: 1.42, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.38, max: 0.94, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: -0.05, max: 1.30, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.03, max: 1.44, stdev: 0.23
New network won 66 and tied 161 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 338 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.56 seconds
Training examples lengths: [64736, 64816, 64853, 64860, 64707, 65015, 64743, 65007, 64975, 64641]
Total value: 473472.08
Training on 648353 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2592 (value: 0.0017, weighted value: 0.0843, policy: 0.1749, weighted policy: 0.1749), Train Mean Max: 0.9326
Epoch 2/10, Train Loss: 0.2359 (value: 0.0014, weighted value: 0.0716, policy: 0.1643, weighted policy: 0.1643), Train Mean Max: 0.9340
Epoch 3/10, Train Loss: 0.2228 (value: 0.0013, weighted value: 0.0642, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9346
Epoch 4/10, Train Loss: 0.2143 (value: 0.0012, weighted value: 0.0590, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9350
Epoch 5/10, Train Loss: 0.2105 (value: 0.0011, weighted value: 0.0565, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9352
Epoch 6/10, Train Loss: 0.2047 (value: 0.0011, weighted value: 0.0527, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9357
Epoch 7/10, Train Loss: 0.2021 (value: 0.0010, weighted value: 0.0506, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9358
Epoch 8/10, Train Loss: 0.1999 (value: 0.0010, weighted value: 0.0496, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9362
Epoch 9/10, Train Loss: 0.1957 (value: 0.0009, weighted value: 0.0454, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9364
Epoch 10/10, Train Loss: 0.1942 (value: 0.0009, weighted value: 0.0447, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9367
..training done in 70.37 seconds
..evaluation done in 15.79 seconds
Old network+MCTS average reward: 0.72, min: 0.19, max: 1.62, stdev: 0.24
New network+MCTS average reward: 0.72, min: 0.13, max: 1.64, stdev: 0.24
Old bare network average reward: 0.68, min: 0.13, max: 1.62, stdev: 0.24
New bare network average reward: 0.68, min: 0.13, max: 1.62, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.40, max: 1.17, stdev: 0.24
External policy "individual greedy" average reward: 0.54, min: -0.06, max: 1.40, stdev: 0.24
External policy "total greedy" average reward: 0.66, min: 0.06, max: 1.52, stdev: 0.23
New network won 91 and tied 124 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 339 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.61 seconds
Training examples lengths: [64816, 64853, 64860, 64707, 65015, 64743, 65007, 64975, 64641, 64821]
Total value: 473738.42
Training on 648438 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2166 (value: 0.0012, weighted value: 0.0577, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9353
Epoch 2/10, Train Loss: 0.2072 (value: 0.0010, weighted value: 0.0524, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9359
Epoch 3/10, Train Loss: 0.2031 (value: 0.0010, weighted value: 0.0511, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9364
Epoch 4/10, Train Loss: 0.1975 (value: 0.0009, weighted value: 0.0464, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9366
Epoch 5/10, Train Loss: 0.1938 (value: 0.0009, weighted value: 0.0441, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9370
Epoch 6/10, Train Loss: 0.1929 (value: 0.0009, weighted value: 0.0444, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9370
Epoch 7/10, Train Loss: 0.1904 (value: 0.0008, weighted value: 0.0417, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1886 (value: 0.0008, weighted value: 0.0402, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1871 (value: 0.0008, weighted value: 0.0393, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1845 (value: 0.0007, weighted value: 0.0374, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9379
..training done in 70.58 seconds
..evaluation done in 15.90 seconds
Old network+MCTS average reward: 0.73, min: 0.07, max: 1.61, stdev: 0.24
New network+MCTS average reward: 0.73, min: 0.11, max: 1.61, stdev: 0.25
Old bare network average reward: 0.69, min: 0.06, max: 1.61, stdev: 0.25
New bare network average reward: 0.69, min: 0.10, max: 1.53, stdev: 0.25
External policy "random" average reward: 0.27, min: -0.41, max: 0.94, stdev: 0.23
External policy "individual greedy" average reward: 0.56, min: 0.01, max: 1.31, stdev: 0.24
External policy "total greedy" average reward: 0.67, min: 0.10, max: 1.40, stdev: 0.24
New network won 84 and tied 158 out of 300 games (54.33% wins where ties are half wins)
Keeping the new network

Training iteration 340 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.97 seconds
Training examples lengths: [64853, 64860, 64707, 65015, 64743, 65007, 64975, 64641, 64821, 65052]
Total value: 474559.06
Training on 648674 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2097 (value: 0.0010, weighted value: 0.0524, policy: 0.1573, weighted policy: 0.1573), Train Mean Max: 0.9363
Epoch 2/10, Train Loss: 0.2006 (value: 0.0010, weighted value: 0.0477, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9371
Epoch 3/10, Train Loss: 0.1963 (value: 0.0009, weighted value: 0.0457, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9373
Epoch 4/10, Train Loss: 0.1921 (value: 0.0009, weighted value: 0.0430, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9377
Epoch 5/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0407, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9378
Epoch 6/10, Train Loss: 0.1863 (value: 0.0008, weighted value: 0.0392, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9381
Epoch 7/10, Train Loss: 0.1867 (value: 0.0008, weighted value: 0.0397, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9382
Epoch 8/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0363, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9385
Epoch 9/10, Train Loss: 0.1838 (value: 0.0007, weighted value: 0.0372, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9386
Epoch 10/10, Train Loss: 0.1822 (value: 0.0007, weighted value: 0.0360, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9386
..training done in 73.77 seconds
..evaluation done in 15.65 seconds
Old network+MCTS average reward: 0.72, min: -0.05, max: 1.63, stdev: 0.23
New network+MCTS average reward: 0.72, min: -0.09, max: 1.63, stdev: 0.23
Old bare network average reward: 0.68, min: -0.09, max: 1.48, stdev: 0.23
New bare network average reward: 0.68, min: -0.09, max: 1.57, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.41, max: 1.10, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: -0.17, max: 1.37, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: -0.04, max: 1.46, stdev: 0.22
New network won 77 and tied 160 out of 300 games (52.33% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1757697740_iter_340

Training iteration 341 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.42 seconds
Training examples lengths: [64860, 64707, 65015, 64743, 65007, 64975, 64641, 64821, 65052, 64809]
Total value: 474161.65
Training on 648630 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2062 (value: 0.0010, weighted value: 0.0496, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9368
Epoch 2/10, Train Loss: 0.1967 (value: 0.0009, weighted value: 0.0445, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9376
Epoch 3/10, Train Loss: 0.1929 (value: 0.0009, weighted value: 0.0429, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9379
Epoch 4/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0423, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9382
Epoch 5/10, Train Loss: 0.1855 (value: 0.0008, weighted value: 0.0384, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9382
Epoch 6/10, Train Loss: 0.1855 (value: 0.0008, weighted value: 0.0385, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9384
Epoch 7/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0364, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9385
Epoch 8/10, Train Loss: 0.1819 (value: 0.0007, weighted value: 0.0357, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9387
Epoch 9/10, Train Loss: 0.1806 (value: 0.0007, weighted value: 0.0354, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9389
Epoch 10/10, Train Loss: 0.1786 (value: 0.0007, weighted value: 0.0336, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9391
..training done in 68.10 seconds
..evaluation done in 16.44 seconds
Old network+MCTS average reward: 0.72, min: -0.03, max: 1.32, stdev: 0.23
New network+MCTS average reward: 0.72, min: -0.04, max: 1.32, stdev: 0.23
Old bare network average reward: 0.68, min: -0.04, max: 1.32, stdev: 0.23
New bare network average reward: 0.69, min: -0.04, max: 1.37, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.37, max: 0.95, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.19, max: 1.18, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: -0.01, max: 1.24, stdev: 0.22
New network won 76 and tied 160 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network

Training iteration 342 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.98 seconds
Training examples lengths: [64707, 65015, 64743, 65007, 64975, 64641, 64821, 65052, 64809, 65124]
Total value: 475157.09
Training on 648894 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2050 (value: 0.0010, weighted value: 0.0494, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9374
Epoch 2/10, Train Loss: 0.1951 (value: 0.0009, weighted value: 0.0439, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9381
Epoch 3/10, Train Loss: 0.1904 (value: 0.0008, weighted value: 0.0423, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9386
Epoch 4/10, Train Loss: 0.1876 (value: 0.0008, weighted value: 0.0403, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9386
Epoch 5/10, Train Loss: 0.1840 (value: 0.0008, weighted value: 0.0379, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9390
Epoch 6/10, Train Loss: 0.1818 (value: 0.0007, weighted value: 0.0362, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9391
Epoch 7/10, Train Loss: 0.1814 (value: 0.0007, weighted value: 0.0366, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9392
Epoch 8/10, Train Loss: 0.1800 (value: 0.0007, weighted value: 0.0347, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9392
Epoch 9/10, Train Loss: 0.1787 (value: 0.0007, weighted value: 0.0339, policy: 0.1447, weighted policy: 0.1447), Train Mean Max: 0.9396
Epoch 10/10, Train Loss: 0.1774 (value: 0.0007, weighted value: 0.0334, policy: 0.1439, weighted policy: 0.1439), Train Mean Max: 0.9396
..training done in 67.11 seconds
..evaluation done in 16.21 seconds
Old network+MCTS average reward: 0.71, min: 0.11, max: 1.41, stdev: 0.25
New network+MCTS average reward: 0.71, min: 0.13, max: 1.41, stdev: 0.25
Old bare network average reward: 0.66, min: -0.01, max: 1.35, stdev: 0.26
New bare network average reward: 0.66, min: 0.06, max: 1.35, stdev: 0.26
External policy "random" average reward: 0.25, min: -0.29, max: 0.88, stdev: 0.23
External policy "individual greedy" average reward: 0.52, min: -0.09, max: 1.19, stdev: 0.25
External policy "total greedy" average reward: 0.63, min: 0.01, max: 1.30, stdev: 0.24
New network won 57 and tied 177 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 343 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.45 seconds
Training examples lengths: [65015, 64743, 65007, 64975, 64641, 64821, 65052, 64809, 65124, 64738]
Total value: 474889.71
Training on 648925 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2296 (value: 0.0013, weighted value: 0.0636, policy: 0.1660, weighted policy: 0.1660), Train Mean Max: 0.9358
Epoch 2/10, Train Loss: 0.2140 (value: 0.0011, weighted value: 0.0557, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9369
Epoch 3/10, Train Loss: 0.2024 (value: 0.0010, weighted value: 0.0505, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9375
Epoch 4/10, Train Loss: 0.1969 (value: 0.0009, weighted value: 0.0473, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9377
Epoch 5/10, Train Loss: 0.1924 (value: 0.0009, weighted value: 0.0440, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9379
Epoch 6/10, Train Loss: 0.1912 (value: 0.0009, weighted value: 0.0435, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9381
Epoch 7/10, Train Loss: 0.1891 (value: 0.0008, weighted value: 0.0422, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9383
Epoch 8/10, Train Loss: 0.1859 (value: 0.0008, weighted value: 0.0390, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9383
Epoch 9/10, Train Loss: 0.1858 (value: 0.0008, weighted value: 0.0386, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9385
Epoch 10/10, Train Loss: 0.1836 (value: 0.0007, weighted value: 0.0372, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9389
..training done in 71.86 seconds
..evaluation done in 16.42 seconds
Old network+MCTS average reward: 0.74, min: 0.01, max: 1.52, stdev: 0.23
New network+MCTS average reward: 0.74, min: -0.04, max: 1.52, stdev: 0.23
Old bare network average reward: 0.70, min: -0.05, max: 1.42, stdev: 0.24
New bare network average reward: 0.70, min: -0.17, max: 1.52, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.43, max: 0.93, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: 0.02, max: 1.35, stdev: 0.22
External policy "total greedy" average reward: 0.67, min: 0.09, max: 1.43, stdev: 0.22
New network won 65 and tied 170 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 344 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.80 seconds
Training examples lengths: [64743, 65007, 64975, 64641, 64821, 65052, 64809, 65124, 64738, 64606]
Total value: 474159.98
Training on 648516 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2064 (value: 0.0010, weighted value: 0.0511, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9372
Epoch 2/10, Train Loss: 0.1993 (value: 0.0009, weighted value: 0.0466, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9376
Epoch 3/10, Train Loss: 0.1923 (value: 0.0009, weighted value: 0.0434, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9382
Epoch 4/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0418, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9383
Epoch 5/10, Train Loss: 0.1873 (value: 0.0008, weighted value: 0.0405, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9384
Epoch 6/10, Train Loss: 0.1844 (value: 0.0008, weighted value: 0.0378, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9387
Epoch 7/10, Train Loss: 0.1843 (value: 0.0008, weighted value: 0.0381, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9388
Epoch 8/10, Train Loss: 0.1821 (value: 0.0007, weighted value: 0.0365, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9389
Epoch 9/10, Train Loss: 0.1803 (value: 0.0007, weighted value: 0.0347, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9391
Epoch 10/10, Train Loss: 0.1800 (value: 0.0007, weighted value: 0.0351, policy: 0.1449, weighted policy: 0.1449), Train Mean Max: 0.9393
..training done in 65.15 seconds
..evaluation done in 16.14 seconds
Old network+MCTS average reward: 0.72, min: 0.10, max: 1.66, stdev: 0.23
New network+MCTS average reward: 0.73, min: 0.10, max: 1.66, stdev: 0.23
Old bare network average reward: 0.69, min: 0.08, max: 1.66, stdev: 0.22
New bare network average reward: 0.68, min: 0.08, max: 1.66, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.31, max: 0.91, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: 0.03, max: 1.54, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.19, max: 1.46, stdev: 0.22
New network won 62 and tied 186 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 345 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.86 seconds
Training examples lengths: [65007, 64975, 64641, 64821, 65052, 64809, 65124, 64738, 64606, 64749]
Total value: 474906.44
Training on 648522 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2051 (value: 0.0010, weighted value: 0.0497, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9372
Epoch 2/10, Train Loss: 0.1957 (value: 0.0009, weighted value: 0.0437, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9380
Epoch 3/10, Train Loss: 0.1912 (value: 0.0008, weighted value: 0.0419, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9383
Epoch 4/10, Train Loss: 0.1864 (value: 0.0008, weighted value: 0.0394, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9387
Epoch 5/10, Train Loss: 0.1856 (value: 0.0008, weighted value: 0.0394, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9388
Epoch 6/10, Train Loss: 0.1822 (value: 0.0007, weighted value: 0.0363, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9391
Epoch 7/10, Train Loss: 0.1822 (value: 0.0007, weighted value: 0.0362, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9391
Epoch 8/10, Train Loss: 0.1787 (value: 0.0007, weighted value: 0.0340, policy: 0.1447, weighted policy: 0.1447), Train Mean Max: 0.9393
Epoch 9/10, Train Loss: 0.1803 (value: 0.0007, weighted value: 0.0352, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9392
Epoch 10/10, Train Loss: 0.1777 (value: 0.0007, weighted value: 0.0329, policy: 0.1447, weighted policy: 0.1447), Train Mean Max: 0.9394
..training done in 65.56 seconds
..evaluation done in 15.80 seconds
Old network+MCTS average reward: 0.73, min: 0.19, max: 1.47, stdev: 0.23
New network+MCTS average reward: 0.73, min: 0.24, max: 1.47, stdev: 0.23
Old bare network average reward: 0.69, min: 0.17, max: 1.40, stdev: 0.23
New bare network average reward: 0.69, min: 0.19, max: 1.47, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.35, max: 1.01, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.04, max: 1.20, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.17, max: 1.35, stdev: 0.21
New network won 75 and tied 150 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 346 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.26 seconds
Training examples lengths: [64975, 64641, 64821, 65052, 64809, 65124, 64738, 64606, 64749, 64630]
Total value: 474252.99
Training on 648145 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2045 (value: 0.0010, weighted value: 0.0480, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9375
Epoch 2/10, Train Loss: 0.1954 (value: 0.0009, weighted value: 0.0435, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9383
Epoch 3/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0405, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9387
Epoch 4/10, Train Loss: 0.1854 (value: 0.0008, weighted value: 0.0390, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9388
Epoch 5/10, Train Loss: 0.1830 (value: 0.0007, weighted value: 0.0374, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9390
Epoch 6/10, Train Loss: 0.1837 (value: 0.0008, weighted value: 0.0375, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9390
Epoch 7/10, Train Loss: 0.1799 (value: 0.0007, weighted value: 0.0341, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9392
Epoch 8/10, Train Loss: 0.1783 (value: 0.0007, weighted value: 0.0336, policy: 0.1447, weighted policy: 0.1447), Train Mean Max: 0.9393
Epoch 9/10, Train Loss: 0.1777 (value: 0.0007, weighted value: 0.0333, policy: 0.1443, weighted policy: 0.1443), Train Mean Max: 0.9396
Epoch 10/10, Train Loss: 0.1780 (value: 0.0006, weighted value: 0.0324, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9395
..training done in 66.29 seconds
..evaluation done in 15.96 seconds
Old network+MCTS average reward: 0.75, min: 0.09, max: 1.39, stdev: 0.24
New network+MCTS average reward: 0.75, min: 0.09, max: 1.39, stdev: 0.24
Old bare network average reward: 0.72, min: 0.08, max: 1.31, stdev: 0.24
New bare network average reward: 0.72, min: 0.08, max: 1.39, stdev: 0.24
External policy "random" average reward: 0.29, min: -0.45, max: 1.03, stdev: 0.24
External policy "individual greedy" average reward: 0.56, min: -0.04, max: 1.27, stdev: 0.24
External policy "total greedy" average reward: 0.68, min: 0.14, max: 1.28, stdev: 0.22
New network won 81 and tied 153 out of 300 games (52.50% wins where ties are half wins)
Keeping the new network

Training iteration 347 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.13 seconds
Training examples lengths: [64641, 64821, 65052, 64809, 65124, 64738, 64606, 64749, 64630, 64904]
Total value: 475239.62
Training on 648074 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2037 (value: 0.0010, weighted value: 0.0482, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9379
Epoch 2/10, Train Loss: 0.1940 (value: 0.0009, weighted value: 0.0431, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9386
Epoch 3/10, Train Loss: 0.1880 (value: 0.0008, weighted value: 0.0392, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9388
Epoch 4/10, Train Loss: 0.1858 (value: 0.0008, weighted value: 0.0389, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9392
Epoch 5/10, Train Loss: 0.1815 (value: 0.0007, weighted value: 0.0360, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9393
Epoch 6/10, Train Loss: 0.1828 (value: 0.0008, weighted value: 0.0377, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9392
Epoch 7/10, Train Loss: 0.1805 (value: 0.0007, weighted value: 0.0351, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9395
Epoch 8/10, Train Loss: 0.1777 (value: 0.0007, weighted value: 0.0331, policy: 0.1446, weighted policy: 0.1446), Train Mean Max: 0.9396
Epoch 9/10, Train Loss: 0.1784 (value: 0.0007, weighted value: 0.0339, policy: 0.1445, weighted policy: 0.1445), Train Mean Max: 0.9396
Epoch 10/10, Train Loss: 0.1755 (value: 0.0006, weighted value: 0.0311, policy: 0.1445, weighted policy: 0.1445), Train Mean Max: 0.9398
..training done in 64.89 seconds
..evaluation done in 15.89 seconds
Old network+MCTS average reward: 0.71, min: 0.24, max: 1.63, stdev: 0.23
New network+MCTS average reward: 0.71, min: 0.22, max: 1.63, stdev: 0.23
Old bare network average reward: 0.67, min: 0.14, max: 1.63, stdev: 0.23
New bare network average reward: 0.67, min: 0.06, max: 1.63, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.31, max: 1.07, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: -0.06, max: 1.35, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.05, max: 1.54, stdev: 0.22
New network won 54 and tied 168 out of 300 games (46.00% wins where ties are half wins)
Reverting to the old network

Training iteration 348 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.62 seconds
Training examples lengths: [64821, 65052, 64809, 65124, 64738, 64606, 64749, 64630, 64904, 64773]
Total value: 475181.74
Training on 648206 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2288 (value: 0.0013, weighted value: 0.0636, policy: 0.1651, weighted policy: 0.1651), Train Mean Max: 0.9362
Epoch 2/10, Train Loss: 0.2096 (value: 0.0011, weighted value: 0.0536, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9373
Epoch 3/10, Train Loss: 0.2005 (value: 0.0010, weighted value: 0.0494, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9379
Epoch 4/10, Train Loss: 0.1955 (value: 0.0009, weighted value: 0.0464, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9380
Epoch 5/10, Train Loss: 0.1916 (value: 0.0009, weighted value: 0.0432, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9381
Epoch 6/10, Train Loss: 0.1889 (value: 0.0008, weighted value: 0.0415, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9384
Epoch 7/10, Train Loss: 0.1870 (value: 0.0008, weighted value: 0.0406, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9387
Epoch 8/10, Train Loss: 0.1858 (value: 0.0008, weighted value: 0.0391, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9385
Epoch 9/10, Train Loss: 0.1836 (value: 0.0007, weighted value: 0.0374, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9388
Epoch 10/10, Train Loss: 0.1834 (value: 0.0007, weighted value: 0.0370, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9390
..training done in 72.33 seconds
..evaluation done in 15.68 seconds
Old network+MCTS average reward: 0.71, min: 0.17, max: 1.55, stdev: 0.25
New network+MCTS average reward: 0.71, min: 0.21, max: 1.55, stdev: 0.25
Old bare network average reward: 0.67, min: 0.16, max: 1.48, stdev: 0.25
New bare network average reward: 0.67, min: 0.16, max: 1.57, stdev: 0.25
External policy "random" average reward: 0.27, min: -0.40, max: 1.05, stdev: 0.24
External policy "individual greedy" average reward: 0.53, min: 0.04, max: 1.24, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.11, max: 1.43, stdev: 0.23
New network won 74 and tied 162 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 349 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.82 seconds
Training examples lengths: [65052, 64809, 65124, 64738, 64606, 64749, 64630, 64904, 64773, 64968]
Total value: 475104.51
Training on 648353 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2060 (value: 0.0010, weighted value: 0.0504, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9372
Epoch 2/10, Train Loss: 0.1974 (value: 0.0009, weighted value: 0.0458, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9380
Epoch 3/10, Train Loss: 0.1922 (value: 0.0008, weighted value: 0.0422, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9384
Epoch 4/10, Train Loss: 0.1886 (value: 0.0008, weighted value: 0.0414, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9386
Epoch 5/10, Train Loss: 0.1847 (value: 0.0008, weighted value: 0.0383, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9388
Epoch 6/10, Train Loss: 0.1844 (value: 0.0008, weighted value: 0.0390, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9388
Epoch 7/10, Train Loss: 0.1817 (value: 0.0007, weighted value: 0.0364, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9389
Epoch 8/10, Train Loss: 0.1823 (value: 0.0007, weighted value: 0.0358, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9390
Epoch 9/10, Train Loss: 0.1796 (value: 0.0007, weighted value: 0.0345, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9394
Epoch 10/10, Train Loss: 0.1796 (value: 0.0007, weighted value: 0.0345, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9393
..training done in 65.30 seconds
..evaluation done in 15.74 seconds
Old network+MCTS average reward: 0.73, min: 0.10, max: 1.39, stdev: 0.24
New network+MCTS average reward: 0.73, min: 0.11, max: 1.43, stdev: 0.23
Old bare network average reward: 0.69, min: 0.04, max: 1.42, stdev: 0.24
New bare network average reward: 0.69, min: 0.07, max: 1.43, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.36, max: 1.06, stdev: 0.23
External policy "individual greedy" average reward: 0.56, min: -0.02, max: 1.29, stdev: 0.24
External policy "total greedy" average reward: 0.66, min: 0.05, max: 1.51, stdev: 0.23
New network won 68 and tied 172 out of 300 games (51.33% wins where ties are half wins)
Keeping the new network

Training iteration 350 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.46 seconds
Training examples lengths: [64809, 65124, 64738, 64606, 64749, 64630, 64904, 64773, 64968, 64855]
Total value: 474766.17
Training on 648156 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2043 (value: 0.0010, weighted value: 0.0482, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9374
Epoch 2/10, Train Loss: 0.1962 (value: 0.0009, weighted value: 0.0444, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9380
Epoch 3/10, Train Loss: 0.1887 (value: 0.0008, weighted value: 0.0406, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9385
Epoch 4/10, Train Loss: 0.1865 (value: 0.0008, weighted value: 0.0390, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9387
Epoch 5/10, Train Loss: 0.1853 (value: 0.0008, weighted value: 0.0392, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9388
Epoch 6/10, Train Loss: 0.1819 (value: 0.0007, weighted value: 0.0359, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9389
Epoch 7/10, Train Loss: 0.1817 (value: 0.0007, weighted value: 0.0359, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9390
Epoch 8/10, Train Loss: 0.1809 (value: 0.0007, weighted value: 0.0350, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9390
Epoch 9/10, Train Loss: 0.1788 (value: 0.0007, weighted value: 0.0338, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9394
Epoch 10/10, Train Loss: 0.1772 (value: 0.0006, weighted value: 0.0324, policy: 0.1447, weighted policy: 0.1447), Train Mean Max: 0.9396
..training done in 71.34 seconds
..evaluation done in 15.84 seconds
Old network+MCTS average reward: 0.73, min: 0.06, max: 1.43, stdev: 0.25
New network+MCTS average reward: 0.73, min: 0.07, max: 1.48, stdev: 0.24
Old bare network average reward: 0.69, min: 0.06, max: 1.40, stdev: 0.25
New bare network average reward: 0.69, min: 0.06, max: 1.42, stdev: 0.25
External policy "random" average reward: 0.27, min: -0.33, max: 0.90, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: -0.02, max: 1.29, stdev: 0.24
External policy "total greedy" average reward: 0.66, min: 0.01, max: 1.28, stdev: 0.24
New network won 70 and tied 165 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 351 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.90 seconds
Training examples lengths: [65124, 64738, 64606, 64749, 64630, 64904, 64773, 64968, 64855, 64605]
Total value: 474684.31
Training on 647952 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2031 (value: 0.0010, weighted value: 0.0479, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9375
Epoch 2/10, Train Loss: 0.1949 (value: 0.0009, weighted value: 0.0432, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9384
Epoch 3/10, Train Loss: 0.1889 (value: 0.0008, weighted value: 0.0406, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9387
Epoch 4/10, Train Loss: 0.1847 (value: 0.0008, weighted value: 0.0388, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9390
Epoch 5/10, Train Loss: 0.1831 (value: 0.0007, weighted value: 0.0371, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9391
Epoch 6/10, Train Loss: 0.1817 (value: 0.0007, weighted value: 0.0355, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9391
Epoch 7/10, Train Loss: 0.1798 (value: 0.0007, weighted value: 0.0342, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9393
Epoch 8/10, Train Loss: 0.1795 (value: 0.0007, weighted value: 0.0343, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9395
Epoch 9/10, Train Loss: 0.1780 (value: 0.0007, weighted value: 0.0327, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9395
Epoch 10/10, Train Loss: 0.1766 (value: 0.0006, weighted value: 0.0324, policy: 0.1442, weighted policy: 0.1442), Train Mean Max: 0.9397
..training done in 67.84 seconds
..evaluation done in 15.50 seconds
Old network+MCTS average reward: 0.74, min: 0.12, max: 1.50, stdev: 0.23
New network+MCTS average reward: 0.74, min: 0.15, max: 1.50, stdev: 0.24
Old bare network average reward: 0.70, min: 0.12, max: 1.50, stdev: 0.23
New bare network average reward: 0.70, min: -0.06, max: 1.50, stdev: 0.24
External policy "random" average reward: 0.28, min: -0.28, max: 1.00, stdev: 0.23
External policy "individual greedy" average reward: 0.56, min: -0.02, max: 1.30, stdev: 0.23
External policy "total greedy" average reward: 0.68, min: 0.16, max: 1.41, stdev: 0.22
New network won 68 and tied 169 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 352 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.22 seconds
Training examples lengths: [64738, 64606, 64749, 64630, 64904, 64773, 64968, 64855, 64605, 64648]
Total value: 474172.83
Training on 647476 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2042 (value: 0.0010, weighted value: 0.0484, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9376
Epoch 2/10, Train Loss: 0.1948 (value: 0.0008, weighted value: 0.0423, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9381
Epoch 3/10, Train Loss: 0.1885 (value: 0.0008, weighted value: 0.0396, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9387
Epoch 4/10, Train Loss: 0.1853 (value: 0.0008, weighted value: 0.0380, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9389
Epoch 5/10, Train Loss: 0.1832 (value: 0.0007, weighted value: 0.0367, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9390
Epoch 6/10, Train Loss: 0.1814 (value: 0.0007, weighted value: 0.0357, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9392
Epoch 7/10, Train Loss: 0.1797 (value: 0.0007, weighted value: 0.0340, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9392
Epoch 8/10, Train Loss: 0.1794 (value: 0.0007, weighted value: 0.0334, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9393
Epoch 9/10, Train Loss: 0.1789 (value: 0.0007, weighted value: 0.0333, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9394
Epoch 10/10, Train Loss: 0.1783 (value: 0.0007, weighted value: 0.0326, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9394
..training done in 68.27 seconds
..evaluation done in 16.25 seconds
Old network+MCTS average reward: 0.70, min: 0.14, max: 1.33, stdev: 0.23
New network+MCTS average reward: 0.70, min: 0.15, max: 1.33, stdev: 0.23
Old bare network average reward: 0.65, min: 0.08, max: 1.33, stdev: 0.23
New bare network average reward: 0.66, min: 0.08, max: 1.33, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.40, max: 0.84, stdev: 0.22
External policy "individual greedy" average reward: 0.52, min: -0.10, max: 1.12, stdev: 0.22
External policy "total greedy" average reward: 0.63, min: 0.10, max: 1.30, stdev: 0.21
New network won 81 and tied 166 out of 300 games (54.67% wins where ties are half wins)
Keeping the new network

Training iteration 353 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.57 seconds
Training examples lengths: [64606, 64749, 64630, 64904, 64773, 64968, 64855, 64605, 64648, 65147]
Total value: 474723.88
Training on 647885 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2043 (value: 0.0009, weighted value: 0.0466, policy: 0.1577, weighted policy: 0.1577), Train Mean Max: 0.9376
Epoch 2/10, Train Loss: 0.1941 (value: 0.0008, weighted value: 0.0422, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9383
Epoch 3/10, Train Loss: 0.1882 (value: 0.0008, weighted value: 0.0397, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9387
Epoch 4/10, Train Loss: 0.1845 (value: 0.0008, weighted value: 0.0378, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9389
Epoch 5/10, Train Loss: 0.1834 (value: 0.0007, weighted value: 0.0370, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9390
Epoch 6/10, Train Loss: 0.1804 (value: 0.0007, weighted value: 0.0346, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9391
Epoch 7/10, Train Loss: 0.1806 (value: 0.0007, weighted value: 0.0342, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9391
Epoch 8/10, Train Loss: 0.1783 (value: 0.0007, weighted value: 0.0332, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9394
Epoch 9/10, Train Loss: 0.1776 (value: 0.0006, weighted value: 0.0317, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9393
Epoch 10/10, Train Loss: 0.1774 (value: 0.0006, weighted value: 0.0324, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9396
..training done in 71.85 seconds
..evaluation done in 16.29 seconds
Old network+MCTS average reward: 0.74, min: 0.06, max: 1.41, stdev: 0.22
New network+MCTS average reward: 0.75, min: 0.18, max: 1.39, stdev: 0.22
Old bare network average reward: 0.70, min: 0.06, max: 1.41, stdev: 0.23
New bare network average reward: 0.70, min: 0.06, max: 1.41, stdev: 0.22
External policy "random" average reward: 0.28, min: -0.33, max: 0.88, stdev: 0.22
External policy "individual greedy" average reward: 0.56, min: -0.05, max: 1.11, stdev: 0.21
External policy "total greedy" average reward: 0.68, min: 0.08, max: 1.39, stdev: 0.21
New network won 83 and tied 157 out of 300 games (53.83% wins where ties are half wins)
Keeping the new network

Training iteration 354 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.87 seconds
Training examples lengths: [64749, 64630, 64904, 64773, 64968, 64855, 64605, 64648, 65147, 64632]
Total value: 474909.98
Training on 647911 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2043 (value: 0.0010, weighted value: 0.0477, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9377
Epoch 2/10, Train Loss: 0.1914 (value: 0.0008, weighted value: 0.0405, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9385
Epoch 3/10, Train Loss: 0.1889 (value: 0.0008, weighted value: 0.0404, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9386
Epoch 4/10, Train Loss: 0.1848 (value: 0.0008, weighted value: 0.0382, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9389
Epoch 5/10, Train Loss: 0.1814 (value: 0.0007, weighted value: 0.0354, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9392
Epoch 6/10, Train Loss: 0.1814 (value: 0.0007, weighted value: 0.0353, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9390
Epoch 7/10, Train Loss: 0.1793 (value: 0.0007, weighted value: 0.0340, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9393
Epoch 8/10, Train Loss: 0.1769 (value: 0.0007, weighted value: 0.0329, policy: 0.1440, weighted policy: 0.1440), Train Mean Max: 0.9395
Epoch 9/10, Train Loss: 0.1773 (value: 0.0006, weighted value: 0.0321, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9395
Epoch 10/10, Train Loss: 0.1771 (value: 0.0006, weighted value: 0.0324, policy: 0.1447, weighted policy: 0.1447), Train Mean Max: 0.9397
..training done in 66.25 seconds
..evaluation done in 15.49 seconds
Old network+MCTS average reward: 0.71, min: 0.09, max: 1.34, stdev: 0.22
New network+MCTS average reward: 0.71, min: 0.09, max: 1.31, stdev: 0.22
Old bare network average reward: 0.68, min: 0.14, max: 1.31, stdev: 0.23
New bare network average reward: 0.67, min: 0.09, max: 1.27, stdev: 0.22
External policy "random" average reward: 0.26, min: -0.36, max: 1.01, stdev: 0.21
External policy "individual greedy" average reward: 0.53, min: -0.07, max: 1.05, stdev: 0.21
External policy "total greedy" average reward: 0.65, min: 0.11, max: 1.22, stdev: 0.21
New network won 67 and tied 159 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 355 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.84 seconds
Training examples lengths: [64630, 64904, 64773, 64968, 64855, 64605, 64648, 65147, 64632, 64726]
Total value: 474018.91
Training on 647888 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2276 (value: 0.0012, weighted value: 0.0614, policy: 0.1662, weighted policy: 0.1662), Train Mean Max: 0.9359
Epoch 2/10, Train Loss: 0.2107 (value: 0.0011, weighted value: 0.0525, policy: 0.1582, weighted policy: 0.1582), Train Mean Max: 0.9369
Epoch 3/10, Train Loss: 0.2000 (value: 0.0010, weighted value: 0.0476, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9375
Epoch 4/10, Train Loss: 0.1964 (value: 0.0009, weighted value: 0.0463, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1915 (value: 0.0009, weighted value: 0.0426, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9377
Epoch 6/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0410, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9380
Epoch 7/10, Train Loss: 0.1869 (value: 0.0008, weighted value: 0.0396, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9382
Epoch 8/10, Train Loss: 0.1860 (value: 0.0008, weighted value: 0.0379, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9382
Epoch 9/10, Train Loss: 0.1833 (value: 0.0007, weighted value: 0.0366, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9386
Epoch 10/10, Train Loss: 0.1810 (value: 0.0007, weighted value: 0.0351, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9387
..training done in 65.72 seconds
..evaluation done in 15.81 seconds
Old network+MCTS average reward: 0.74, min: 0.03, max: 1.49, stdev: 0.22
New network+MCTS average reward: 0.75, min: 0.03, max: 1.56, stdev: 0.22
Old bare network average reward: 0.70, min: 0.03, max: 1.49, stdev: 0.23
New bare network average reward: 0.70, min: 0.02, max: 1.56, stdev: 0.23
External policy "random" average reward: 0.28, min: -0.23, max: 0.98, stdev: 0.22
External policy "individual greedy" average reward: 0.56, min: -0.07, max: 1.14, stdev: 0.22
External policy "total greedy" average reward: 0.68, min: 0.15, max: 1.33, stdev: 0.22
New network won 62 and tied 181 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 356 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.32 seconds
Training examples lengths: [64904, 64773, 64968, 64855, 64605, 64648, 65147, 64632, 64726, 64753]
Total value: 474690.32
Training on 648011 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2081 (value: 0.0010, weighted value: 0.0518, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9369
Epoch 2/10, Train Loss: 0.1976 (value: 0.0009, weighted value: 0.0447, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9374
Epoch 3/10, Train Loss: 0.1916 (value: 0.0008, weighted value: 0.0424, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9380
Epoch 4/10, Train Loss: 0.1890 (value: 0.0008, weighted value: 0.0405, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9381
Epoch 5/10, Train Loss: 0.1871 (value: 0.0008, weighted value: 0.0398, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9383
Epoch 6/10, Train Loss: 0.1828 (value: 0.0007, weighted value: 0.0358, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9385
Epoch 7/10, Train Loss: 0.1830 (value: 0.0007, weighted value: 0.0367, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9387
Epoch 8/10, Train Loss: 0.1814 (value: 0.0007, weighted value: 0.0352, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9387
Epoch 9/10, Train Loss: 0.1797 (value: 0.0007, weighted value: 0.0345, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9391
Epoch 10/10, Train Loss: 0.1793 (value: 0.0007, weighted value: 0.0336, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9390
..training done in 72.85 seconds
..evaluation done in 15.85 seconds
Old network+MCTS average reward: 0.71, min: 0.14, max: 1.44, stdev: 0.24
New network+MCTS average reward: 0.71, min: 0.14, max: 1.44, stdev: 0.24
Old bare network average reward: 0.68, min: 0.11, max: 1.44, stdev: 0.24
New bare network average reward: 0.67, min: 0.06, max: 1.44, stdev: 0.24
External policy "random" average reward: 0.25, min: -0.36, max: 1.07, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: -0.05, max: 1.24, stdev: 0.24
External policy "total greedy" average reward: 0.64, min: 0.05, max: 1.38, stdev: 0.22
New network won 65 and tied 170 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 357 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.14 seconds
Training examples lengths: [64773, 64968, 64855, 64605, 64648, 65147, 64632, 64726, 64753, 64717]
Total value: 473886.25
Training on 647824 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2045 (value: 0.0010, weighted value: 0.0488, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9372
Epoch 2/10, Train Loss: 0.1964 (value: 0.0009, weighted value: 0.0430, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9379
Epoch 3/10, Train Loss: 0.1907 (value: 0.0008, weighted value: 0.0409, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9383
Epoch 4/10, Train Loss: 0.1852 (value: 0.0008, weighted value: 0.0381, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9386
Epoch 5/10, Train Loss: 0.1855 (value: 0.0008, weighted value: 0.0382, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9386
Epoch 6/10, Train Loss: 0.1816 (value: 0.0007, weighted value: 0.0358, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9389
Epoch 7/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0355, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9389
Epoch 8/10, Train Loss: 0.1788 (value: 0.0007, weighted value: 0.0329, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9391
Epoch 9/10, Train Loss: 0.1791 (value: 0.0007, weighted value: 0.0327, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9390
Epoch 10/10, Train Loss: 0.1782 (value: 0.0007, weighted value: 0.0332, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9394
..training done in 66.08 seconds
..evaluation done in 16.18 seconds
Old network+MCTS average reward: 0.73, min: 0.07, max: 1.41, stdev: 0.25
New network+MCTS average reward: 0.73, min: 0.07, max: 1.41, stdev: 0.25
Old bare network average reward: 0.69, min: 0.05, max: 1.41, stdev: 0.25
New bare network average reward: 0.69, min: 0.05, max: 1.41, stdev: 0.26
External policy "random" average reward: 0.27, min: -0.36, max: 1.06, stdev: 0.25
External policy "individual greedy" average reward: 0.55, min: 0.01, max: 1.20, stdev: 0.24
External policy "total greedy" average reward: 0.66, min: 0.02, max: 1.29, stdev: 0.23
New network won 68 and tied 165 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 358 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.91 seconds
Training examples lengths: [64968, 64855, 64605, 64648, 65147, 64632, 64726, 64753, 64717, 64629]
Total value: 473185.25
Training on 647680 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2030 (value: 0.0009, weighted value: 0.0469, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9374
Epoch 2/10, Train Loss: 0.1938 (value: 0.0008, weighted value: 0.0417, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9380
Epoch 3/10, Train Loss: 0.1893 (value: 0.0008, weighted value: 0.0401, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9384
Epoch 4/10, Train Loss: 0.1852 (value: 0.0008, weighted value: 0.0379, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9386
Epoch 5/10, Train Loss: 0.1817 (value: 0.0007, weighted value: 0.0348, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9386
Epoch 6/10, Train Loss: 0.1824 (value: 0.0007, weighted value: 0.0352, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9387
Epoch 7/10, Train Loss: 0.1806 (value: 0.0007, weighted value: 0.0343, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9390
Epoch 8/10, Train Loss: 0.1808 (value: 0.0007, weighted value: 0.0345, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9391
Epoch 9/10, Train Loss: 0.1755 (value: 0.0006, weighted value: 0.0311, policy: 0.1445, weighted policy: 0.1445), Train Mean Max: 0.9393
Epoch 10/10, Train Loss: 0.1773 (value: 0.0006, weighted value: 0.0318, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9393
..training done in 66.04 seconds
..evaluation done in 16.34 seconds
Old network+MCTS average reward: 0.73, min: 0.12, max: 1.38, stdev: 0.22
New network+MCTS average reward: 0.73, min: 0.09, max: 1.38, stdev: 0.22
Old bare network average reward: 0.68, min: 0.06, max: 1.36, stdev: 0.22
New bare network average reward: 0.68, min: 0.06, max: 1.36, stdev: 0.22
External policy "random" average reward: 0.25, min: -0.36, max: 0.93, stdev: 0.20
External policy "individual greedy" average reward: 0.53, min: 0.01, max: 1.08, stdev: 0.20
External policy "total greedy" average reward: 0.65, min: 0.15, max: 1.22, stdev: 0.21
New network won 75 and tied 157 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 359 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.97 seconds
Training examples lengths: [64855, 64605, 64648, 65147, 64632, 64726, 64753, 64717, 64629, 64786]
Total value: 473571.46
Training on 647498 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2034 (value: 0.0009, weighted value: 0.0471, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9375
Epoch 2/10, Train Loss: 0.1954 (value: 0.0008, weighted value: 0.0419, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9380
Epoch 3/10, Train Loss: 0.1890 (value: 0.0008, weighted value: 0.0392, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9385
Epoch 4/10, Train Loss: 0.1857 (value: 0.0008, weighted value: 0.0380, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9386
Epoch 5/10, Train Loss: 0.1832 (value: 0.0007, weighted value: 0.0363, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9388
Epoch 6/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0349, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9390
Epoch 7/10, Train Loss: 0.1803 (value: 0.0007, weighted value: 0.0342, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9389
Epoch 8/10, Train Loss: 0.1787 (value: 0.0007, weighted value: 0.0327, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9390
Epoch 9/10, Train Loss: 0.1791 (value: 0.0007, weighted value: 0.0328, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9393
Epoch 10/10, Train Loss: 0.1765 (value: 0.0006, weighted value: 0.0311, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9394
..training done in 66.39 seconds
..evaluation done in 15.44 seconds
Old network+MCTS average reward: 0.71, min: 0.19, max: 1.31, stdev: 0.22
New network+MCTS average reward: 0.71, min: 0.19, max: 1.35, stdev: 0.22
Old bare network average reward: 0.67, min: 0.07, max: 1.31, stdev: 0.23
New bare network average reward: 0.68, min: 0.07, max: 1.31, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.38, max: 1.00, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: 0.09, max: 1.14, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.17, max: 1.32, stdev: 0.22
New network won 63 and tied 162 out of 300 games (48.00% wins where ties are half wins)
Reverting to the old network

Training iteration 360 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.50 seconds
Training examples lengths: [64605, 64648, 65147, 64632, 64726, 64753, 64717, 64629, 64786, 64820]
Total value: 473380.64
Training on 647463 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2308 (value: 0.0013, weighted value: 0.0636, policy: 0.1672, weighted policy: 0.1672), Train Mean Max: 0.9355
Epoch 2/10, Train Loss: 0.2112 (value: 0.0010, weighted value: 0.0522, policy: 0.1590, weighted policy: 0.1590), Train Mean Max: 0.9367
Epoch 3/10, Train Loss: 0.2017 (value: 0.0010, weighted value: 0.0488, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9373
Epoch 4/10, Train Loss: 0.1957 (value: 0.0009, weighted value: 0.0450, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9375
Epoch 5/10, Train Loss: 0.1925 (value: 0.0009, weighted value: 0.0432, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1914 (value: 0.0009, weighted value: 0.0426, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9377
Epoch 7/10, Train Loss: 0.1874 (value: 0.0008, weighted value: 0.0393, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1863 (value: 0.0008, weighted value: 0.0381, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1835 (value: 0.0007, weighted value: 0.0364, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9383
Epoch 10/10, Train Loss: 0.1825 (value: 0.0007, weighted value: 0.0357, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9384
..training done in 73.39 seconds
..evaluation done in 15.60 seconds
Old network+MCTS average reward: 0.73, min: 0.19, max: 1.60, stdev: 0.23
New network+MCTS average reward: 0.73, min: 0.19, max: 1.55, stdev: 0.23
Old bare network average reward: 0.69, min: 0.13, max: 1.55, stdev: 0.24
New bare network average reward: 0.69, min: 0.09, max: 1.55, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.24, max: 0.93, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: -0.07, max: 1.46, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.08, max: 1.39, stdev: 0.22
New network won 73 and tied 162 out of 300 games (51.33% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1757697740_iter_360

Training iteration 361 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.21 seconds
Training examples lengths: [64648, 65147, 64632, 64726, 64753, 64717, 64629, 64786, 64820, 64744]
Total value: 474059.12
Training on 647602 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2076 (value: 0.0010, weighted value: 0.0505, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9366
Epoch 2/10, Train Loss: 0.1989 (value: 0.0009, weighted value: 0.0452, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9373
Epoch 3/10, Train Loss: 0.1925 (value: 0.0008, weighted value: 0.0418, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9377
Epoch 4/10, Train Loss: 0.1890 (value: 0.0008, weighted value: 0.0404, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9379
Epoch 5/10, Train Loss: 0.1870 (value: 0.0008, weighted value: 0.0392, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9380
Epoch 6/10, Train Loss: 0.1849 (value: 0.0007, weighted value: 0.0369, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9381
Epoch 7/10, Train Loss: 0.1841 (value: 0.0007, weighted value: 0.0366, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9384
Epoch 8/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0348, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9385
Epoch 9/10, Train Loss: 0.1799 (value: 0.0007, weighted value: 0.0338, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9387
Epoch 10/10, Train Loss: 0.1801 (value: 0.0007, weighted value: 0.0341, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9388
..training done in 73.87 seconds
..evaluation done in 16.15 seconds
Old network+MCTS average reward: 0.74, min: 0.19, max: 1.54, stdev: 0.21
New network+MCTS average reward: 0.73, min: 0.23, max: 1.54, stdev: 0.22
Old bare network average reward: 0.70, min: 0.19, max: 1.54, stdev: 0.22
New bare network average reward: 0.70, min: 0.19, max: 1.46, stdev: 0.22
External policy "random" average reward: 0.28, min: -0.44, max: 1.06, stdev: 0.23
External policy "individual greedy" average reward: 0.56, min: -0.03, max: 1.52, stdev: 0.22
External policy "total greedy" average reward: 0.67, min: 0.07, max: 1.49, stdev: 0.21
New network won 65 and tied 154 out of 300 games (47.33% wins where ties are half wins)
Reverting to the old network

Training iteration 362 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.63 seconds
Training examples lengths: [65147, 64632, 64726, 64753, 64717, 64629, 64786, 64820, 64744, 64791]
Total value: 474203.85
Training on 647745 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2322 (value: 0.0013, weighted value: 0.0651, policy: 0.1671, weighted policy: 0.1671), Train Mean Max: 0.9349
Epoch 2/10, Train Loss: 0.2140 (value: 0.0011, weighted value: 0.0544, policy: 0.1596, weighted policy: 0.1596), Train Mean Max: 0.9360
Epoch 3/10, Train Loss: 0.2043 (value: 0.0010, weighted value: 0.0508, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9365
Epoch 4/10, Train Loss: 0.1996 (value: 0.0010, weighted value: 0.0479, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9367
Epoch 5/10, Train Loss: 0.1947 (value: 0.0009, weighted value: 0.0440, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1938 (value: 0.0009, weighted value: 0.0437, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9370
Epoch 7/10, Train Loss: 0.1902 (value: 0.0008, weighted value: 0.0409, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9372
Epoch 8/10, Train Loss: 0.1888 (value: 0.0008, weighted value: 0.0400, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1875 (value: 0.0008, weighted value: 0.0390, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1851 (value: 0.0007, weighted value: 0.0368, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9378
..training done in 66.59 seconds
..evaluation done in 16.81 seconds
Old network+MCTS average reward: 0.72, min: 0.10, max: 1.32, stdev: 0.23
New network+MCTS average reward: 0.72, min: 0.20, max: 1.33, stdev: 0.22
Old bare network average reward: 0.68, min: 0.18, max: 1.32, stdev: 0.23
New bare network average reward: 0.68, min: 0.14, max: 1.25, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.45, max: 0.93, stdev: 0.24
External policy "individual greedy" average reward: 0.53, min: -0.04, max: 1.09, stdev: 0.21
External policy "total greedy" average reward: 0.65, min: -0.02, max: 1.12, stdev: 0.21
New network won 80 and tied 143 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 363 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.23 seconds
Training examples lengths: [64632, 64726, 64753, 64717, 64629, 64786, 64820, 64744, 64791, 64586]
Total value: 473645.75
Training on 647184 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2091 (value: 0.0010, weighted value: 0.0499, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9360
Epoch 2/10, Train Loss: 0.2074 (value: 0.0009, weighted value: 0.0470, policy: 0.1604, weighted policy: 0.1604), Train Mean Max: 0.9362
Epoch 3/10, Train Loss: 0.2044 (value: 0.0009, weighted value: 0.0428, policy: 0.1616, weighted policy: 0.1616), Train Mean Max: 0.9365
Epoch 4/10, Train Loss: 0.1991 (value: 0.0009, weighted value: 0.0439, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9369
Epoch 5/10, Train Loss: 0.1970 (value: 0.0008, weighted value: 0.0390, policy: 0.1580, weighted policy: 0.1580), Train Mean Max: 0.9370
Epoch 6/10, Train Loss: 0.1923 (value: 0.0008, weighted value: 0.0385, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9373
Epoch 7/10, Train Loss: 0.1855 (value: 0.0008, weighted value: 0.0383, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1890 (value: 0.0008, weighted value: 0.0378, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1925 (value: 0.0007, weighted value: 0.0356, policy: 0.1569, weighted policy: 0.1569), Train Mean Max: 0.9374
Epoch 10/10, Train Loss: 0.1842 (value: 0.0007, weighted value: 0.0353, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9381
..training done in 65.03 seconds
..evaluation done in 15.68 seconds
Old network+MCTS average reward: 0.71, min: 0.04, max: 1.45, stdev: 0.23
New network+MCTS average reward: 0.71, min: 0.03, max: 1.45, stdev: 0.23
Old bare network average reward: 0.67, min: -0.10, max: 1.45, stdev: 0.24
New bare network average reward: 0.67, min: -0.06, max: 1.36, stdev: 0.24
External policy "random" average reward: 0.25, min: -0.77, max: 0.82, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.24, max: 1.15, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: -0.20, max: 1.37, stdev: 0.22
New network won 64 and tied 173 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 364 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.87 seconds
Training examples lengths: [64726, 64753, 64717, 64629, 64786, 64820, 64744, 64791, 64586, 64851]
Total value: 473887.81
Training on 647403 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2052 (value: 0.0010, weighted value: 0.0501, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9364
Epoch 2/10, Train Loss: 0.1963 (value: 0.0009, weighted value: 0.0438, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9369
Epoch 3/10, Train Loss: 0.1919 (value: 0.0008, weighted value: 0.0408, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9370
Epoch 4/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0402, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9373
Epoch 5/10, Train Loss: 0.1872 (value: 0.0008, weighted value: 0.0380, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9374
Epoch 6/10, Train Loss: 0.1849 (value: 0.0007, weighted value: 0.0366, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9377
Epoch 7/10, Train Loss: 0.1832 (value: 0.0007, weighted value: 0.0355, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1824 (value: 0.0007, weighted value: 0.0345, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9380
Epoch 9/10, Train Loss: 0.1808 (value: 0.0007, weighted value: 0.0331, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9382
Epoch 10/10, Train Loss: 0.1807 (value: 0.0007, weighted value: 0.0330, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9382
..training done in 65.99 seconds
..evaluation done in 15.40 seconds
Old network+MCTS average reward: 0.71, min: 0.04, max: 1.38, stdev: 0.23
New network+MCTS average reward: 0.71, min: 0.04, max: 1.38, stdev: 0.23
Old bare network average reward: 0.67, min: -0.02, max: 1.32, stdev: 0.23
New bare network average reward: 0.67, min: 0.01, max: 1.32, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.48, max: 0.98, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: -0.09, max: 1.19, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: 0.09, max: 1.25, stdev: 0.22
New network won 75 and tied 153 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 365 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.82 seconds
Training examples lengths: [64753, 64717, 64629, 64786, 64820, 64744, 64791, 64586, 64851, 64718]
Total value: 473617.60
Training on 647395 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2059 (value: 0.0009, weighted value: 0.0471, policy: 0.1587, weighted policy: 0.1587), Train Mean Max: 0.9364
Epoch 2/10, Train Loss: 0.1982 (value: 0.0008, weighted value: 0.0424, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9368
Epoch 3/10, Train Loss: 0.1925 (value: 0.0008, weighted value: 0.0405, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9375
Epoch 4/10, Train Loss: 0.1876 (value: 0.0008, weighted value: 0.0379, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9375
Epoch 5/10, Train Loss: 0.1858 (value: 0.0007, weighted value: 0.0369, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9377
Epoch 6/10, Train Loss: 0.1840 (value: 0.0007, weighted value: 0.0357, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9378
Epoch 7/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0340, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9378
Epoch 8/10, Train Loss: 0.1821 (value: 0.0007, weighted value: 0.0335, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9380
Epoch 9/10, Train Loss: 0.1804 (value: 0.0007, weighted value: 0.0326, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9382
Epoch 10/10, Train Loss: 0.1793 (value: 0.0006, weighted value: 0.0314, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9383
..training done in 63.90 seconds
..evaluation done in 16.17 seconds
Old network+MCTS average reward: 0.70, min: -0.06, max: 1.31, stdev: 0.23
New network+MCTS average reward: 0.70, min: -0.02, max: 1.31, stdev: 0.24
Old bare network average reward: 0.66, min: -0.05, max: 1.33, stdev: 0.24
New bare network average reward: 0.66, min: 0.06, max: 1.31, stdev: 0.24
External policy "random" average reward: 0.25, min: -0.52, max: 0.91, stdev: 0.23
External policy "individual greedy" average reward: 0.52, min: -0.17, max: 1.14, stdev: 0.24
External policy "total greedy" average reward: 0.64, min: 0.02, max: 1.19, stdev: 0.22
New network won 80 and tied 152 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network

Training iteration 366 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.76 seconds
Training examples lengths: [64717, 64629, 64786, 64820, 64744, 64791, 64586, 64851, 64718, 64734]
Total value: 473578.81
Training on 647376 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2071 (value: 0.0010, weighted value: 0.0477, policy: 0.1595, weighted policy: 0.1595), Train Mean Max: 0.9363
Epoch 2/10, Train Loss: 0.1977 (value: 0.0009, weighted value: 0.0429, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9370
Epoch 3/10, Train Loss: 0.1902 (value: 0.0008, weighted value: 0.0387, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9374
Epoch 4/10, Train Loss: 0.1878 (value: 0.0007, weighted value: 0.0372, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9375
Epoch 5/10, Train Loss: 0.1854 (value: 0.0007, weighted value: 0.0363, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1859 (value: 0.0007, weighted value: 0.0355, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9376
Epoch 7/10, Train Loss: 0.1818 (value: 0.0007, weighted value: 0.0333, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9378
Epoch 8/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0333, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9380
Epoch 9/10, Train Loss: 0.1803 (value: 0.0006, weighted value: 0.0320, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9380
Epoch 10/10, Train Loss: 0.1799 (value: 0.0006, weighted value: 0.0316, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9381
..training done in 68.38 seconds
..evaluation done in 16.18 seconds
Old network+MCTS average reward: 0.73, min: 0.16, max: 1.37, stdev: 0.24
New network+MCTS average reward: 0.74, min: 0.17, max: 1.37, stdev: 0.24
Old bare network average reward: 0.70, min: 0.09, max: 1.35, stdev: 0.24
New bare network average reward: 0.70, min: 0.14, max: 1.36, stdev: 0.24
External policy "random" average reward: 0.28, min: -0.39, max: 1.23, stdev: 0.23
External policy "individual greedy" average reward: 0.56, min: -0.15, max: 1.31, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.02, max: 1.38, stdev: 0.23
New network won 88 and tied 147 out of 300 games (53.83% wins where ties are half wins)
Keeping the new network

Training iteration 367 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.04 seconds
Training examples lengths: [64629, 64786, 64820, 64744, 64791, 64586, 64851, 64718, 64734, 64831]
Total value: 473443.68
Training on 647490 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2049 (value: 0.0009, weighted value: 0.0462, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9363
Epoch 2/10, Train Loss: 0.1969 (value: 0.0008, weighted value: 0.0422, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9370
Epoch 3/10, Train Loss: 0.1907 (value: 0.0008, weighted value: 0.0382, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9371
Epoch 4/10, Train Loss: 0.1876 (value: 0.0007, weighted value: 0.0374, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9374
Epoch 5/10, Train Loss: 0.1855 (value: 0.0007, weighted value: 0.0363, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9375
Epoch 6/10, Train Loss: 0.1830 (value: 0.0007, weighted value: 0.0338, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9376
Epoch 7/10, Train Loss: 0.1834 (value: 0.0007, weighted value: 0.0334, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9375
Epoch 8/10, Train Loss: 0.1822 (value: 0.0007, weighted value: 0.0333, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1799 (value: 0.0006, weighted value: 0.0316, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1786 (value: 0.0006, weighted value: 0.0308, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9381
..training done in 65.83 seconds
..evaluation done in 16.45 seconds
Old network+MCTS average reward: 0.74, min: 0.04, max: 1.45, stdev: 0.25
New network+MCTS average reward: 0.74, min: 0.00, max: 1.45, stdev: 0.25
Old bare network average reward: 0.70, min: 0.00, max: 1.45, stdev: 0.26
New bare network average reward: 0.70, min: 0.00, max: 1.45, stdev: 0.25
External policy "random" average reward: 0.27, min: -0.29, max: 0.99, stdev: 0.24
External policy "individual greedy" average reward: 0.55, min: -0.10, max: 1.37, stdev: 0.25
External policy "total greedy" average reward: 0.68, min: 0.08, max: 1.47, stdev: 0.23
New network won 65 and tied 158 out of 300 games (48.00% wins where ties are half wins)
Reverting to the old network

Training iteration 368 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.29 seconds
Training examples lengths: [64786, 64820, 64744, 64791, 64586, 64851, 64718, 64734, 64831, 64822]
Total value: 474277.81
Training on 647683 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2288 (value: 0.0012, weighted value: 0.0599, policy: 0.1689, weighted policy: 0.1689), Train Mean Max: 0.9346
Epoch 2/10, Train Loss: 0.2139 (value: 0.0010, weighted value: 0.0522, policy: 0.1616, weighted policy: 0.1616), Train Mean Max: 0.9356
Epoch 3/10, Train Loss: 0.2039 (value: 0.0010, weighted value: 0.0482, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9359
Epoch 4/10, Train Loss: 0.1983 (value: 0.0009, weighted value: 0.0442, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9362
Epoch 5/10, Train Loss: 0.1949 (value: 0.0008, weighted value: 0.0424, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9364
Epoch 6/10, Train Loss: 0.1917 (value: 0.0008, weighted value: 0.0406, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9365
Epoch 7/10, Train Loss: 0.1888 (value: 0.0008, weighted value: 0.0379, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9367
Epoch 8/10, Train Loss: 0.1882 (value: 0.0008, weighted value: 0.0379, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9368
Epoch 9/10, Train Loss: 0.1873 (value: 0.0007, weighted value: 0.0371, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9370
Epoch 10/10, Train Loss: 0.1843 (value: 0.0007, weighted value: 0.0348, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9371
..training done in 67.27 seconds
..evaluation done in 16.02 seconds
Old network+MCTS average reward: 0.75, min: 0.19, max: 1.53, stdev: 0.22
New network+MCTS average reward: 0.75, min: 0.20, max: 1.52, stdev: 0.22
Old bare network average reward: 0.71, min: 0.00, max: 1.53, stdev: 0.23
New bare network average reward: 0.71, min: 0.10, max: 1.49, stdev: 0.23
External policy "random" average reward: 0.28, min: -0.34, max: 1.08, stdev: 0.22
External policy "individual greedy" average reward: 0.56, min: 0.07, max: 1.37, stdev: 0.22
External policy "total greedy" average reward: 0.67, min: 0.13, max: 1.31, stdev: 0.21
New network won 73 and tied 149 out of 300 games (49.17% wins where ties are half wins)
Reverting to the old network

Training iteration 369 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.74 seconds
Training examples lengths: [64820, 64744, 64791, 64586, 64851, 64718, 64734, 64831, 64822, 65236]
Total value: 474415.45
Training on 648133 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2546 (value: 0.0015, weighted value: 0.0766, policy: 0.1780, weighted policy: 0.1780), Train Mean Max: 0.9329
Epoch 2/10, Train Loss: 0.2262 (value: 0.0012, weighted value: 0.0616, policy: 0.1647, weighted policy: 0.1647), Train Mean Max: 0.9345
Epoch 3/10, Train Loss: 0.2166 (value: 0.0011, weighted value: 0.0565, policy: 0.1601, weighted policy: 0.1601), Train Mean Max: 0.9347
Epoch 4/10, Train Loss: 0.2078 (value: 0.0010, weighted value: 0.0515, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9352
Epoch 5/10, Train Loss: 0.2048 (value: 0.0010, weighted value: 0.0496, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9352
Epoch 6/10, Train Loss: 0.2002 (value: 0.0009, weighted value: 0.0460, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9356
Epoch 7/10, Train Loss: 0.1966 (value: 0.0009, weighted value: 0.0435, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9357
Epoch 8/10, Train Loss: 0.1952 (value: 0.0009, weighted value: 0.0428, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9359
Epoch 9/10, Train Loss: 0.1921 (value: 0.0008, weighted value: 0.0404, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9360
Epoch 10/10, Train Loss: 0.1902 (value: 0.0008, weighted value: 0.0387, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9362
..training done in 65.53 seconds
..evaluation done in 16.21 seconds
Old network+MCTS average reward: 0.75, min: 0.11, max: 1.52, stdev: 0.25
New network+MCTS average reward: 0.75, min: 0.11, max: 1.52, stdev: 0.24
Old bare network average reward: 0.71, min: 0.11, max: 1.52, stdev: 0.25
New bare network average reward: 0.71, min: 0.11, max: 1.52, stdev: 0.25
External policy "random" average reward: 0.28, min: -0.30, max: 1.10, stdev: 0.23
External policy "individual greedy" average reward: 0.56, min: -0.02, max: 1.32, stdev: 0.24
External policy "total greedy" average reward: 0.67, min: 0.12, max: 1.35, stdev: 0.23
New network won 70 and tied 145 out of 300 games (47.50% wins where ties are half wins)
Reverting to the old network

Training iteration 370 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.81 seconds
Training examples lengths: [64744, 64791, 64586, 64851, 64718, 64734, 64831, 64822, 65236, 64914]
Total value: 475302.92
Training on 648227 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2731 (value: 0.0018, weighted value: 0.0889, policy: 0.1843, weighted policy: 0.1843), Train Mean Max: 0.9319
Epoch 2/10, Train Loss: 0.2439 (value: 0.0015, weighted value: 0.0729, policy: 0.1710, weighted policy: 0.1710), Train Mean Max: 0.9332
Epoch 3/10, Train Loss: 0.2290 (value: 0.0013, weighted value: 0.0651, policy: 0.1640, weighted policy: 0.1640), Train Mean Max: 0.9337
Epoch 4/10, Train Loss: 0.2184 (value: 0.0012, weighted value: 0.0584, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9341
Epoch 5/10, Train Loss: 0.2114 (value: 0.0011, weighted value: 0.0542, policy: 0.1572, weighted policy: 0.1572), Train Mean Max: 0.9343
Epoch 6/10, Train Loss: 0.2076 (value: 0.0010, weighted value: 0.0518, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9345
Epoch 7/10, Train Loss: 0.2028 (value: 0.0010, weighted value: 0.0481, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9347
Epoch 8/10, Train Loss: 0.2008 (value: 0.0009, weighted value: 0.0463, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9350
Epoch 9/10, Train Loss: 0.1985 (value: 0.0009, weighted value: 0.0448, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9353
Epoch 10/10, Train Loss: 0.1958 (value: 0.0009, weighted value: 0.0429, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9355
..training done in 66.45 seconds
..evaluation done in 16.59 seconds
Old network+MCTS average reward: 0.73, min: 0.19, max: 1.48, stdev: 0.24
New network+MCTS average reward: 0.73, min: 0.19, max: 1.43, stdev: 0.23
Old bare network average reward: 0.69, min: 0.15, max: 1.44, stdev: 0.24
New bare network average reward: 0.69, min: 0.11, max: 1.37, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.36, max: 0.98, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: -0.03, max: 1.22, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.08, max: 1.24, stdev: 0.23
New network won 72 and tied 156 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 371 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.37 seconds
Training examples lengths: [64791, 64586, 64851, 64718, 64734, 64831, 64822, 65236, 64914, 64558]
Total value: 475465.34
Training on 648041 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2157 (value: 0.0011, weighted value: 0.0543, policy: 0.1614, weighted policy: 0.1614), Train Mean Max: 0.9341
Epoch 2/10, Train Loss: 0.2100 (value: 0.0011, weighted value: 0.0528, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9347
Epoch 3/10, Train Loss: 0.2016 (value: 0.0009, weighted value: 0.0461, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9350
Epoch 4/10, Train Loss: 0.1978 (value: 0.0009, weighted value: 0.0438, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9354
Epoch 5/10, Train Loss: 0.1964 (value: 0.0009, weighted value: 0.0439, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9355
Epoch 6/10, Train Loss: 0.1933 (value: 0.0008, weighted value: 0.0406, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9358
Epoch 7/10, Train Loss: 0.1918 (value: 0.0008, weighted value: 0.0393, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9360
Epoch 8/10, Train Loss: 0.1897 (value: 0.0008, weighted value: 0.0386, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9362
Epoch 9/10, Train Loss: 0.1882 (value: 0.0008, weighted value: 0.0376, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9363
Epoch 10/10, Train Loss: 0.1870 (value: 0.0007, weighted value: 0.0368, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9366
..training done in 66.19 seconds
..evaluation done in 16.37 seconds
Old network+MCTS average reward: 0.72, min: 0.14, max: 1.43, stdev: 0.24
New network+MCTS average reward: 0.72, min: 0.11, max: 1.43, stdev: 0.24
Old bare network average reward: 0.68, min: 0.06, max: 1.41, stdev: 0.24
New bare network average reward: 0.68, min: 0.11, max: 1.43, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.44, max: 0.96, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: -0.06, max: 1.10, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.05, max: 1.27, stdev: 0.23
New network won 69 and tied 164 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 372 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.12 seconds
Training examples lengths: [64586, 64851, 64718, 64734, 64831, 64822, 65236, 64914, 64558, 64701]
Total value: 475900.52
Training on 647951 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2101 (value: 0.0010, weighted value: 0.0500, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9349
Epoch 2/10, Train Loss: 0.2002 (value: 0.0009, weighted value: 0.0438, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9357
Epoch 3/10, Train Loss: 0.1966 (value: 0.0009, weighted value: 0.0428, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9360
Epoch 4/10, Train Loss: 0.1929 (value: 0.0008, weighted value: 0.0406, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9361
Epoch 5/10, Train Loss: 0.1897 (value: 0.0008, weighted value: 0.0385, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9364
Epoch 6/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0377, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9366
Epoch 7/10, Train Loss: 0.1870 (value: 0.0007, weighted value: 0.0367, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9367
Epoch 8/10, Train Loss: 0.1853 (value: 0.0007, weighted value: 0.0353, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9369
Epoch 9/10, Train Loss: 0.1836 (value: 0.0007, weighted value: 0.0342, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9371
Epoch 10/10, Train Loss: 0.1822 (value: 0.0007, weighted value: 0.0337, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9372
..training done in 71.86 seconds
..evaluation done in 16.48 seconds
Old network+MCTS average reward: 0.72, min: 0.00, max: 1.64, stdev: 0.24
New network+MCTS average reward: 0.72, min: 0.00, max: 1.67, stdev: 0.24
Old bare network average reward: 0.68, min: -0.03, max: 1.58, stdev: 0.24
New bare network average reward: 0.68, min: -0.06, max: 1.58, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.31, max: 1.19, stdev: 0.21
External policy "individual greedy" average reward: 0.53, min: -0.07, max: 1.45, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: 0.06, max: 1.49, stdev: 0.23
New network won 55 and tied 177 out of 300 games (47.83% wins where ties are half wins)
Reverting to the old network

Training iteration 373 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.37 seconds
Training examples lengths: [64851, 64718, 64734, 64831, 64822, 65236, 64914, 64558, 64701, 64882]
Total value: 476431.26
Training on 648247 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2323 (value: 0.0013, weighted value: 0.0637, policy: 0.1686, weighted policy: 0.1686), Train Mean Max: 0.9335
Epoch 2/10, Train Loss: 0.2167 (value: 0.0011, weighted value: 0.0552, policy: 0.1615, weighted policy: 0.1615), Train Mean Max: 0.9345
Epoch 3/10, Train Loss: 0.2068 (value: 0.0010, weighted value: 0.0502, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9349
Epoch 4/10, Train Loss: 0.2022 (value: 0.0010, weighted value: 0.0481, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9351
Epoch 5/10, Train Loss: 0.1973 (value: 0.0009, weighted value: 0.0445, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9355
Epoch 6/10, Train Loss: 0.1962 (value: 0.0009, weighted value: 0.0432, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9356
Epoch 7/10, Train Loss: 0.1950 (value: 0.0009, weighted value: 0.0427, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9358
Epoch 8/10, Train Loss: 0.1899 (value: 0.0008, weighted value: 0.0386, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9362
Epoch 9/10, Train Loss: 0.1898 (value: 0.0008, weighted value: 0.0389, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9363
Epoch 10/10, Train Loss: 0.1873 (value: 0.0007, weighted value: 0.0369, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9364
..training done in 72.11 seconds
..evaluation done in 15.83 seconds
Old network+MCTS average reward: 0.72, min: 0.06, max: 1.65, stdev: 0.25
New network+MCTS average reward: 0.72, min: 0.06, max: 1.65, stdev: 0.25
Old bare network average reward: 0.68, min: -0.06, max: 1.65, stdev: 0.26
New bare network average reward: 0.68, min: -0.06, max: 1.65, stdev: 0.25
External policy "random" average reward: 0.26, min: -0.38, max: 1.28, stdev: 0.24
External policy "individual greedy" average reward: 0.53, min: 0.02, max: 1.48, stdev: 0.24
External policy "total greedy" average reward: 0.64, min: 0.04, max: 1.47, stdev: 0.24
New network won 56 and tied 171 out of 300 games (47.17% wins where ties are half wins)
Reverting to the old network

Training iteration 374 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.45 seconds
Training examples lengths: [64718, 64734, 64831, 64822, 65236, 64914, 64558, 64701, 64882, 64776]
Total value: 476334.71
Training on 648172 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2531 (value: 0.0015, weighted value: 0.0769, policy: 0.1762, weighted policy: 0.1762), Train Mean Max: 0.9319
Epoch 2/10, Train Loss: 0.2298 (value: 0.0013, weighted value: 0.0636, policy: 0.1661, weighted policy: 0.1661), Train Mean Max: 0.9332
Epoch 3/10, Train Loss: 0.2193 (value: 0.0012, weighted value: 0.0589, policy: 0.1604, weighted policy: 0.1604), Train Mean Max: 0.9337
Epoch 4/10, Train Loss: 0.2116 (value: 0.0011, weighted value: 0.0537, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9341
Epoch 5/10, Train Loss: 0.2063 (value: 0.0010, weighted value: 0.0506, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9344
Epoch 6/10, Train Loss: 0.2031 (value: 0.0010, weighted value: 0.0483, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9346
Epoch 7/10, Train Loss: 0.1998 (value: 0.0009, weighted value: 0.0462, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9349
Epoch 8/10, Train Loss: 0.1967 (value: 0.0009, weighted value: 0.0433, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9351
Epoch 9/10, Train Loss: 0.1945 (value: 0.0008, weighted value: 0.0422, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9354
Epoch 10/10, Train Loss: 0.1922 (value: 0.0008, weighted value: 0.0410, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9356
..training done in 73.18 seconds
..evaluation done in 15.96 seconds
Old network+MCTS average reward: 0.70, min: 0.20, max: 1.51, stdev: 0.25
New network+MCTS average reward: 0.71, min: 0.20, max: 1.51, stdev: 0.24
Old bare network average reward: 0.66, min: 0.13, max: 1.56, stdev: 0.25
New bare network average reward: 0.67, min: 0.13, max: 1.51, stdev: 0.25
External policy "random" average reward: 0.24, min: -0.29, max: 1.02, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: -0.05, max: 1.33, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.17, max: 1.31, stdev: 0.22
New network won 81 and tied 148 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 375 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.20 seconds
Training examples lengths: [64734, 64831, 64822, 65236, 64914, 64558, 64701, 64882, 64776, 64970]
Total value: 477362.15
Training on 648424 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2147 (value: 0.0011, weighted value: 0.0539, policy: 0.1608, weighted policy: 0.1608), Train Mean Max: 0.9344
Epoch 2/10, Train Loss: 0.2055 (value: 0.0010, weighted value: 0.0484, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9350
Epoch 3/10, Train Loss: 0.2005 (value: 0.0009, weighted value: 0.0461, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9354
Epoch 4/10, Train Loss: 0.1968 (value: 0.0009, weighted value: 0.0434, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9356
Epoch 5/10, Train Loss: 0.1930 (value: 0.0008, weighted value: 0.0410, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9359
Epoch 6/10, Train Loss: 0.1910 (value: 0.0008, weighted value: 0.0402, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9360
Epoch 7/10, Train Loss: 0.1885 (value: 0.0008, weighted value: 0.0378, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9363
Epoch 8/10, Train Loss: 0.1881 (value: 0.0008, weighted value: 0.0377, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9363
Epoch 9/10, Train Loss: 0.1864 (value: 0.0007, weighted value: 0.0363, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9367
Epoch 10/10, Train Loss: 0.1857 (value: 0.0007, weighted value: 0.0359, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9368
..training done in 72.85 seconds
..evaluation done in 16.03 seconds
Old network+MCTS average reward: 0.73, min: 0.10, max: 1.37, stdev: 0.23
New network+MCTS average reward: 0.74, min: 0.17, max: 1.42, stdev: 0.23
Old bare network average reward: 0.69, min: 0.10, max: 1.38, stdev: 0.24
New bare network average reward: 0.69, min: 0.07, max: 1.36, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.34, max: 0.94, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.08, max: 1.23, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.01, max: 1.43, stdev: 0.22
New network won 90 and tied 159 out of 300 games (56.50% wins where ties are half wins)
Keeping the new network

Training iteration 376 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.05 seconds
Training examples lengths: [64831, 64822, 65236, 64914, 64558, 64701, 64882, 64776, 64970, 65024]
Total value: 477624.76
Training on 648714 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2072 (value: 0.0010, weighted value: 0.0488, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9355
Epoch 2/10, Train Loss: 0.1983 (value: 0.0009, weighted value: 0.0440, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9362
Epoch 3/10, Train Loss: 0.1947 (value: 0.0008, weighted value: 0.0422, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9366
Epoch 4/10, Train Loss: 0.1907 (value: 0.0008, weighted value: 0.0398, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9367
Epoch 5/10, Train Loss: 0.1863 (value: 0.0007, weighted value: 0.0375, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9370
Epoch 6/10, Train Loss: 0.1859 (value: 0.0007, weighted value: 0.0368, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1839 (value: 0.0007, weighted value: 0.0348, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9372
Epoch 8/10, Train Loss: 0.1839 (value: 0.0007, weighted value: 0.0350, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1820 (value: 0.0007, weighted value: 0.0335, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1807 (value: 0.0007, weighted value: 0.0329, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9377
..training done in 71.75 seconds
..evaluation done in 16.22 seconds
Old network+MCTS average reward: 0.71, min: -0.04, max: 1.42, stdev: 0.26
New network+MCTS average reward: 0.70, min: -0.02, max: 1.42, stdev: 0.25
Old bare network average reward: 0.67, min: -0.05, max: 1.42, stdev: 0.25
New bare network average reward: 0.67, min: -0.05, max: 1.42, stdev: 0.26
External policy "random" average reward: 0.26, min: -0.32, max: 0.88, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: -0.05, max: 1.19, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.05, max: 1.29, stdev: 0.23
New network won 51 and tied 189 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 377 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.32 seconds
Training examples lengths: [64822, 65236, 64914, 64558, 64701, 64882, 64776, 64970, 65024, 64556]
Total value: 477384.44
Training on 648439 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2284 (value: 0.0013, weighted value: 0.0628, policy: 0.1655, weighted policy: 0.1655), Train Mean Max: 0.9341
Epoch 2/10, Train Loss: 0.2124 (value: 0.0011, weighted value: 0.0534, policy: 0.1590, weighted policy: 0.1590), Train Mean Max: 0.9352
Epoch 3/10, Train Loss: 0.2050 (value: 0.0010, weighted value: 0.0502, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9357
Epoch 4/10, Train Loss: 0.1992 (value: 0.0009, weighted value: 0.0465, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9360
Epoch 5/10, Train Loss: 0.1951 (value: 0.0009, weighted value: 0.0439, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9362
Epoch 6/10, Train Loss: 0.1916 (value: 0.0008, weighted value: 0.0416, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9365
Epoch 7/10, Train Loss: 0.1910 (value: 0.0008, weighted value: 0.0411, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9367
Epoch 8/10, Train Loss: 0.1885 (value: 0.0008, weighted value: 0.0392, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9369
Epoch 9/10, Train Loss: 0.1860 (value: 0.0007, weighted value: 0.0372, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9371
Epoch 10/10, Train Loss: 0.1850 (value: 0.0007, weighted value: 0.0361, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9372
..training done in 67.49 seconds
..evaluation done in 15.26 seconds
Old network+MCTS average reward: 0.74, min: 0.19, max: 1.39, stdev: 0.24
New network+MCTS average reward: 0.74, min: 0.12, max: 1.42, stdev: 0.24
Old bare network average reward: 0.70, min: 0.06, max: 1.39, stdev: 0.25
New bare network average reward: 0.70, min: 0.12, max: 1.37, stdev: 0.25
External policy "random" average reward: 0.28, min: -0.34, max: 1.21, stdev: 0.24
External policy "individual greedy" average reward: 0.57, min: -0.07, max: 1.31, stdev: 0.23
External policy "total greedy" average reward: 0.68, min: 0.18, max: 1.38, stdev: 0.22
New network won 58 and tied 178 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 378 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.96 seconds
Training examples lengths: [65236, 64914, 64558, 64701, 64882, 64776, 64970, 65024, 64556, 64642]
Total value: 477486.56
Training on 648259 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2492 (value: 0.0015, weighted value: 0.0767, policy: 0.1725, weighted policy: 0.1725), Train Mean Max: 0.9330
Epoch 2/10, Train Loss: 0.2262 (value: 0.0013, weighted value: 0.0631, policy: 0.1631, weighted policy: 0.1631), Train Mean Max: 0.9343
Epoch 3/10, Train Loss: 0.2155 (value: 0.0012, weighted value: 0.0575, policy: 0.1580, weighted policy: 0.1580), Train Mean Max: 0.9349
Epoch 4/10, Train Loss: 0.2089 (value: 0.0011, weighted value: 0.0544, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9353
Epoch 5/10, Train Loss: 0.2032 (value: 0.0010, weighted value: 0.0504, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9356
Epoch 6/10, Train Loss: 0.2001 (value: 0.0010, weighted value: 0.0477, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9357
Epoch 7/10, Train Loss: 0.1966 (value: 0.0009, weighted value: 0.0454, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9361
Epoch 8/10, Train Loss: 0.1935 (value: 0.0009, weighted value: 0.0434, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9364
Epoch 9/10, Train Loss: 0.1907 (value: 0.0008, weighted value: 0.0408, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9366
Epoch 10/10, Train Loss: 0.1898 (value: 0.0008, weighted value: 0.0406, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9368
..training done in 71.14 seconds
..evaluation done in 16.64 seconds
Old network+MCTS average reward: 0.74, min: 0.22, max: 1.91, stdev: 0.23
New network+MCTS average reward: 0.74, min: 0.16, max: 1.89, stdev: 0.23
Old bare network average reward: 0.70, min: 0.12, max: 1.91, stdev: 0.24
New bare network average reward: 0.70, min: 0.17, max: 1.89, stdev: 0.24
External policy "random" average reward: 0.28, min: -0.29, max: 1.05, stdev: 0.23
External policy "individual greedy" average reward: 0.56, min: -0.13, max: 1.78, stdev: 0.24
External policy "total greedy" average reward: 0.68, min: 0.07, max: 1.72, stdev: 0.24
New network won 65 and tied 161 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 379 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.34 seconds
Training examples lengths: [64914, 64558, 64701, 64882, 64776, 64970, 65024, 64556, 64642, 64567]
Total value: 477289.40
Training on 647590 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2678 (value: 0.0018, weighted value: 0.0892, policy: 0.1786, weighted policy: 0.1786), Train Mean Max: 0.9317
Epoch 2/10, Train Loss: 0.2385 (value: 0.0014, weighted value: 0.0717, policy: 0.1668, weighted policy: 0.1668), Train Mean Max: 0.9332
Epoch 3/10, Train Loss: 0.2268 (value: 0.0013, weighted value: 0.0658, policy: 0.1609, weighted policy: 0.1609), Train Mean Max: 0.9338
Epoch 4/10, Train Loss: 0.2170 (value: 0.0012, weighted value: 0.0594, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9343
Epoch 5/10, Train Loss: 0.2103 (value: 0.0011, weighted value: 0.0553, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9348
Epoch 6/10, Train Loss: 0.2061 (value: 0.0010, weighted value: 0.0525, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9350
Epoch 7/10, Train Loss: 0.2029 (value: 0.0010, weighted value: 0.0501, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9353
Epoch 8/10, Train Loss: 0.1972 (value: 0.0009, weighted value: 0.0457, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9358
Epoch 9/10, Train Loss: 0.1969 (value: 0.0009, weighted value: 0.0460, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9360
Epoch 10/10, Train Loss: 0.1931 (value: 0.0008, weighted value: 0.0425, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9362
..training done in 72.36 seconds
..evaluation done in 16.33 seconds
Old network+MCTS average reward: 0.71, min: 0.09, max: 1.54, stdev: 0.24
New network+MCTS average reward: 0.71, min: 0.13, max: 1.41, stdev: 0.24
Old bare network average reward: 0.67, min: 0.09, max: 1.36, stdev: 0.24
New bare network average reward: 0.67, min: 0.08, max: 1.41, stdev: 0.23
External policy "random" average reward: 0.24, min: -0.35, max: 0.94, stdev: 0.23
External policy "individual greedy" average reward: 0.52, min: -0.10, max: 1.14, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.10, max: 1.32, stdev: 0.23
New network won 78 and tied 156 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network

Training iteration 380 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 48.15 seconds
Training examples lengths: [64558, 64701, 64882, 64776, 64970, 65024, 64556, 64642, 64567, 65012]
Total value: 476984.26
Training on 647688 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2141 (value: 0.0011, weighted value: 0.0551, policy: 0.1590, weighted policy: 0.1590), Train Mean Max: 0.9348
Epoch 2/10, Train Loss: 0.2057 (value: 0.0010, weighted value: 0.0507, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9356
Epoch 3/10, Train Loss: 0.1995 (value: 0.0009, weighted value: 0.0467, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9360
Epoch 4/10, Train Loss: 0.1960 (value: 0.0009, weighted value: 0.0447, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9362
Epoch 5/10, Train Loss: 0.1926 (value: 0.0009, weighted value: 0.0425, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9365
Epoch 6/10, Train Loss: 0.1908 (value: 0.0008, weighted value: 0.0403, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9367
Epoch 7/10, Train Loss: 0.1902 (value: 0.0008, weighted value: 0.0410, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9370
Epoch 8/10, Train Loss: 0.1867 (value: 0.0008, weighted value: 0.0381, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9372
Epoch 9/10, Train Loss: 0.1846 (value: 0.0007, weighted value: 0.0366, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9375
Epoch 10/10, Train Loss: 0.1842 (value: 0.0007, weighted value: 0.0362, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9376
..training done in 67.58 seconds
..evaluation done in 15.97 seconds
Old network+MCTS average reward: 0.74, min: 0.01, max: 1.39, stdev: 0.24
New network+MCTS average reward: 0.74, min: -0.01, max: 1.39, stdev: 0.24
Old bare network average reward: 0.71, min: -0.14, max: 1.39, stdev: 0.24
New bare network average reward: 0.70, min: -0.19, max: 1.39, stdev: 0.25
External policy "random" average reward: 0.28, min: -0.17, max: 0.95, stdev: 0.23
External policy "individual greedy" average reward: 0.57, min: -0.05, max: 1.23, stdev: 0.24
External policy "total greedy" average reward: 0.67, min: 0.13, max: 1.37, stdev: 0.23
New network won 70 and tied 154 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1757697740_iter_380

Training iteration 381 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.93 seconds
Training examples lengths: [64701, 64882, 64776, 64970, 65024, 64556, 64642, 64567, 65012, 64846]
Total value: 476775.39
Training on 647976 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2350 (value: 0.0014, weighted value: 0.0684, policy: 0.1666, weighted policy: 0.1666), Train Mean Max: 0.9333
Epoch 2/10, Train Loss: 0.2201 (value: 0.0012, weighted value: 0.0599, policy: 0.1602, weighted policy: 0.1602), Train Mean Max: 0.9344
Epoch 3/10, Train Loss: 0.2103 (value: 0.0011, weighted value: 0.0539, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9348
Epoch 4/10, Train Loss: 0.2053 (value: 0.0010, weighted value: 0.0512, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9352
Epoch 5/10, Train Loss: 0.2006 (value: 0.0010, weighted value: 0.0477, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9355
Epoch 6/10, Train Loss: 0.1976 (value: 0.0009, weighted value: 0.0463, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9360
Epoch 7/10, Train Loss: 0.1946 (value: 0.0009, weighted value: 0.0445, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9362
Epoch 8/10, Train Loss: 0.1917 (value: 0.0008, weighted value: 0.0413, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9363
Epoch 9/10, Train Loss: 0.1904 (value: 0.0008, weighted value: 0.0416, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9367
Epoch 10/10, Train Loss: 0.1883 (value: 0.0008, weighted value: 0.0385, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9367
..training done in 67.66 seconds
..evaluation done in 15.92 seconds
Old network+MCTS average reward: 0.71, min: 0.13, max: 1.47, stdev: 0.24
New network+MCTS average reward: 0.72, min: 0.11, max: 1.47, stdev: 0.24
Old bare network average reward: 0.68, min: 0.04, max: 1.47, stdev: 0.25
New bare network average reward: 0.68, min: 0.02, max: 1.47, stdev: 0.25
External policy "random" average reward: 0.26, min: -0.40, max: 0.91, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: -0.11, max: 1.18, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.05, max: 1.29, stdev: 0.22
New network won 72 and tied 148 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 382 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.09 seconds
Training examples lengths: [64882, 64776, 64970, 65024, 64556, 64642, 64567, 65012, 64846, 64997]
Total value: 476911.34
Training on 648272 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2549 (value: 0.0016, weighted value: 0.0806, policy: 0.1744, weighted policy: 0.1744), Train Mean Max: 0.9320
Epoch 2/10, Train Loss: 0.2355 (value: 0.0014, weighted value: 0.0697, policy: 0.1659, weighted policy: 0.1659), Train Mean Max: 0.9331
Epoch 3/10, Train Loss: 0.2207 (value: 0.0012, weighted value: 0.0613, policy: 0.1595, weighted policy: 0.1595), Train Mean Max: 0.9338
Epoch 4/10, Train Loss: 0.2131 (value: 0.0011, weighted value: 0.0571, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9342
Epoch 5/10, Train Loss: 0.2083 (value: 0.0011, weighted value: 0.0535, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9346
Epoch 6/10, Train Loss: 0.2040 (value: 0.0010, weighted value: 0.0504, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9349
Epoch 7/10, Train Loss: 0.2002 (value: 0.0010, weighted value: 0.0475, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9352
Epoch 8/10, Train Loss: 0.1980 (value: 0.0009, weighted value: 0.0463, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9356
Epoch 9/10, Train Loss: 0.1946 (value: 0.0009, weighted value: 0.0441, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9358
Epoch 10/10, Train Loss: 0.1928 (value: 0.0008, weighted value: 0.0420, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9360
..training done in 67.04 seconds
..evaluation done in 16.02 seconds
Old network+MCTS average reward: 0.74, min: 0.23, max: 1.60, stdev: 0.23
New network+MCTS average reward: 0.74, min: 0.19, max: 1.83, stdev: 0.24
Old bare network average reward: 0.70, min: 0.13, max: 1.51, stdev: 0.24
New bare network average reward: 0.70, min: 0.17, max: 1.83, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.46, max: 0.91, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: 0.01, max: 1.59, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.16, max: 1.70, stdev: 0.22
New network won 87 and tied 150 out of 300 games (54.00% wins where ties are half wins)
Keeping the new network

Training iteration 383 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.33 seconds
Training examples lengths: [64776, 64970, 65024, 64556, 64642, 64567, 65012, 64846, 64997, 64892]
Total value: 476930.18
Training on 648282 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2154 (value: 0.0011, weighted value: 0.0549, policy: 0.1605, weighted policy: 0.1605), Train Mean Max: 0.9347
Epoch 2/10, Train Loss: 0.2055 (value: 0.0010, weighted value: 0.0493, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9353
Epoch 3/10, Train Loss: 0.2002 (value: 0.0009, weighted value: 0.0469, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9357
Epoch 4/10, Train Loss: 0.1947 (value: 0.0009, weighted value: 0.0432, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9360
Epoch 5/10, Train Loss: 0.1925 (value: 0.0008, weighted value: 0.0418, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9361
Epoch 6/10, Train Loss: 0.1906 (value: 0.0008, weighted value: 0.0406, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9366
Epoch 7/10, Train Loss: 0.1889 (value: 0.0008, weighted value: 0.0394, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9367
Epoch 8/10, Train Loss: 0.1875 (value: 0.0008, weighted value: 0.0379, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9369
Epoch 9/10, Train Loss: 0.1860 (value: 0.0007, weighted value: 0.0370, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9371
Epoch 10/10, Train Loss: 0.1850 (value: 0.0007, weighted value: 0.0365, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9373
..training done in 67.75 seconds
..evaluation done in 16.20 seconds
Old network+MCTS average reward: 0.75, min: 0.21, max: 1.45, stdev: 0.24
New network+MCTS average reward: 0.75, min: 0.21, max: 1.52, stdev: 0.24
Old bare network average reward: 0.72, min: 0.19, max: 1.50, stdev: 0.24
New bare network average reward: 0.71, min: 0.13, max: 1.42, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.25, max: 0.97, stdev: 0.22
External policy "individual greedy" average reward: 0.56, min: -0.07, max: 1.36, stdev: 0.24
External policy "total greedy" average reward: 0.68, min: 0.20, max: 1.34, stdev: 0.22
New network won 68 and tied 172 out of 300 games (51.33% wins where ties are half wins)
Keeping the new network

Training iteration 384 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.73 seconds
Training examples lengths: [64970, 65024, 64556, 64642, 64567, 65012, 64846, 64997, 64892, 64708]
Total value: 477486.59
Training on 648214 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2070 (value: 0.0010, weighted value: 0.0489, policy: 0.1581, weighted policy: 0.1581), Train Mean Max: 0.9357
Epoch 2/10, Train Loss: 0.1990 (value: 0.0009, weighted value: 0.0449, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9364
Epoch 3/10, Train Loss: 0.1930 (value: 0.0008, weighted value: 0.0421, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9369
Epoch 4/10, Train Loss: 0.1894 (value: 0.0008, weighted value: 0.0399, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9371
Epoch 5/10, Train Loss: 0.1868 (value: 0.0008, weighted value: 0.0380, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9373
Epoch 6/10, Train Loss: 0.1851 (value: 0.0007, weighted value: 0.0373, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9375
Epoch 7/10, Train Loss: 0.1850 (value: 0.0007, weighted value: 0.0366, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9376
Epoch 8/10, Train Loss: 0.1825 (value: 0.0007, weighted value: 0.0344, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9376
Epoch 9/10, Train Loss: 0.1834 (value: 0.0007, weighted value: 0.0355, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1798 (value: 0.0007, weighted value: 0.0330, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9382
..training done in 67.11 seconds
..evaluation done in 15.70 seconds
Old network+MCTS average reward: 0.72, min: 0.13, max: 1.44, stdev: 0.23
New network+MCTS average reward: 0.72, min: 0.13, max: 1.46, stdev: 0.23
Old bare network average reward: 0.68, min: 0.06, max: 1.48, stdev: 0.23
New bare network average reward: 0.68, min: 0.09, max: 1.37, stdev: 0.22
External policy "random" average reward: 0.26, min: -0.37, max: 0.93, stdev: 0.21
External policy "individual greedy" average reward: 0.53, min: -0.08, max: 1.24, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.10, max: 1.29, stdev: 0.21
New network won 70 and tied 161 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 385 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.94 seconds
Training examples lengths: [65024, 64556, 64642, 64567, 65012, 64846, 64997, 64892, 64708, 64907]
Total value: 478043.94
Training on 648151 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2047 (value: 0.0010, weighted value: 0.0485, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9366
Epoch 2/10, Train Loss: 0.1950 (value: 0.0008, weighted value: 0.0424, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9374
Epoch 3/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0401, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9377
Epoch 4/10, Train Loss: 0.1871 (value: 0.0008, weighted value: 0.0386, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9380
Epoch 5/10, Train Loss: 0.1848 (value: 0.0008, weighted value: 0.0377, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9381
Epoch 6/10, Train Loss: 0.1826 (value: 0.0007, weighted value: 0.0361, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9383
Epoch 7/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0344, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9385
Epoch 8/10, Train Loss: 0.1802 (value: 0.0007, weighted value: 0.0344, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9387
Epoch 9/10, Train Loss: 0.1796 (value: 0.0007, weighted value: 0.0335, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9387
Epoch 10/10, Train Loss: 0.1780 (value: 0.0006, weighted value: 0.0318, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9388
..training done in 67.38 seconds
..evaluation done in 16.08 seconds
Old network+MCTS average reward: 0.72, min: 0.06, max: 1.47, stdev: 0.23
New network+MCTS average reward: 0.72, min: 0.06, max: 1.49, stdev: 0.24
Old bare network average reward: 0.68, min: 0.05, max: 1.37, stdev: 0.24
New bare network average reward: 0.68, min: 0.06, max: 1.47, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.33, max: 0.96, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: -0.08, max: 1.15, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.13, max: 1.19, stdev: 0.22
New network won 59 and tied 164 out of 300 games (47.00% wins where ties are half wins)
Reverting to the old network

Training iteration 386 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.83 seconds
Training examples lengths: [64556, 64642, 64567, 65012, 64846, 64997, 64892, 64708, 64907, 64658]
Total value: 477517.29
Training on 647785 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2282 (value: 0.0012, weighted value: 0.0617, policy: 0.1665, weighted policy: 0.1665), Train Mean Max: 0.9349
Epoch 2/10, Train Loss: 0.2119 (value: 0.0011, weighted value: 0.0528, policy: 0.1591, weighted policy: 0.1591), Train Mean Max: 0.9361
Epoch 3/10, Train Loss: 0.2028 (value: 0.0010, weighted value: 0.0492, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9365
Epoch 4/10, Train Loss: 0.1969 (value: 0.0009, weighted value: 0.0452, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9368
Epoch 5/10, Train Loss: 0.1946 (value: 0.0009, weighted value: 0.0442, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9370
Epoch 6/10, Train Loss: 0.1915 (value: 0.0008, weighted value: 0.0417, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9372
Epoch 7/10, Train Loss: 0.1881 (value: 0.0008, weighted value: 0.0392, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9375
Epoch 8/10, Train Loss: 0.1859 (value: 0.0008, weighted value: 0.0378, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9377
Epoch 9/10, Train Loss: 0.1864 (value: 0.0008, weighted value: 0.0380, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1839 (value: 0.0007, weighted value: 0.0360, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9379
..training done in 70.18 seconds
..evaluation done in 15.89 seconds
Old network+MCTS average reward: 0.72, min: 0.13, max: 1.44, stdev: 0.24
New network+MCTS average reward: 0.72, min: 0.16, max: 1.45, stdev: 0.24
Old bare network average reward: 0.69, min: 0.13, max: 1.42, stdev: 0.24
New bare network average reward: 0.69, min: 0.08, max: 1.42, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.37, max: 0.81, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.05, max: 1.30, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.05, max: 1.29, stdev: 0.23
New network won 78 and tied 154 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 387 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.84 seconds
Training examples lengths: [64642, 64567, 65012, 64846, 64997, 64892, 64708, 64907, 64658, 64679]
Total value: 478322.33
Training on 647908 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2066 (value: 0.0010, weighted value: 0.0486, policy: 0.1580, weighted policy: 0.1580), Train Mean Max: 0.9363
Epoch 2/10, Train Loss: 0.1969 (value: 0.0009, weighted value: 0.0440, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9370
Epoch 3/10, Train Loss: 0.1938 (value: 0.0008, weighted value: 0.0423, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9372
Epoch 4/10, Train Loss: 0.1893 (value: 0.0008, weighted value: 0.0395, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1860 (value: 0.0008, weighted value: 0.0376, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9378
Epoch 6/10, Train Loss: 0.1854 (value: 0.0008, weighted value: 0.0375, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9379
Epoch 7/10, Train Loss: 0.1837 (value: 0.0007, weighted value: 0.0355, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9381
Epoch 8/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0343, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1811 (value: 0.0007, weighted value: 0.0341, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1796 (value: 0.0007, weighted value: 0.0327, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9385
..training done in 67.88 seconds
..evaluation done in 16.05 seconds
Old network+MCTS average reward: 0.71, min: 0.06, max: 1.44, stdev: 0.24
New network+MCTS average reward: 0.71, min: -0.09, max: 1.44, stdev: 0.23
Old bare network average reward: 0.67, min: -0.03, max: 1.43, stdev: 0.24
New bare network average reward: 0.67, min: -0.10, max: 1.44, stdev: 0.24
External policy "random" average reward: 0.25, min: -0.27, max: 1.13, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: 0.00, max: 1.29, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.06, max: 1.40, stdev: 0.23
New network won 73 and tied 159 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 388 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.35 seconds
Training examples lengths: [64567, 65012, 64846, 64997, 64892, 64708, 64907, 64658, 64679, 65067]
Total value: 478906.75
Training on 648333 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2047 (value: 0.0009, weighted value: 0.0472, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9367
Epoch 2/10, Train Loss: 0.1949 (value: 0.0008, weighted value: 0.0417, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9375
Epoch 3/10, Train Loss: 0.1899 (value: 0.0008, weighted value: 0.0396, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9377
Epoch 4/10, Train Loss: 0.1856 (value: 0.0007, weighted value: 0.0374, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9380
Epoch 5/10, Train Loss: 0.1848 (value: 0.0007, weighted value: 0.0366, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9381
Epoch 6/10, Train Loss: 0.1826 (value: 0.0007, weighted value: 0.0348, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9383
Epoch 7/10, Train Loss: 0.1832 (value: 0.0007, weighted value: 0.0356, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9383
Epoch 8/10, Train Loss: 0.1792 (value: 0.0007, weighted value: 0.0325, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9385
Epoch 9/10, Train Loss: 0.1781 (value: 0.0006, weighted value: 0.0323, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9387
Epoch 10/10, Train Loss: 0.1784 (value: 0.0006, weighted value: 0.0315, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9387
..training done in 66.63 seconds
..evaluation done in 15.91 seconds
Old network+MCTS average reward: 0.74, min: 0.24, max: 1.38, stdev: 0.22
New network+MCTS average reward: 0.74, min: 0.17, max: 1.38, stdev: 0.22
Old bare network average reward: 0.70, min: 0.19, max: 1.38, stdev: 0.22
New bare network average reward: 0.70, min: 0.16, max: 1.38, stdev: 0.22
External policy "random" average reward: 0.27, min: -0.25, max: 0.96, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: 0.05, max: 1.24, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.21, max: 1.27, stdev: 0.21
New network won 69 and tied 160 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 389 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.59 seconds
Training examples lengths: [65012, 64846, 64997, 64892, 64708, 64907, 64658, 64679, 65067, 64630]
Total value: 479705.31
Training on 648396 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2278 (value: 0.0012, weighted value: 0.0607, policy: 0.1671, weighted policy: 0.1671), Train Mean Max: 0.9349
Epoch 2/10, Train Loss: 0.2110 (value: 0.0010, weighted value: 0.0522, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9361
Epoch 3/10, Train Loss: 0.2019 (value: 0.0010, weighted value: 0.0481, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9364
Epoch 4/10, Train Loss: 0.1955 (value: 0.0009, weighted value: 0.0443, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9369
Epoch 5/10, Train Loss: 0.1924 (value: 0.0009, weighted value: 0.0425, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9370
Epoch 6/10, Train Loss: 0.1905 (value: 0.0008, weighted value: 0.0411, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1875 (value: 0.0008, weighted value: 0.0385, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1863 (value: 0.0008, weighted value: 0.0379, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9376
Epoch 9/10, Train Loss: 0.1845 (value: 0.0007, weighted value: 0.0362, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9376
Epoch 10/10, Train Loss: 0.1843 (value: 0.0007, weighted value: 0.0358, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9377
..training done in 67.28 seconds
..evaluation done in 15.82 seconds
Old network+MCTS average reward: 0.74, min: 0.19, max: 1.44, stdev: 0.23
New network+MCTS average reward: 0.74, min: 0.21, max: 1.44, stdev: 0.23
Old bare network average reward: 0.70, min: 0.15, max: 1.44, stdev: 0.24
New bare network average reward: 0.70, min: 0.17, max: 1.44, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.25, max: 1.05, stdev: 0.21
External policy "individual greedy" average reward: 0.55, min: -0.04, max: 1.23, stdev: 0.21
External policy "total greedy" average reward: 0.66, min: 0.10, max: 1.35, stdev: 0.21
New network won 75 and tied 165 out of 300 games (52.50% wins where ties are half wins)
Keeping the new network

Training iteration 390 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.34 seconds
Training examples lengths: [64846, 64997, 64892, 64708, 64907, 64658, 64679, 65067, 64630, 64654]
Total value: 479103.22
Training on 648038 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2063 (value: 0.0010, weighted value: 0.0488, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9363
Epoch 2/10, Train Loss: 0.1981 (value: 0.0009, weighted value: 0.0442, policy: 0.1539, weighted policy: 0.1539), Train Mean Max: 0.9370
Epoch 3/10, Train Loss: 0.1918 (value: 0.0008, weighted value: 0.0408, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9374
Epoch 4/10, Train Loss: 0.1895 (value: 0.0008, weighted value: 0.0394, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9375
Epoch 5/10, Train Loss: 0.1873 (value: 0.0008, weighted value: 0.0386, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1853 (value: 0.0007, weighted value: 0.0370, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9378
Epoch 7/10, Train Loss: 0.1821 (value: 0.0007, weighted value: 0.0344, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9380
Epoch 8/10, Train Loss: 0.1825 (value: 0.0007, weighted value: 0.0353, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1808 (value: 0.0007, weighted value: 0.0332, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9381
Epoch 10/10, Train Loss: 0.1807 (value: 0.0007, weighted value: 0.0334, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9385
..training done in 66.32 seconds
..evaluation done in 16.26 seconds
Old network+MCTS average reward: 0.75, min: 0.16, max: 1.52, stdev: 0.24
New network+MCTS average reward: 0.75, min: 0.16, max: 1.52, stdev: 0.24
Old bare network average reward: 0.71, min: 0.16, max: 1.52, stdev: 0.24
New bare network average reward: 0.72, min: 0.16, max: 1.52, stdev: 0.25
External policy "random" average reward: 0.26, min: -0.34, max: 1.18, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: 0.06, max: 1.20, stdev: 0.23
External policy "total greedy" average reward: 0.68, min: 0.13, max: 1.46, stdev: 0.22
New network won 64 and tied 163 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 391 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.35 seconds
Training examples lengths: [64997, 64892, 64708, 64907, 64658, 64679, 65067, 64630, 64654, 64926]
Total value: 478941.94
Training on 648118 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2297 (value: 0.0013, weighted value: 0.0629, policy: 0.1668, weighted policy: 0.1668), Train Mean Max: 0.9348
Epoch 2/10, Train Loss: 0.2130 (value: 0.0011, weighted value: 0.0539, policy: 0.1591, weighted policy: 0.1591), Train Mean Max: 0.9356
Epoch 3/10, Train Loss: 0.2048 (value: 0.0010, weighted value: 0.0499, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9361
Epoch 4/10, Train Loss: 0.1980 (value: 0.0009, weighted value: 0.0457, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9365
Epoch 5/10, Train Loss: 0.1956 (value: 0.0009, weighted value: 0.0443, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9365
Epoch 6/10, Train Loss: 0.1938 (value: 0.0009, weighted value: 0.0435, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9368
Epoch 7/10, Train Loss: 0.1888 (value: 0.0008, weighted value: 0.0392, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9370
Epoch 8/10, Train Loss: 0.1878 (value: 0.0008, weighted value: 0.0390, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9371
Epoch 9/10, Train Loss: 0.1864 (value: 0.0007, weighted value: 0.0372, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9373
Epoch 10/10, Train Loss: 0.1865 (value: 0.0007, weighted value: 0.0374, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9374
..training done in 67.22 seconds
..evaluation done in 15.85 seconds
Old network+MCTS average reward: 0.74, min: -0.06, max: 1.44, stdev: 0.24
New network+MCTS average reward: 0.74, min: -0.12, max: 1.44, stdev: 0.24
Old bare network average reward: 0.70, min: -0.17, max: 1.44, stdev: 0.24
New bare network average reward: 0.70, min: -0.17, max: 1.44, stdev: 0.24
External policy "random" average reward: 0.28, min: -0.48, max: 0.94, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: -0.31, max: 1.22, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: -0.12, max: 1.37, stdev: 0.22
New network won 68 and tied 162 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 392 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.74 seconds
Training examples lengths: [64892, 64708, 64907, 64658, 64679, 65067, 64630, 64654, 64926, 64625]
Total value: 478488.31
Training on 647746 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2521 (value: 0.0015, weighted value: 0.0770, policy: 0.1751, weighted policy: 0.1751), Train Mean Max: 0.9331
Epoch 2/10, Train Loss: 0.2277 (value: 0.0013, weighted value: 0.0635, policy: 0.1643, weighted policy: 0.1643), Train Mean Max: 0.9344
Epoch 3/10, Train Loss: 0.2155 (value: 0.0012, weighted value: 0.0577, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9348
Epoch 4/10, Train Loss: 0.2087 (value: 0.0011, weighted value: 0.0533, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9352
Epoch 5/10, Train Loss: 0.2042 (value: 0.0010, weighted value: 0.0510, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9355
Epoch 6/10, Train Loss: 0.1994 (value: 0.0009, weighted value: 0.0474, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9357
Epoch 7/10, Train Loss: 0.1974 (value: 0.0009, weighted value: 0.0455, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9358
Epoch 8/10, Train Loss: 0.1948 (value: 0.0009, weighted value: 0.0429, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9361
Epoch 9/10, Train Loss: 0.1922 (value: 0.0008, weighted value: 0.0412, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9364
Epoch 10/10, Train Loss: 0.1910 (value: 0.0008, weighted value: 0.0404, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9367
..training done in 72.69 seconds
..evaluation done in 15.93 seconds
Old network+MCTS average reward: 0.70, min: 0.14, max: 1.34, stdev: 0.23
New network+MCTS average reward: 0.70, min: 0.14, max: 1.36, stdev: 0.23
Old bare network average reward: 0.67, min: 0.12, max: 1.34, stdev: 0.23
New bare network average reward: 0.67, min: 0.12, max: 1.28, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.28, max: 0.96, stdev: 0.22
External policy "individual greedy" average reward: 0.52, min: -0.03, max: 1.38, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: 0.10, max: 1.26, stdev: 0.22
New network won 62 and tied 175 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 393 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.18 seconds
Training examples lengths: [64708, 64907, 64658, 64679, 65067, 64630, 64654, 64926, 64625, 64555]
Total value: 478193.89
Training on 647409 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2716 (value: 0.0018, weighted value: 0.0898, policy: 0.1818, weighted policy: 0.1818), Train Mean Max: 0.9317
Epoch 2/10, Train Loss: 0.2448 (value: 0.0015, weighted value: 0.0744, policy: 0.1704, weighted policy: 0.1704), Train Mean Max: 0.9331
Epoch 3/10, Train Loss: 0.2265 (value: 0.0013, weighted value: 0.0646, policy: 0.1619, weighted policy: 0.1619), Train Mean Max: 0.9337
Epoch 4/10, Train Loss: 0.2187 (value: 0.0012, weighted value: 0.0604, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9341
Epoch 5/10, Train Loss: 0.2118 (value: 0.0011, weighted value: 0.0557, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9345
Epoch 6/10, Train Loss: 0.2086 (value: 0.0011, weighted value: 0.0534, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9346
Epoch 7/10, Train Loss: 0.2030 (value: 0.0010, weighted value: 0.0492, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9350
Epoch 8/10, Train Loss: 0.2000 (value: 0.0009, weighted value: 0.0472, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9353
Epoch 9/10, Train Loss: 0.1977 (value: 0.0009, weighted value: 0.0454, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9354
Epoch 10/10, Train Loss: 0.1950 (value: 0.0009, weighted value: 0.0432, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9358
..training done in 70.68 seconds
..evaluation done in 16.29 seconds
Old network+MCTS average reward: 0.75, min: 0.26, max: 1.54, stdev: 0.23
New network+MCTS average reward: 0.75, min: 0.19, max: 1.54, stdev: 0.23
Old bare network average reward: 0.71, min: 0.19, max: 1.54, stdev: 0.23
New bare network average reward: 0.71, min: 0.17, max: 1.54, stdev: 0.23
External policy "random" average reward: 0.28, min: -0.21, max: 1.03, stdev: 0.23
External policy "individual greedy" average reward: 0.56, min: -0.04, max: 1.44, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: 0.08, max: 1.54, stdev: 0.22
New network won 64 and tied 161 out of 300 games (48.17% wins where ties are half wins)
Reverting to the old network

Training iteration 394 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.91 seconds
Training examples lengths: [64907, 64658, 64679, 65067, 64630, 64654, 64926, 64625, 64555, 65141]
Total value: 478287.27
Training on 647842 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2919 (value: 0.0021, weighted value: 0.1033, policy: 0.1886, weighted policy: 0.1886), Train Mean Max: 0.9304
Epoch 2/10, Train Loss: 0.2548 (value: 0.0016, weighted value: 0.0817, policy: 0.1730, weighted policy: 0.1730), Train Mean Max: 0.9320
Epoch 3/10, Train Loss: 0.2369 (value: 0.0014, weighted value: 0.0722, policy: 0.1648, weighted policy: 0.1648), Train Mean Max: 0.9326
Epoch 4/10, Train Loss: 0.2281 (value: 0.0013, weighted value: 0.0671, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9330
Epoch 5/10, Train Loss: 0.2188 (value: 0.0012, weighted value: 0.0605, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9333
Epoch 6/10, Train Loss: 0.2135 (value: 0.0011, weighted value: 0.0569, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9338
Epoch 7/10, Train Loss: 0.2101 (value: 0.0011, weighted value: 0.0542, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9339
Epoch 8/10, Train Loss: 0.2042 (value: 0.0010, weighted value: 0.0496, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9344
Epoch 9/10, Train Loss: 0.2024 (value: 0.0010, weighted value: 0.0488, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9347
Epoch 10/10, Train Loss: 0.1995 (value: 0.0009, weighted value: 0.0460, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9348
..training done in 70.50 seconds
..evaluation done in 16.56 seconds
Old network+MCTS average reward: 0.73, min: 0.13, max: 1.48, stdev: 0.24
New network+MCTS average reward: 0.74, min: 0.13, max: 1.48, stdev: 0.24
Old bare network average reward: 0.70, min: 0.04, max: 1.44, stdev: 0.24
New bare network average reward: 0.70, min: 0.04, max: 1.48, stdev: 0.24
External policy "random" average reward: 0.29, min: -0.23, max: 0.98, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: 0.02, max: 1.18, stdev: 0.24
External policy "total greedy" average reward: 0.67, min: 0.14, max: 1.25, stdev: 0.22
New network won 91 and tied 151 out of 300 games (55.50% wins where ties are half wins)
Keeping the new network

Training iteration 395 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.53 seconds
Training examples lengths: [64658, 64679, 65067, 64630, 64654, 64926, 64625, 64555, 65141, 64809]
Total value: 477975.24
Training on 647744 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2196 (value: 0.0012, weighted value: 0.0577, policy: 0.1619, weighted policy: 0.1619), Train Mean Max: 0.9335
Epoch 2/10, Train Loss: 0.2097 (value: 0.0010, weighted value: 0.0517, policy: 0.1580, weighted policy: 0.1580), Train Mean Max: 0.9342
Epoch 3/10, Train Loss: 0.2048 (value: 0.0010, weighted value: 0.0493, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9346
Epoch 4/10, Train Loss: 0.2000 (value: 0.0009, weighted value: 0.0462, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9349
Epoch 5/10, Train Loss: 0.1965 (value: 0.0009, weighted value: 0.0437, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9350
Epoch 6/10, Train Loss: 0.1950 (value: 0.0009, weighted value: 0.0431, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9354
Epoch 7/10, Train Loss: 0.1934 (value: 0.0008, weighted value: 0.0413, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9355
Epoch 8/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0390, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9359
Epoch 9/10, Train Loss: 0.1903 (value: 0.0008, weighted value: 0.0391, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9360
Epoch 10/10, Train Loss: 0.1878 (value: 0.0007, weighted value: 0.0370, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9362
..training done in 68.19 seconds
..evaluation done in 16.04 seconds
Old network+MCTS average reward: 0.74, min: 0.14, max: 1.50, stdev: 0.24
New network+MCTS average reward: 0.73, min: 0.14, max: 1.51, stdev: 0.24
Old bare network average reward: 0.70, min: 0.14, max: 1.50, stdev: 0.24
New bare network average reward: 0.69, min: 0.14, max: 1.51, stdev: 0.24
External policy "random" average reward: 0.25, min: -0.51, max: 0.94, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: -0.09, max: 1.23, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: -0.03, max: 1.28, stdev: 0.22
New network won 71 and tied 156 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 396 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.81 seconds
Training examples lengths: [64679, 65067, 64630, 64654, 64926, 64625, 64555, 65141, 64809, 65019]
Total value: 478323.09
Training on 648105 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2377 (value: 0.0014, weighted value: 0.0687, policy: 0.1690, weighted policy: 0.1690), Train Mean Max: 0.9323
Epoch 2/10, Train Loss: 0.2237 (value: 0.0012, weighted value: 0.0606, policy: 0.1630, weighted policy: 0.1630), Train Mean Max: 0.9331
Epoch 3/10, Train Loss: 0.2152 (value: 0.0011, weighted value: 0.0565, policy: 0.1587, weighted policy: 0.1587), Train Mean Max: 0.9336
Epoch 4/10, Train Loss: 0.2080 (value: 0.0010, weighted value: 0.0524, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9341
Epoch 5/10, Train Loss: 0.2031 (value: 0.0010, weighted value: 0.0485, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9343
Epoch 6/10, Train Loss: 0.2013 (value: 0.0009, weighted value: 0.0471, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9345
Epoch 7/10, Train Loss: 0.1996 (value: 0.0009, weighted value: 0.0460, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9349
Epoch 8/10, Train Loss: 0.1955 (value: 0.0009, weighted value: 0.0433, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9352
Epoch 9/10, Train Loss: 0.1925 (value: 0.0008, weighted value: 0.0414, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9355
Epoch 10/10, Train Loss: 0.1933 (value: 0.0008, weighted value: 0.0418, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9356
..training done in 71.00 seconds
..evaluation done in 15.94 seconds
Old network+MCTS average reward: 0.75, min: 0.11, max: 1.41, stdev: 0.24
New network+MCTS average reward: 0.75, min: 0.11, max: 1.41, stdev: 0.24
Old bare network average reward: 0.71, min: 0.06, max: 1.34, stdev: 0.24
New bare network average reward: 0.71, min: 0.03, max: 1.40, stdev: 0.25
External policy "random" average reward: 0.27, min: -0.40, max: 0.95, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: -0.19, max: 1.29, stdev: 0.22
External policy "total greedy" average reward: 0.67, min: -0.09, max: 1.35, stdev: 0.22
New network won 71 and tied 159 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 397 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.46 seconds
Training examples lengths: [65067, 64630, 64654, 64926, 64625, 64555, 65141, 64809, 65019, 64863]
Total value: 478184.30
Training on 648289 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2121 (value: 0.0010, weighted value: 0.0523, policy: 0.1599, weighted policy: 0.1599), Train Mean Max: 0.9342
Epoch 2/10, Train Loss: 0.2058 (value: 0.0010, weighted value: 0.0483, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9348
Epoch 3/10, Train Loss: 0.1983 (value: 0.0009, weighted value: 0.0443, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9354
Epoch 4/10, Train Loss: 0.1968 (value: 0.0009, weighted value: 0.0445, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9354
Epoch 5/10, Train Loss: 0.1932 (value: 0.0008, weighted value: 0.0417, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9358
Epoch 6/10, Train Loss: 0.1906 (value: 0.0008, weighted value: 0.0393, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9361
Epoch 7/10, Train Loss: 0.1897 (value: 0.0008, weighted value: 0.0387, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9362
Epoch 8/10, Train Loss: 0.1886 (value: 0.0007, weighted value: 0.0373, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9362
Epoch 9/10, Train Loss: 0.1858 (value: 0.0007, weighted value: 0.0360, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9367
Epoch 10/10, Train Loss: 0.1855 (value: 0.0007, weighted value: 0.0356, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9367
..training done in 68.04 seconds
..evaluation done in 16.35 seconds
Old network+MCTS average reward: 0.73, min: 0.15, max: 1.31, stdev: 0.24
New network+MCTS average reward: 0.73, min: 0.15, max: 1.38, stdev: 0.24
Old bare network average reward: 0.69, min: 0.11, max: 1.31, stdev: 0.25
New bare network average reward: 0.69, min: 0.15, max: 1.31, stdev: 0.25
External policy "random" average reward: 0.25, min: -0.42, max: 0.88, stdev: 0.24
External policy "individual greedy" average reward: 0.54, min: -0.09, max: 1.30, stdev: 0.24
External policy "total greedy" average reward: 0.66, min: 0.11, max: 1.43, stdev: 0.23
New network won 58 and tied 165 out of 300 games (46.83% wins where ties are half wins)
Reverting to the old network

Training iteration 398 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.81 seconds
Training examples lengths: [64630, 64654, 64926, 64625, 64555, 65141, 64809, 65019, 64863, 65031]
Total value: 478614.23
Training on 648253 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2334 (value: 0.0013, weighted value: 0.0646, policy: 0.1688, weighted policy: 0.1688), Train Mean Max: 0.9330
Epoch 2/10, Train Loss: 0.2199 (value: 0.0012, weighted value: 0.0578, policy: 0.1621, weighted policy: 0.1621), Train Mean Max: 0.9340
Epoch 3/10, Train Loss: 0.2093 (value: 0.0011, weighted value: 0.0525, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9345
Epoch 4/10, Train Loss: 0.2036 (value: 0.0010, weighted value: 0.0491, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9348
Epoch 5/10, Train Loss: 0.2015 (value: 0.0010, weighted value: 0.0477, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9350
Epoch 6/10, Train Loss: 0.1980 (value: 0.0009, weighted value: 0.0447, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9352
Epoch 7/10, Train Loss: 0.1950 (value: 0.0009, weighted value: 0.0426, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9355
Epoch 8/10, Train Loss: 0.1939 (value: 0.0008, weighted value: 0.0421, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9357
Epoch 9/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0390, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9360
Epoch 10/10, Train Loss: 0.1899 (value: 0.0008, weighted value: 0.0396, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9361
..training done in 66.40 seconds
..evaluation done in 16.17 seconds
Old network+MCTS average reward: 0.72, min: 0.07, max: 1.62, stdev: 0.25
New network+MCTS average reward: 0.72, min: 0.09, max: 1.58, stdev: 0.25
Old bare network average reward: 0.68, min: 0.04, max: 1.55, stdev: 0.26
New bare network average reward: 0.68, min: 0.06, max: 1.55, stdev: 0.25
External policy "random" average reward: 0.25, min: -0.59, max: 0.84, stdev: 0.22
External policy "individual greedy" average reward: 0.52, min: -0.03, max: 1.29, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.04, max: 1.28, stdev: 0.23
New network won 70 and tied 166 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 399 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.94 seconds
Training examples lengths: [64654, 64926, 64625, 64555, 65141, 64809, 65019, 64863, 65031, 64817]
Total value: 478624.14
Training on 648440 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2118 (value: 0.0010, weighted value: 0.0514, policy: 0.1604, weighted policy: 0.1604), Train Mean Max: 0.9347
Epoch 2/10, Train Loss: 0.2023 (value: 0.0009, weighted value: 0.0463, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9355
Epoch 3/10, Train Loss: 0.1973 (value: 0.0009, weighted value: 0.0442, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9359
Epoch 4/10, Train Loss: 0.1949 (value: 0.0008, weighted value: 0.0420, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9361
Epoch 5/10, Train Loss: 0.1909 (value: 0.0008, weighted value: 0.0398, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9363
Epoch 6/10, Train Loss: 0.1893 (value: 0.0008, weighted value: 0.0390, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9364
Epoch 7/10, Train Loss: 0.1869 (value: 0.0007, weighted value: 0.0373, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9366
Epoch 8/10, Train Loss: 0.1864 (value: 0.0007, weighted value: 0.0366, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9367
Epoch 9/10, Train Loss: 0.1853 (value: 0.0007, weighted value: 0.0356, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9370
Epoch 10/10, Train Loss: 0.1849 (value: 0.0007, weighted value: 0.0356, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9371
..training done in 66.62 seconds
..evaluation done in 15.83 seconds
Old network+MCTS average reward: 0.72, min: 0.01, max: 1.56, stdev: 0.23
New network+MCTS average reward: 0.72, min: 0.01, max: 1.56, stdev: 0.23
Old bare network average reward: 0.69, min: 0.01, max: 1.35, stdev: 0.23
New bare network average reward: 0.69, min: 0.01, max: 1.47, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.42, max: 1.06, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.06, max: 1.18, stdev: 0.21
External policy "total greedy" average reward: 0.66, min: 0.01, max: 1.32, stdev: 0.21
New network won 66 and tied 173 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 400 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.22 seconds
Training examples lengths: [64926, 64625, 64555, 65141, 64809, 65019, 64863, 65031, 64817, 64589]
Total value: 479182.17
Training on 648375 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2070 (value: 0.0010, weighted value: 0.0478, policy: 0.1592, weighted policy: 0.1592), Train Mean Max: 0.9357
Epoch 2/10, Train Loss: 0.1980 (value: 0.0008, weighted value: 0.0425, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9363
Epoch 3/10, Train Loss: 0.1932 (value: 0.0008, weighted value: 0.0411, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9369
Epoch 4/10, Train Loss: 0.1883 (value: 0.0008, weighted value: 0.0383, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9371
Epoch 5/10, Train Loss: 0.1873 (value: 0.0008, weighted value: 0.0380, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9371
Epoch 6/10, Train Loss: 0.1848 (value: 0.0007, weighted value: 0.0356, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9372
Epoch 7/10, Train Loss: 0.1846 (value: 0.0007, weighted value: 0.0357, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1831 (value: 0.0007, weighted value: 0.0343, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1818 (value: 0.0007, weighted value: 0.0332, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1804 (value: 0.0006, weighted value: 0.0318, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9378
..training done in 66.65 seconds
..evaluation done in 16.14 seconds
Old network+MCTS average reward: 0.74, min: 0.06, max: 1.61, stdev: 0.23
New network+MCTS average reward: 0.74, min: 0.16, max: 1.53, stdev: 0.23
Old bare network average reward: 0.71, min: 0.01, max: 1.64, stdev: 0.23
New bare network average reward: 0.70, min: 0.15, max: 1.53, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.35, max: 0.96, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: -0.07, max: 1.37, stdev: 0.22
External policy "total greedy" average reward: 0.67, min: 0.13, max: 1.51, stdev: 0.22
New network won 70 and tied 158 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1757697740_iter_400

Training iteration 401 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.06 seconds
Training examples lengths: [64625, 64555, 65141, 64809, 65019, 64863, 65031, 64817, 64589, 64750]
Total value: 480105.02
Training on 648199 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2287 (value: 0.0012, weighted value: 0.0617, policy: 0.1671, weighted policy: 0.1671), Train Mean Max: 0.9346
Epoch 2/10, Train Loss: 0.2131 (value: 0.0011, weighted value: 0.0540, policy: 0.1591, weighted policy: 0.1591), Train Mean Max: 0.9356
Epoch 3/10, Train Loss: 0.2027 (value: 0.0010, weighted value: 0.0484, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9361
Epoch 4/10, Train Loss: 0.1990 (value: 0.0009, weighted value: 0.0465, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9363
Epoch 5/10, Train Loss: 0.1949 (value: 0.0009, weighted value: 0.0441, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9365
Epoch 6/10, Train Loss: 0.1912 (value: 0.0008, weighted value: 0.0407, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9368
Epoch 7/10, Train Loss: 0.1904 (value: 0.0008, weighted value: 0.0404, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9370
Epoch 8/10, Train Loss: 0.1879 (value: 0.0008, weighted value: 0.0389, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9372
Epoch 9/10, Train Loss: 0.1853 (value: 0.0007, weighted value: 0.0362, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9374
Epoch 10/10, Train Loss: 0.1857 (value: 0.0007, weighted value: 0.0368, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9374
..training done in 71.61 seconds
..evaluation done in 16.21 seconds
Old network+MCTS average reward: 0.72, min: 0.04, max: 1.35, stdev: 0.23
New network+MCTS average reward: 0.72, min: 0.04, max: 1.42, stdev: 0.23
Old bare network average reward: 0.68, min: 0.04, max: 1.29, stdev: 0.24
New bare network average reward: 0.68, min: -0.03, max: 1.29, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.29, max: 1.03, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: -0.08, max: 1.30, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: 0.05, max: 1.34, stdev: 0.21
New network won 71 and tied 165 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 402 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.78 seconds
Training examples lengths: [64555, 65141, 64809, 65019, 64863, 65031, 64817, 64589, 64750, 64797]
Total value: 480842.21
Training on 648371 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2058 (value: 0.0009, weighted value: 0.0474, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9360
Epoch 2/10, Train Loss: 0.1989 (value: 0.0009, weighted value: 0.0446, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9368
Epoch 3/10, Train Loss: 0.1941 (value: 0.0008, weighted value: 0.0418, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9369
Epoch 4/10, Train Loss: 0.1898 (value: 0.0008, weighted value: 0.0398, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9372
Epoch 5/10, Train Loss: 0.1865 (value: 0.0008, weighted value: 0.0376, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9375
Epoch 6/10, Train Loss: 0.1860 (value: 0.0007, weighted value: 0.0372, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9376
Epoch 7/10, Train Loss: 0.1845 (value: 0.0007, weighted value: 0.0360, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9377
Epoch 8/10, Train Loss: 0.1825 (value: 0.0007, weighted value: 0.0337, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9379
Epoch 9/10, Train Loss: 0.1821 (value: 0.0007, weighted value: 0.0344, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9381
Epoch 10/10, Train Loss: 0.1808 (value: 0.0007, weighted value: 0.0333, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9381
..training done in 68.32 seconds
..evaluation done in 16.34 seconds
Old network+MCTS average reward: 0.74, min: 0.03, max: 1.47, stdev: 0.24
New network+MCTS average reward: 0.74, min: 0.03, max: 1.47, stdev: 0.24
Old bare network average reward: 0.70, min: 0.01, max: 1.47, stdev: 0.25
New bare network average reward: 0.70, min: 0.03, max: 1.38, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.40, max: 0.85, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: -0.06, max: 1.16, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.05, max: 1.47, stdev: 0.23
New network won 83 and tied 147 out of 300 games (52.17% wins where ties are half wins)
Keeping the new network

Training iteration 403 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.30 seconds
Training examples lengths: [65141, 64809, 65019, 64863, 65031, 64817, 64589, 64750, 64797, 64849]
Total value: 481300.27
Training on 648665 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2029 (value: 0.0009, weighted value: 0.0460, policy: 0.1569, weighted policy: 0.1569), Train Mean Max: 0.9365
Epoch 2/10, Train Loss: 0.1941 (value: 0.0008, weighted value: 0.0407, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9372
Epoch 3/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0391, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9376
Epoch 4/10, Train Loss: 0.1865 (value: 0.0007, weighted value: 0.0372, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9378
Epoch 5/10, Train Loss: 0.1840 (value: 0.0007, weighted value: 0.0363, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9381
Epoch 6/10, Train Loss: 0.1837 (value: 0.0007, weighted value: 0.0360, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9380
Epoch 7/10, Train Loss: 0.1817 (value: 0.0007, weighted value: 0.0332, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9381
Epoch 8/10, Train Loss: 0.1805 (value: 0.0007, weighted value: 0.0328, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9382
Epoch 9/10, Train Loss: 0.1791 (value: 0.0006, weighted value: 0.0324, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9385
Epoch 10/10, Train Loss: 0.1777 (value: 0.0006, weighted value: 0.0306, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9385
..training done in 71.10 seconds
..evaluation done in 16.88 seconds
Old network+MCTS average reward: 0.73, min: 0.10, max: 1.69, stdev: 0.24
New network+MCTS average reward: 0.73, min: 0.10, max: 1.69, stdev: 0.24
Old bare network average reward: 0.70, min: 0.10, max: 1.69, stdev: 0.24
New bare network average reward: 0.69, min: 0.10, max: 1.68, stdev: 0.24
External policy "random" average reward: 0.25, min: -0.41, max: 1.21, stdev: 0.24
External policy "individual greedy" average reward: 0.54, min: -0.16, max: 1.54, stdev: 0.25
External policy "total greedy" average reward: 0.64, min: -0.06, max: 1.64, stdev: 0.24
New network won 62 and tied 174 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 404 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.56 seconds
Training examples lengths: [64809, 65019, 64863, 65031, 64817, 64589, 64750, 64797, 64849, 64779]
Total value: 481012.91
Training on 648303 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2269 (value: 0.0012, weighted value: 0.0608, policy: 0.1661, weighted policy: 0.1661), Train Mean Max: 0.9348
Epoch 2/10, Train Loss: 0.2102 (value: 0.0010, weighted value: 0.0513, policy: 0.1589, weighted policy: 0.1589), Train Mean Max: 0.9361
Epoch 3/10, Train Loss: 0.2007 (value: 0.0009, weighted value: 0.0472, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9366
Epoch 4/10, Train Loss: 0.1968 (value: 0.0009, weighted value: 0.0450, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9367
Epoch 5/10, Train Loss: 0.1926 (value: 0.0009, weighted value: 0.0428, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9370
Epoch 6/10, Train Loss: 0.1903 (value: 0.0008, weighted value: 0.0406, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1868 (value: 0.0008, weighted value: 0.0380, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9374
Epoch 8/10, Train Loss: 0.1859 (value: 0.0007, weighted value: 0.0375, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9376
Epoch 9/10, Train Loss: 0.1853 (value: 0.0007, weighted value: 0.0368, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1833 (value: 0.0007, weighted value: 0.0352, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9379
..training done in 72.85 seconds
..evaluation done in 16.37 seconds
Old network+MCTS average reward: 0.73, min: 0.06, max: 1.48, stdev: 0.25
New network+MCTS average reward: 0.73, min: 0.03, max: 1.56, stdev: 0.25
Old bare network average reward: 0.69, min: 0.01, max: 1.34, stdev: 0.25
New bare network average reward: 0.69, min: 0.01, max: 1.36, stdev: 0.25
External policy "random" average reward: 0.26, min: -0.34, max: 0.87, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: -0.04, max: 1.39, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.01, max: 1.41, stdev: 0.23
New network won 61 and tied 166 out of 300 games (48.00% wins where ties are half wins)
Reverting to the old network

Training iteration 405 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.77 seconds
Training examples lengths: [65019, 64863, 65031, 64817, 64589, 64750, 64797, 64849, 64779, 64895]
Total value: 480699.76
Training on 648389 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2481 (value: 0.0015, weighted value: 0.0740, policy: 0.1741, weighted policy: 0.1741), Train Mean Max: 0.9334
Epoch 2/10, Train Loss: 0.2231 (value: 0.0012, weighted value: 0.0606, policy: 0.1625, weighted policy: 0.1625), Train Mean Max: 0.9347
Epoch 3/10, Train Loss: 0.2126 (value: 0.0011, weighted value: 0.0556, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9354
Epoch 4/10, Train Loss: 0.2066 (value: 0.0010, weighted value: 0.0519, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9356
Epoch 5/10, Train Loss: 0.2005 (value: 0.0010, weighted value: 0.0477, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9360
Epoch 6/10, Train Loss: 0.1974 (value: 0.0009, weighted value: 0.0461, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9363
Epoch 7/10, Train Loss: 0.1956 (value: 0.0009, weighted value: 0.0448, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9364
Epoch 8/10, Train Loss: 0.1910 (value: 0.0008, weighted value: 0.0409, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9366
Epoch 9/10, Train Loss: 0.1894 (value: 0.0008, weighted value: 0.0395, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9369
Epoch 10/10, Train Loss: 0.1888 (value: 0.0008, weighted value: 0.0396, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9371
..training done in 71.60 seconds
..evaluation done in 16.01 seconds
Old network+MCTS average reward: 0.73, min: 0.14, max: 1.28, stdev: 0.21
New network+MCTS average reward: 0.74, min: 0.14, max: 1.26, stdev: 0.20
Old bare network average reward: 0.69, min: 0.13, max: 1.12, stdev: 0.21
New bare network average reward: 0.70, min: 0.14, max: 1.12, stdev: 0.21
External policy "random" average reward: 0.26, min: -0.44, max: 0.87, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.09, max: 1.06, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.17, max: 1.23, stdev: 0.21
New network won 62 and tied 178 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 406 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.73 seconds
Training examples lengths: [64863, 65031, 64817, 64589, 64750, 64797, 64849, 64779, 64895, 64648]
Total value: 481035.60
Training on 648018 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2088 (value: 0.0010, weighted value: 0.0509, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9355
Epoch 2/10, Train Loss: 0.2014 (value: 0.0009, weighted value: 0.0462, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9361
Epoch 3/10, Train Loss: 0.1953 (value: 0.0009, weighted value: 0.0430, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9365
Epoch 4/10, Train Loss: 0.1921 (value: 0.0008, weighted value: 0.0415, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9368
Epoch 5/10, Train Loss: 0.1891 (value: 0.0008, weighted value: 0.0398, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9370
Epoch 6/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0388, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1868 (value: 0.0008, weighted value: 0.0378, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9372
Epoch 8/10, Train Loss: 0.1832 (value: 0.0007, weighted value: 0.0350, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9376
Epoch 9/10, Train Loss: 0.1830 (value: 0.0007, weighted value: 0.0347, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9376
Epoch 10/10, Train Loss: 0.1832 (value: 0.0007, weighted value: 0.0354, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9378
..training done in 67.95 seconds
..evaluation done in 16.25 seconds
Old network+MCTS average reward: 0.74, min: 0.21, max: 1.40, stdev: 0.22
New network+MCTS average reward: 0.75, min: 0.16, max: 1.44, stdev: 0.22
Old bare network average reward: 0.71, min: 0.19, max: 1.40, stdev: 0.22
New bare network average reward: 0.71, min: 0.16, max: 1.37, stdev: 0.22
External policy "random" average reward: 0.26, min: -0.35, max: 0.79, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.06, max: 1.10, stdev: 0.20
External policy "total greedy" average reward: 0.66, min: 0.06, max: 1.24, stdev: 0.20
New network won 76 and tied 169 out of 300 games (53.50% wins where ties are half wins)
Keeping the new network

Training iteration 407 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.22 seconds
Training examples lengths: [65031, 64817, 64589, 64750, 64797, 64849, 64779, 64895, 64648, 64725]
Total value: 481237.39
Training on 647880 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2053 (value: 0.0010, weighted value: 0.0481, policy: 0.1572, weighted policy: 0.1572), Train Mean Max: 0.9363
Epoch 2/10, Train Loss: 0.1958 (value: 0.0008, weighted value: 0.0421, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9370
Epoch 3/10, Train Loss: 0.1898 (value: 0.0008, weighted value: 0.0393, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9374
Epoch 4/10, Train Loss: 0.1885 (value: 0.0008, weighted value: 0.0389, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9375
Epoch 5/10, Train Loss: 0.1849 (value: 0.0007, weighted value: 0.0365, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9377
Epoch 6/10, Train Loss: 0.1837 (value: 0.0007, weighted value: 0.0359, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9380
Epoch 7/10, Train Loss: 0.1820 (value: 0.0007, weighted value: 0.0348, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9381
Epoch 8/10, Train Loss: 0.1802 (value: 0.0007, weighted value: 0.0336, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1792 (value: 0.0006, weighted value: 0.0322, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1788 (value: 0.0006, weighted value: 0.0321, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9384
..training done in 67.68 seconds
..evaluation done in 15.78 seconds
Old network+MCTS average reward: 0.76, min: 0.18, max: 1.48, stdev: 0.24
New network+MCTS average reward: 0.76, min: 0.13, max: 1.52, stdev: 0.23
Old bare network average reward: 0.72, min: 0.12, max: 1.48, stdev: 0.24
New bare network average reward: 0.72, min: 0.13, max: 1.48, stdev: 0.24
External policy "random" average reward: 0.29, min: -0.35, max: 1.05, stdev: 0.22
External policy "individual greedy" average reward: 0.56, min: 0.04, max: 1.20, stdev: 0.22
External policy "total greedy" average reward: 0.68, min: 0.04, max: 1.31, stdev: 0.22
New network won 65 and tied 166 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 408 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 48.21 seconds
Training examples lengths: [64817, 64589, 64750, 64797, 64849, 64779, 64895, 64648, 64725, 65031]
Total value: 481449.24
Training on 647880 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2272 (value: 0.0012, weighted value: 0.0616, policy: 0.1655, weighted policy: 0.1655), Train Mean Max: 0.9349
Epoch 2/10, Train Loss: 0.2105 (value: 0.0010, weighted value: 0.0524, policy: 0.1581, weighted policy: 0.1581), Train Mean Max: 0.9359
Epoch 3/10, Train Loss: 0.2000 (value: 0.0009, weighted value: 0.0468, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9367
Epoch 4/10, Train Loss: 0.1967 (value: 0.0009, weighted value: 0.0458, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9368
Epoch 5/10, Train Loss: 0.1933 (value: 0.0009, weighted value: 0.0433, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1905 (value: 0.0008, weighted value: 0.0415, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9372
Epoch 7/10, Train Loss: 0.1878 (value: 0.0008, weighted value: 0.0391, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1859 (value: 0.0008, weighted value: 0.0380, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9376
Epoch 9/10, Train Loss: 0.1846 (value: 0.0007, weighted value: 0.0365, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1836 (value: 0.0007, weighted value: 0.0357, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9379
..training done in 65.97 seconds
..evaluation done in 16.24 seconds
Old network+MCTS average reward: 0.75, min: 0.13, max: 1.68, stdev: 0.23
New network+MCTS average reward: 0.75, min: 0.13, max: 1.67, stdev: 0.23
Old bare network average reward: 0.72, min: 0.10, max: 1.68, stdev: 0.24
New bare network average reward: 0.71, min: 0.05, max: 1.67, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.28, max: 1.13, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: 0.00, max: 1.22, stdev: 0.24
External policy "total greedy" average reward: 0.67, min: 0.10, max: 1.44, stdev: 0.23
New network won 69 and tied 171 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 409 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.92 seconds
Training examples lengths: [64589, 64750, 64797, 64849, 64779, 64895, 64648, 64725, 65031, 64750]
Total value: 480614.17
Training on 647813 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2057 (value: 0.0010, weighted value: 0.0490, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9363
Epoch 2/10, Train Loss: 0.1975 (value: 0.0009, weighted value: 0.0444, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9370
Epoch 3/10, Train Loss: 0.1921 (value: 0.0008, weighted value: 0.0414, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9374
Epoch 4/10, Train Loss: 0.1891 (value: 0.0008, weighted value: 0.0398, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9375
Epoch 5/10, Train Loss: 0.1870 (value: 0.0008, weighted value: 0.0378, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1844 (value: 0.0007, weighted value: 0.0363, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9379
Epoch 7/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0357, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9381
Epoch 8/10, Train Loss: 0.1812 (value: 0.0007, weighted value: 0.0340, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9382
Epoch 9/10, Train Loss: 0.1804 (value: 0.0007, weighted value: 0.0334, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1800 (value: 0.0007, weighted value: 0.0331, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9385
..training done in 65.98 seconds
..evaluation done in 15.92 seconds
Old network+MCTS average reward: 0.72, min: 0.19, max: 1.30, stdev: 0.22
New network+MCTS average reward: 0.72, min: 0.19, max: 1.33, stdev: 0.22
Old bare network average reward: 0.69, min: 0.16, max: 1.32, stdev: 0.23
New bare network average reward: 0.69, min: 0.19, max: 1.28, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.33, max: 0.89, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: 0.04, max: 1.22, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.06, max: 1.27, stdev: 0.23
New network won 59 and tied 167 out of 300 games (47.50% wins where ties are half wins)
Reverting to the old network

Training iteration 410 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.26 seconds
Training examples lengths: [64750, 64797, 64849, 64779, 64895, 64648, 64725, 65031, 64750, 64729]
Total value: 480607.71
Training on 647953 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2270 (value: 0.0013, weighted value: 0.0627, policy: 0.1643, weighted policy: 0.1643), Train Mean Max: 0.9348
Epoch 2/10, Train Loss: 0.2124 (value: 0.0011, weighted value: 0.0544, policy: 0.1581, weighted policy: 0.1581), Train Mean Max: 0.9358
Epoch 3/10, Train Loss: 0.2014 (value: 0.0010, weighted value: 0.0478, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9364
Epoch 4/10, Train Loss: 0.1980 (value: 0.0009, weighted value: 0.0464, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9367
Epoch 5/10, Train Loss: 0.1936 (value: 0.0009, weighted value: 0.0438, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1915 (value: 0.0008, weighted value: 0.0419, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9370
Epoch 7/10, Train Loss: 0.1906 (value: 0.0008, weighted value: 0.0405, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9372
Epoch 8/10, Train Loss: 0.1864 (value: 0.0008, weighted value: 0.0382, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1864 (value: 0.0008, weighted value: 0.0381, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9376
Epoch 10/10, Train Loss: 0.1847 (value: 0.0007, weighted value: 0.0371, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9379
..training done in 65.85 seconds
..evaluation done in 16.35 seconds
Old network+MCTS average reward: 0.73, min: 0.09, max: 1.44, stdev: 0.23
New network+MCTS average reward: 0.73, min: 0.06, max: 1.45, stdev: 0.23
Old bare network average reward: 0.69, min: 0.00, max: 1.40, stdev: 0.23
New bare network average reward: 0.69, min: 0.00, max: 1.38, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.39, max: 0.94, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: -0.04, max: 1.23, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.10, max: 1.23, stdev: 0.22
New network won 74 and tied 153 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 411 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.36 seconds
Training examples lengths: [64797, 64849, 64779, 64895, 64648, 64725, 65031, 64750, 64729, 64604]
Total value: 480199.78
Training on 647807 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2049 (value: 0.0010, weighted value: 0.0483, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9362
Epoch 2/10, Train Loss: 0.1969 (value: 0.0009, weighted value: 0.0435, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9369
Epoch 3/10, Train Loss: 0.1920 (value: 0.0008, weighted value: 0.0412, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9372
Epoch 4/10, Train Loss: 0.1895 (value: 0.0008, weighted value: 0.0402, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9374
Epoch 5/10, Train Loss: 0.1877 (value: 0.0008, weighted value: 0.0385, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9374
Epoch 6/10, Train Loss: 0.1851 (value: 0.0007, weighted value: 0.0368, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9378
Epoch 7/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0359, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1816 (value: 0.0007, weighted value: 0.0343, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9380
Epoch 9/10, Train Loss: 0.1808 (value: 0.0007, weighted value: 0.0336, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9381
Epoch 10/10, Train Loss: 0.1802 (value: 0.0007, weighted value: 0.0327, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9383
..training done in 73.31 seconds
..evaluation done in 15.82 seconds
Old network+MCTS average reward: 0.73, min: 0.08, max: 1.45, stdev: 0.23
New network+MCTS average reward: 0.73, min: 0.06, max: 1.41, stdev: 0.23
Old bare network average reward: 0.69, min: 0.08, max: 1.44, stdev: 0.24
New bare network average reward: 0.70, min: 0.07, max: 1.41, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.25, max: 0.92, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: -0.04, max: 1.41, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.10, max: 1.46, stdev: 0.22
New network won 69 and tied 168 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 412 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.35 seconds
Training examples lengths: [64849, 64779, 64895, 64648, 64725, 65031, 64750, 64729, 64604, 64965]
Total value: 480377.58
Training on 647975 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2030 (value: 0.0009, weighted value: 0.0469, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9367
Epoch 2/10, Train Loss: 0.1937 (value: 0.0008, weighted value: 0.0408, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9373
Epoch 3/10, Train Loss: 0.1894 (value: 0.0008, weighted value: 0.0397, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9376
Epoch 4/10, Train Loss: 0.1856 (value: 0.0008, weighted value: 0.0376, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9378
Epoch 5/10, Train Loss: 0.1841 (value: 0.0007, weighted value: 0.0366, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9380
Epoch 6/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0340, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9382
Epoch 7/10, Train Loss: 0.1809 (value: 0.0007, weighted value: 0.0337, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9383
Epoch 8/10, Train Loss: 0.1790 (value: 0.0007, weighted value: 0.0326, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9385
Epoch 9/10, Train Loss: 0.1787 (value: 0.0006, weighted value: 0.0324, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9387
Epoch 10/10, Train Loss: 0.1783 (value: 0.0006, weighted value: 0.0317, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9386
..training done in 71.16 seconds
..evaluation done in 16.47 seconds
Old network+MCTS average reward: 0.74, min: 0.23, max: 1.46, stdev: 0.24
New network+MCTS average reward: 0.74, min: 0.23, max: 1.46, stdev: 0.24
Old bare network average reward: 0.70, min: 0.22, max: 1.46, stdev: 0.24
New bare network average reward: 0.70, min: 0.22, max: 1.46, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.35, max: 0.95, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: -0.02, max: 1.23, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.14, max: 1.46, stdev: 0.22
New network won 69 and tied 157 out of 300 games (49.17% wins where ties are half wins)
Reverting to the old network

Training iteration 413 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.45 seconds
Training examples lengths: [64779, 64895, 64648, 64725, 65031, 64750, 64729, 64604, 64965, 64672]
Total value: 480651.51
Training on 647798 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2254 (value: 0.0012, weighted value: 0.0613, policy: 0.1641, weighted policy: 0.1641), Train Mean Max: 0.9352
Epoch 2/10, Train Loss: 0.2082 (value: 0.0010, weighted value: 0.0508, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9363
Epoch 3/10, Train Loss: 0.2010 (value: 0.0010, weighted value: 0.0480, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9368
Epoch 4/10, Train Loss: 0.1941 (value: 0.0009, weighted value: 0.0438, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9370
Epoch 5/10, Train Loss: 0.1919 (value: 0.0009, weighted value: 0.0426, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9372
Epoch 6/10, Train Loss: 0.1883 (value: 0.0008, weighted value: 0.0399, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9375
Epoch 7/10, Train Loss: 0.1864 (value: 0.0008, weighted value: 0.0384, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9377
Epoch 8/10, Train Loss: 0.1847 (value: 0.0007, weighted value: 0.0367, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9377
Epoch 9/10, Train Loss: 0.1847 (value: 0.0007, weighted value: 0.0366, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9380
Epoch 10/10, Train Loss: 0.1830 (value: 0.0007, weighted value: 0.0355, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9381
..training done in 71.61 seconds
..evaluation done in 16.67 seconds
Old network+MCTS average reward: 0.73, min: -0.03, max: 1.40, stdev: 0.23
New network+MCTS average reward: 0.73, min: 0.14, max: 1.40, stdev: 0.23
Old bare network average reward: 0.70, min: -0.03, max: 1.32, stdev: 0.23
New bare network average reward: 0.69, min: -0.13, max: 1.32, stdev: 0.24
External policy "random" average reward: 0.25, min: -0.30, max: 1.05, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: 0.03, max: 1.18, stdev: 0.21
External policy "total greedy" average reward: 0.66, min: 0.16, max: 1.31, stdev: 0.22
New network won 66 and tied 167 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 414 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.98 seconds
Training examples lengths: [64895, 64648, 64725, 65031, 64750, 64729, 64604, 64965, 64672, 64828]
Total value: 481352.02
Training on 647847 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2454 (value: 0.0015, weighted value: 0.0737, policy: 0.1717, weighted policy: 0.1717), Train Mean Max: 0.9340
Epoch 2/10, Train Loss: 0.2227 (value: 0.0012, weighted value: 0.0609, policy: 0.1618, weighted policy: 0.1618), Train Mean Max: 0.9352
Epoch 3/10, Train Loss: 0.2106 (value: 0.0011, weighted value: 0.0544, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9359
Epoch 4/10, Train Loss: 0.2041 (value: 0.0010, weighted value: 0.0510, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9362
Epoch 5/10, Train Loss: 0.2005 (value: 0.0010, weighted value: 0.0488, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9363
Epoch 6/10, Train Loss: 0.1956 (value: 0.0009, weighted value: 0.0450, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9365
Epoch 7/10, Train Loss: 0.1925 (value: 0.0009, weighted value: 0.0436, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9369
Epoch 8/10, Train Loss: 0.1894 (value: 0.0008, weighted value: 0.0407, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9371
Epoch 9/10, Train Loss: 0.1880 (value: 0.0008, weighted value: 0.0396, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9371
Epoch 10/10, Train Loss: 0.1869 (value: 0.0008, weighted value: 0.0381, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9374
..training done in 70.23 seconds
..evaluation done in 16.73 seconds
Old network+MCTS average reward: 0.74, min: 0.11, max: 1.43, stdev: 0.24
New network+MCTS average reward: 0.74, min: 0.11, max: 1.43, stdev: 0.24
Old bare network average reward: 0.70, min: 0.15, max: 1.33, stdev: 0.24
New bare network average reward: 0.71, min: 0.11, max: 1.42, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.30, max: 1.02, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: 0.03, max: 1.32, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.16, max: 1.44, stdev: 0.23
New network won 73 and tied 167 out of 300 games (52.17% wins where ties are half wins)
Keeping the new network

Training iteration 415 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.86 seconds
Training examples lengths: [64648, 64725, 65031, 64750, 64729, 64604, 64965, 64672, 64828, 64916]
Total value: 481843.98
Training on 647868 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2089 (value: 0.0010, weighted value: 0.0509, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9361
Epoch 2/10, Train Loss: 0.2020 (value: 0.0010, weighted value: 0.0485, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9366
Epoch 3/10, Train Loss: 0.1935 (value: 0.0009, weighted value: 0.0430, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9370
Epoch 4/10, Train Loss: 0.1904 (value: 0.0008, weighted value: 0.0406, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9373
Epoch 5/10, Train Loss: 0.1881 (value: 0.0008, weighted value: 0.0390, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9374
Epoch 6/10, Train Loss: 0.1872 (value: 0.0008, weighted value: 0.0388, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9376
Epoch 7/10, Train Loss: 0.1840 (value: 0.0007, weighted value: 0.0362, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9377
Epoch 8/10, Train Loss: 0.1843 (value: 0.0007, weighted value: 0.0365, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9380
Epoch 9/10, Train Loss: 0.1809 (value: 0.0007, weighted value: 0.0350, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9382
Epoch 10/10, Train Loss: 0.1800 (value: 0.0007, weighted value: 0.0334, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9382
..training done in 65.65 seconds
..evaluation done in 15.67 seconds
Old network+MCTS average reward: 0.72, min: 0.05, max: 1.45, stdev: 0.23
New network+MCTS average reward: 0.72, min: 0.07, max: 1.45, stdev: 0.23
Old bare network average reward: 0.68, min: 0.03, max: 1.45, stdev: 0.23
New bare network average reward: 0.68, min: 0.05, max: 1.45, stdev: 0.24
External policy "random" average reward: 0.25, min: -0.55, max: 0.93, stdev: 0.21
External policy "individual greedy" average reward: 0.51, min: -0.04, max: 1.17, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: 0.02, max: 1.27, stdev: 0.21
New network won 69 and tied 156 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 416 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 48.08 seconds
Training examples lengths: [64725, 65031, 64750, 64729, 64604, 64965, 64672, 64828, 64916, 64860]
Total value: 481971.86
Training on 648080 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2301 (value: 0.0013, weighted value: 0.0647, policy: 0.1654, weighted policy: 0.1654), Train Mean Max: 0.9344
Epoch 2/10, Train Loss: 0.2137 (value: 0.0011, weighted value: 0.0546, policy: 0.1591, weighted policy: 0.1591), Train Mean Max: 0.9355
Epoch 3/10, Train Loss: 0.2052 (value: 0.0010, weighted value: 0.0509, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9360
Epoch 4/10, Train Loss: 0.2000 (value: 0.0010, weighted value: 0.0482, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9364
Epoch 5/10, Train Loss: 0.1949 (value: 0.0009, weighted value: 0.0447, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9365
Epoch 6/10, Train Loss: 0.1931 (value: 0.0009, weighted value: 0.0432, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9368
Epoch 7/10, Train Loss: 0.1907 (value: 0.0008, weighted value: 0.0415, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9370
Epoch 8/10, Train Loss: 0.1878 (value: 0.0008, weighted value: 0.0392, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9373
Epoch 9/10, Train Loss: 0.1868 (value: 0.0008, weighted value: 0.0383, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9374
Epoch 10/10, Train Loss: 0.1843 (value: 0.0007, weighted value: 0.0368, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9377
..training done in 73.89 seconds
..evaluation done in 16.40 seconds
Old network+MCTS average reward: 0.72, min: 0.17, max: 1.44, stdev: 0.22
New network+MCTS average reward: 0.71, min: 0.17, max: 1.44, stdev: 0.22
Old bare network average reward: 0.68, min: 0.10, max: 1.44, stdev: 0.22
New bare network average reward: 0.67, min: 0.10, max: 1.44, stdev: 0.22
External policy "random" average reward: 0.24, min: -0.34, max: 0.88, stdev: 0.21
External policy "individual greedy" average reward: 0.52, min: -0.04, max: 1.19, stdev: 0.22
External policy "total greedy" average reward: 0.63, min: 0.05, max: 1.19, stdev: 0.20
New network won 63 and tied 151 out of 300 games (46.17% wins where ties are half wins)
Reverting to the old network

Training iteration 417 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.52 seconds
Training examples lengths: [65031, 64750, 64729, 64604, 64965, 64672, 64828, 64916, 64860, 64877]
Total value: 481220.68
Training on 648232 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2489 (value: 0.0015, weighted value: 0.0756, policy: 0.1733, weighted policy: 0.1733), Train Mean Max: 0.9329
Epoch 2/10, Train Loss: 0.2282 (value: 0.0013, weighted value: 0.0639, policy: 0.1643, weighted policy: 0.1643), Train Mean Max: 0.9341
Epoch 3/10, Train Loss: 0.2149 (value: 0.0011, weighted value: 0.0572, policy: 0.1577, weighted policy: 0.1577), Train Mean Max: 0.9350
Epoch 4/10, Train Loss: 0.2084 (value: 0.0011, weighted value: 0.0529, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9351
Epoch 5/10, Train Loss: 0.2029 (value: 0.0010, weighted value: 0.0501, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9355
Epoch 6/10, Train Loss: 0.2002 (value: 0.0010, weighted value: 0.0481, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9358
Epoch 7/10, Train Loss: 0.1959 (value: 0.0009, weighted value: 0.0447, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9358
Epoch 8/10, Train Loss: 0.1934 (value: 0.0008, weighted value: 0.0424, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9362
Epoch 9/10, Train Loss: 0.1911 (value: 0.0008, weighted value: 0.0408, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9365
Epoch 10/10, Train Loss: 0.1898 (value: 0.0008, weighted value: 0.0405, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9366
..training done in 73.07 seconds
..evaluation done in 15.98 seconds
Old network+MCTS average reward: 0.73, min: 0.16, max: 1.39, stdev: 0.24
New network+MCTS average reward: 0.73, min: 0.05, max: 1.39, stdev: 0.24
Old bare network average reward: 0.70, min: 0.07, max: 1.34, stdev: 0.24
New bare network average reward: 0.70, min: 0.05, max: 1.36, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.36, max: 0.88, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: -0.06, max: 1.25, stdev: 0.24
External policy "total greedy" average reward: 0.65, min: 0.11, max: 1.31, stdev: 0.23
New network won 73 and tied 162 out of 300 games (51.33% wins where ties are half wins)
Keeping the new network

Training iteration 418 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.36 seconds
Training examples lengths: [64750, 64729, 64604, 64965, 64672, 64828, 64916, 64860, 64877, 65155]
Total value: 481081.80
Training on 648356 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2111 (value: 0.0010, weighted value: 0.0514, policy: 0.1596, weighted policy: 0.1596), Train Mean Max: 0.9351
Epoch 2/10, Train Loss: 0.2009 (value: 0.0009, weighted value: 0.0466, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9359
Epoch 3/10, Train Loss: 0.1951 (value: 0.0009, weighted value: 0.0431, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9363
Epoch 4/10, Train Loss: 0.1943 (value: 0.0009, weighted value: 0.0431, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9364
Epoch 5/10, Train Loss: 0.1890 (value: 0.0008, weighted value: 0.0391, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9368
Epoch 6/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0388, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9369
Epoch 7/10, Train Loss: 0.1855 (value: 0.0007, weighted value: 0.0368, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9372
Epoch 8/10, Train Loss: 0.1849 (value: 0.0007, weighted value: 0.0364, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1835 (value: 0.0007, weighted value: 0.0347, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9375
Epoch 10/10, Train Loss: 0.1828 (value: 0.0007, weighted value: 0.0348, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9377
..training done in 72.50 seconds
..evaluation done in 16.82 seconds
Old network+MCTS average reward: 0.73, min: 0.10, max: 1.61, stdev: 0.23
New network+MCTS average reward: 0.73, min: 0.10, max: 1.61, stdev: 0.23
Old bare network average reward: 0.70, min: 0.10, max: 1.61, stdev: 0.23
New bare network average reward: 0.70, min: 0.04, max: 1.61, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.25, max: 1.00, stdev: 0.21
External policy "individual greedy" average reward: 0.53, min: 0.05, max: 1.42, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.15, max: 1.59, stdev: 0.21
New network won 68 and tied 170 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 419 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.19 seconds
Training examples lengths: [64729, 64604, 64965, 64672, 64828, 64916, 64860, 64877, 65155, 64637]
Total value: 481546.35
Training on 648243 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2032 (value: 0.0009, weighted value: 0.0465, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9362
Epoch 2/10, Train Loss: 0.1966 (value: 0.0009, weighted value: 0.0428, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9368
Epoch 3/10, Train Loss: 0.1904 (value: 0.0008, weighted value: 0.0395, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9371
Epoch 4/10, Train Loss: 0.1879 (value: 0.0008, weighted value: 0.0384, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9373
Epoch 5/10, Train Loss: 0.1853 (value: 0.0007, weighted value: 0.0365, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1842 (value: 0.0007, weighted value: 0.0357, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9377
Epoch 7/10, Train Loss: 0.1826 (value: 0.0007, weighted value: 0.0344, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1801 (value: 0.0007, weighted value: 0.0326, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1801 (value: 0.0007, weighted value: 0.0331, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9382
Epoch 10/10, Train Loss: 0.1786 (value: 0.0006, weighted value: 0.0315, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9383
..training done in 67.81 seconds
..evaluation done in 16.26 seconds
Old network+MCTS average reward: 0.71, min: 0.13, max: 1.41, stdev: 0.23
New network+MCTS average reward: 0.71, min: 0.14, max: 1.41, stdev: 0.23
Old bare network average reward: 0.68, min: 0.13, max: 1.41, stdev: 0.23
New bare network average reward: 0.67, min: 0.11, max: 1.41, stdev: 0.23
External policy "random" average reward: 0.24, min: -0.36, max: 0.86, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: -0.03, max: 1.23, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.13, max: 1.18, stdev: 0.22
New network won 70 and tied 154 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 420 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.21 seconds
Training examples lengths: [64604, 64965, 64672, 64828, 64916, 64860, 64877, 65155, 64637, 64906]
Total value: 482144.20
Training on 648420 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2268 (value: 0.0012, weighted value: 0.0603, policy: 0.1666, weighted policy: 0.1666), Train Mean Max: 0.9345
Epoch 2/10, Train Loss: 0.2120 (value: 0.0011, weighted value: 0.0527, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9356
Epoch 3/10, Train Loss: 0.2018 (value: 0.0009, weighted value: 0.0471, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9360
Epoch 4/10, Train Loss: 0.1980 (value: 0.0009, weighted value: 0.0466, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9363
Epoch 5/10, Train Loss: 0.1927 (value: 0.0008, weighted value: 0.0416, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9365
Epoch 6/10, Train Loss: 0.1895 (value: 0.0008, weighted value: 0.0397, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9368
Epoch 7/10, Train Loss: 0.1888 (value: 0.0008, weighted value: 0.0392, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9368
Epoch 8/10, Train Loss: 0.1879 (value: 0.0008, weighted value: 0.0384, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9370
Epoch 9/10, Train Loss: 0.1862 (value: 0.0007, weighted value: 0.0361, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9372
Epoch 10/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0348, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9375
..training done in 66.45 seconds
..evaluation done in 15.82 seconds
Old network+MCTS average reward: 0.72, min: 0.14, max: 1.39, stdev: 0.23
New network+MCTS average reward: 0.72, min: 0.12, max: 1.39, stdev: 0.24
Old bare network average reward: 0.68, min: 0.08, max: 1.41, stdev: 0.24
New bare network average reward: 0.69, min: 0.12, max: 1.39, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.51, max: 1.06, stdev: 0.24
External policy "individual greedy" average reward: 0.52, min: -0.11, max: 1.27, stdev: 0.24
External policy "total greedy" average reward: 0.63, min: -0.02, max: 1.35, stdev: 0.23
New network won 62 and tied 176 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1757697740_iter_420

Training iteration 421 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.67 seconds
Training examples lengths: [64965, 64672, 64828, 64916, 64860, 64877, 65155, 64637, 64906, 64767]
Total value: 482489.75
Training on 648583 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2066 (value: 0.0010, weighted value: 0.0496, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9361
Epoch 2/10, Train Loss: 0.1967 (value: 0.0009, weighted value: 0.0431, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9367
Epoch 3/10, Train Loss: 0.1923 (value: 0.0008, weighted value: 0.0414, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9370
Epoch 4/10, Train Loss: 0.1897 (value: 0.0008, weighted value: 0.0393, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9371
Epoch 5/10, Train Loss: 0.1864 (value: 0.0007, weighted value: 0.0374, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9373
Epoch 6/10, Train Loss: 0.1840 (value: 0.0007, weighted value: 0.0360, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9376
Epoch 7/10, Train Loss: 0.1828 (value: 0.0007, weighted value: 0.0349, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9377
Epoch 8/10, Train Loss: 0.1820 (value: 0.0007, weighted value: 0.0344, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1807 (value: 0.0007, weighted value: 0.0331, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1792 (value: 0.0006, weighted value: 0.0318, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9382
..training done in 72.13 seconds
..evaluation done in 16.76 seconds
Old network+MCTS average reward: 0.71, min: 0.20, max: 1.50, stdev: 0.23
New network+MCTS average reward: 0.72, min: 0.19, max: 1.50, stdev: 0.23
Old bare network average reward: 0.68, min: 0.12, max: 1.44, stdev: 0.23
New bare network average reward: 0.68, min: 0.16, max: 1.42, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.35, max: 1.00, stdev: 0.23
External policy "individual greedy" average reward: 0.52, min: -0.04, max: 1.44, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.15, max: 1.39, stdev: 0.23
New network won 85 and tied 155 out of 300 games (54.17% wins where ties are half wins)
Keeping the new network

Training iteration 422 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.01 seconds
Training examples lengths: [64672, 64828, 64916, 64860, 64877, 65155, 64637, 64906, 64767, 64998]
Total value: 482859.76
Training on 648616 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2020 (value: 0.0009, weighted value: 0.0444, policy: 0.1577, weighted policy: 0.1577), Train Mean Max: 0.9366
Epoch 2/10, Train Loss: 0.1950 (value: 0.0008, weighted value: 0.0419, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9373
Epoch 3/10, Train Loss: 0.1880 (value: 0.0008, weighted value: 0.0382, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9376
Epoch 4/10, Train Loss: 0.1856 (value: 0.0007, weighted value: 0.0369, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9378
Epoch 5/10, Train Loss: 0.1825 (value: 0.0007, weighted value: 0.0342, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9379
Epoch 6/10, Train Loss: 0.1825 (value: 0.0007, weighted value: 0.0344, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9381
Epoch 7/10, Train Loss: 0.1804 (value: 0.0007, weighted value: 0.0331, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9382
Epoch 8/10, Train Loss: 0.1795 (value: 0.0006, weighted value: 0.0322, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1776 (value: 0.0006, weighted value: 0.0308, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1772 (value: 0.0006, weighted value: 0.0305, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9386
..training done in 71.11 seconds
..evaluation done in 15.80 seconds
Old network+MCTS average reward: 0.72, min: 0.12, max: 1.43, stdev: 0.24
New network+MCTS average reward: 0.73, min: 0.07, max: 1.45, stdev: 0.25
Old bare network average reward: 0.69, min: 0.06, max: 1.43, stdev: 0.25
New bare network average reward: 0.69, min: -0.06, max: 1.43, stdev: 0.25
External policy "random" average reward: 0.25, min: -0.33, max: 1.19, stdev: 0.24
External policy "individual greedy" average reward: 0.53, min: -0.03, max: 1.30, stdev: 0.24
External policy "total greedy" average reward: 0.65, min: 0.11, max: 1.44, stdev: 0.23
New network won 75 and tied 169 out of 300 games (53.17% wins where ties are half wins)
Keeping the new network

Training iteration 423 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.91 seconds
Training examples lengths: [64828, 64916, 64860, 64877, 65155, 64637, 64906, 64767, 64998, 64809]
Total value: 482950.24
Training on 648753 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2009 (value: 0.0009, weighted value: 0.0443, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9368
Epoch 2/10, Train Loss: 0.1928 (value: 0.0008, weighted value: 0.0399, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9375
Epoch 3/10, Train Loss: 0.1855 (value: 0.0007, weighted value: 0.0365, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9380
Epoch 4/10, Train Loss: 0.1847 (value: 0.0007, weighted value: 0.0360, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9381
Epoch 5/10, Train Loss: 0.1809 (value: 0.0007, weighted value: 0.0341, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9383
Epoch 6/10, Train Loss: 0.1797 (value: 0.0007, weighted value: 0.0330, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9384
Epoch 7/10, Train Loss: 0.1788 (value: 0.0006, weighted value: 0.0321, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9384
Epoch 8/10, Train Loss: 0.1780 (value: 0.0006, weighted value: 0.0303, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9385
Epoch 9/10, Train Loss: 0.1766 (value: 0.0006, weighted value: 0.0303, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9389
Epoch 10/10, Train Loss: 0.1764 (value: 0.0006, weighted value: 0.0309, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9388
..training done in 67.00 seconds
..evaluation done in 15.70 seconds
Old network+MCTS average reward: 0.74, min: 0.19, max: 1.41, stdev: 0.22
New network+MCTS average reward: 0.74, min: 0.19, max: 1.41, stdev: 0.22
Old bare network average reward: 0.71, min: 0.15, max: 1.41, stdev: 0.23
New bare network average reward: 0.71, min: 0.14, max: 1.41, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.31, max: 1.03, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: -0.05, max: 1.25, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.10, max: 1.31, stdev: 0.23
New network won 56 and tied 175 out of 300 games (47.83% wins where ties are half wins)
Reverting to the old network

Training iteration 424 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.49 seconds
Training examples lengths: [64916, 64860, 64877, 65155, 64637, 64906, 64767, 64998, 64809, 64572]
Total value: 482115.94
Training on 648497 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2239 (value: 0.0012, weighted value: 0.0583, policy: 0.1657, weighted policy: 0.1657), Train Mean Max: 0.9353
Epoch 2/10, Train Loss: 0.2079 (value: 0.0010, weighted value: 0.0500, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9363
Epoch 3/10, Train Loss: 0.1985 (value: 0.0009, weighted value: 0.0457, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9368
Epoch 4/10, Train Loss: 0.1936 (value: 0.0009, weighted value: 0.0426, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9369
Epoch 5/10, Train Loss: 0.1891 (value: 0.0008, weighted value: 0.0393, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9371
Epoch 6/10, Train Loss: 0.1878 (value: 0.0008, weighted value: 0.0387, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9374
Epoch 7/10, Train Loss: 0.1855 (value: 0.0007, weighted value: 0.0369, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9375
Epoch 8/10, Train Loss: 0.1842 (value: 0.0007, weighted value: 0.0361, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9377
Epoch 9/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0349, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9378
Epoch 10/10, Train Loss: 0.1806 (value: 0.0007, weighted value: 0.0332, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9380
..training done in 69.58 seconds
..evaluation done in 16.41 seconds
Old network+MCTS average reward: 0.73, min: 0.11, max: 1.33, stdev: 0.23
New network+MCTS average reward: 0.73, min: 0.11, max: 1.37, stdev: 0.23
Old bare network average reward: 0.69, min: 0.06, max: 1.32, stdev: 0.23
New bare network average reward: 0.69, min: 0.11, max: 1.32, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.31, max: 0.82, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: 0.06, max: 1.26, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.04, max: 1.20, stdev: 0.22
New network won 72 and tied 161 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 425 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.85 seconds
Training examples lengths: [64860, 64877, 65155, 64637, 64906, 64767, 64998, 64809, 64572, 64808]
Total value: 482203.29
Training on 648389 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2043 (value: 0.0009, weighted value: 0.0465, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9364
Epoch 2/10, Train Loss: 0.1952 (value: 0.0008, weighted value: 0.0420, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9372
Epoch 3/10, Train Loss: 0.1892 (value: 0.0008, weighted value: 0.0387, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9375
Epoch 4/10, Train Loss: 0.1863 (value: 0.0007, weighted value: 0.0375, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9378
Epoch 5/10, Train Loss: 0.1837 (value: 0.0007, weighted value: 0.0353, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9378
Epoch 6/10, Train Loss: 0.1831 (value: 0.0007, weighted value: 0.0352, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9379
Epoch 7/10, Train Loss: 0.1798 (value: 0.0006, weighted value: 0.0324, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9381
Epoch 8/10, Train Loss: 0.1802 (value: 0.0007, weighted value: 0.0332, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1776 (value: 0.0006, weighted value: 0.0310, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9385
Epoch 10/10, Train Loss: 0.1775 (value: 0.0006, weighted value: 0.0313, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9386
..training done in 67.04 seconds
..evaluation done in 16.53 seconds
Old network+MCTS average reward: 0.74, min: 0.13, max: 1.44, stdev: 0.23
New network+MCTS average reward: 0.74, min: 0.11, max: 1.44, stdev: 0.23
Old bare network average reward: 0.70, min: 0.10, max: 1.44, stdev: 0.24
New bare network average reward: 0.70, min: 0.10, max: 1.44, stdev: 0.24
External policy "random" average reward: 0.25, min: -0.42, max: 0.96, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: -0.27, max: 1.37, stdev: 0.24
External policy "total greedy" average reward: 0.64, min: 0.12, max: 1.33, stdev: 0.22
New network won 68 and tied 175 out of 300 games (51.83% wins where ties are half wins)
Keeping the new network

Training iteration 426 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.35 seconds
Training examples lengths: [64877, 65155, 64637, 64906, 64767, 64998, 64809, 64572, 64808, 64985]
Total value: 482493.00
Training on 648514 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2008 (value: 0.0009, weighted value: 0.0448, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9369
Epoch 2/10, Train Loss: 0.1926 (value: 0.0008, weighted value: 0.0398, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9376
Epoch 3/10, Train Loss: 0.1862 (value: 0.0007, weighted value: 0.0371, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9381
Epoch 4/10, Train Loss: 0.1836 (value: 0.0007, weighted value: 0.0354, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9382
Epoch 5/10, Train Loss: 0.1820 (value: 0.0007, weighted value: 0.0342, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9382
Epoch 6/10, Train Loss: 0.1803 (value: 0.0007, weighted value: 0.0336, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9384
Epoch 7/10, Train Loss: 0.1787 (value: 0.0007, weighted value: 0.0326, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9386
Epoch 8/10, Train Loss: 0.1771 (value: 0.0006, weighted value: 0.0308, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9387
Epoch 9/10, Train Loss: 0.1779 (value: 0.0006, weighted value: 0.0317, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9388
Epoch 10/10, Train Loss: 0.1744 (value: 0.0006, weighted value: 0.0287, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9390
..training done in 66.25 seconds
..evaluation done in 16.42 seconds
Old network+MCTS average reward: 0.75, min: 0.07, max: 1.40, stdev: 0.24
New network+MCTS average reward: 0.75, min: 0.07, max: 1.40, stdev: 0.23
Old bare network average reward: 0.72, min: 0.07, max: 1.36, stdev: 0.24
New bare network average reward: 0.72, min: 0.07, max: 1.36, stdev: 0.24
External policy "random" average reward: 0.28, min: -0.34, max: 0.88, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: -0.08, max: 1.31, stdev: 0.24
External policy "total greedy" average reward: 0.66, min: 0.07, max: 1.40, stdev: 0.23
New network won 64 and tied 171 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 427 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.54 seconds
Training examples lengths: [65155, 64637, 64906, 64767, 64998, 64809, 64572, 64808, 64985, 64640]
Total value: 483140.86
Training on 648277 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2237 (value: 0.0012, weighted value: 0.0593, policy: 0.1644, weighted policy: 0.1644), Train Mean Max: 0.9355
Epoch 2/10, Train Loss: 0.2062 (value: 0.0010, weighted value: 0.0487, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9365
Epoch 3/10, Train Loss: 0.1985 (value: 0.0009, weighted value: 0.0459, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9370
Epoch 4/10, Train Loss: 0.1941 (value: 0.0009, weighted value: 0.0431, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9372
Epoch 5/10, Train Loss: 0.1899 (value: 0.0008, weighted value: 0.0407, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1854 (value: 0.0008, weighted value: 0.0378, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9377
Epoch 7/10, Train Loss: 0.1851 (value: 0.0007, weighted value: 0.0375, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9378
Epoch 8/10, Train Loss: 0.1830 (value: 0.0007, weighted value: 0.0357, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9380
Epoch 9/10, Train Loss: 0.1818 (value: 0.0007, weighted value: 0.0348, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9381
Epoch 10/10, Train Loss: 0.1799 (value: 0.0006, weighted value: 0.0323, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9383
..training done in 66.66 seconds
..evaluation done in 16.35 seconds
Old network+MCTS average reward: 0.72, min: 0.15, max: 1.32, stdev: 0.23
New network+MCTS average reward: 0.72, min: 0.15, max: 1.32, stdev: 0.23
Old bare network average reward: 0.68, min: 0.11, max: 1.31, stdev: 0.24
New bare network average reward: 0.69, min: 0.11, max: 1.31, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.30, max: 0.87, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: 0.04, max: 1.23, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.14, max: 1.35, stdev: 0.22
New network won 60 and tied 180 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 428 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.84 seconds
Training examples lengths: [64637, 64906, 64767, 64998, 64809, 64572, 64808, 64985, 64640, 64733]
Total value: 483122.15
Training on 647855 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2036 (value: 0.0009, weighted value: 0.0470, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9368
Epoch 2/10, Train Loss: 0.1946 (value: 0.0008, weighted value: 0.0422, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9373
Epoch 3/10, Train Loss: 0.1885 (value: 0.0008, weighted value: 0.0388, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9377
Epoch 4/10, Train Loss: 0.1856 (value: 0.0007, weighted value: 0.0373, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9379
Epoch 5/10, Train Loss: 0.1834 (value: 0.0007, weighted value: 0.0362, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9380
Epoch 6/10, Train Loss: 0.1811 (value: 0.0007, weighted value: 0.0337, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9382
Epoch 7/10, Train Loss: 0.1804 (value: 0.0007, weighted value: 0.0332, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9384
Epoch 8/10, Train Loss: 0.1803 (value: 0.0007, weighted value: 0.0335, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1780 (value: 0.0006, weighted value: 0.0315, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9387
Epoch 10/10, Train Loss: 0.1775 (value: 0.0006, weighted value: 0.0314, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9388
..training done in 66.17 seconds
..evaluation done in 16.45 seconds
Old network+MCTS average reward: 0.73, min: 0.11, max: 1.52, stdev: 0.23
New network+MCTS average reward: 0.73, min: 0.11, max: 1.52, stdev: 0.23
Old bare network average reward: 0.69, min: 0.11, max: 1.52, stdev: 0.23
New bare network average reward: 0.69, min: 0.10, max: 1.52, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.32, max: 0.99, stdev: 0.23
External policy "individual greedy" average reward: 0.52, min: 0.01, max: 1.31, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: 0.01, max: 1.43, stdev: 0.22
New network won 59 and tied 175 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 429 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.51 seconds
Training examples lengths: [64906, 64767, 64998, 64809, 64572, 64808, 64985, 64640, 64733, 64518]
Total value: 482975.65
Training on 647736 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2256 (value: 0.0012, weighted value: 0.0600, policy: 0.1656, weighted policy: 0.1656), Train Mean Max: 0.9349
Epoch 2/10, Train Loss: 0.2096 (value: 0.0010, weighted value: 0.0511, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9361
Epoch 3/10, Train Loss: 0.2006 (value: 0.0009, weighted value: 0.0473, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9366
Epoch 4/10, Train Loss: 0.1943 (value: 0.0009, weighted value: 0.0438, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9369
Epoch 5/10, Train Loss: 0.1908 (value: 0.0008, weighted value: 0.0413, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9371
Epoch 6/10, Train Loss: 0.1887 (value: 0.0008, weighted value: 0.0397, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9373
Epoch 7/10, Train Loss: 0.1862 (value: 0.0008, weighted value: 0.0384, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9376
Epoch 8/10, Train Loss: 0.1847 (value: 0.0007, weighted value: 0.0365, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0351, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1821 (value: 0.0007, weighted value: 0.0342, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9381
..training done in 71.02 seconds
..evaluation done in 16.23 seconds
Old network+MCTS average reward: 0.74, min: 0.22, max: 1.49, stdev: 0.24
New network+MCTS average reward: 0.74, min: 0.16, max: 1.49, stdev: 0.24
Old bare network average reward: 0.70, min: 0.06, max: 1.49, stdev: 0.24
New bare network average reward: 0.71, min: 0.06, max: 1.45, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.39, max: 0.93, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: 0.01, max: 1.23, stdev: 0.22
External policy "total greedy" average reward: 0.67, min: 0.10, max: 1.41, stdev: 0.22
New network won 51 and tied 183 out of 300 games (47.50% wins where ties are half wins)
Reverting to the old network

Training iteration 430 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.92 seconds
Training examples lengths: [64767, 64998, 64809, 64572, 64808, 64985, 64640, 64733, 64518, 65205]
Total value: 483789.31
Training on 648035 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2458 (value: 0.0015, weighted value: 0.0736, policy: 0.1722, weighted policy: 0.1722), Train Mean Max: 0.9335
Epoch 2/10, Train Loss: 0.2226 (value: 0.0012, weighted value: 0.0599, policy: 0.1628, weighted policy: 0.1628), Train Mean Max: 0.9350
Epoch 3/10, Train Loss: 0.2109 (value: 0.0011, weighted value: 0.0547, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9357
Epoch 4/10, Train Loss: 0.2031 (value: 0.0010, weighted value: 0.0500, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9361
Epoch 5/10, Train Loss: 0.1985 (value: 0.0009, weighted value: 0.0473, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9362
Epoch 6/10, Train Loss: 0.1952 (value: 0.0009, weighted value: 0.0450, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9364
Epoch 7/10, Train Loss: 0.1923 (value: 0.0009, weighted value: 0.0426, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9367
Epoch 8/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0410, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9371
Epoch 9/10, Train Loss: 0.1869 (value: 0.0008, weighted value: 0.0385, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9371
Epoch 10/10, Train Loss: 0.1867 (value: 0.0008, weighted value: 0.0379, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9373
..training done in 67.41 seconds
..evaluation done in 16.73 seconds
Old network+MCTS average reward: 0.74, min: 0.18, max: 1.42, stdev: 0.25
New network+MCTS average reward: 0.74, min: 0.18, max: 1.44, stdev: 0.25
Old bare network average reward: 0.70, min: 0.13, max: 1.35, stdev: 0.25
New bare network average reward: 0.70, min: 0.11, max: 1.41, stdev: 0.26
External policy "random" average reward: 0.26, min: -0.56, max: 0.97, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: -0.16, max: 1.25, stdev: 0.24
External policy "total greedy" average reward: 0.66, min: -0.06, max: 1.31, stdev: 0.24
New network won 63 and tied 179 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 431 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.43 seconds
Training examples lengths: [64998, 64809, 64572, 64808, 64985, 64640, 64733, 64518, 65205, 64759]
Total value: 484014.44
Training on 648027 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2063 (value: 0.0010, weighted value: 0.0493, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9359
Epoch 2/10, Train Loss: 0.1976 (value: 0.0009, weighted value: 0.0441, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9365
Epoch 3/10, Train Loss: 0.1933 (value: 0.0009, weighted value: 0.0427, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9370
Epoch 4/10, Train Loss: 0.1877 (value: 0.0008, weighted value: 0.0387, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9373
Epoch 5/10, Train Loss: 0.1883 (value: 0.0008, weighted value: 0.0399, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1841 (value: 0.0007, weighted value: 0.0358, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9376
Epoch 7/10, Train Loss: 0.1830 (value: 0.0007, weighted value: 0.0356, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1803 (value: 0.0007, weighted value: 0.0340, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9380
Epoch 9/10, Train Loss: 0.1819 (value: 0.0007, weighted value: 0.0350, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9380
Epoch 10/10, Train Loss: 0.1794 (value: 0.0006, weighted value: 0.0320, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9383
..training done in 66.50 seconds
..evaluation done in 15.74 seconds
Old network+MCTS average reward: 0.74, min: 0.25, max: 1.46, stdev: 0.23
New network+MCTS average reward: 0.74, min: 0.25, max: 1.46, stdev: 0.23
Old bare network average reward: 0.71, min: 0.18, max: 1.32, stdev: 0.23
New bare network average reward: 0.70, min: 0.19, max: 1.35, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.18, max: 0.87, stdev: 0.21
External policy "individual greedy" average reward: 0.55, min: 0.00, max: 1.33, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: 0.06, max: 1.44, stdev: 0.22
New network won 54 and tied 186 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 432 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.20 seconds
Training examples lengths: [64809, 64572, 64808, 64985, 64640, 64733, 64518, 65205, 64759, 64685]
Total value: 483317.49
Training on 647714 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2276 (value: 0.0013, weighted value: 0.0625, policy: 0.1650, weighted policy: 0.1650), Train Mean Max: 0.9344
Epoch 2/10, Train Loss: 0.2127 (value: 0.0011, weighted value: 0.0532, policy: 0.1596, weighted policy: 0.1596), Train Mean Max: 0.9352
Epoch 3/10, Train Loss: 0.2055 (value: 0.0010, weighted value: 0.0505, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9358
Epoch 4/10, Train Loss: 0.1976 (value: 0.0009, weighted value: 0.0460, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9363
Epoch 5/10, Train Loss: 0.1938 (value: 0.0009, weighted value: 0.0435, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9365
Epoch 6/10, Train Loss: 0.1911 (value: 0.0008, weighted value: 0.0412, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9368
Epoch 7/10, Train Loss: 0.1909 (value: 0.0008, weighted value: 0.0409, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9367
Epoch 8/10, Train Loss: 0.1879 (value: 0.0008, weighted value: 0.0389, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9371
Epoch 9/10, Train Loss: 0.1849 (value: 0.0007, weighted value: 0.0369, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9374
Epoch 10/10, Train Loss: 0.1832 (value: 0.0007, weighted value: 0.0357, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9375
..training done in 67.01 seconds
..evaluation done in 16.42 seconds
Old network+MCTS average reward: 0.73, min: 0.16, max: 1.36, stdev: 0.24
New network+MCTS average reward: 0.73, min: 0.11, max: 1.36, stdev: 0.24
Old bare network average reward: 0.69, min: 0.00, max: 1.36, stdev: 0.25
New bare network average reward: 0.69, min: 0.00, max: 1.35, stdev: 0.25
External policy "random" average reward: 0.25, min: -0.31, max: 1.13, stdev: 0.24
External policy "individual greedy" average reward: 0.53, min: 0.02, max: 1.20, stdev: 0.24
External policy "total greedy" average reward: 0.65, min: 0.09, max: 1.35, stdev: 0.23
New network won 63 and tied 167 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 433 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.58 seconds
Training examples lengths: [64572, 64808, 64985, 64640, 64733, 64518, 65205, 64759, 64685, 64582]
Total value: 483374.85
Training on 647487 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2486 (value: 0.0015, weighted value: 0.0757, policy: 0.1728, weighted policy: 0.1728), Train Mean Max: 0.9328
Epoch 2/10, Train Loss: 0.2272 (value: 0.0013, weighted value: 0.0629, policy: 0.1643, weighted policy: 0.1643), Train Mean Max: 0.9341
Epoch 3/10, Train Loss: 0.2140 (value: 0.0011, weighted value: 0.0562, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9348
Epoch 4/10, Train Loss: 0.2077 (value: 0.0010, weighted value: 0.0521, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9351
Epoch 5/10, Train Loss: 0.2030 (value: 0.0010, weighted value: 0.0498, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9355
Epoch 6/10, Train Loss: 0.1983 (value: 0.0009, weighted value: 0.0462, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9359
Epoch 7/10, Train Loss: 0.1946 (value: 0.0009, weighted value: 0.0433, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9361
Epoch 8/10, Train Loss: 0.1925 (value: 0.0008, weighted value: 0.0418, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9362
Epoch 9/10, Train Loss: 0.1906 (value: 0.0008, weighted value: 0.0411, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9366
Epoch 10/10, Train Loss: 0.1885 (value: 0.0008, weighted value: 0.0392, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9367
..training done in 72.86 seconds
..evaluation done in 17.37 seconds
Old network+MCTS average reward: 0.75, min: 0.04, max: 1.58, stdev: 0.24
New network+MCTS average reward: 0.76, min: 0.13, max: 1.58, stdev: 0.24
Old bare network average reward: 0.72, min: 0.07, max: 1.58, stdev: 0.24
New bare network average reward: 0.72, min: 0.06, max: 1.58, stdev: 0.23
External policy "random" average reward: 0.28, min: -0.47, max: 1.10, stdev: 0.23
External policy "individual greedy" average reward: 0.56, min: 0.09, max: 1.44, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: 0.10, max: 1.51, stdev: 0.23
New network won 64 and tied 177 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 434 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.92 seconds
Training examples lengths: [64808, 64985, 64640, 64733, 64518, 65205, 64759, 64685, 64582, 64777]
Total value: 484050.29
Training on 647692 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2086 (value: 0.0010, weighted value: 0.0508, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9352
Epoch 2/10, Train Loss: 0.1996 (value: 0.0009, weighted value: 0.0449, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9360
Epoch 3/10, Train Loss: 0.1956 (value: 0.0009, weighted value: 0.0435, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9364
Epoch 4/10, Train Loss: 0.1909 (value: 0.0008, weighted value: 0.0407, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9367
Epoch 5/10, Train Loss: 0.1891 (value: 0.0008, weighted value: 0.0402, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1864 (value: 0.0007, weighted value: 0.0373, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1855 (value: 0.0007, weighted value: 0.0369, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1836 (value: 0.0007, weighted value: 0.0352, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1815 (value: 0.0007, weighted value: 0.0339, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1806 (value: 0.0007, weighted value: 0.0330, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9378
..training done in 68.33 seconds
..evaluation done in 17.13 seconds
Old network+MCTS average reward: 0.74, min: 0.03, max: 1.43, stdev: 0.24
New network+MCTS average reward: 0.74, min: 0.06, max: 1.43, stdev: 0.24
Old bare network average reward: 0.71, min: -0.06, max: 1.43, stdev: 0.24
New bare network average reward: 0.71, min: 0.00, max: 1.43, stdev: 0.24
External policy "random" average reward: 0.28, min: -0.31, max: 0.96, stdev: 0.23
External policy "individual greedy" average reward: 0.56, min: -0.06, max: 1.19, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.06, max: 1.24, stdev: 0.21
New network won 71 and tied 165 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 435 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.33 seconds
Training examples lengths: [64985, 64640, 64733, 64518, 65205, 64759, 64685, 64582, 64777, 64578]
Total value: 484005.94
Training on 647462 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2025 (value: 0.0009, weighted value: 0.0462, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9365
Epoch 2/10, Train Loss: 0.1944 (value: 0.0008, weighted value: 0.0416, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9371
Epoch 3/10, Train Loss: 0.1909 (value: 0.0008, weighted value: 0.0395, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9372
Epoch 4/10, Train Loss: 0.1851 (value: 0.0007, weighted value: 0.0363, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1843 (value: 0.0007, weighted value: 0.0364, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9377
Epoch 6/10, Train Loss: 0.1821 (value: 0.0007, weighted value: 0.0346, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9379
Epoch 7/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0335, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9380
Epoch 8/10, Train Loss: 0.1788 (value: 0.0006, weighted value: 0.0322, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9383
Epoch 9/10, Train Loss: 0.1781 (value: 0.0006, weighted value: 0.0317, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1785 (value: 0.0006, weighted value: 0.0318, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9384
..training done in 65.93 seconds
..evaluation done in 15.98 seconds
Old network+MCTS average reward: 0.73, min: 0.10, max: 1.55, stdev: 0.25
New network+MCTS average reward: 0.73, min: 0.17, max: 1.56, stdev: 0.25
Old bare network average reward: 0.70, min: 0.06, max: 1.55, stdev: 0.25
New bare network average reward: 0.70, min: 0.10, max: 1.56, stdev: 0.25
External policy "random" average reward: 0.26, min: -0.28, max: 0.96, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: -0.03, max: 1.33, stdev: 0.24
External policy "total greedy" average reward: 0.65, min: 0.14, max: 1.50, stdev: 0.23
New network won 65 and tied 169 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 436 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.00 seconds
Training examples lengths: [64640, 64733, 64518, 65205, 64759, 64685, 64582, 64777, 64578, 65080]
Total value: 484331.86
Training on 647557 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2252 (value: 0.0012, weighted value: 0.0599, policy: 0.1652, weighted policy: 0.1652), Train Mean Max: 0.9347
Epoch 2/10, Train Loss: 0.2090 (value: 0.0010, weighted value: 0.0502, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9358
Epoch 3/10, Train Loss: 0.2009 (value: 0.0010, weighted value: 0.0478, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9365
Epoch 4/10, Train Loss: 0.1953 (value: 0.0009, weighted value: 0.0437, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9366
Epoch 5/10, Train Loss: 0.1913 (value: 0.0008, weighted value: 0.0415, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9370
Epoch 6/10, Train Loss: 0.1890 (value: 0.0008, weighted value: 0.0399, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9372
Epoch 7/10, Train Loss: 0.1862 (value: 0.0008, weighted value: 0.0380, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1843 (value: 0.0007, weighted value: 0.0366, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1837 (value: 0.0007, weighted value: 0.0360, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9376
Epoch 10/10, Train Loss: 0.1820 (value: 0.0007, weighted value: 0.0344, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9379
..training done in 69.20 seconds
..evaluation done in 16.39 seconds
Old network+MCTS average reward: 0.73, min: 0.24, max: 1.42, stdev: 0.23
New network+MCTS average reward: 0.73, min: 0.19, max: 1.42, stdev: 0.23
Old bare network average reward: 0.70, min: 0.18, max: 1.42, stdev: 0.23
New bare network average reward: 0.70, min: 0.18, max: 1.42, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.35, max: 1.00, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: 0.00, max: 1.20, stdev: 0.21
External policy "total greedy" average reward: 0.65, min: 0.03, max: 1.44, stdev: 0.21
New network won 70 and tied 164 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 437 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.72 seconds
Training examples lengths: [64733, 64518, 65205, 64759, 64685, 64582, 64777, 64578, 65080, 64746]
Total value: 484595.72
Training on 647663 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2026 (value: 0.0009, weighted value: 0.0467, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9365
Epoch 2/10, Train Loss: 0.1952 (value: 0.0009, weighted value: 0.0426, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9372
Epoch 3/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0403, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9377
Epoch 4/10, Train Loss: 0.1864 (value: 0.0008, weighted value: 0.0377, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9377
Epoch 5/10, Train Loss: 0.1840 (value: 0.0007, weighted value: 0.0358, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9379
Epoch 6/10, Train Loss: 0.1831 (value: 0.0007, weighted value: 0.0360, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9381
Epoch 7/10, Train Loss: 0.1800 (value: 0.0007, weighted value: 0.0333, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9381
Epoch 8/10, Train Loss: 0.1798 (value: 0.0007, weighted value: 0.0332, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9385
Epoch 9/10, Train Loss: 0.1782 (value: 0.0006, weighted value: 0.0318, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9386
Epoch 10/10, Train Loss: 0.1788 (value: 0.0007, weighted value: 0.0327, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9386
..training done in 72.13 seconds
..evaluation done in 16.54 seconds
Old network+MCTS average reward: 0.75, min: 0.13, max: 1.38, stdev: 0.22
New network+MCTS average reward: 0.75, min: 0.16, max: 1.44, stdev: 0.22
Old bare network average reward: 0.72, min: 0.01, max: 1.38, stdev: 0.22
New bare network average reward: 0.72, min: 0.01, max: 1.38, stdev: 0.22
External policy "random" average reward: 0.26, min: -0.40, max: 1.09, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: -0.01, max: 1.37, stdev: 0.22
External policy "total greedy" average reward: 0.67, min: 0.17, max: 1.39, stdev: 0.21
New network won 56 and tied 170 out of 300 games (47.00% wins where ties are half wins)
Reverting to the old network

Training iteration 438 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 48.17 seconds
Training examples lengths: [64518, 65205, 64759, 64685, 64582, 64777, 64578, 65080, 64746, 64884]
Total value: 484873.81
Training on 647814 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2250 (value: 0.0012, weighted value: 0.0602, policy: 0.1648, weighted policy: 0.1648), Train Mean Max: 0.9349
Epoch 2/10, Train Loss: 0.2085 (value: 0.0010, weighted value: 0.0509, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9361
Epoch 3/10, Train Loss: 0.1994 (value: 0.0009, weighted value: 0.0466, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9365
Epoch 4/10, Train Loss: 0.1960 (value: 0.0009, weighted value: 0.0455, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9369
Epoch 5/10, Train Loss: 0.1913 (value: 0.0008, weighted value: 0.0413, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9370
Epoch 6/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0399, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1859 (value: 0.0008, weighted value: 0.0383, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9375
Epoch 8/10, Train Loss: 0.1848 (value: 0.0007, weighted value: 0.0366, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1842 (value: 0.0007, weighted value: 0.0366, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1806 (value: 0.0007, weighted value: 0.0335, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9381
..training done in 65.52 seconds
..evaluation done in 16.26 seconds
Old network+MCTS average reward: 0.74, min: 0.10, max: 1.54, stdev: 0.23
New network+MCTS average reward: 0.74, min: 0.13, max: 1.56, stdev: 0.23
Old bare network average reward: 0.70, min: 0.10, max: 1.47, stdev: 0.23
New bare network average reward: 0.70, min: 0.16, max: 1.56, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.46, max: 0.86, stdev: 0.23
External policy "individual greedy" average reward: 0.52, min: -0.20, max: 1.30, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.06, max: 1.25, stdev: 0.22
New network won 73 and tied 173 out of 300 games (53.17% wins where ties are half wins)
Keeping the new network

Training iteration 439 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.64 seconds
Training examples lengths: [65205, 64759, 64685, 64582, 64777, 64578, 65080, 64746, 64884, 64907]
Total value: 485321.58
Training on 648203 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2041 (value: 0.0010, weighted value: 0.0481, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9363
Epoch 2/10, Train Loss: 0.1938 (value: 0.0008, weighted value: 0.0410, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9371
Epoch 3/10, Train Loss: 0.1903 (value: 0.0008, weighted value: 0.0402, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9376
Epoch 4/10, Train Loss: 0.1869 (value: 0.0008, weighted value: 0.0383, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9377
Epoch 5/10, Train Loss: 0.1832 (value: 0.0007, weighted value: 0.0356, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9380
Epoch 6/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0343, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9380
Epoch 7/10, Train Loss: 0.1808 (value: 0.0007, weighted value: 0.0345, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9383
Epoch 8/10, Train Loss: 0.1796 (value: 0.0007, weighted value: 0.0330, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1785 (value: 0.0006, weighted value: 0.0317, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9387
Epoch 10/10, Train Loss: 0.1783 (value: 0.0006, weighted value: 0.0314, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9386
..training done in 65.41 seconds
..evaluation done in 16.65 seconds
Old network+MCTS average reward: 0.76, min: 0.08, max: 1.48, stdev: 0.23
New network+MCTS average reward: 0.75, min: 0.08, max: 1.48, stdev: 0.23
Old bare network average reward: 0.72, min: 0.08, max: 1.48, stdev: 0.23
New bare network average reward: 0.72, min: 0.08, max: 1.48, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.27, max: 0.91, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: 0.04, max: 1.28, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.10, max: 1.25, stdev: 0.21
New network won 73 and tied 151 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 440 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.16 seconds
Training examples lengths: [64759, 64685, 64582, 64777, 64578, 65080, 64746, 64884, 64907, 65009]
Total value: 485229.69
Training on 648007 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2254 (value: 0.0012, weighted value: 0.0607, policy: 0.1648, weighted policy: 0.1648), Train Mean Max: 0.9347
Epoch 2/10, Train Loss: 0.2104 (value: 0.0010, weighted value: 0.0519, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9359
Epoch 3/10, Train Loss: 0.2013 (value: 0.0010, weighted value: 0.0480, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9364
Epoch 4/10, Train Loss: 0.1952 (value: 0.0009, weighted value: 0.0436, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9366
Epoch 5/10, Train Loss: 0.1910 (value: 0.0008, weighted value: 0.0414, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9371
Epoch 6/10, Train Loss: 0.1897 (value: 0.0008, weighted value: 0.0403, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9370
Epoch 7/10, Train Loss: 0.1864 (value: 0.0008, weighted value: 0.0382, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9374
Epoch 8/10, Train Loss: 0.1854 (value: 0.0007, weighted value: 0.0369, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1850 (value: 0.0007, weighted value: 0.0366, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1816 (value: 0.0007, weighted value: 0.0340, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9378
..training done in 66.28 seconds
..evaluation done in 16.61 seconds
Old network+MCTS average reward: 0.75, min: 0.10, max: 1.48, stdev: 0.23
New network+MCTS average reward: 0.75, min: 0.15, max: 1.48, stdev: 0.23
Old bare network average reward: 0.71, min: 0.11, max: 1.48, stdev: 0.24
New bare network average reward: 0.71, min: 0.15, max: 1.54, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.35, max: 0.91, stdev: 0.22
External policy "individual greedy" average reward: 0.55, min: -0.04, max: 1.31, stdev: 0.23
External policy "total greedy" average reward: 0.67, min: 0.09, max: 1.46, stdev: 0.23
New network won 68 and tied 165 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1757697740_iter_440

Training iteration 441 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.69 seconds
Training examples lengths: [64685, 64582, 64777, 64578, 65080, 64746, 64884, 64907, 65009, 64759]
Total value: 485032.33
Training on 648007 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2035 (value: 0.0009, weighted value: 0.0471, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9363
Epoch 2/10, Train Loss: 0.1952 (value: 0.0008, weighted value: 0.0415, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9369
Epoch 3/10, Train Loss: 0.1895 (value: 0.0008, weighted value: 0.0393, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9374
Epoch 4/10, Train Loss: 0.1866 (value: 0.0008, weighted value: 0.0381, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1832 (value: 0.0007, weighted value: 0.0357, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9378
Epoch 6/10, Train Loss: 0.1820 (value: 0.0007, weighted value: 0.0346, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9380
Epoch 7/10, Train Loss: 0.1819 (value: 0.0007, weighted value: 0.0342, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9380
Epoch 8/10, Train Loss: 0.1793 (value: 0.0007, weighted value: 0.0326, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9383
Epoch 9/10, Train Loss: 0.1784 (value: 0.0006, weighted value: 0.0323, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9385
Epoch 10/10, Train Loss: 0.1769 (value: 0.0006, weighted value: 0.0308, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9384
..training done in 65.71 seconds
..evaluation done in 16.78 seconds
Old network+MCTS average reward: 0.74, min: -0.04, max: 1.61, stdev: 0.24
New network+MCTS average reward: 0.74, min: -0.05, max: 1.61, stdev: 0.24
Old bare network average reward: 0.71, min: -0.18, max: 1.61, stdev: 0.24
New bare network average reward: 0.71, min: -0.06, max: 1.61, stdev: 0.24
External policy "random" average reward: 0.27, min: -0.34, max: 1.12, stdev: 0.24
External policy "individual greedy" average reward: 0.55, min: -0.15, max: 1.31, stdev: 0.24
External policy "total greedy" average reward: 0.66, min: -0.06, max: 1.28, stdev: 0.23
New network won 64 and tied 178 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 442 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.58 seconds
Training examples lengths: [64582, 64777, 64578, 65080, 64746, 64884, 64907, 65009, 64759, 64533]
Total value: 484634.33
Training on 647855 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1999 (value: 0.0009, weighted value: 0.0439, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9370
Epoch 2/10, Train Loss: 0.1913 (value: 0.0008, weighted value: 0.0386, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9376
Epoch 3/10, Train Loss: 0.1871 (value: 0.0008, weighted value: 0.0379, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9380
Epoch 4/10, Train Loss: 0.1819 (value: 0.0007, weighted value: 0.0350, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9383
Epoch 5/10, Train Loss: 0.1826 (value: 0.0007, weighted value: 0.0350, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9382
Epoch 6/10, Train Loss: 0.1783 (value: 0.0006, weighted value: 0.0321, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9385
Epoch 7/10, Train Loss: 0.1781 (value: 0.0006, weighted value: 0.0319, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9386
Epoch 8/10, Train Loss: 0.1783 (value: 0.0006, weighted value: 0.0315, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9387
Epoch 9/10, Train Loss: 0.1762 (value: 0.0006, weighted value: 0.0302, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9390
Epoch 10/10, Train Loss: 0.1753 (value: 0.0006, weighted value: 0.0300, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9391
..training done in 66.33 seconds
..evaluation done in 17.39 seconds
Old network+MCTS average reward: 0.74, min: 0.19, max: 1.42, stdev: 0.23
New network+MCTS average reward: 0.74, min: 0.19, max: 1.44, stdev: 0.23
Old bare network average reward: 0.70, min: 0.19, max: 1.42, stdev: 0.24
New bare network average reward: 0.70, min: 0.19, max: 1.44, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.38, max: 1.05, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: 0.02, max: 1.28, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.11, max: 1.37, stdev: 0.22
New network won 61 and tied 168 out of 300 games (48.33% wins where ties are half wins)
Reverting to the old network

Training iteration 443 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.85 seconds
Training examples lengths: [64777, 64578, 65080, 64746, 64884, 64907, 65009, 64759, 64533, 64874]
Total value: 484878.47
Training on 648147 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2205 (value: 0.0011, weighted value: 0.0571, policy: 0.1634, weighted policy: 0.1634), Train Mean Max: 0.9354
Epoch 2/10, Train Loss: 0.2063 (value: 0.0010, weighted value: 0.0492, policy: 0.1572, weighted policy: 0.1572), Train Mean Max: 0.9365
Epoch 3/10, Train Loss: 0.1971 (value: 0.0009, weighted value: 0.0450, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9372
Epoch 4/10, Train Loss: 0.1916 (value: 0.0008, weighted value: 0.0413, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9373
Epoch 5/10, Train Loss: 0.1880 (value: 0.0008, weighted value: 0.0401, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1852 (value: 0.0008, weighted value: 0.0378, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9377
Epoch 7/10, Train Loss: 0.1855 (value: 0.0007, weighted value: 0.0373, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9380
Epoch 8/10, Train Loss: 0.1825 (value: 0.0007, weighted value: 0.0348, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9380
Epoch 9/10, Train Loss: 0.1816 (value: 0.0007, weighted value: 0.0343, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1797 (value: 0.0007, weighted value: 0.0332, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9386
..training done in 65.86 seconds
..evaluation done in 16.60 seconds
Old network+MCTS average reward: 0.73, min: 0.03, max: 1.56, stdev: 0.23
New network+MCTS average reward: 0.74, min: -0.12, max: 1.56, stdev: 0.23
Old bare network average reward: 0.70, min: -0.12, max: 1.56, stdev: 0.23
New bare network average reward: 0.70, min: -0.12, max: 1.56, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.39, max: 0.86, stdev: 0.24
External policy "individual greedy" average reward: 0.55, min: -0.06, max: 1.31, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.11, max: 1.49, stdev: 0.22
New network won 62 and tied 173 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 444 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.88 seconds
Training examples lengths: [64578, 65080, 64746, 64884, 64907, 65009, 64759, 64533, 64874, 64697]
Total value: 484934.59
Training on 648067 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2426 (value: 0.0014, weighted value: 0.0704, policy: 0.1723, weighted policy: 0.1723), Train Mean Max: 0.9341
Epoch 2/10, Train Loss: 0.2199 (value: 0.0012, weighted value: 0.0589, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9354
Epoch 3/10, Train Loss: 0.2071 (value: 0.0011, weighted value: 0.0526, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9361
Epoch 4/10, Train Loss: 0.2002 (value: 0.0010, weighted value: 0.0480, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9363
Epoch 5/10, Train Loss: 0.1968 (value: 0.0009, weighted value: 0.0455, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9365
Epoch 6/10, Train Loss: 0.1919 (value: 0.0009, weighted value: 0.0427, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9370
Epoch 7/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0408, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9371
Epoch 8/10, Train Loss: 0.1876 (value: 0.0008, weighted value: 0.0391, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1851 (value: 0.0008, weighted value: 0.0376, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9376
Epoch 10/10, Train Loss: 0.1833 (value: 0.0007, weighted value: 0.0365, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9378
..training done in 65.24 seconds
..evaluation done in 16.85 seconds
Old network+MCTS average reward: 0.73, min: 0.13, max: 1.53, stdev: 0.22
New network+MCTS average reward: 0.73, min: 0.11, max: 1.52, stdev: 0.22
Old bare network average reward: 0.69, min: 0.07, max: 1.53, stdev: 0.23
New bare network average reward: 0.70, min: 0.11, max: 1.53, stdev: 0.22
External policy "random" average reward: 0.28, min: -0.30, max: 0.95, stdev: 0.20
External policy "individual greedy" average reward: 0.53, min: -0.06, max: 1.23, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: -0.08, max: 1.37, stdev: 0.21
New network won 64 and tied 178 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 445 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.16 seconds
Training examples lengths: [65080, 64746, 64884, 64907, 65009, 64759, 64533, 64874, 64697, 64584]
Total value: 485058.52
Training on 648073 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2057 (value: 0.0010, weighted value: 0.0485, policy: 0.1572, weighted policy: 0.1572), Train Mean Max: 0.9360
Epoch 2/10, Train Loss: 0.1971 (value: 0.0009, weighted value: 0.0436, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9368
Epoch 3/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0401, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9373
Epoch 4/10, Train Loss: 0.1886 (value: 0.0008, weighted value: 0.0395, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9375
Epoch 5/10, Train Loss: 0.1863 (value: 0.0008, weighted value: 0.0383, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0352, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9378
Epoch 7/10, Train Loss: 0.1812 (value: 0.0007, weighted value: 0.0344, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9380
Epoch 8/10, Train Loss: 0.1801 (value: 0.0007, weighted value: 0.0334, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9382
Epoch 9/10, Train Loss: 0.1798 (value: 0.0007, weighted value: 0.0333, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1785 (value: 0.0006, weighted value: 0.0319, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9386
..training done in 71.40 seconds
..evaluation done in 16.25 seconds
Old network+MCTS average reward: 0.72, min: 0.08, max: 1.43, stdev: 0.23
New network+MCTS average reward: 0.72, min: 0.17, max: 1.43, stdev: 0.23
Old bare network average reward: 0.69, min: 0.08, max: 1.43, stdev: 0.23
New bare network average reward: 0.68, min: 0.08, max: 1.40, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.36, max: 0.87, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: -0.01, max: 1.21, stdev: 0.23
External policy "total greedy" average reward: 0.65, min: 0.18, max: 1.33, stdev: 0.22
New network won 69 and tied 171 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 446 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.51 seconds
Training examples lengths: [64746, 64884, 64907, 65009, 64759, 64533, 64874, 64697, 64584, 64801]
Total value: 484657.88
Training on 647794 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2018 (value: 0.0009, weighted value: 0.0452, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9367
Epoch 2/10, Train Loss: 0.1926 (value: 0.0008, weighted value: 0.0399, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9374
Epoch 3/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0383, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9378
Epoch 4/10, Train Loss: 0.1838 (value: 0.0007, weighted value: 0.0360, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9381
Epoch 5/10, Train Loss: 0.1823 (value: 0.0007, weighted value: 0.0349, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9381
Epoch 6/10, Train Loss: 0.1816 (value: 0.0007, weighted value: 0.0344, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9383
Epoch 7/10, Train Loss: 0.1777 (value: 0.0006, weighted value: 0.0318, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9385
Epoch 8/10, Train Loss: 0.1780 (value: 0.0006, weighted value: 0.0320, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9386
Epoch 9/10, Train Loss: 0.1772 (value: 0.0006, weighted value: 0.0308, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9385
Epoch 10/10, Train Loss: 0.1767 (value: 0.0006, weighted value: 0.0298, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9387
..training done in 65.98 seconds
..evaluation done in 17.43 seconds
Old network+MCTS average reward: 0.77, min: 0.23, max: 1.44, stdev: 0.21
New network+MCTS average reward: 0.77, min: 0.23, max: 1.44, stdev: 0.21
Old bare network average reward: 0.73, min: 0.18, max: 1.41, stdev: 0.21
New bare network average reward: 0.73, min: 0.18, max: 1.41, stdev: 0.22
External policy "random" average reward: 0.27, min: -0.33, max: 1.13, stdev: 0.23
External policy "individual greedy" average reward: 0.56, min: -0.04, max: 1.19, stdev: 0.22
External policy "total greedy" average reward: 0.68, min: 0.16, max: 1.25, stdev: 0.21
New network won 76 and tied 143 out of 300 games (49.17% wins where ties are half wins)
Reverting to the old network

Training iteration 447 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.91 seconds
Training examples lengths: [64884, 64907, 65009, 64759, 64533, 64874, 64697, 64584, 64801, 64651]
Total value: 484941.71
Training on 647699 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2244 (value: 0.0012, weighted value: 0.0585, policy: 0.1659, weighted policy: 0.1659), Train Mean Max: 0.9352
Epoch 2/10, Train Loss: 0.2072 (value: 0.0010, weighted value: 0.0487, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9362
Epoch 3/10, Train Loss: 0.1989 (value: 0.0009, weighted value: 0.0459, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9368
Epoch 4/10, Train Loss: 0.1941 (value: 0.0009, weighted value: 0.0427, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9370
Epoch 5/10, Train Loss: 0.1890 (value: 0.0008, weighted value: 0.0399, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9373
Epoch 6/10, Train Loss: 0.1878 (value: 0.0008, weighted value: 0.0385, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9374
Epoch 7/10, Train Loss: 0.1864 (value: 0.0008, weighted value: 0.0376, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9375
Epoch 8/10, Train Loss: 0.1832 (value: 0.0007, weighted value: 0.0359, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1812 (value: 0.0007, weighted value: 0.0340, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1810 (value: 0.0007, weighted value: 0.0341, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9379
..training done in 73.72 seconds
..evaluation done in 16.59 seconds
Old network+MCTS average reward: 0.74, min: -0.02, max: 1.44, stdev: 0.25
New network+MCTS average reward: 0.74, min: 0.02, max: 1.45, stdev: 0.25
Old bare network average reward: 0.70, min: -0.03, max: 1.44, stdev: 0.26
New bare network average reward: 0.70, min: 0.03, max: 1.44, stdev: 0.25
External policy "random" average reward: 0.25, min: -0.45, max: 1.07, stdev: 0.24
External policy "individual greedy" average reward: 0.54, min: -0.15, max: 1.34, stdev: 0.24
External policy "total greedy" average reward: 0.65, min: 0.00, max: 1.48, stdev: 0.23
New network won 85 and tied 157 out of 300 games (54.50% wins where ties are half wins)
Keeping the new network

Training iteration 448 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.72 seconds
Training examples lengths: [64907, 65009, 64759, 64533, 64874, 64697, 64584, 64801, 64651, 64919]
Total value: 484325.14
Training on 647734 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2039 (value: 0.0009, weighted value: 0.0464, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9364
Epoch 2/10, Train Loss: 0.1943 (value: 0.0008, weighted value: 0.0408, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9371
Epoch 3/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0399, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9376
Epoch 4/10, Train Loss: 0.1862 (value: 0.0007, weighted value: 0.0368, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9375
Epoch 5/10, Train Loss: 0.1840 (value: 0.0007, weighted value: 0.0359, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9379
Epoch 6/10, Train Loss: 0.1820 (value: 0.0007, weighted value: 0.0343, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9380
Epoch 7/10, Train Loss: 0.1795 (value: 0.0007, weighted value: 0.0327, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9383
Epoch 8/10, Train Loss: 0.1789 (value: 0.0006, weighted value: 0.0323, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1786 (value: 0.0006, weighted value: 0.0324, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1767 (value: 0.0006, weighted value: 0.0303, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9386
..training done in 72.81 seconds
..evaluation done in 16.67 seconds
Old network+MCTS average reward: 0.73, min: 0.12, max: 1.39, stdev: 0.24
New network+MCTS average reward: 0.73, min: 0.14, max: 1.39, stdev: 0.24
Old bare network average reward: 0.69, min: 0.12, max: 1.39, stdev: 0.25
New bare network average reward: 0.69, min: 0.08, max: 1.38, stdev: 0.25
External policy "random" average reward: 0.26, min: -0.31, max: 1.01, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: 0.01, max: 1.24, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.04, max: 1.25, stdev: 0.23
New network won 73 and tied 155 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 449 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.27 seconds
Training examples lengths: [65009, 64759, 64533, 64874, 64697, 64584, 64801, 64651, 64919, 64950]
Total value: 484437.81
Training on 647777 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1993 (value: 0.0009, weighted value: 0.0440, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9370
Epoch 2/10, Train Loss: 0.1921 (value: 0.0008, weighted value: 0.0398, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9377
Epoch 3/10, Train Loss: 0.1854 (value: 0.0007, weighted value: 0.0365, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9381
Epoch 4/10, Train Loss: 0.1821 (value: 0.0007, weighted value: 0.0350, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9384
Epoch 5/10, Train Loss: 0.1811 (value: 0.0007, weighted value: 0.0342, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9383
Epoch 6/10, Train Loss: 0.1794 (value: 0.0007, weighted value: 0.0327, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9385
Epoch 7/10, Train Loss: 0.1787 (value: 0.0007, weighted value: 0.0327, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9388
Epoch 8/10, Train Loss: 0.1771 (value: 0.0006, weighted value: 0.0305, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9388
Epoch 9/10, Train Loss: 0.1765 (value: 0.0006, weighted value: 0.0307, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9389
Epoch 10/10, Train Loss: 0.1748 (value: 0.0006, weighted value: 0.0293, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9391
..training done in 73.53 seconds
..evaluation done in 16.40 seconds
Old network+MCTS average reward: 0.71, min: 0.14, max: 1.30, stdev: 0.24
New network+MCTS average reward: 0.71, min: 0.14, max: 1.30, stdev: 0.24
Old bare network average reward: 0.68, min: 0.13, max: 1.26, stdev: 0.24
New bare network average reward: 0.68, min: 0.13, max: 1.30, stdev: 0.24
External policy "random" average reward: 0.25, min: -0.39, max: 1.01, stdev: 0.21
External policy "individual greedy" average reward: 0.52, min: -0.10, max: 1.07, stdev: 0.22
External policy "total greedy" average reward: 0.63, min: 0.10, max: 1.22, stdev: 0.22
New network won 66 and tied 169 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 450 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.62 seconds
Training examples lengths: [64759, 64533, 64874, 64697, 64584, 64801, 64651, 64919, 64950, 64663]
Total value: 483645.31
Training on 647431 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1985 (value: 0.0008, weighted value: 0.0421, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9373
Epoch 2/10, Train Loss: 0.1894 (value: 0.0007, weighted value: 0.0374, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9383
Epoch 3/10, Train Loss: 0.1857 (value: 0.0007, weighted value: 0.0368, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9384
Epoch 4/10, Train Loss: 0.1812 (value: 0.0007, weighted value: 0.0343, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9387
Epoch 5/10, Train Loss: 0.1784 (value: 0.0006, weighted value: 0.0323, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9388
Epoch 6/10, Train Loss: 0.1780 (value: 0.0006, weighted value: 0.0323, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9390
Epoch 7/10, Train Loss: 0.1776 (value: 0.0006, weighted value: 0.0320, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9390
Epoch 8/10, Train Loss: 0.1743 (value: 0.0006, weighted value: 0.0289, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9391
Epoch 9/10, Train Loss: 0.1745 (value: 0.0006, weighted value: 0.0293, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9393
Epoch 10/10, Train Loss: 0.1738 (value: 0.0006, weighted value: 0.0285, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9394
..training done in 71.21 seconds
..evaluation done in 16.64 seconds
Old network+MCTS average reward: 0.73, min: 0.19, max: 1.56, stdev: 0.24
New network+MCTS average reward: 0.73, min: 0.19, max: 1.56, stdev: 0.24
Old bare network average reward: 0.69, min: 0.19, max: 1.56, stdev: 0.24
New bare network average reward: 0.69, min: 0.19, max: 1.38, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.39, max: 0.93, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: -0.08, max: 1.30, stdev: 0.23
External policy "total greedy" average reward: 0.66, min: 0.06, max: 1.34, stdev: 0.23
New network won 69 and tied 168 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 451 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.60 seconds
Training examples lengths: [64533, 64874, 64697, 64584, 64801, 64651, 64919, 64950, 64663, 64818]
Total value: 483583.93
Training on 647490 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1983 (value: 0.0008, weighted value: 0.0424, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9376
Epoch 2/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0379, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9382
Epoch 3/10, Train Loss: 0.1835 (value: 0.0007, weighted value: 0.0354, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9387
Epoch 4/10, Train Loss: 0.1794 (value: 0.0007, weighted value: 0.0330, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9387
Epoch 5/10, Train Loss: 0.1780 (value: 0.0006, weighted value: 0.0322, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9390
Epoch 6/10, Train Loss: 0.1773 (value: 0.0006, weighted value: 0.0315, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9389
Epoch 7/10, Train Loss: 0.1757 (value: 0.0006, weighted value: 0.0294, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9391
Epoch 8/10, Train Loss: 0.1747 (value: 0.0006, weighted value: 0.0295, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9393
Epoch 9/10, Train Loss: 0.1740 (value: 0.0006, weighted value: 0.0290, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9396
Epoch 10/10, Train Loss: 0.1727 (value: 0.0006, weighted value: 0.0283, policy: 0.1444, weighted policy: 0.1444), Train Mean Max: 0.9396
..training done in 66.23 seconds
..evaluation done in 16.94 seconds
Old network+MCTS average reward: 0.72, min: 0.12, max: 1.53, stdev: 0.24
New network+MCTS average reward: 0.72, min: 0.18, max: 1.53, stdev: 0.24
Old bare network average reward: 0.69, min: 0.11, max: 1.44, stdev: 0.24
New bare network average reward: 0.69, min: 0.11, max: 1.53, stdev: 0.25
External policy "random" average reward: 0.24, min: -0.44, max: 0.98, stdev: 0.22
External policy "individual greedy" average reward: 0.52, min: 0.02, max: 1.27, stdev: 0.22
External policy "total greedy" average reward: 0.63, min: 0.09, max: 1.45, stdev: 0.23
New network won 69 and tied 159 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 452 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.38 seconds
Training examples lengths: [64874, 64697, 64584, 64801, 64651, 64919, 64950, 64663, 64818, 65064]
Total value: 484876.92
Training on 648021 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2214 (value: 0.0011, weighted value: 0.0555, policy: 0.1659, weighted policy: 0.1659), Train Mean Max: 0.9358
Epoch 2/10, Train Loss: 0.2042 (value: 0.0009, weighted value: 0.0473, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9371
Epoch 3/10, Train Loss: 0.1947 (value: 0.0009, weighted value: 0.0429, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9376
Epoch 4/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0405, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9378
Epoch 5/10, Train Loss: 0.1863 (value: 0.0008, weighted value: 0.0386, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9380
Epoch 6/10, Train Loss: 0.1824 (value: 0.0007, weighted value: 0.0353, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9381
Epoch 7/10, Train Loss: 0.1810 (value: 0.0007, weighted value: 0.0340, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9383
Epoch 8/10, Train Loss: 0.1822 (value: 0.0007, weighted value: 0.0350, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1792 (value: 0.0007, weighted value: 0.0328, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9387
Epoch 10/10, Train Loss: 0.1777 (value: 0.0006, weighted value: 0.0318, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9388
..training done in 73.39 seconds
..evaluation done in 15.90 seconds
Old network+MCTS average reward: 0.73, min: 0.17, max: 1.58, stdev: 0.24
New network+MCTS average reward: 0.73, min: 0.16, max: 1.58, stdev: 0.25
Old bare network average reward: 0.70, min: 0.12, max: 1.58, stdev: 0.25
New bare network average reward: 0.70, min: 0.11, max: 1.54, stdev: 0.25
External policy "random" average reward: 0.28, min: -0.31, max: 1.37, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: -0.02, max: 1.44, stdev: 0.24
External policy "total greedy" average reward: 0.65, min: 0.15, max: 1.55, stdev: 0.23
New network won 69 and tied 171 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 453 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.89 seconds
Training examples lengths: [64697, 64584, 64801, 64651, 64919, 64950, 64663, 64818, 65064, 64602]
Total value: 484508.19
Training on 647749 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2011 (value: 0.0009, weighted value: 0.0450, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9371
Epoch 2/10, Train Loss: 0.1922 (value: 0.0008, weighted value: 0.0394, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9377
Epoch 3/10, Train Loss: 0.1863 (value: 0.0007, weighted value: 0.0373, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9382
Epoch 4/10, Train Loss: 0.1834 (value: 0.0007, weighted value: 0.0356, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9383
Epoch 5/10, Train Loss: 0.1814 (value: 0.0007, weighted value: 0.0342, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9384
Epoch 6/10, Train Loss: 0.1796 (value: 0.0007, weighted value: 0.0330, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9386
Epoch 7/10, Train Loss: 0.1775 (value: 0.0006, weighted value: 0.0318, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9388
Epoch 8/10, Train Loss: 0.1765 (value: 0.0006, weighted value: 0.0304, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9389
Epoch 9/10, Train Loss: 0.1765 (value: 0.0006, weighted value: 0.0305, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9390
Epoch 10/10, Train Loss: 0.1745 (value: 0.0006, weighted value: 0.0294, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9393
..training done in 68.06 seconds
..evaluation done in 16.27 seconds
Old network+MCTS average reward: 0.74, min: 0.18, max: 1.40, stdev: 0.23
New network+MCTS average reward: 0.73, min: 0.18, max: 1.40, stdev: 0.23
Old bare network average reward: 0.70, min: 0.18, max: 1.40, stdev: 0.23
New bare network average reward: 0.70, min: 0.18, max: 1.35, stdev: 0.23
External policy "random" average reward: 0.26, min: -0.42, max: 0.92, stdev: 0.24
External policy "individual greedy" average reward: 0.54, min: -0.03, max: 1.20, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.16, max: 1.31, stdev: 0.22
New network won 62 and tied 171 out of 300 games (49.17% wins where ties are half wins)
Reverting to the old network

Training iteration 454 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.84 seconds
Training examples lengths: [64584, 64801, 64651, 64919, 64950, 64663, 64818, 65064, 64602, 64754]
Total value: 484782.37
Training on 647806 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2244 (value: 0.0012, weighted value: 0.0596, policy: 0.1648, weighted policy: 0.1648), Train Mean Max: 0.9354
Epoch 2/10, Train Loss: 0.2072 (value: 0.0010, weighted value: 0.0492, policy: 0.1580, weighted policy: 0.1580), Train Mean Max: 0.9365
Epoch 3/10, Train Loss: 0.1975 (value: 0.0009, weighted value: 0.0454, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9372
Epoch 4/10, Train Loss: 0.1927 (value: 0.0009, weighted value: 0.0426, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9375
Epoch 5/10, Train Loss: 0.1897 (value: 0.0008, weighted value: 0.0408, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1868 (value: 0.0008, weighted value: 0.0383, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9376
Epoch 7/10, Train Loss: 0.1847 (value: 0.0007, weighted value: 0.0369, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1823 (value: 0.0007, weighted value: 0.0354, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9380
Epoch 9/10, Train Loss: 0.1821 (value: 0.0007, weighted value: 0.0351, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9383
Epoch 10/10, Train Loss: 0.1807 (value: 0.0007, weighted value: 0.0336, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9385
..training done in 68.62 seconds
..evaluation done in 16.14 seconds
Old network+MCTS average reward: 0.72, min: 0.09, max: 1.31, stdev: 0.23
New network+MCTS average reward: 0.72, min: 0.07, max: 1.31, stdev: 0.23
Old bare network average reward: 0.69, min: -0.01, max: 1.31, stdev: 0.23
New bare network average reward: 0.69, min: 0.06, max: 1.31, stdev: 0.23
External policy "random" average reward: 0.24, min: -0.35, max: 0.81, stdev: 0.23
External policy "individual greedy" average reward: 0.52, min: -0.18, max: 1.06, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: 0.03, max: 1.32, stdev: 0.22
New network won 72 and tied 152 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 455 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.52 seconds
Training examples lengths: [64801, 64651, 64919, 64950, 64663, 64818, 65064, 64602, 64754, 64721]
Total value: 485068.62
Training on 647943 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2440 (value: 0.0014, weighted value: 0.0710, policy: 0.1730, weighted policy: 0.1730), Train Mean Max: 0.9341
Epoch 2/10, Train Loss: 0.2197 (value: 0.0012, weighted value: 0.0576, policy: 0.1621, weighted policy: 0.1621), Train Mean Max: 0.9356
Epoch 3/10, Train Loss: 0.2075 (value: 0.0010, weighted value: 0.0524, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9360
Epoch 4/10, Train Loss: 0.2003 (value: 0.0010, weighted value: 0.0480, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9364
Epoch 5/10, Train Loss: 0.1976 (value: 0.0009, weighted value: 0.0462, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9366
Epoch 6/10, Train Loss: 0.1926 (value: 0.0009, weighted value: 0.0431, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9369
Epoch 7/10, Train Loss: 0.1908 (value: 0.0008, weighted value: 0.0414, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9372
Epoch 8/10, Train Loss: 0.1873 (value: 0.0008, weighted value: 0.0390, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1856 (value: 0.0008, weighted value: 0.0376, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9376
Epoch 10/10, Train Loss: 0.1852 (value: 0.0007, weighted value: 0.0363, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9377
..training done in 67.78 seconds
..evaluation done in 16.80 seconds
Old network+MCTS average reward: 0.73, min: 0.19, max: 1.56, stdev: 0.21
New network+MCTS average reward: 0.73, min: 0.21, max: 1.56, stdev: 0.22
Old bare network average reward: 0.69, min: 0.15, max: 1.47, stdev: 0.22
New bare network average reward: 0.69, min: 0.08, max: 1.44, stdev: 0.22
External policy "random" average reward: 0.26, min: -0.31, max: 0.90, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: 0.05, max: 1.15, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.20, max: 1.38, stdev: 0.20
New network won 62 and tied 176 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 456 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.71 seconds
Training examples lengths: [64651, 64919, 64950, 64663, 64818, 65064, 64602, 64754, 64721, 64647]
Total value: 485358.80
Training on 647789 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2054 (value: 0.0010, weighted value: 0.0490, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9364
Epoch 2/10, Train Loss: 0.1965 (value: 0.0009, weighted value: 0.0439, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9371
Epoch 3/10, Train Loss: 0.1912 (value: 0.0008, weighted value: 0.0404, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9374
Epoch 4/10, Train Loss: 0.1880 (value: 0.0008, weighted value: 0.0391, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1862 (value: 0.0008, weighted value: 0.0383, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9378
Epoch 6/10, Train Loss: 0.1819 (value: 0.0007, weighted value: 0.0349, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9380
Epoch 7/10, Train Loss: 0.1817 (value: 0.0007, weighted value: 0.0347, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9382
Epoch 8/10, Train Loss: 0.1819 (value: 0.0007, weighted value: 0.0344, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9383
Epoch 9/10, Train Loss: 0.1785 (value: 0.0006, weighted value: 0.0322, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9386
Epoch 10/10, Train Loss: 0.1776 (value: 0.0006, weighted value: 0.0318, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9387
..training done in 72.52 seconds
..evaluation done in 16.89 seconds
Old network+MCTS average reward: 0.73, min: 0.16, max: 1.38, stdev: 0.24
New network+MCTS average reward: 0.73, min: 0.13, max: 1.36, stdev: 0.24
Old bare network average reward: 0.69, min: 0.05, max: 1.31, stdev: 0.24
New bare network average reward: 0.69, min: 0.13, max: 1.31, stdev: 0.24
External policy "random" average reward: 0.25, min: -0.28, max: 0.97, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: -0.11, max: 1.18, stdev: 0.24
External policy "total greedy" average reward: 0.65, min: 0.11, max: 1.26, stdev: 0.23
New network won 65 and tied 161 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 457 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.51 seconds
Training examples lengths: [64919, 64950, 64663, 64818, 65064, 64602, 64754, 64721, 64647, 64521]
Total value: 485323.29
Training on 647659 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2261 (value: 0.0012, weighted value: 0.0612, policy: 0.1648, weighted policy: 0.1648), Train Mean Max: 0.9349
Epoch 2/10, Train Loss: 0.2094 (value: 0.0010, weighted value: 0.0520, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9358
Epoch 3/10, Train Loss: 0.2026 (value: 0.0010, weighted value: 0.0486, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9363
Epoch 4/10, Train Loss: 0.1959 (value: 0.0009, weighted value: 0.0447, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9367
Epoch 5/10, Train Loss: 0.1925 (value: 0.0008, weighted value: 0.0425, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1895 (value: 0.0008, weighted value: 0.0406, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9372
Epoch 7/10, Train Loss: 0.1865 (value: 0.0008, weighted value: 0.0384, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9374
Epoch 8/10, Train Loss: 0.1848 (value: 0.0007, weighted value: 0.0371, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9376
Epoch 9/10, Train Loss: 0.1836 (value: 0.0007, weighted value: 0.0360, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9378
Epoch 10/10, Train Loss: 0.1831 (value: 0.0007, weighted value: 0.0355, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9379
..training done in 72.05 seconds
..evaluation done in 16.51 seconds
Old network+MCTS average reward: 0.76, min: 0.23, max: 1.70, stdev: 0.25
New network+MCTS average reward: 0.76, min: 0.23, max: 1.70, stdev: 0.25
Old bare network average reward: 0.73, min: 0.18, max: 1.70, stdev: 0.25
New bare network average reward: 0.73, min: 0.18, max: 1.70, stdev: 0.25
External policy "random" average reward: 0.28, min: -0.25, max: 1.08, stdev: 0.23
External policy "individual greedy" average reward: 0.57, min: -0.02, max: 1.55, stdev: 0.24
External policy "total greedy" average reward: 0.68, min: 0.16, max: 1.57, stdev: 0.24
New network won 58 and tied 173 out of 300 games (48.17% wins where ties are half wins)
Reverting to the old network

Training iteration 458 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.28 seconds
Training examples lengths: [64950, 64663, 64818, 65064, 64602, 64754, 64721, 64647, 64521, 64704]
Total value: 484921.21
Training on 647444 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2436 (value: 0.0014, weighted value: 0.0724, policy: 0.1712, weighted policy: 0.1712), Train Mean Max: 0.9335
Epoch 2/10, Train Loss: 0.2234 (value: 0.0012, weighted value: 0.0611, policy: 0.1622, weighted policy: 0.1622), Train Mean Max: 0.9347
Epoch 3/10, Train Loss: 0.2108 (value: 0.0011, weighted value: 0.0540, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9352
Epoch 4/10, Train Loss: 0.2047 (value: 0.0010, weighted value: 0.0505, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9357
Epoch 5/10, Train Loss: 0.1999 (value: 0.0009, weighted value: 0.0473, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9360
Epoch 6/10, Train Loss: 0.1966 (value: 0.0009, weighted value: 0.0451, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9363
Epoch 7/10, Train Loss: 0.1928 (value: 0.0009, weighted value: 0.0427, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9366
Epoch 8/10, Train Loss: 0.1899 (value: 0.0008, weighted value: 0.0409, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9369
Epoch 9/10, Train Loss: 0.1885 (value: 0.0008, weighted value: 0.0395, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9369
Epoch 10/10, Train Loss: 0.1861 (value: 0.0008, weighted value: 0.0377, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9373
..training done in 65.68 seconds
..evaluation done in 16.07 seconds
Old network+MCTS average reward: 0.74, min: 0.16, max: 1.44, stdev: 0.23
New network+MCTS average reward: 0.74, min: 0.16, max: 1.48, stdev: 0.23
Old bare network average reward: 0.70, min: 0.10, max: 1.41, stdev: 0.23
New bare network average reward: 0.70, min: 0.16, max: 1.44, stdev: 0.23
External policy "random" average reward: 0.24, min: -0.48, max: 0.88, stdev: 0.21
External policy "individual greedy" average reward: 0.54, min: 0.01, max: 1.30, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.09, max: 1.23, stdev: 0.22
New network won 80 and tied 143 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 459 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.67 seconds
Training examples lengths: [64663, 64818, 65064, 64602, 64754, 64721, 64647, 64521, 64704, 64859]
Total value: 485476.85
Training on 647353 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2064 (value: 0.0010, weighted value: 0.0496, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9359
Epoch 2/10, Train Loss: 0.2001 (value: 0.0009, weighted value: 0.0460, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9364
Epoch 3/10, Train Loss: 0.1931 (value: 0.0008, weighted value: 0.0424, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9371
Epoch 4/10, Train Loss: 0.1886 (value: 0.0008, weighted value: 0.0393, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9373
Epoch 5/10, Train Loss: 0.1865 (value: 0.0008, weighted value: 0.0383, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9375
Epoch 6/10, Train Loss: 0.1854 (value: 0.0007, weighted value: 0.0371, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9377
Epoch 7/10, Train Loss: 0.1841 (value: 0.0007, weighted value: 0.0359, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1805 (value: 0.0007, weighted value: 0.0338, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1806 (value: 0.0007, weighted value: 0.0341, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9383
Epoch 10/10, Train Loss: 0.1799 (value: 0.0007, weighted value: 0.0329, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9382
..training done in 65.52 seconds
..evaluation done in 16.48 seconds
Old network+MCTS average reward: 0.73, min: -0.06, max: 1.51, stdev: 0.23
New network+MCTS average reward: 0.73, min: -0.06, max: 1.48, stdev: 0.24
Old bare network average reward: 0.70, min: -0.09, max: 1.51, stdev: 0.24
New bare network average reward: 0.70, min: -0.06, max: 1.46, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.40, max: 0.98, stdev: 0.21
External policy "individual greedy" average reward: 0.53, min: -0.09, max: 1.27, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: 0.06, max: 1.36, stdev: 0.22
New network won 73 and tied 163 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 460 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.75 seconds
Training examples lengths: [64818, 65064, 64602, 64754, 64721, 64647, 64521, 64704, 64859, 65104]
Total value: 486318.85
Training on 647794 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2029 (value: 0.0009, weighted value: 0.0455, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9367
Epoch 2/10, Train Loss: 0.1926 (value: 0.0008, weighted value: 0.0396, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9375
Epoch 3/10, Train Loss: 0.1891 (value: 0.0008, weighted value: 0.0388, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9379
Epoch 4/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0353, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9381
Epoch 5/10, Train Loss: 0.1819 (value: 0.0007, weighted value: 0.0350, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9382
Epoch 6/10, Train Loss: 0.1811 (value: 0.0007, weighted value: 0.0340, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9382
Epoch 7/10, Train Loss: 0.1783 (value: 0.0006, weighted value: 0.0323, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9385
Epoch 8/10, Train Loss: 0.1779 (value: 0.0006, weighted value: 0.0317, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9385
Epoch 9/10, Train Loss: 0.1771 (value: 0.0006, weighted value: 0.0309, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9387
Epoch 10/10, Train Loss: 0.1758 (value: 0.0006, weighted value: 0.0301, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9390
..training done in 66.00 seconds
..evaluation done in 16.34 seconds
Old network+MCTS average reward: 0.71, min: 0.18, max: 1.47, stdev: 0.23
New network+MCTS average reward: 0.71, min: 0.12, max: 1.47, stdev: 0.23
Old bare network average reward: 0.68, min: 0.11, max: 1.38, stdev: 0.23
New bare network average reward: 0.68, min: 0.12, max: 1.47, stdev: 0.23
External policy "random" average reward: 0.23, min: -0.25, max: 0.99, stdev: 0.22
External policy "individual greedy" average reward: 0.52, min: 0.01, max: 1.43, stdev: 0.23
External policy "total greedy" average reward: 0.63, min: 0.11, max: 1.36, stdev: 0.22
New network won 69 and tied 164 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1757697740_iter_460

Training iteration 461 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.39 seconds
Training examples lengths: [65064, 64602, 64754, 64721, 64647, 64521, 64704, 64859, 65104, 64693]
Total value: 486437.99
Training on 647669 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1998 (value: 0.0009, weighted value: 0.0434, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9372
Epoch 2/10, Train Loss: 0.1894 (value: 0.0008, weighted value: 0.0379, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9379
Epoch 3/10, Train Loss: 0.1867 (value: 0.0007, weighted value: 0.0370, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9382
Epoch 4/10, Train Loss: 0.1828 (value: 0.0007, weighted value: 0.0350, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9384
Epoch 5/10, Train Loss: 0.1808 (value: 0.0007, weighted value: 0.0338, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9386
Epoch 6/10, Train Loss: 0.1775 (value: 0.0006, weighted value: 0.0314, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9387
Epoch 7/10, Train Loss: 0.1772 (value: 0.0006, weighted value: 0.0317, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9388
Epoch 8/10, Train Loss: 0.1760 (value: 0.0006, weighted value: 0.0299, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9390
Epoch 9/10, Train Loss: 0.1749 (value: 0.0006, weighted value: 0.0298, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9392
Epoch 10/10, Train Loss: 0.1754 (value: 0.0006, weighted value: 0.0297, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9391
..training done in 71.89 seconds
..evaluation done in 16.06 seconds
Old network+MCTS average reward: 0.75, min: 0.04, max: 1.28, stdev: 0.23
New network+MCTS average reward: 0.75, min: 0.12, max: 1.28, stdev: 0.23
Old bare network average reward: 0.71, min: 0.01, max: 1.28, stdev: 0.23
New bare network average reward: 0.71, min: -0.06, max: 1.24, stdev: 0.23
External policy "random" average reward: 0.27, min: -0.51, max: 0.96, stdev: 0.24
External policy "individual greedy" average reward: 0.54, min: -0.07, max: 1.16, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.07, max: 1.30, stdev: 0.22
New network won 78 and tied 165 out of 300 games (53.50% wins where ties are half wins)
Keeping the new network

Training iteration 462 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.11 seconds
Training examples lengths: [64602, 64754, 64721, 64647, 64521, 64704, 64859, 65104, 64693, 64391]
Total value: 485400.92
Training on 646996 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.1967 (value: 0.0008, weighted value: 0.0413, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9375
Epoch 2/10, Train Loss: 0.1891 (value: 0.0008, weighted value: 0.0380, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9381
Epoch 3/10, Train Loss: 0.1823 (value: 0.0007, weighted value: 0.0346, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9386
Epoch 4/10, Train Loss: 0.1800 (value: 0.0007, weighted value: 0.0335, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9388
Epoch 5/10, Train Loss: 0.1777 (value: 0.0006, weighted value: 0.0316, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9388
Epoch 6/10, Train Loss: 0.1777 (value: 0.0006, weighted value: 0.0319, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9390
Epoch 7/10, Train Loss: 0.1745 (value: 0.0006, weighted value: 0.0295, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9392
Epoch 8/10, Train Loss: 0.1748 (value: 0.0006, weighted value: 0.0300, policy: 0.1449, weighted policy: 0.1449), Train Mean Max: 0.9393
Epoch 9/10, Train Loss: 0.1732 (value: 0.0006, weighted value: 0.0284, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9394
Epoch 10/10, Train Loss: 0.1725 (value: 0.0006, weighted value: 0.0281, policy: 0.1444, weighted policy: 0.1444), Train Mean Max: 0.9395
..training done in 67.44 seconds
..evaluation done in 16.55 seconds
Old network+MCTS average reward: 0.77, min: 0.16, max: 1.69, stdev: 0.24
New network+MCTS average reward: 0.77, min: 0.16, max: 1.69, stdev: 0.24
Old bare network average reward: 0.74, min: 0.12, max: 1.61, stdev: 0.25
New bare network average reward: 0.74, min: 0.12, max: 1.61, stdev: 0.25
External policy "random" average reward: 0.29, min: -0.35, max: 1.09, stdev: 0.24
External policy "individual greedy" average reward: 0.57, min: -0.07, max: 1.34, stdev: 0.25
External policy "total greedy" average reward: 0.70, min: 0.09, max: 1.63, stdev: 0.24
New network won 59 and tied 179 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 463 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.36 seconds
Training examples lengths: [64754, 64721, 64647, 64521, 64704, 64859, 65104, 64693, 64391, 64622]
Total value: 485399.84
Training on 647016 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2169 (value: 0.0011, weighted value: 0.0534, policy: 0.1635, weighted policy: 0.1635), Train Mean Max: 0.9360
Epoch 2/10, Train Loss: 0.2025 (value: 0.0009, weighted value: 0.0466, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9372
Epoch 3/10, Train Loss: 0.1940 (value: 0.0009, weighted value: 0.0426, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9377
Epoch 4/10, Train Loss: 0.1882 (value: 0.0008, weighted value: 0.0389, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9379
Epoch 5/10, Train Loss: 0.1857 (value: 0.0008, weighted value: 0.0383, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9382
Epoch 6/10, Train Loss: 0.1831 (value: 0.0007, weighted value: 0.0359, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9383
Epoch 7/10, Train Loss: 0.1806 (value: 0.0007, weighted value: 0.0343, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9385
Epoch 8/10, Train Loss: 0.1793 (value: 0.0007, weighted value: 0.0337, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9387
Epoch 9/10, Train Loss: 0.1784 (value: 0.0006, weighted value: 0.0324, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9388
Epoch 10/10, Train Loss: 0.1768 (value: 0.0006, weighted value: 0.0314, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9389
..training done in 67.34 seconds
..evaluation done in 16.40 seconds
Old network+MCTS average reward: 0.74, min: 0.19, max: 1.29, stdev: 0.22
New network+MCTS average reward: 0.74, min: 0.14, max: 1.29, stdev: 0.22
Old bare network average reward: 0.71, min: 0.08, max: 1.29, stdev: 0.22
New bare network average reward: 0.71, min: 0.08, max: 1.22, stdev: 0.22
External policy "random" average reward: 0.26, min: -0.35, max: 0.84, stdev: 0.22
External policy "individual greedy" average reward: 0.54, min: 0.00, max: 1.08, stdev: 0.22
External policy "total greedy" average reward: 0.66, min: 0.19, max: 1.20, stdev: 0.22
New network won 57 and tied 170 out of 300 games (47.33% wins where ties are half wins)
Reverting to the old network

Training iteration 464 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.45 seconds
Training examples lengths: [64721, 64647, 64521, 64704, 64859, 65104, 64693, 64391, 64622, 64507]
Total value: 484828.51
Training on 646769 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2388 (value: 0.0013, weighted value: 0.0659, policy: 0.1729, weighted policy: 0.1729), Train Mean Max: 0.9345
Epoch 2/10, Train Loss: 0.2164 (value: 0.0011, weighted value: 0.0557, policy: 0.1607, weighted policy: 0.1607), Train Mean Max: 0.9360
Epoch 3/10, Train Loss: 0.2027 (value: 0.0010, weighted value: 0.0486, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9366
Epoch 4/10, Train Loss: 0.1982 (value: 0.0009, weighted value: 0.0467, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9369
Epoch 5/10, Train Loss: 0.1923 (value: 0.0008, weighted value: 0.0424, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9370
Epoch 6/10, Train Loss: 0.1894 (value: 0.0008, weighted value: 0.0409, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9374
Epoch 7/10, Train Loss: 0.1872 (value: 0.0008, weighted value: 0.0387, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9375
Epoch 8/10, Train Loss: 0.1860 (value: 0.0008, weighted value: 0.0377, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1820 (value: 0.0007, weighted value: 0.0353, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9382
Epoch 10/10, Train Loss: 0.1815 (value: 0.0007, weighted value: 0.0352, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9382
..training done in 67.54 seconds
..evaluation done in 16.76 seconds
Old network+MCTS average reward: 0.76, min: 0.19, max: 1.37, stdev: 0.22
New network+MCTS average reward: 0.75, min: 0.19, max: 1.37, stdev: 0.22
Old bare network average reward: 0.72, min: 0.10, max: 1.34, stdev: 0.22
New bare network average reward: 0.73, min: 0.11, max: 1.37, stdev: 0.22
External policy "random" average reward: 0.26, min: -0.36, max: 0.92, stdev: 0.20
External policy "individual greedy" average reward: 0.55, min: 0.01, max: 1.27, stdev: 0.22
External policy "total greedy" average reward: 0.67, min: 0.11, max: 1.28, stdev: 0.21
New network won 66 and tied 163 out of 300 games (49.17% wins where ties are half wins)
Reverting to the old network

Training iteration 465 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.95 seconds
Training examples lengths: [64647, 64521, 64704, 64859, 65104, 64693, 64391, 64622, 64507, 64766]
Total value: 484784.78
Training on 646814 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2566 (value: 0.0016, weighted value: 0.0778, policy: 0.1788, weighted policy: 0.1788), Train Mean Max: 0.9332
Epoch 2/10, Train Loss: 0.2293 (value: 0.0013, weighted value: 0.0643, policy: 0.1650, weighted policy: 0.1650), Train Mean Max: 0.9347
Epoch 3/10, Train Loss: 0.2146 (value: 0.0011, weighted value: 0.0570, policy: 0.1577, weighted policy: 0.1577), Train Mean Max: 0.9354
Epoch 4/10, Train Loss: 0.2053 (value: 0.0010, weighted value: 0.0513, policy: 0.1539, weighted policy: 0.1539), Train Mean Max: 0.9358
Epoch 5/10, Train Loss: 0.2003 (value: 0.0010, weighted value: 0.0487, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9361
Epoch 6/10, Train Loss: 0.1956 (value: 0.0009, weighted value: 0.0450, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9364
Epoch 7/10, Train Loss: 0.1922 (value: 0.0008, weighted value: 0.0425, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9367
Epoch 8/10, Train Loss: 0.1895 (value: 0.0008, weighted value: 0.0409, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9370
Epoch 9/10, Train Loss: 0.1878 (value: 0.0008, weighted value: 0.0393, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9371
Epoch 10/10, Train Loss: 0.1846 (value: 0.0007, weighted value: 0.0369, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9374
..training done in 71.60 seconds
..evaluation done in 16.84 seconds
Old network+MCTS average reward: 0.73, min: 0.13, max: 1.56, stdev: 0.24
New network+MCTS average reward: 0.73, min: 0.11, max: 1.46, stdev: 0.24
Old bare network average reward: 0.70, min: 0.08, max: 1.43, stdev: 0.25
New bare network average reward: 0.70, min: 0.08, max: 1.46, stdev: 0.24
External policy "random" average reward: 0.25, min: -0.42, max: 0.96, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: -0.18, max: 1.31, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.07, max: 1.33, stdev: 0.23
New network won 79 and tied 157 out of 300 games (52.50% wins where ties are half wins)
Keeping the new network

Training iteration 466 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.72 seconds
Training examples lengths: [64521, 64704, 64859, 65104, 64693, 64391, 64622, 64507, 64766, 64716]
Total value: 484499.37
Training on 646883 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2052 (value: 0.0010, weighted value: 0.0489, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9359
Epoch 2/10, Train Loss: 0.1974 (value: 0.0009, weighted value: 0.0440, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9368
Epoch 3/10, Train Loss: 0.1920 (value: 0.0008, weighted value: 0.0421, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9372
Epoch 4/10, Train Loss: 0.1871 (value: 0.0008, weighted value: 0.0386, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9375
Epoch 5/10, Train Loss: 0.1848 (value: 0.0007, weighted value: 0.0374, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9378
Epoch 6/10, Train Loss: 0.1832 (value: 0.0007, weighted value: 0.0362, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9378
Epoch 7/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0345, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9380
Epoch 8/10, Train Loss: 0.1807 (value: 0.0007, weighted value: 0.0340, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9383
Epoch 9/10, Train Loss: 0.1793 (value: 0.0007, weighted value: 0.0333, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9385
Epoch 10/10, Train Loss: 0.1781 (value: 0.0006, weighted value: 0.0323, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9387
..training done in 66.20 seconds
..evaluation done in 16.74 seconds
Old network+MCTS average reward: 0.72, min: 0.22, max: 1.57, stdev: 0.23
New network+MCTS average reward: 0.72, min: 0.23, max: 1.57, stdev: 0.24
Old bare network average reward: 0.69, min: 0.08, max: 1.49, stdev: 0.24
New bare network average reward: 0.69, min: 0.09, max: 1.46, stdev: 0.24
External policy "random" average reward: 0.26, min: -0.33, max: 0.95, stdev: 0.22
External policy "individual greedy" average reward: 0.53, min: 0.05, max: 1.41, stdev: 0.22
External policy "total greedy" average reward: 0.64, min: 0.14, max: 1.41, stdev: 0.23
New network won 68 and tied 160 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 467 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.47 seconds
Training examples lengths: [64704, 64859, 65104, 64693, 64391, 64622, 64507, 64766, 64716, 64835]
Total value: 484711.37
Training on 647197 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2272 (value: 0.0013, weighted value: 0.0628, policy: 0.1644, weighted policy: 0.1644), Train Mean Max: 0.9344
Epoch 2/10, Train Loss: 0.2182 (value: 0.0011, weighted value: 0.0529, policy: 0.1653, weighted policy: 0.1653), Train Mean Max: 0.9349
Epoch 3/10, Train Loss: 0.2026 (value: 0.0010, weighted value: 0.0483, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9361
Epoch 4/10, Train Loss: 0.2005 (value: 0.0009, weighted value: 0.0463, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9364
Epoch 5/10, Train Loss: 0.1924 (value: 0.0009, weighted value: 0.0436, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9370
Epoch 6/10, Train Loss: 0.1897 (value: 0.0008, weighted value: 0.0415, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1933 (value: 0.0008, weighted value: 0.0391, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9366
Epoch 8/10, Train Loss: 0.1872 (value: 0.0008, weighted value: 0.0382, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1854 (value: 0.0008, weighted value: 0.0378, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9378
Epoch 10/10, Train Loss: 0.1853 (value: 0.0007, weighted value: 0.0348, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9375
..training done in 69.20 seconds
..evaluation done in 16.81 seconds
Old network+MCTS average reward: 0.73, min: 0.08, max: 1.41, stdev: 0.24
New network+MCTS average reward: 0.73, min: 0.06, max: 1.41, stdev: 0.25
Old bare network average reward: 0.69, min: -0.02, max: 1.41, stdev: 0.25
New bare network average reward: 0.70, min: -0.02, max: 1.41, stdev: 0.25
External policy "random" average reward: 0.26, min: -0.29, max: 0.94, stdev: 0.23
External policy "individual greedy" average reward: 0.53, min: 0.04, max: 1.22, stdev: 0.23
External policy "total greedy" average reward: 0.64, min: 0.04, max: 1.41, stdev: 0.23
New network won 68 and tied 165 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 468 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.97 seconds
Training examples lengths: [64859, 65104, 64693, 64391, 64622, 64507, 64766, 64716, 64835, 64750]
Total value: 485408.06
Training on 647243 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2083 (value: 0.0010, weighted value: 0.0482, policy: 0.1601, weighted policy: 0.1601), Train Mean Max: 0.9360
Epoch 2/10, Train Loss: 0.1995 (value: 0.0009, weighted value: 0.0448, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9369
Epoch 3/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0395, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9372
Epoch 4/10, Train Loss: 0.1914 (value: 0.0008, weighted value: 0.0388, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9372
Epoch 5/10, Train Loss: 0.1851 (value: 0.0007, weighted value: 0.0363, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9378
Epoch 6/10, Train Loss: 0.1837 (value: 0.0007, weighted value: 0.0358, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9378
Epoch 7/10, Train Loss: 0.1826 (value: 0.0007, weighted value: 0.0348, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9380
Epoch 8/10, Train Loss: 0.1803 (value: 0.0007, weighted value: 0.0326, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1798 (value: 0.0007, weighted value: 0.0325, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1804 (value: 0.0006, weighted value: 0.0318, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9384
..training done in 65.69 seconds
..evaluation done in 16.43 seconds
Old network+MCTS average reward: 0.74, min: 0.06, max: 1.34, stdev: 0.25
New network+MCTS average reward: 0.73, min: 0.13, max: 1.34, stdev: 0.24
Old bare network average reward: 0.70, min: 0.09, max: 1.31, stdev: 0.24
New bare network average reward: 0.70, min: 0.02, max: 1.31, stdev: 0.25
External policy "random" average reward: 0.26, min: -0.43, max: 1.01, stdev: 0.23
External policy "individual greedy" average reward: 0.54, min: -0.19, max: 1.12, stdev: 0.24
External policy "total greedy" average reward: 0.65, min: 0.12, max: 1.21, stdev: 0.22
New network won 70 and tied 148 out of 300 games (48.00% wins where ties are half wins)
Reverting to the old network

Training iteration 469 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.38 seconds
Training examples lengths: [65104, 64693, 64391, 64622, 64507, 64766, 64716, 64835, 64750, 64936]
Total value: 485079.30
Training on 647320 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2295 (value: 0.0012, weighted value: 0.0606, policy: 0.1689, weighted policy: 0.1689), Train Mean Max: 0.9344
Epoch 2/10, Train Loss: 0.2129 (value: 0.0010, weighted value: 0.0519, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9355
Epoch 3/10, Train Loss: 0.2027 (value: 0.0009, weighted value: 0.0470, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9360
Epoch 4/10, Train Loss: 0.1966 (value: 0.0009, weighted value: 0.0437, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9362
Epoch 5/10, Train Loss: 0.1939 (value: 0.0008, weighted value: 0.0419, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9364
Epoch 6/10, Train Loss: 0.1910 (value: 0.0008, weighted value: 0.0405, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9367
Epoch 7/10, Train Loss: 0.1865 (value: 0.0007, weighted value: 0.0374, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9372
Epoch 8/10, Train Loss: 0.1864 (value: 0.0007, weighted value: 0.0372, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9370
Epoch 9/10, Train Loss: 0.1844 (value: 0.0007, weighted value: 0.0349, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9373
Epoch 10/10, Train Loss: 0.1843 (value: 0.0007, weighted value: 0.0347, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9374
..training done in 67.13 seconds
..evaluation done in 16.67 seconds
Old network+MCTS average reward: 0.74, min: 0.06, max: 1.47, stdev: 0.23
New network+MCTS average reward: 0.74, min: 0.06, max: 1.47, stdev: 0.23
Old bare network average reward: 0.70, min: 0.06, max: 1.47, stdev: 0.23
New bare network average reward: 0.70, min: 0.06, max: 1.47, stdev: 0.23
External policy "random" average reward: 0.25, min: -0.28, max: 0.87, stdev: 0.21
External policy "individual greedy" average reward: 0.53, min: -0.14, max: 1.35, stdev: 0.22
External policy "total greedy" average reward: 0.65, min: -0.02, max: 1.31, stdev: 0.21
New network won 71 and tied 156 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 470 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.69 seconds
Training examples lengths: [64693, 64391, 64622, 64507, 64766, 64716, 64835, 64750, 64936, 64844]
Total value: 484830.43
Training on 647060 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2491 (value: 0.0014, weighted value: 0.0720, policy: 0.1771, weighted policy: 0.1771), Train Mean Max: 0.9331
Epoch 2/10, Train Loss: 0.2248 (value: 0.0012, weighted value: 0.0606, policy: 0.1642, weighted policy: 0.1642), Train Mean Max: 0.9345
Epoch 3/10, Train Loss: 0.2127 (value: 0.0011, weighted value: 0.0537, policy: 0.1590, weighted policy: 0.1590), Train Mean Max: 0.9348
Epoch 4/10, Train Loss: 0.2056 (value: 0.0010, weighted value: 0.0500, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9351
Epoch 5/10, Train Loss: 0.2006 (value: 0.0009, weighted value: 0.0471, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9356
Epoch 6/10, Train Loss: 0.1958 (value: 0.0009, weighted value: 0.0436, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9357
Epoch 7/10, Train Loss: 0.1929 (value: 0.0008, weighted value: 0.0415, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9359
Epoch 8/10, Train Loss: 0.1915 (value: 0.0008, weighted value: 0.0405, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9363
Epoch 9/10, Train Loss: 0.1891 (value: 0.0008, weighted value: 0.0385, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9364
Epoch 10/10, Train Loss: 0.1856 (value: 0.0007, weighted value: 0.0363, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9367
..training done in 70.22 seconds
..evaluation done in 16.22 seconds
Old network+MCTS average reward: 0.76, min: 0.11, max: 1.49, stdev: 0.23
New network+MCTS average reward: 0.76, min: 0.16, max: 1.49, stdev: 0.23
Old bare network average reward: 0.72, min: 0.07, max: 1.44, stdev: 0.23
New bare network average reward: 0.72, min: 0.11, max: 1.46, stdev: 0.23
External policy "random" average reward: 0.28, min: -0.33, max: 0.96, stdev: 0.23
External policy "individual greedy" average reward: 0.55, min: -0.03, max: 1.21, stdev: 0.22
External policy "total greedy" average reward: 0.67, min: 0.16, max: 1.37, stdev: 0.21
New network won 86 and tied 156 out of 300 games (54.67% wins where ties are half wins)
Keeping the new network
