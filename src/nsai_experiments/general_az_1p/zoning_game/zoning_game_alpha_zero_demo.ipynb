{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d34cddc4",
   "metadata": {},
   "source": [
    "# Zoning Game AlphaZero Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b005e1",
   "metadata": {},
   "source": [
    "## Individually construct bits and pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e41415c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nsai_experiments.general_az_1p.game import Game\n",
    "from nsai_experiments.general_az_1p.policy_value_net import PolicyValueNet\n",
    "from nsai_experiments.general_az_1p.agent import Agent\n",
    "\n",
    "from nsai_experiments.general_az_1p.zoning_game.zoning_game_az_impl import ZoningGameGame\n",
    "from nsai_experiments.general_az_1p.zoning_game.zoning_game_az_impl import ZoningGamePolicyValueNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97adf08",
   "metadata": {},
   "source": [
    "### The `Game`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ddfb6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tile grid:\n",
      "[[0 0 0 5 1 0]\n",
      " [0 4 0 0 0 0]\n",
      " [0 3 0 3 2 4]\n",
      " [0 0 0 0 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 0 3 1]]\n",
      "Tile queue (leftmost next): [1 4 2 1 5 2 3 3 2 3 1 1 1 4 2 2 1 5 5 2 1 5 3 2 5 1 0 0 0 0 0 0 0 0 0 0]\n",
      "where 0 = EMPTY, 1 = RESIDENTIAL, 2 = COMMERCIAL, 3 = INDUSTRIAL, 4 = DOWNTOWN, 5 = PARK.\n",
      "After 0 moves, current grid score is 3; terminated = False, truncated = False.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mygame = ZoningGameGame()\n",
    "assert isinstance(mygame, Game)\n",
    "mygame.reset_wrapper(seed=47)\n",
    "print(mygame.render().read())  # type: ignore[union-attr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbc3da8",
   "metadata": {},
   "source": [
    "### The `PolicyValueNet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cd02249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from disk: ../../zoning_game/zg_data/create_policy_indiv_greedy__20000\n",
      "Done loading, shuffling, splitting data\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nsai_experiments.zoning_game.notebook_utils import get_zg_data\n",
    "from nsai_experiments.zoning_game.zg_policy import create_policy_indiv_greedy\n",
    "\n",
    "torch.manual_seed(47)\n",
    "n_games = 20_000\n",
    "savedir = \"../../zoning_game/zg_data\"\n",
    "valid_frac = 0.15\n",
    "test_frac = 0.15\n",
    "\n",
    "states_tensor, values_tensor, moves_tensor = get_zg_data(create_policy_indiv_greedy, n_games = n_games, savedir = savedir)\n",
    "indices = torch.randperm(len(values_tensor))\n",
    "full_dataset_3 = torch.utils.data.TensorDataset(states_tensor[indices], moves_tensor[indices], values_tensor[indices])\n",
    "\n",
    "valid_size_3 = int(valid_frac * len(full_dataset_3))\n",
    "test_size_3 = int(test_frac * len(full_dataset_3))\n",
    "train_size_3 = len(full_dataset_3) - valid_size_3 - test_size_3\n",
    "train_dataset_3, valid_dataset_3, test_dataset_3 = torch.utils.data.random_split(full_dataset_3, [train_size_3, valid_size_3, test_size_3])\n",
    "print(\"Done loading, shuffling, splitting data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ae01ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.02878359, 0.02750564, 0.02725394, 0.02644118, 0.02703556,\n",
       "        0.02739549, 0.02875161, 0.02773597, 0.02912001, 0.02731342,\n",
       "        0.02837674, 0.02837252, 0.02701781, 0.02933569, 0.02749422,\n",
       "        0.02898956, 0.02915286, 0.025719  , 0.02784868, 0.02766767,\n",
       "        0.02893063, 0.02690739, 0.02791698, 0.0266863 , 0.02847183,\n",
       "        0.0276081 , 0.02711248, 0.02722818, 0.02664321, 0.02836256,\n",
       "        0.02602571, 0.02749578, 0.02793976, 0.02626782, 0.02930332,\n",
       "        0.02978883], dtype=float32),\n",
       " 0.02528020739555359)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mynet = ZoningGamePolicyValueNet()\n",
    "assert isinstance(mynet, PolicyValueNet)\n",
    "# mynet.train(train_dataset_3, needs_reshape=False)  # takes a little while\n",
    "mynet.predict(mygame.obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f12d2d",
   "metadata": {},
   "source": [
    "### The `Agent` and `MCTS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7a9d5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.WARN)  # TODO\n",
    "myagent = Agent(mygame, mynet)\n",
    "train_examples = myagent.play_single_game()\n",
    "print(len(train_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d47fd6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 2190 examples\n",
      "Epoch 1/10, Train Loss: 1361.8778\n",
      "Epoch 10/10, Train Loss: 498.0550\n",
      "Old network average reward: 27.0\n",
      "New network average reward: 25.4\n",
      "New network won 4 out of 10 games (40.00%)\n",
      "Reverting to the old network\n"
     ]
    }
   ],
   "source": [
    "myagent.play_and_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08db6f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration 1 of 10: will play 100 games, train, and evaluate on 10 games\n",
      "Training on 4399 examples\n",
      "Epoch 1/10, Train Loss: 1160.3925\n",
      "Epoch 10/10, Train Loss: 380.0371\n",
      "Old network average reward: 18.7\n",
      "New network average reward: 26.4\n",
      "New network won 6 out of 10 games (60.00%)\n",
      "Keeping the new network\n",
      "Training iteration 2 of 10: will play 100 games, train, and evaluate on 10 games\n",
      "Training on 6572 examples\n",
      "Epoch 1/10, Train Loss: 311.0075\n",
      "Epoch 10/10, Train Loss: 25.6942\n",
      "Old network average reward: 32.1\n",
      "New network average reward: 30.9\n",
      "New network won 3 out of 10 games (30.00%)\n",
      "Reverting to the old network\n",
      "Training iteration 3 of 10: will play 100 games, train, and evaluate on 10 games\n",
      "Training on 8751 examples\n",
      "Epoch 1/10, Train Loss: 281.8561\n",
      "Epoch 10/10, Train Loss: 21.5699\n",
      "Old network average reward: 34.3\n",
      "New network average reward: 34.9\n",
      "New network won 7 out of 10 games (70.00%)\n",
      "Keeping the new network\n",
      "Training iteration 4 of 10: will play 100 games, train, and evaluate on 10 games\n",
      "Training on 10884 examples\n",
      "Epoch 1/10, Train Loss: 73.5955\n",
      "Epoch 10/10, Train Loss: 15.9720\n",
      "Old network average reward: 23.0\n",
      "New network average reward: 21.9\n",
      "New network won 4 out of 10 games (40.00%)\n",
      "Reverting to the old network\n",
      "Training iteration 5 of 10: will play 100 games, train, and evaluate on 10 games\n",
      "Training on 13007 examples\n",
      "Epoch 1/10, Train Loss: 94.3367\n",
      "Epoch 10/10, Train Loss: 17.0352\n",
      "Old network average reward: 32.3\n",
      "New network average reward: 24.2\n",
      "New network won 3 out of 10 games (30.00%)\n",
      "Reverting to the old network\n",
      "Training iteration 6 of 10: will play 100 games, train, and evaluate on 10 games\n",
      "Training on 15165 examples\n",
      "Epoch 1/10, Train Loss: 99.0416\n",
      "Epoch 10/10, Train Loss: 15.3198\n",
      "Old network average reward: 20.4\n",
      "New network average reward: 12.9\n",
      "New network won 2 out of 10 games (20.00%)\n",
      "Reverting to the old network\n",
      "Training iteration 7 of 10: will play 100 games, train, and evaluate on 10 games\n",
      "Training on 17348 examples\n",
      "Epoch 1/10, Train Loss: 110.8669\n",
      "Epoch 10/10, Train Loss: 15.9945\n",
      "Old network average reward: 46.2\n",
      "New network average reward: 42.5\n",
      "New network won 5 out of 10 games (50.00%)\n",
      "Reverting to the old network\n",
      "Training iteration 8 of 10: will play 100 games, train, and evaluate on 10 games\n",
      "Training on 19500 examples\n",
      "Epoch 1/10, Train Loss: 114.2108\n",
      "Epoch 10/10, Train Loss: 16.4618\n",
      "Old network average reward: 26.7\n",
      "New network average reward: 27.5\n",
      "New network won 4 out of 10 games (40.00%)\n",
      "Reverting to the old network\n",
      "Training iteration 9 of 10: will play 100 games, train, and evaluate on 10 games\n",
      "Training on 21667 examples\n",
      "Epoch 1/10, Train Loss: 103.8188\n",
      "Epoch 10/10, Train Loss: 15.1923\n",
      "Old network average reward: 16.8\n",
      "New network average reward: 17.2\n",
      "New network won 6 out of 10 games (60.00%)\n",
      "Keeping the new network\n",
      "Training iteration 10 of 10: will play 100 games, train, and evaluate on 10 games\n",
      "Training on 23818 examples\n",
      "Epoch 1/10, Train Loss: 92.7347\n",
      "Epoch 10/10, Train Loss: 15.4088\n",
      "Old network average reward: 36.0\n",
      "New network average reward: 30.3\n",
      "New network won 2 out of 10 games (20.00%)\n",
      "Reverting to the old network\n"
     ]
    }
   ],
   "source": [
    "myagent.play_train_multiple(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b5deaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
