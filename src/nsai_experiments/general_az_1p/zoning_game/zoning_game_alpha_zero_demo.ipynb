{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d34cddc4",
   "metadata": {},
   "source": [
    "# Zoning Game AlphaZero Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b005e1",
   "metadata": {},
   "source": [
    "## Individually construct bits and pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e41415c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nsai_experiments.general_az_1p.utils import disable_numpy_multithreading, use_deterministic_cuda\n",
    "disable_numpy_multithreading()\n",
    "use_deterministic_cuda()\n",
    "\n",
    "from nsai_experiments.general_az_1p.game import Game\n",
    "from nsai_experiments.general_az_1p.policy_value_net import PolicyValueNet\n",
    "from nsai_experiments.general_az_1p.agent import Agent\n",
    "\n",
    "from nsai_experiments.general_az_1p.zoning_game.zoning_game_az_impl import ZoningGameGame\n",
    "from nsai_experiments.general_az_1p.zoning_game.zoning_game_az_impl import ZoningGamePolicyValueNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97adf08",
   "metadata": {},
   "source": [
    "### The `Game`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ddfb6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tile grid:\n",
      "[[0 0 0 5 1 0]\n",
      " [0 4 0 0 0 0]\n",
      " [0 3 0 3 2 4]\n",
      " [0 0 0 0 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 0 3 1]]\n",
      "Tile queue (leftmost next): [1 4 2 1 5 2 3 3 2 3 1 1 1 4 2 2 1 5 5 2 1 5 3 2 5 1 0 0 0 0 0 0 0 0 0 0]\n",
      "where 0 = EMPTY, 1 = RESIDENTIAL, 2 = COMMERCIAL, 3 = INDUSTRIAL, 4 = DOWNTOWN, 5 = PARK.\n",
      "After 0 moves, current grid score is 3; terminated = False, truncated = False.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mygame = ZoningGameGame()\n",
    "assert isinstance(mygame, Game)\n",
    "mygame.reset_wrapper(seed=47)\n",
    "print(mygame.render().read())  # type: ignore[union-attr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbc3da8",
   "metadata": {},
   "source": [
    "### The `PolicyValueNet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cd02249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from disk: ../../zoning_game/zg_data/create_policy_indiv_greedy__20000\n",
      "Done loading, shuffling, splitting data\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nsai_experiments.zoning_game.notebook_utils import get_zg_data\n",
    "from nsai_experiments.zoning_game.zg_policy import create_policy_indiv_greedy\n",
    "\n",
    "torch.manual_seed(47)\n",
    "n_games = 20_000\n",
    "savedir = \"../../zoning_game/zg_data\"\n",
    "valid_frac = 0.15\n",
    "test_frac = 0.15\n",
    "\n",
    "states_tensor, values_tensor, moves_tensor = get_zg_data(create_policy_indiv_greedy, n_games = n_games, savedir = savedir)\n",
    "indices = torch.randperm(len(values_tensor))\n",
    "full_dataset_3 = torch.utils.data.TensorDataset(states_tensor[indices], moves_tensor[indices], values_tensor[indices])\n",
    "\n",
    "valid_size_3 = int(valid_frac * len(full_dataset_3))\n",
    "test_size_3 = int(test_frac * len(full_dataset_3))\n",
    "train_size_3 = len(full_dataset_3) - valid_size_3 - test_size_3\n",
    "train_dataset_3, valid_dataset_3, test_dataset_3 = torch.utils.data.random_split(full_dataset_3, [train_size_3, valid_size_3, test_size_3])\n",
    "print(\"Done loading, shuffling, splitting data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ae01ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network training will occur on device 'mps'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.02805594, 0.02738343, 0.02839885, 0.02788622, 0.0291001 ,\n",
       "        0.02763609, 0.02681519, 0.02864029, 0.02788756, 0.02838092,\n",
       "        0.02778699, 0.02877449, 0.02703443, 0.02906117, 0.02899825,\n",
       "        0.02776492, 0.02859806, 0.02702451, 0.02705516, 0.02807434,\n",
       "        0.02658405, 0.02678845, 0.02949526, 0.02644843, 0.02754996,\n",
       "        0.02703502, 0.02698193, 0.02768094, 0.02770268, 0.02843505,\n",
       "        0.02523457, 0.02692246, 0.02690465, 0.0284914 , 0.02925407,\n",
       "        0.02813419], dtype=float32),\n",
       " array(0.02830549, dtype=float32))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mynet = ZoningGamePolicyValueNet(random_seed=47)\n",
    "assert isinstance(mynet, PolicyValueNet)\n",
    "mynet.predict(mygame.obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f12d2d",
   "metadata": {},
   "source": [
    "### The `Agent` and `MCTS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7a9d5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNG seeds are fully specified\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.WARN)  # TODO\n",
    "myagent = Agent(mygame, mynet, random_seeds={\"mcts\": 48, \"train\": 49, \"eval\": 50})\n",
    "train_examples = myagent.play_single_game()\n",
    "print(len(train_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d47fd6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..games done in 4.81 seconds\n",
      "Training on 2146 examples\n",
      "Training with 3 batches of size 1024\n",
      "Epoch 1/10, Train Loss: 14.3882\n",
      "Epoch 10/10, Train Loss: 12.5672\n",
      "..training done in 1.87 seconds\n",
      "..evaluation done in 2.77 seconds\n",
      "Old network+MCTS average reward: 0.31, min: -0.05, max: 0.77, stdev: 0.23\n",
      "New network+MCTS average reward: 0.33, min: -0.01, max: 0.73, stdev: 0.20\n",
      "Old bare network average reward: 0.33, min: 0.00, max: 0.76, stdev: 0.22\n",
      "New bare network average reward: 0.31, min: 0.03, max: 0.75, stdev: 0.20\n",
      "New network won 9 and tied 3 out of 20 games (52.50% wins where ties are half wins)\n",
      "Reverting to the old network\n"
     ]
    }
   ],
   "source": [
    "myagent.play_and_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ab5688",
   "metadata": {},
   "source": [
    "Does it perform better with some supervised pretraining?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a62e8ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping reshape of `examples`.\n",
      "Training with 296 batches of size 1024\n",
      "Epoch 1/10, Train Loss: 204.2436\n",
      "Epoch 10/10, Train Loss: 15.3870\n"
     ]
    }
   ],
   "source": [
    "mynet.train(train_dataset_3, needs_reshape=False);  # takes a little while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad85f8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..games done in 4.62 seconds\n",
      "Training on 4328 examples\n",
      "Training with 5 batches of size 1024\n",
      "Epoch 1/10, Train Loss: 14.3741\n",
      "Epoch 10/10, Train Loss: 11.5443\n",
      "..training done in 1.21 seconds\n",
      "..evaluation done in 2.76 seconds\n",
      "Old network+MCTS average reward: 0.26, min: -0.19, max: 0.85, stdev: 0.23\n",
      "New network+MCTS average reward: 0.30, min: -0.14, max: 0.88, stdev: 0.22\n",
      "Old bare network average reward: 0.29, min: -0.24, max: 0.84, stdev: 0.24\n",
      "New bare network average reward: 0.27, min: -0.27, max: 0.84, stdev: 0.26\n",
      "New network won 14 and tied 1 out of 20 games (72.50% wins where ties are half wins)\n",
      "Keeping the new network\n"
     ]
    }
   ],
   "source": [
    "myagent.play_and_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d10a124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
