Neural network training params are {'epochs': 10, 'batch_size': 2048, 'learning_rate': 0.0003, 'l1_lambda': 0, 'weight_decay': 1e-05, 'value_weight': 50.0, 'policy_weight': 1.0, 'persist_optimizer': True}
Neural network training will occur on device 'cuda'
Agent config: n_games_per_train=3000, n_games_per_eval=300, n_past_iterations_to_train=10, threshold_to_keep=0.5, reward_discount=1.0, mcts_params={'n_simulations': 100, 'c_exploration': 0.5}, n_procs=None, external_policy=None, external_policy_creators_to_pit={'random': <function create_policy_random at 0x7f8f1b58ed40>, 'individual greedy': <function create_policy_indiv_greedy at 0x7f8f1b58efc0>, 'total greedy': <function create_policy_total_greedy at 0x7f8f1b58f1a0>}
RNG seeds are fully specified

Training iteration 1 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 34.99 seconds
Training examples lengths: [64795]
Total value: 18670.01
Training on 64795 examples
Initializing optimizer
Training with 32 batches of size 2048
Epoch 1/10, Train Loss: 6.1523 (value: 0.0514, weighted value: 2.5697, policy: 3.5827, weighted policy: 3.5827), Train Mean Max: 0.0298
Epoch 2/10, Train Loss: 4.7697 (value: 0.0238, weighted value: 1.1894, policy: 3.5803, weighted policy: 3.5803), Train Mean Max: 0.0300
Epoch 3/10, Train Loss: 4.1761 (value: 0.0120, weighted value: 0.5983, policy: 3.5778, weighted policy: 3.5778), Train Mean Max: 0.0298
Epoch 4/10, Train Loss: 4.0958 (value: 0.0104, weighted value: 0.5208, policy: 3.5751, weighted policy: 3.5751), Train Mean Max: 0.0303
Epoch 5/10, Train Loss: 4.0360 (value: 0.0093, weighted value: 0.4643, policy: 3.5718, weighted policy: 3.5718), Train Mean Max: 0.0304
Epoch 6/10, Train Loss: 3.9896 (value: 0.0084, weighted value: 0.4219, policy: 3.5677, weighted policy: 3.5677), Train Mean Max: 0.0303
Epoch 7/10, Train Loss: 3.9258 (value: 0.0073, weighted value: 0.3632, policy: 3.5626, weighted policy: 3.5626), Train Mean Max: 0.0306
Epoch 8/10, Train Loss: 3.8826 (value: 0.0065, weighted value: 0.3264, policy: 3.5562, weighted policy: 3.5562), Train Mean Max: 0.0309
Epoch 9/10, Train Loss: 3.8286 (value: 0.0056, weighted value: 0.2804, policy: 3.5481, weighted policy: 3.5481), Train Mean Max: 0.0312
Epoch 10/10, Train Loss: 3.7795 (value: 0.0048, weighted value: 0.2416, policy: 3.5380, weighted policy: 3.5380), Train Mean Max: 0.0321
..training done in 63.11 seconds
..evaluation done in 10.66 seconds
Old network+MCTS average reward: 0.3034, min: -0.2037, max: 1.0556, stdev: 0.2149
New network+MCTS average reward: 0.3628, min: -0.2315, max: 1.1667, stdev: 0.2194
Old bare network average reward: 0.2638, min: -0.3426, max: 0.9815, stdev: 0.2244
New bare network average reward: 0.2622, min: -0.3426, max: 0.9815, stdev: 0.2282
External policy "random" average reward: 0.2546, min: -0.2407, max: 0.9444, stdev: 0.2163
External policy "individual greedy" average reward: 0.5443, min: 0.0463, max: 1.4259, stdev: 0.2286
External policy "total greedy" average reward: 0.6527, min: 0.2222, max: 1.5093, stdev: 0.2184
New network won 196 and tied 8 out of 300 games (66.67% wins where ties are half wins)
Keeping the new network

Training iteration 2 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 43.33 seconds
Training examples lengths: [64795, 65039]
Total value: 42842.87
Training on 129834 examples
Training with 64 batches of size 2048
Epoch 1/10, Train Loss: 4.0782 (value: 0.0112, weighted value: 0.5599, policy: 3.5184, weighted policy: 3.5184), Train Mean Max: 0.0325
Epoch 2/10, Train Loss: 3.8966 (value: 0.0085, weighted value: 0.4255, policy: 3.4711, weighted policy: 3.4711), Train Mean Max: 0.0359
Epoch 3/10, Train Loss: 3.7445 (value: 0.0070, weighted value: 0.3500, policy: 3.3945, weighted policy: 3.3945), Train Mean Max: 0.0430
Epoch 4/10, Train Loss: 3.5882 (value: 0.0061, weighted value: 0.3031, policy: 3.2851, weighted policy: 3.2851), Train Mean Max: 0.0558
Epoch 5/10, Train Loss: 3.4825 (value: 0.0065, weighted value: 0.3258, policy: 3.1567, weighted policy: 3.1567), Train Mean Max: 0.0746
Epoch 6/10, Train Loss: 3.2475 (value: 0.0046, weighted value: 0.2288, policy: 3.0187, weighted policy: 3.0187), Train Mean Max: 0.1014
Epoch 7/10, Train Loss: 3.1175 (value: 0.0046, weighted value: 0.2312, policy: 2.8864, weighted policy: 2.8864), Train Mean Max: 0.1300
Epoch 8/10, Train Loss: 2.9785 (value: 0.0042, weighted value: 0.2087, policy: 2.7698, weighted policy: 2.7698), Train Mean Max: 0.1564
Epoch 9/10, Train Loss: 2.8711 (value: 0.0041, weighted value: 0.2029, policy: 2.6682, weighted policy: 2.6682), Train Mean Max: 0.1782
Epoch 10/10, Train Loss: 2.7987 (value: 0.0043, weighted value: 0.2150, policy: 2.5837, weighted policy: 2.5837), Train Mean Max: 0.1951
..training done in 14.83 seconds
..evaluation done in 12.18 seconds
Old network+MCTS average reward: 0.3767, min: -0.3611, max: 1.0093, stdev: 0.2228
New network+MCTS average reward: 0.4164, min: -0.2037, max: 1.1574, stdev: 0.2282
Old bare network average reward: 0.2698, min: -0.2407, max: 0.9352, stdev: 0.2177
New bare network average reward: 0.2716, min: -0.3056, max: 1.0093, stdev: 0.2274
External policy "random" average reward: 0.2725, min: -0.2037, max: 1.0926, stdev: 0.2262
External policy "individual greedy" average reward: 0.5437, min: 0.0926, max: 1.2130, stdev: 0.2183
External policy "total greedy" average reward: 0.6620, min: 0.1204, max: 1.3796, stdev: 0.2189
New network won 182 and tied 9 out of 300 games (62.17% wins where ties are half wins)
Keeping the new network

Training iteration 3 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 45.73 seconds
Training examples lengths: [64795, 65039, 64860]
Total value: 70447.01
Training on 194694 examples
Training with 96 batches of size 2048
Epoch 1/10, Train Loss: 2.7664 (value: 0.0081, weighted value: 0.4053, policy: 2.3611, weighted policy: 2.3611), Train Mean Max: 0.2197
Epoch 2/10, Train Loss: 2.5922 (value: 0.0066, weighted value: 0.3303, policy: 2.2618, weighted policy: 2.2618), Train Mean Max: 0.2549
Epoch 3/10, Train Loss: 2.4906 (value: 0.0058, weighted value: 0.2884, policy: 2.2022, weighted policy: 2.2022), Train Mean Max: 0.2712
Epoch 4/10, Train Loss: 2.4250 (value: 0.0053, weighted value: 0.2635, policy: 2.1615, weighted policy: 2.1615), Train Mean Max: 0.2780
Epoch 5/10, Train Loss: 2.3942 (value: 0.0052, weighted value: 0.2602, policy: 2.1341, weighted policy: 2.1341), Train Mean Max: 0.2798
Epoch 6/10, Train Loss: 2.3380 (value: 0.0045, weighted value: 0.2253, policy: 2.1127, weighted policy: 2.1127), Train Mean Max: 0.2822
Epoch 7/10, Train Loss: 2.3232 (value: 0.0045, weighted value: 0.2228, policy: 2.1004, weighted policy: 2.1004), Train Mean Max: 0.2819
Epoch 8/10, Train Loss: 2.2954 (value: 0.0041, weighted value: 0.2058, policy: 2.0897, weighted policy: 2.0897), Train Mean Max: 0.2834
Epoch 9/10, Train Loss: 2.2629 (value: 0.0036, weighted value: 0.1804, policy: 2.0824, weighted policy: 2.0824), Train Mean Max: 0.2838
Epoch 10/10, Train Loss: 2.2946 (value: 0.0043, weighted value: 0.2146, policy: 2.0800, weighted policy: 2.0800), Train Mean Max: 0.2833
..training done in 15.66 seconds
..evaluation done in 12.67 seconds
Old network+MCTS average reward: 0.4126, min: -0.1759, max: 1.0556, stdev: 0.2293
New network+MCTS average reward: 0.4468, min: -0.3148, max: 1.2130, stdev: 0.2461
Old bare network average reward: 0.2564, min: -0.2685, max: 1.1019, stdev: 0.2159
New bare network average reward: 0.2667, min: -0.3333, max: 0.9352, stdev: 0.2106
External policy "random" average reward: 0.2434, min: -0.3241, max: 0.8796, stdev: 0.2200
External policy "individual greedy" average reward: 0.5201, min: -0.0185, max: 1.3056, stdev: 0.2182
External policy "total greedy" average reward: 0.6379, min: 0.1389, max: 1.3148, stdev: 0.2100
New network won 165 and tied 12 out of 300 games (57.00% wins where ties are half wins)
Keeping the new network

Training iteration 4 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 46.18 seconds
Training examples lengths: [64795, 65039, 64860, 64704]
Total value: 100047.57
Training on 259398 examples
Training with 127 batches of size 2048
Epoch 1/10, Train Loss: 2.3001 (value: 0.0064, weighted value: 0.3207, policy: 1.9793, weighted policy: 1.9793), Train Mean Max: 0.2993
Epoch 2/10, Train Loss: 2.2416 (value: 0.0054, weighted value: 0.2718, policy: 1.9698, weighted policy: 1.9698), Train Mean Max: 0.3116
Epoch 3/10, Train Loss: 2.2315 (value: 0.0053, weighted value: 0.2653, policy: 1.9662, weighted policy: 1.9662), Train Mean Max: 0.3156
Epoch 4/10, Train Loss: 2.1885 (value: 0.0045, weighted value: 0.2260, policy: 1.9625, weighted policy: 1.9625), Train Mean Max: 0.3192
Epoch 5/10, Train Loss: 2.1798 (value: 0.0044, weighted value: 0.2207, policy: 1.9591, weighted policy: 1.9591), Train Mean Max: 0.3211
Epoch 6/10, Train Loss: 2.1599 (value: 0.0041, weighted value: 0.2029, policy: 1.9570, weighted policy: 1.9570), Train Mean Max: 0.3225
Epoch 7/10, Train Loss: 2.1462 (value: 0.0038, weighted value: 0.1915, policy: 1.9546, weighted policy: 1.9546), Train Mean Max: 0.3243
Epoch 8/10, Train Loss: 2.1300 (value: 0.0035, weighted value: 0.1775, policy: 1.9525, weighted policy: 1.9525), Train Mean Max: 0.3245
Epoch 9/10, Train Loss: 2.1264 (value: 0.0035, weighted value: 0.1759, policy: 1.9505, weighted policy: 1.9505), Train Mean Max: 0.3254
Epoch 10/10, Train Loss: 2.1117 (value: 0.0033, weighted value: 0.1633, policy: 1.9484, weighted policy: 1.9484), Train Mean Max: 0.3264
..training done in 22.79 seconds
..evaluation done in 12.79 seconds
Old network+MCTS average reward: 0.4367, min: -0.1574, max: 1.1574, stdev: 0.2491
New network+MCTS average reward: 0.4556, min: -0.3056, max: 1.1204, stdev: 0.2364
Old bare network average reward: 0.2677, min: -0.3241, max: 1.1019, stdev: 0.2482
New bare network average reward: 0.2799, min: -0.3519, max: 1.0278, stdev: 0.2457
External policy "random" average reward: 0.2594, min: -0.6759, max: 1.1204, stdev: 0.2543
External policy "individual greedy" average reward: 0.5376, min: -0.2593, max: 1.3241, stdev: 0.2377
External policy "total greedy" average reward: 0.6424, min: 0.0556, max: 1.3796, stdev: 0.2246
New network won 152 and tied 12 out of 300 games (52.67% wins where ties are half wins)
Keeping the new network

Training iteration 5 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 48.04 seconds
Training examples lengths: [64795, 65039, 64860, 64704, 64923]
Total value: 130395.74
Training on 324321 examples
Training with 159 batches of size 2048
Epoch 1/10, Train Loss: 2.1469 (value: 0.0056, weighted value: 0.2785, policy: 1.8685, weighted policy: 1.8685), Train Mean Max: 0.3403
Epoch 2/10, Train Loss: 2.1051 (value: 0.0048, weighted value: 0.2405, policy: 1.8646, weighted policy: 1.8646), Train Mean Max: 0.3492
Epoch 3/10, Train Loss: 2.0843 (value: 0.0045, weighted value: 0.2226, policy: 1.8618, weighted policy: 1.8618), Train Mean Max: 0.3521
Epoch 4/10, Train Loss: 2.0759 (value: 0.0043, weighted value: 0.2155, policy: 1.8604, weighted policy: 1.8604), Train Mean Max: 0.3534
Epoch 5/10, Train Loss: 2.0395 (value: 0.0036, weighted value: 0.1817, policy: 1.8578, weighted policy: 1.8578), Train Mean Max: 0.3562
Epoch 6/10, Train Loss: 2.0401 (value: 0.0037, weighted value: 0.1844, policy: 1.8557, weighted policy: 1.8557), Train Mean Max: 0.3563
Epoch 7/10, Train Loss: 2.0354 (value: 0.0036, weighted value: 0.1812, policy: 1.8542, weighted policy: 1.8542), Train Mean Max: 0.3569
Epoch 8/10, Train Loss: 2.0171 (value: 0.0033, weighted value: 0.1644, policy: 1.8527, weighted policy: 1.8527), Train Mean Max: 0.3583
Epoch 9/10, Train Loss: 2.0103 (value: 0.0032, weighted value: 0.1593, policy: 1.8510, weighted policy: 1.8510), Train Mean Max: 0.3589
Epoch 10/10, Train Loss: 2.0030 (value: 0.0031, weighted value: 0.1536, policy: 1.8495, weighted policy: 1.8495), Train Mean Max: 0.3594
..training done in 29.35 seconds
..evaluation done in 13.85 seconds
Old network+MCTS average reward: 0.4780, min: -0.2315, max: 1.2130, stdev: 0.2455
New network+MCTS average reward: 0.5017, min: -0.1481, max: 1.2222, stdev: 0.2473
Old bare network average reward: 0.2972, min: -0.1944, max: 1.0463, stdev: 0.2348
New bare network average reward: 0.3138, min: -0.1944, max: 1.0741, stdev: 0.2333
External policy "random" average reward: 0.2890, min: -0.1852, max: 1.0370, stdev: 0.2335
External policy "individual greedy" average reward: 0.5510, min: -0.1759, max: 1.2778, stdev: 0.2422
External policy "total greedy" average reward: 0.6720, min: 0.0926, max: 1.4722, stdev: 0.2285
New network won 169 and tied 11 out of 300 games (58.17% wins where ties are half wins)
Keeping the new network

Training iteration 6 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 48.22 seconds
Training examples lengths: [64795, 65039, 64860, 64704, 64923, 64703]
Total value: 161151.51
Training on 389024 examples
Training with 190 batches of size 2048
Epoch 1/10, Train Loss: 2.0248 (value: 0.0048, weighted value: 0.2415, policy: 1.7833, weighted policy: 1.7833), Train Mean Max: 0.3726
Epoch 2/10, Train Loss: 1.9857 (value: 0.0041, weighted value: 0.2061, policy: 1.7796, weighted policy: 1.7796), Train Mean Max: 0.3791
Epoch 3/10, Train Loss: 1.9768 (value: 0.0040, weighted value: 0.1990, policy: 1.7778, weighted policy: 1.7778), Train Mean Max: 0.3805
Epoch 4/10, Train Loss: 1.9592 (value: 0.0037, weighted value: 0.1831, policy: 1.7761, weighted policy: 1.7761), Train Mean Max: 0.3827
Epoch 5/10, Train Loss: 1.9480 (value: 0.0035, weighted value: 0.1733, policy: 1.7747, weighted policy: 1.7747), Train Mean Max: 0.3836
Epoch 6/10, Train Loss: 1.9437 (value: 0.0034, weighted value: 0.1703, policy: 1.7734, weighted policy: 1.7734), Train Mean Max: 0.3842
Epoch 7/10, Train Loss: 1.9301 (value: 0.0032, weighted value: 0.1582, policy: 1.7719, weighted policy: 1.7719), Train Mean Max: 0.3857
Epoch 8/10, Train Loss: 1.9206 (value: 0.0030, weighted value: 0.1504, policy: 1.7703, weighted policy: 1.7703), Train Mean Max: 0.3863
Epoch 9/10, Train Loss: 1.9170 (value: 0.0030, weighted value: 0.1478, policy: 1.7692, weighted policy: 1.7692), Train Mean Max: 0.3867
Epoch 10/10, Train Loss: 1.9125 (value: 0.0029, weighted value: 0.1452, policy: 1.7673, weighted policy: 1.7673), Train Mean Max: 0.3873
..training done in 33.28 seconds
..evaluation done in 13.35 seconds
Old network+MCTS average reward: 0.4983, min: -0.2778, max: 1.2870, stdev: 0.2556
New network+MCTS average reward: 0.5098, min: -0.2407, max: 1.2407, stdev: 0.2581
Old bare network average reward: 0.3166, min: -0.3426, max: 0.9907, stdev: 0.2402
New bare network average reward: 0.3171, min: -0.2407, max: 0.9907, stdev: 0.2379
External policy "random" average reward: 0.2746, min: -0.4444, max: 1.1574, stdev: 0.2418
External policy "individual greedy" average reward: 0.5628, min: -0.0926, max: 1.4259, stdev: 0.2474
External policy "total greedy" average reward: 0.6756, min: -0.0185, max: 1.3981, stdev: 0.2321
New network won 148 and tied 7 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 7 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 47.32 seconds
Training examples lengths: [64795, 65039, 64860, 64704, 64923, 64703, 64600]
Total value: 192623.19
Training on 453624 examples
Training with 222 batches of size 2048
Epoch 1/10, Train Loss: 1.9310 (value: 0.0044, weighted value: 0.2181, policy: 1.7130, weighted policy: 1.7130), Train Mean Max: 0.3986
Epoch 2/10, Train Loss: 1.9024 (value: 0.0038, weighted value: 0.1917, policy: 1.7108, weighted policy: 1.7108), Train Mean Max: 0.4035
Epoch 3/10, Train Loss: 1.8877 (value: 0.0036, weighted value: 0.1786, policy: 1.7091, weighted policy: 1.7091), Train Mean Max: 0.4053
Epoch 4/10, Train Loss: 1.8766 (value: 0.0034, weighted value: 0.1683, policy: 1.7082, weighted policy: 1.7082), Train Mean Max: 0.4069
Epoch 5/10, Train Loss: 1.8737 (value: 0.0033, weighted value: 0.1665, policy: 1.7071, weighted policy: 1.7071), Train Mean Max: 0.4070
Epoch 6/10, Train Loss: 1.8653 (value: 0.0032, weighted value: 0.1593, policy: 1.7060, weighted policy: 1.7060), Train Mean Max: 0.4081
Epoch 7/10, Train Loss: 1.8475 (value: 0.0029, weighted value: 0.1435, policy: 1.7041, weighted policy: 1.7041), Train Mean Max: 0.4092
Epoch 8/10, Train Loss: 1.8487 (value: 0.0029, weighted value: 0.1453, policy: 1.7034, weighted policy: 1.7034), Train Mean Max: 0.4092
Epoch 9/10, Train Loss: 1.8379 (value: 0.0027, weighted value: 0.1356, policy: 1.7024, weighted policy: 1.7024), Train Mean Max: 0.4101
Epoch 10/10, Train Loss: 1.8323 (value: 0.0026, weighted value: 0.1312, policy: 1.7010, weighted policy: 1.7010), Train Mean Max: 0.4106
..training done in 35.22 seconds
..evaluation done in 14.30 seconds
Old network+MCTS average reward: 0.4682, min: -0.2870, max: 1.5370, stdev: 0.2536
New network+MCTS average reward: 0.4680, min: -0.2222, max: 1.3889, stdev: 0.2536
Old bare network average reward: 0.2791, min: -0.2778, max: 1.1204, stdev: 0.2246
New bare network average reward: 0.2890, min: -0.3333, max: 1.4815, stdev: 0.2415
External policy "random" average reward: 0.2456, min: -0.3241, max: 1.2222, stdev: 0.2447
External policy "individual greedy" average reward: 0.5176, min: -0.0926, max: 1.2315, stdev: 0.2325
External policy "total greedy" average reward: 0.6393, min: 0.0000, max: 1.5278, stdev: 0.2319
New network won 147 and tied 8 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 8 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 48.53 seconds
Training examples lengths: [64795, 65039, 64860, 64704, 64923, 64703, 64600, 64654]
Total value: 224150.56
Training on 518278 examples
Training with 254 batches of size 2048
Epoch 1/10, Train Loss: 1.8564 (value: 0.0041, weighted value: 0.2037, policy: 1.6526, weighted policy: 1.6526), Train Mean Max: 0.4202
Epoch 2/10, Train Loss: 1.8330 (value: 0.0036, weighted value: 0.1806, policy: 1.6524, weighted policy: 1.6524), Train Mean Max: 0.4240
Epoch 3/10, Train Loss: 1.8194 (value: 0.0034, weighted value: 0.1685, policy: 1.6508, weighted policy: 1.6508), Train Mean Max: 0.4258
Epoch 4/10, Train Loss: 1.8114 (value: 0.0032, weighted value: 0.1618, policy: 1.6496, weighted policy: 1.6496), Train Mean Max: 0.4267
Epoch 5/10, Train Loss: 1.8014 (value: 0.0030, weighted value: 0.1524, policy: 1.6490, weighted policy: 1.6490), Train Mean Max: 0.4271
Epoch 6/10, Train Loss: 1.7931 (value: 0.0029, weighted value: 0.1451, policy: 1.6480, weighted policy: 1.6480), Train Mean Max: 0.4278
Epoch 7/10, Train Loss: 1.7837 (value: 0.0027, weighted value: 0.1368, policy: 1.6469, weighted policy: 1.6469), Train Mean Max: 0.4286
Epoch 8/10, Train Loss: 1.7812 (value: 0.0027, weighted value: 0.1347, policy: 1.6465, weighted policy: 1.6465), Train Mean Max: 0.4288
Epoch 9/10, Train Loss: 1.7790 (value: 0.0027, weighted value: 0.1329, policy: 1.6462, weighted policy: 1.6462), Train Mean Max: 0.4293
Epoch 10/10, Train Loss: 1.7712 (value: 0.0025, weighted value: 0.1254, policy: 1.6458, weighted policy: 1.6458), Train Mean Max: 0.4294
..training done in 51.45 seconds
..evaluation done in 14.49 seconds
Old network+MCTS average reward: 0.4955, min: -0.1111, max: 1.2685, stdev: 0.2429
New network+MCTS average reward: 0.4869, min: -0.1019, max: 1.2130, stdev: 0.2244
Old bare network average reward: 0.3060, min: -0.4907, max: 1.0741, stdev: 0.2239
New bare network average reward: 0.3098, min: -0.4259, max: 1.0370, stdev: 0.2240
External policy "random" average reward: 0.2635, min: -0.2870, max: 1.0833, stdev: 0.2242
External policy "individual greedy" average reward: 0.5324, min: -0.1667, max: 1.3796, stdev: 0.2282
External policy "total greedy" average reward: 0.6581, min: -0.0463, max: 1.3426, stdev: 0.2276
New network won 125 and tied 19 out of 300 games (44.83% wins where ties are half wins)
Reverting to the old network

Training iteration 9 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 49.84 seconds
Training examples lengths: [64795, 65039, 64860, 64704, 64923, 64703, 64600, 64654, 65101]
Total value: 256248.63
Training on 583379 examples
Training with 285 batches of size 2048
Epoch 1/10, Train Loss: 1.8666 (value: 0.0051, weighted value: 0.2534, policy: 1.6132, weighted policy: 1.6132), Train Mean Max: 0.4287
Epoch 2/10, Train Loss: 1.8325 (value: 0.0044, weighted value: 0.2212, policy: 1.6113, weighted policy: 1.6113), Train Mean Max: 0.4353
Epoch 3/10, Train Loss: 1.8143 (value: 0.0041, weighted value: 0.2045, policy: 1.6098, weighted policy: 1.6098), Train Mean Max: 0.4375
Epoch 4/10, Train Loss: 1.7974 (value: 0.0038, weighted value: 0.1887, policy: 1.6087, weighted policy: 1.6087), Train Mean Max: 0.4388
Epoch 5/10, Train Loss: 1.7851 (value: 0.0036, weighted value: 0.1776, policy: 1.6074, weighted policy: 1.6074), Train Mean Max: 0.4401
Epoch 6/10, Train Loss: 1.7807 (value: 0.0035, weighted value: 0.1739, policy: 1.6068, weighted policy: 1.6068), Train Mean Max: 0.4405
Epoch 7/10, Train Loss: 1.7662 (value: 0.0032, weighted value: 0.1603, policy: 1.6059, weighted policy: 1.6059), Train Mean Max: 0.4417
Epoch 8/10, Train Loss: 1.7622 (value: 0.0031, weighted value: 0.1572, policy: 1.6050, weighted policy: 1.6050), Train Mean Max: 0.4418
Epoch 9/10, Train Loss: 1.7528 (value: 0.0030, weighted value: 0.1484, policy: 1.6044, weighted policy: 1.6044), Train Mean Max: 0.4427
Epoch 10/10, Train Loss: 1.7437 (value: 0.0028, weighted value: 0.1405, policy: 1.6032, weighted policy: 1.6032), Train Mean Max: 0.4433
..training done in 51.86 seconds
..evaluation done in 14.38 seconds
Old network+MCTS average reward: 0.5069, min: -0.1019, max: 1.3148, stdev: 0.2296
New network+MCTS average reward: 0.4991, min: -0.1111, max: 1.2778, stdev: 0.2332
Old bare network average reward: 0.3261, min: -0.2407, max: 0.9907, stdev: 0.2226
New bare network average reward: 0.3469, min: -0.2222, max: 1.1389, stdev: 0.2411
External policy "random" average reward: 0.2802, min: -0.2222, max: 1.1204, stdev: 0.2254
External policy "individual greedy" average reward: 0.5637, min: -0.0278, max: 1.2222, stdev: 0.2276
External policy "total greedy" average reward: 0.6778, min: 0.1019, max: 1.3426, stdev: 0.2282
New network won 134 and tied 14 out of 300 games (47.00% wins where ties are half wins)
Reverting to the old network

Training iteration 10 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 49.61 seconds
Training examples lengths: [64795, 65039, 64860, 64704, 64923, 64703, 64600, 64654, 65101, 64821]
Total value: 288441.19
Training on 648200 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 1.8757 (value: 0.0059, weighted value: 0.2950, policy: 1.5807, weighted policy: 1.5807), Train Mean Max: 0.4357
Epoch 2/10, Train Loss: 1.8212 (value: 0.0049, weighted value: 0.2444, policy: 1.5768, weighted policy: 1.5768), Train Mean Max: 0.4454
Epoch 3/10, Train Loss: 1.8056 (value: 0.0046, weighted value: 0.2301, policy: 1.5756, weighted policy: 1.5756), Train Mean Max: 0.4478
Epoch 4/10, Train Loss: 1.7905 (value: 0.0043, weighted value: 0.2160, policy: 1.5744, weighted policy: 1.5744), Train Mean Max: 0.4494
Epoch 5/10, Train Loss: 1.7761 (value: 0.0041, weighted value: 0.2025, policy: 1.5736, weighted policy: 1.5736), Train Mean Max: 0.4508
Epoch 6/10, Train Loss: 1.7592 (value: 0.0037, weighted value: 0.1864, policy: 1.5728, weighted policy: 1.5728), Train Mean Max: 0.4521
Epoch 7/10, Train Loss: 1.7539 (value: 0.0036, weighted value: 0.1822, policy: 1.5717, weighted policy: 1.5717), Train Mean Max: 0.4524
Epoch 8/10, Train Loss: 1.7424 (value: 0.0034, weighted value: 0.1715, policy: 1.5709, weighted policy: 1.5709), Train Mean Max: 0.4532
Epoch 9/10, Train Loss: 1.7359 (value: 0.0033, weighted value: 0.1658, policy: 1.5701, weighted policy: 1.5701), Train Mean Max: 0.4536
Epoch 10/10, Train Loss: 1.7269 (value: 0.0031, weighted value: 0.1572, policy: 1.5696, weighted policy: 1.5696), Train Mean Max: 0.4543
..training done in 62.02 seconds
..evaluation done in 15.53 seconds
Old network+MCTS average reward: 0.4839, min: -0.1204, max: 1.1296, stdev: 0.2335
New network+MCTS average reward: 0.5111, min: -0.1667, max: 1.0463, stdev: 0.2403
Old bare network average reward: 0.3175, min: -0.2963, max: 0.9630, stdev: 0.2218
New bare network average reward: 0.3240, min: -0.4352, max: 0.9907, stdev: 0.2388
External policy "random" average reward: 0.2656, min: -0.3148, max: 0.9259, stdev: 0.2329
External policy "individual greedy" average reward: 0.5496, min: 0.0278, max: 1.1389, stdev: 0.2117
External policy "total greedy" average reward: 0.6601, min: 0.1759, max: 1.2222, stdev: 0.2066
New network won 172 and tied 9 out of 300 games (58.83% wins where ties are half wins)
Keeping the new network

Training iteration 11 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 49.37 seconds
Training examples lengths: [65039, 64860, 64704, 64923, 64703, 64600, 64654, 65101, 64821, 65024]
Total value: 302500.47
Training on 648429 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 1.6108 (value: 0.0040, weighted value: 0.2005, policy: 1.4103, weighted policy: 1.4103), Train Mean Max: 0.4899
Epoch 2/10, Train Loss: 1.5872 (value: 0.0036, weighted value: 0.1809, policy: 1.4063, weighted policy: 1.4063), Train Mean Max: 0.4989
Epoch 3/10, Train Loss: 1.5735 (value: 0.0034, weighted value: 0.1690, policy: 1.4045, weighted policy: 1.4045), Train Mean Max: 0.5020
Epoch 4/10, Train Loss: 1.5660 (value: 0.0033, weighted value: 0.1627, policy: 1.4034, weighted policy: 1.4034), Train Mean Max: 0.5033
Epoch 5/10, Train Loss: 1.5517 (value: 0.0030, weighted value: 0.1498, policy: 1.4019, weighted policy: 1.4019), Train Mean Max: 0.5051
Epoch 6/10, Train Loss: 1.5483 (value: 0.0029, weighted value: 0.1469, policy: 1.4014, weighted policy: 1.4014), Train Mean Max: 0.5055
Epoch 7/10, Train Loss: 1.5422 (value: 0.0028, weighted value: 0.1418, policy: 1.4004, weighted policy: 1.4004), Train Mean Max: 0.5062
Epoch 8/10, Train Loss: 1.5367 (value: 0.0027, weighted value: 0.1372, policy: 1.3995, weighted policy: 1.3995), Train Mean Max: 0.5066
Epoch 9/10, Train Loss: 1.5318 (value: 0.0027, weighted value: 0.1328, policy: 1.3991, weighted policy: 1.3991), Train Mean Max: 0.5070
Epoch 10/10, Train Loss: 1.5248 (value: 0.0025, weighted value: 0.1267, policy: 1.3980, weighted policy: 1.3980), Train Mean Max: 0.5076
..training done in 63.39 seconds
..evaluation done in 15.84 seconds
Old network+MCTS average reward: 0.5106, min: -0.0833, max: 1.1204, stdev: 0.2343
New network+MCTS average reward: 0.5243, min: -0.0648, max: 1.0833, stdev: 0.2248
Old bare network average reward: 0.3268, min: -0.2407, max: 0.9815, stdev: 0.2378
New bare network average reward: 0.3298, min: -0.2870, max: 0.9352, stdev: 0.2345
External policy "random" average reward: 0.2772, min: -0.2778, max: 0.8889, stdev: 0.2153
External policy "individual greedy" average reward: 0.5455, min: 0.0000, max: 1.1667, stdev: 0.2186
External policy "total greedy" average reward: 0.6647, min: 0.1389, max: 1.2593, stdev: 0.2195
New network won 153 and tied 14 out of 300 games (53.33% wins where ties are half wins)
Keeping the new network

Training iteration 12 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 50.03 seconds
Training examples lengths: [64860, 64704, 64923, 64703, 64600, 64654, 65101, 64821, 65024, 64996]
Total value: 312296.36
Training on 648386 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 1.4738 (value: 0.0035, weighted value: 0.1774, policy: 1.2964, weighted policy: 1.2964), Train Mean Max: 0.5290
Epoch 2/10, Train Loss: 1.4457 (value: 0.0030, weighted value: 0.1520, policy: 1.2937, weighted policy: 1.2937), Train Mean Max: 0.5362
Epoch 3/10, Train Loss: 1.4391 (value: 0.0029, weighted value: 0.1464, policy: 1.2927, weighted policy: 1.2927), Train Mean Max: 0.5381
Epoch 4/10, Train Loss: 1.4339 (value: 0.0028, weighted value: 0.1421, policy: 1.2918, weighted policy: 1.2918), Train Mean Max: 0.5394
Epoch 5/10, Train Loss: 1.4225 (value: 0.0026, weighted value: 0.1315, policy: 1.2910, weighted policy: 1.2910), Train Mean Max: 0.5405
Epoch 6/10, Train Loss: 1.4178 (value: 0.0026, weighted value: 0.1276, policy: 1.2901, weighted policy: 1.2901), Train Mean Max: 0.5411
Epoch 7/10, Train Loss: 1.4109 (value: 0.0024, weighted value: 0.1217, policy: 1.2892, weighted policy: 1.2892), Train Mean Max: 0.5418
Epoch 8/10, Train Loss: 1.4092 (value: 0.0024, weighted value: 0.1209, policy: 1.2883, weighted policy: 1.2883), Train Mean Max: 0.5418
Epoch 9/10, Train Loss: 1.4057 (value: 0.0023, weighted value: 0.1172, policy: 1.2885, weighted policy: 1.2885), Train Mean Max: 0.5422
Epoch 10/10, Train Loss: 1.4011 (value: 0.0023, weighted value: 0.1134, policy: 1.2876, weighted policy: 1.2876), Train Mean Max: 0.5425
..training done in 61.43 seconds
..evaluation done in 15.53 seconds
Old network+MCTS average reward: 0.5086, min: -0.1852, max: 1.3519, stdev: 0.2333
New network+MCTS average reward: 0.4986, min: -0.1111, max: 1.1944, stdev: 0.2429
Old bare network average reward: 0.3178, min: -0.3148, max: 1.0278, stdev: 0.2399
New bare network average reward: 0.3349, min: -0.2037, max: 1.2037, stdev: 0.2482
External policy "random" average reward: 0.2485, min: -0.3704, max: 1.1019, stdev: 0.2325
External policy "individual greedy" average reward: 0.5368, min: -0.0185, max: 1.6019, stdev: 0.2342
External policy "total greedy" average reward: 0.6568, min: -0.0833, max: 1.6296, stdev: 0.2251
New network won 126 and tied 16 out of 300 games (44.67% wins where ties are half wins)
Reverting to the old network

Training iteration 13 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 48.52 seconds
Training examples lengths: [64704, 64923, 64703, 64600, 64654, 65101, 64821, 65024, 64996, 64921]
Total value: 318779.56
Training on 648447 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 1.4514 (value: 0.0043, weighted value: 0.2167, policy: 1.2347, weighted policy: 1.2347), Train Mean Max: 0.5401
Epoch 2/10, Train Loss: 1.4164 (value: 0.0037, weighted value: 0.1867, policy: 1.2298, weighted policy: 1.2298), Train Mean Max: 0.5513
Epoch 3/10, Train Loss: 1.3961 (value: 0.0034, weighted value: 0.1685, policy: 1.2277, weighted policy: 1.2277), Train Mean Max: 0.5555
Epoch 4/10, Train Loss: 1.3885 (value: 0.0032, weighted value: 0.1624, policy: 1.2261, weighted policy: 1.2261), Train Mean Max: 0.5575
Epoch 5/10, Train Loss: 1.3741 (value: 0.0030, weighted value: 0.1492, policy: 1.2249, weighted policy: 1.2249), Train Mean Max: 0.5592
Epoch 6/10, Train Loss: 1.3721 (value: 0.0029, weighted value: 0.1475, policy: 1.2246, weighted policy: 1.2246), Train Mean Max: 0.5603
Epoch 7/10, Train Loss: 1.3581 (value: 0.0027, weighted value: 0.1349, policy: 1.2232, weighted policy: 1.2232), Train Mean Max: 0.5614
Epoch 8/10, Train Loss: 1.3537 (value: 0.0026, weighted value: 0.1312, policy: 1.2225, weighted policy: 1.2225), Train Mean Max: 0.5622
Epoch 9/10, Train Loss: 1.3460 (value: 0.0025, weighted value: 0.1242, policy: 1.2218, weighted policy: 1.2218), Train Mean Max: 0.5628
Epoch 10/10, Train Loss: 1.3444 (value: 0.0025, weighted value: 0.1235, policy: 1.2209, weighted policy: 1.2209), Train Mean Max: 0.5631
..training done in 63.29 seconds
..evaluation done in 15.55 seconds
Old network+MCTS average reward: 0.5240, min: -0.3148, max: 1.2963, stdev: 0.2399
New network+MCTS average reward: 0.5199, min: -0.2130, max: 1.2407, stdev: 0.2381
Old bare network average reward: 0.3141, min: -0.3611, max: 0.9167, stdev: 0.2377
New bare network average reward: 0.3405, min: -0.3426, max: 0.9630, stdev: 0.2370
External policy "random" average reward: 0.2598, min: -0.2870, max: 1.0648, stdev: 0.2242
External policy "individual greedy" average reward: 0.5566, min: -0.0648, max: 1.2778, stdev: 0.2303
External policy "total greedy" average reward: 0.6601, min: -0.0185, max: 1.2315, stdev: 0.2216
New network won 138 and tied 18 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 14 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 49.31 seconds
Training examples lengths: [64923, 64703, 64600, 64654, 65101, 64821, 65024, 64996, 64921, 64566]
Total value: 322565.72
Training on 648309 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 1.4536 (value: 0.0053, weighted value: 0.2627, policy: 1.1909, weighted policy: 1.1909), Train Mean Max: 0.5472
Epoch 2/10, Train Loss: 1.3985 (value: 0.0043, weighted value: 0.2149, policy: 1.1836, weighted policy: 1.1836), Train Mean Max: 0.5624
Epoch 3/10, Train Loss: 1.3747 (value: 0.0039, weighted value: 0.1940, policy: 1.1807, weighted policy: 1.1807), Train Mean Max: 0.5677
Epoch 4/10, Train Loss: 1.3581 (value: 0.0036, weighted value: 0.1787, policy: 1.1794, weighted policy: 1.1794), Train Mean Max: 0.5709
Epoch 5/10, Train Loss: 1.3438 (value: 0.0033, weighted value: 0.1663, policy: 1.1775, weighted policy: 1.1775), Train Mean Max: 0.5732
Epoch 6/10, Train Loss: 1.3334 (value: 0.0031, weighted value: 0.1571, policy: 1.1764, weighted policy: 1.1764), Train Mean Max: 0.5749
Epoch 7/10, Train Loss: 1.3266 (value: 0.0030, weighted value: 0.1509, policy: 1.1756, weighted policy: 1.1756), Train Mean Max: 0.5758
Epoch 8/10, Train Loss: 1.3156 (value: 0.0028, weighted value: 0.1408, policy: 1.1749, weighted policy: 1.1749), Train Mean Max: 0.5770
Epoch 9/10, Train Loss: 1.3077 (value: 0.0027, weighted value: 0.1338, policy: 1.1740, weighted policy: 1.1740), Train Mean Max: 0.5779
Epoch 10/10, Train Loss: 1.3054 (value: 0.0026, weighted value: 0.1322, policy: 1.1731, weighted policy: 1.1731), Train Mean Max: 0.5781
..training done in 63.82 seconds
..evaluation done in 15.04 seconds
Old network+MCTS average reward: 0.4998, min: -0.2407, max: 1.1574, stdev: 0.2323
New network+MCTS average reward: 0.4885, min: -0.2037, max: 1.1204, stdev: 0.2341
Old bare network average reward: 0.3072, min: -0.3333, max: 1.0370, stdev: 0.2265
New bare network average reward: 0.3292, min: -0.2407, max: 0.9907, stdev: 0.2258
External policy "random" average reward: 0.2417, min: -0.3426, max: 0.8796, stdev: 0.2101
External policy "individual greedy" average reward: 0.5200, min: -0.0463, max: 1.2593, stdev: 0.2163
External policy "total greedy" average reward: 0.6359, min: 0.1574, max: 1.2593, stdev: 0.2084
New network won 121 and tied 23 out of 300 games (44.17% wins where ties are half wins)
Reverting to the old network

Training iteration 15 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 50.19 seconds
Training examples lengths: [64703, 64600, 64654, 65101, 64821, 65024, 64996, 64921, 64566, 64747]
Total value: 325670.56
Training on 648133 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 1.4573 (value: 0.0060, weighted value: 0.2996, policy: 1.1576, weighted policy: 1.1576), Train Mean Max: 0.5525
Epoch 2/10, Train Loss: 1.3890 (value: 0.0048, weighted value: 0.2407, policy: 1.1483, weighted policy: 1.1483), Train Mean Max: 0.5706
Epoch 3/10, Train Loss: 1.3590 (value: 0.0043, weighted value: 0.2140, policy: 1.1450, weighted policy: 1.1450), Train Mean Max: 0.5770
Epoch 4/10, Train Loss: 1.3409 (value: 0.0040, weighted value: 0.1985, policy: 1.1424, weighted policy: 1.1424), Train Mean Max: 0.5807
Epoch 5/10, Train Loss: 1.3225 (value: 0.0036, weighted value: 0.1812, policy: 1.1413, weighted policy: 1.1413), Train Mean Max: 0.5838
Epoch 6/10, Train Loss: 1.3121 (value: 0.0034, weighted value: 0.1722, policy: 1.1399, weighted policy: 1.1399), Train Mean Max: 0.5856
Epoch 7/10, Train Loss: 1.3020 (value: 0.0033, weighted value: 0.1629, policy: 1.1390, weighted policy: 1.1390), Train Mean Max: 0.5868
Epoch 8/10, Train Loss: 1.2856 (value: 0.0030, weighted value: 0.1476, policy: 1.1380, weighted policy: 1.1380), Train Mean Max: 0.5885
Epoch 9/10, Train Loss: 1.2812 (value: 0.0029, weighted value: 0.1445, policy: 1.1367, weighted policy: 1.1367), Train Mean Max: 0.5893
Epoch 10/10, Train Loss: 1.2762 (value: 0.0028, weighted value: 0.1399, policy: 1.1363, weighted policy: 1.1363), Train Mean Max: 0.5897
..training done in 61.16 seconds
..evaluation done in 14.78 seconds
Old network+MCTS average reward: 0.5300, min: -0.3333, max: 1.4537, stdev: 0.2516
New network+MCTS average reward: 0.5217, min: -0.2037, max: 1.2130, stdev: 0.2396
Old bare network average reward: 0.3284, min: -0.2685, max: 1.1944, stdev: 0.2293
New bare network average reward: 0.3465, min: -0.2778, max: 1.0463, stdev: 0.2342
External policy "random" average reward: 0.2619, min: -0.5463, max: 0.9259, stdev: 0.2450
External policy "individual greedy" average reward: 0.5477, min: -0.1389, max: 1.1759, stdev: 0.2215
External policy "total greedy" average reward: 0.6581, min: 0.0741, max: 1.2222, stdev: 0.2191
New network won 126 and tied 19 out of 300 games (45.17% wins where ties are half wins)
Reverting to the old network

Training iteration 16 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 55.40 seconds
Training examples lengths: [64600, 64654, 65101, 64821, 65024, 64996, 64921, 64566, 64747, 64617]
Total value: 328366.19
Training on 648047 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 1.4619 (value: 0.0066, weighted value: 0.3294, policy: 1.1325, weighted policy: 1.1325), Train Mean Max: 0.5568
Epoch 2/10, Train Loss: 1.3883 (value: 0.0053, weighted value: 0.2670, policy: 1.1213, weighted policy: 1.1213), Train Mean Max: 0.5762
Epoch 3/10, Train Loss: 1.3478 (value: 0.0046, weighted value: 0.2306, policy: 1.1172, weighted policy: 1.1172), Train Mean Max: 0.5841
Epoch 4/10, Train Loss: 1.3270 (value: 0.0043, weighted value: 0.2128, policy: 1.1142, weighted policy: 1.1142), Train Mean Max: 0.5884
Epoch 5/10, Train Loss: 1.3089 (value: 0.0039, weighted value: 0.1962, policy: 1.1127, weighted policy: 1.1127), Train Mean Max: 0.5917
Epoch 6/10, Train Loss: 1.2937 (value: 0.0036, weighted value: 0.1825, policy: 1.1112, weighted policy: 1.1112), Train Mean Max: 0.5938
Epoch 7/10, Train Loss: 1.2790 (value: 0.0034, weighted value: 0.1691, policy: 1.1099, weighted policy: 1.1099), Train Mean Max: 0.5959
Epoch 8/10, Train Loss: 1.2708 (value: 0.0032, weighted value: 0.1616, policy: 1.1093, weighted policy: 1.1093), Train Mean Max: 0.5972
Epoch 9/10, Train Loss: 1.2604 (value: 0.0030, weighted value: 0.1522, policy: 1.1082, weighted policy: 1.1082), Train Mean Max: 0.5981
Epoch 10/10, Train Loss: 1.2491 (value: 0.0029, weighted value: 0.1426, policy: 1.1065, weighted policy: 1.1065), Train Mean Max: 0.5992
..training done in 72.99 seconds
..evaluation done in 17.22 seconds
Old network+MCTS average reward: 0.5198, min: -0.2870, max: 1.2315, stdev: 0.2405
New network+MCTS average reward: 0.5147, min: -0.1759, max: 1.3241, stdev: 0.2376
Old bare network average reward: 0.3165, min: -0.5185, max: 1.0093, stdev: 0.2354
New bare network average reward: 0.3421, min: -0.3796, max: 1.0648, stdev: 0.2410
External policy "random" average reward: 0.2716, min: -0.4259, max: 0.8611, stdev: 0.2119
External policy "individual greedy" average reward: 0.5351, min: -0.1204, max: 1.2315, stdev: 0.2209
External policy "total greedy" average reward: 0.6483, min: 0.1667, max: 1.4259, stdev: 0.2138
New network won 131 and tied 18 out of 300 games (46.67% wins where ties are half wins)
Reverting to the old network

Training iteration 17 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 49.01 seconds
Training examples lengths: [64654, 65101, 64821, 65024, 64996, 64921, 64566, 64747, 64617, 64661]
Total value: 330388.87
Training on 648108 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 1.4747 (value: 0.0072, weighted value: 0.3619, policy: 1.1128, weighted policy: 1.1128), Train Mean Max: 0.5600
Epoch 2/10, Train Loss: 1.3828 (value: 0.0057, weighted value: 0.2832, policy: 1.0996, weighted policy: 1.0996), Train Mean Max: 0.5817
Epoch 3/10, Train Loss: 1.3511 (value: 0.0051, weighted value: 0.2563, policy: 1.0948, weighted policy: 1.0948), Train Mean Max: 0.5892
Epoch 4/10, Train Loss: 1.3171 (value: 0.0045, weighted value: 0.2251, policy: 1.0920, weighted policy: 1.0920), Train Mean Max: 0.5948
Epoch 5/10, Train Loss: 1.2974 (value: 0.0042, weighted value: 0.2077, policy: 1.0897, weighted policy: 1.0897), Train Mean Max: 0.5980
Epoch 6/10, Train Loss: 1.2807 (value: 0.0038, weighted value: 0.1925, policy: 1.0882, weighted policy: 1.0882), Train Mean Max: 0.6006
Epoch 7/10, Train Loss: 1.2671 (value: 0.0036, weighted value: 0.1807, policy: 1.0864, weighted policy: 1.0864), Train Mean Max: 0.6025
Epoch 8/10, Train Loss: 1.2562 (value: 0.0034, weighted value: 0.1704, policy: 1.0857, weighted policy: 1.0857), Train Mean Max: 0.6039
Epoch 9/10, Train Loss: 1.2480 (value: 0.0033, weighted value: 0.1633, policy: 1.0846, weighted policy: 1.0846), Train Mean Max: 0.6049
Epoch 10/10, Train Loss: 1.2326 (value: 0.0030, weighted value: 0.1488, policy: 1.0837, weighted policy: 1.0837), Train Mean Max: 0.6065
..training done in 57.96 seconds
..evaluation done in 16.99 seconds
Old network+MCTS average reward: 0.5185, min: -0.2130, max: 1.1019, stdev: 0.2444
New network+MCTS average reward: 0.5204, min: -0.1481, max: 1.1296, stdev: 0.2351
Old bare network average reward: 0.3379, min: -0.2222, max: 1.0278, stdev: 0.2171
New bare network average reward: 0.3662, min: -0.2037, max: 0.9722, stdev: 0.2120
External policy "random" average reward: 0.2608, min: -0.3704, max: 0.8889, stdev: 0.2146
External policy "individual greedy" average reward: 0.5547, min: -0.0556, max: 1.2685, stdev: 0.2131
External policy "total greedy" average reward: 0.6758, min: 0.1574, max: 1.2593, stdev: 0.2045
New network won 135 and tied 18 out of 300 games (48.00% wins where ties are half wins)
Reverting to the old network

Training iteration 18 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 48.96 seconds
Training examples lengths: [65101, 64821, 65024, 64996, 64921, 64566, 64747, 64617, 64661, 64869]
Total value: 332368.55
Training on 648323 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 1.4898 (value: 0.0078, weighted value: 0.3906, policy: 1.0991, weighted policy: 1.0991), Train Mean Max: 0.5622
Epoch 2/10, Train Loss: 1.3954 (value: 0.0062, weighted value: 0.3108, policy: 1.0847, weighted policy: 1.0847), Train Mean Max: 0.5851
Epoch 3/10, Train Loss: 1.3507 (value: 0.0054, weighted value: 0.2713, policy: 1.0794, weighted policy: 1.0794), Train Mean Max: 0.5934
Epoch 4/10, Train Loss: 1.3178 (value: 0.0048, weighted value: 0.2415, policy: 1.0763, weighted policy: 1.0763), Train Mean Max: 0.5990
Epoch 5/10, Train Loss: 1.2982 (value: 0.0045, weighted value: 0.2245, policy: 1.0737, weighted policy: 1.0737), Train Mean Max: 0.6024
Epoch 6/10, Train Loss: 1.2778 (value: 0.0041, weighted value: 0.2052, policy: 1.0725, weighted policy: 1.0725), Train Mean Max: 0.6053
Epoch 7/10, Train Loss: 1.2601 (value: 0.0038, weighted value: 0.1894, policy: 1.0707, weighted policy: 1.0707), Train Mean Max: 0.6075
Epoch 8/10, Train Loss: 1.2492 (value: 0.0036, weighted value: 0.1794, policy: 1.0698, weighted policy: 1.0698), Train Mean Max: 0.6091
Epoch 9/10, Train Loss: 1.2366 (value: 0.0034, weighted value: 0.1681, policy: 1.0684, weighted policy: 1.0684), Train Mean Max: 0.6102
Epoch 10/10, Train Loss: 1.2241 (value: 0.0031, weighted value: 0.1567, policy: 1.0674, weighted policy: 1.0674), Train Mean Max: 0.6117
..training done in 59.28 seconds
..evaluation done in 15.18 seconds
Old network+MCTS average reward: 0.5060, min: -0.2222, max: 1.2407, stdev: 0.2496
New network+MCTS average reward: 0.5076, min: -0.1389, max: 1.3056, stdev: 0.2473
Old bare network average reward: 0.3182, min: -0.2963, max: 1.1111, stdev: 0.2498
New bare network average reward: 0.3573, min: -0.3981, max: 1.1204, stdev: 0.2573
External policy "random" average reward: 0.2510, min: -0.4167, max: 0.9630, stdev: 0.2317
External policy "individual greedy" average reward: 0.5231, min: -0.2037, max: 1.2315, stdev: 0.2382
External policy "total greedy" average reward: 0.6442, min: 0.0093, max: 1.2593, stdev: 0.2309
New network won 143 and tied 11 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 19 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 48.18 seconds
Training examples lengths: [64821, 65024, 64996, 64921, 64566, 64747, 64617, 64661, 64869, 64438]
Total value: 333427.12
Training on 647660 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 1.5141 (value: 0.0085, weighted value: 0.4263, policy: 1.0877, weighted policy: 1.0877), Train Mean Max: 0.5628
Epoch 2/10, Train Loss: 1.3978 (value: 0.0066, weighted value: 0.3276, policy: 1.0702, weighted policy: 1.0702), Train Mean Max: 0.5885
Epoch 3/10, Train Loss: 1.3509 (value: 0.0057, weighted value: 0.2857, policy: 1.0651, weighted policy: 1.0651), Train Mean Max: 0.5977
Epoch 4/10, Train Loss: 1.3199 (value: 0.0052, weighted value: 0.2588, policy: 1.0611, weighted policy: 1.0611), Train Mean Max: 0.6030
Epoch 5/10, Train Loss: 1.2954 (value: 0.0047, weighted value: 0.2360, policy: 1.0594, weighted policy: 1.0594), Train Mean Max: 0.6069
Epoch 6/10, Train Loss: 1.2761 (value: 0.0044, weighted value: 0.2187, policy: 1.0574, weighted policy: 1.0574), Train Mean Max: 0.6097
Epoch 7/10, Train Loss: 1.2544 (value: 0.0040, weighted value: 0.1990, policy: 1.0553, weighted policy: 1.0553), Train Mean Max: 0.6122
Epoch 8/10, Train Loss: 1.2409 (value: 0.0037, weighted value: 0.1863, policy: 1.0546, weighted policy: 1.0546), Train Mean Max: 0.6138
Epoch 9/10, Train Loss: 1.2304 (value: 0.0035, weighted value: 0.1773, policy: 1.0531, weighted policy: 1.0531), Train Mean Max: 0.6150
Epoch 10/10, Train Loss: 1.2163 (value: 0.0033, weighted value: 0.1642, policy: 1.0522, weighted policy: 1.0522), Train Mean Max: 0.6164
..training done in 57.63 seconds
..evaluation done in 15.63 seconds
Old network+MCTS average reward: 0.4953, min: -0.1481, max: 1.1481, stdev: 0.2343
New network+MCTS average reward: 0.4975, min: -0.1759, max: 1.2778, stdev: 0.2385
Old bare network average reward: 0.3085, min: -0.3796, max: 1.0648, stdev: 0.2422
New bare network average reward: 0.3410, min: -0.2778, max: 0.9630, stdev: 0.2272
External policy "random" average reward: 0.2516, min: -0.2130, max: 0.9352, stdev: 0.2129
External policy "individual greedy" average reward: 0.5091, min: -0.0278, max: 1.2870, stdev: 0.2257
External policy "total greedy" average reward: 0.6378, min: -0.0648, max: 1.3148, stdev: 0.2210
New network won 144 and tied 17 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 20 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 50.83 seconds
Training examples lengths: [65024, 64996, 64921, 64566, 64747, 64617, 64661, 64869, 64438, 64737]
Total value: 335281.59
Training on 647576 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 1.2109 (value: 0.0039, weighted value: 0.1928, policy: 1.0181, weighted policy: 1.0181), Train Mean Max: 0.6220
Epoch 2/10, Train Loss: 1.1901 (value: 0.0035, weighted value: 0.1731, policy: 1.0170, weighted policy: 1.0170), Train Mean Max: 0.6256
Epoch 3/10, Train Loss: 1.1775 (value: 0.0032, weighted value: 0.1613, policy: 1.0162, weighted policy: 1.0162), Train Mean Max: 0.6273
Epoch 4/10, Train Loss: 1.1692 (value: 0.0031, weighted value: 0.1544, policy: 1.0148, weighted policy: 1.0148), Train Mean Max: 0.6283
Epoch 5/10, Train Loss: 1.1586 (value: 0.0029, weighted value: 0.1442, policy: 1.0145, weighted policy: 1.0145), Train Mean Max: 0.6295
Epoch 6/10, Train Loss: 1.1540 (value: 0.0028, weighted value: 0.1409, policy: 1.0132, weighted policy: 1.0132), Train Mean Max: 0.6300
Epoch 7/10, Train Loss: 1.1433 (value: 0.0026, weighted value: 0.1305, policy: 1.0128, weighted policy: 1.0128), Train Mean Max: 0.6310
Epoch 8/10, Train Loss: 1.1358 (value: 0.0025, weighted value: 0.1240, policy: 1.0118, weighted policy: 1.0118), Train Mean Max: 0.6316
Epoch 9/10, Train Loss: 1.1347 (value: 0.0025, weighted value: 0.1238, policy: 1.0109, weighted policy: 1.0109), Train Mean Max: 0.6317
Epoch 10/10, Train Loss: 1.1300 (value: 0.0024, weighted value: 0.1193, policy: 1.0106, weighted policy: 1.0106), Train Mean Max: 0.6321
..training done in 68.32 seconds
..evaluation done in 17.09 seconds
Old network+MCTS average reward: 0.5168, min: -0.0926, max: 1.5648, stdev: 0.2359
New network+MCTS average reward: 0.5244, min: -0.0926, max: 1.3981, stdev: 0.2396
Old bare network average reward: 0.3660, min: -0.2870, max: 1.2222, stdev: 0.2355
New bare network average reward: 0.3615, min: -0.2778, max: 1.2037, stdev: 0.2364
External policy "random" average reward: 0.2783, min: -0.3148, max: 1.0093, stdev: 0.2145
External policy "individual greedy" average reward: 0.5462, min: -0.0556, max: 1.6667, stdev: 0.2292
External policy "total greedy" average reward: 0.6554, min: 0.0833, max: 1.7315, stdev: 0.2315
New network won 144 and tied 32 out of 300 games (53.33% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_20

Training iteration 21 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 50.35 seconds
Training examples lengths: [64996, 64921, 64566, 64747, 64617, 64661, 64869, 64438, 64737, 64808]
Total value: 337345.09
Training on 647360 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 1.1322 (value: 0.0030, weighted value: 0.1510, policy: 0.9812, weighted policy: 0.9812), Train Mean Max: 0.6370
Epoch 2/10, Train Loss: 1.1189 (value: 0.0028, weighted value: 0.1384, policy: 0.9805, weighted policy: 0.9805), Train Mean Max: 0.6394
Epoch 3/10, Train Loss: 1.1086 (value: 0.0026, weighted value: 0.1289, policy: 0.9797, weighted policy: 0.9797), Train Mean Max: 0.6407
Epoch 4/10, Train Loss: 1.1067 (value: 0.0025, weighted value: 0.1275, policy: 0.9792, weighted policy: 0.9792), Train Mean Max: 0.6414
Epoch 5/10, Train Loss: 1.0911 (value: 0.0023, weighted value: 0.1132, policy: 0.9779, weighted policy: 0.9779), Train Mean Max: 0.6425
Epoch 6/10, Train Loss: 1.0936 (value: 0.0023, weighted value: 0.1155, policy: 0.9780, weighted policy: 0.9780), Train Mean Max: 0.6428
Epoch 7/10, Train Loss: 1.0852 (value: 0.0022, weighted value: 0.1086, policy: 0.9766, weighted policy: 0.9766), Train Mean Max: 0.6432
Epoch 8/10, Train Loss: 1.0803 (value: 0.0021, weighted value: 0.1044, policy: 0.9759, weighted policy: 0.9759), Train Mean Max: 0.6440
Epoch 9/10, Train Loss: 1.0758 (value: 0.0020, weighted value: 0.1010, policy: 0.9748, weighted policy: 0.9748), Train Mean Max: 0.6443
Epoch 10/10, Train Loss: 1.0738 (value: 0.0020, weighted value: 0.0990, policy: 0.9748, weighted policy: 0.9748), Train Mean Max: 0.6446
..training done in 58.14 seconds
..evaluation done in 15.68 seconds
Old network+MCTS average reward: 0.5203, min: -0.3056, max: 1.2685, stdev: 0.2395
New network+MCTS average reward: 0.5222, min: -0.3056, max: 1.2315, stdev: 0.2456
Old bare network average reward: 0.3581, min: -0.2593, max: 1.0833, stdev: 0.2408
New bare network average reward: 0.3577, min: -0.2315, max: 1.0648, stdev: 0.2378
External policy "random" average reward: 0.2577, min: -0.3889, max: 0.8704, stdev: 0.2140
External policy "individual greedy" average reward: 0.5377, min: -0.1574, max: 1.2593, stdev: 0.2334
External policy "total greedy" average reward: 0.6539, min: 0.1389, max: 1.3056, stdev: 0.2276
New network won 130 and tied 39 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 22 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 51.16 seconds
Training examples lengths: [64921, 64566, 64747, 64617, 64661, 64869, 64438, 64737, 64808, 64726]
Total value: 337409.36
Training on 647090 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 1.1563 (value: 0.0039, weighted value: 0.1931, policy: 0.9631, weighted policy: 0.9631), Train Mean Max: 0.6389
Epoch 2/10, Train Loss: 1.1178 (value: 0.0031, weighted value: 0.1575, policy: 0.9604, weighted policy: 0.9604), Train Mean Max: 0.6436
Epoch 3/10, Train Loss: 1.1071 (value: 0.0029, weighted value: 0.1471, policy: 0.9600, weighted policy: 0.9600), Train Mean Max: 0.6457
Epoch 4/10, Train Loss: 1.0986 (value: 0.0028, weighted value: 0.1398, policy: 0.9588, weighted policy: 0.9588), Train Mean Max: 0.6465
Epoch 5/10, Train Loss: 1.0897 (value: 0.0026, weighted value: 0.1324, policy: 0.9573, weighted policy: 0.9573), Train Mean Max: 0.6478
Epoch 6/10, Train Loss: 1.0786 (value: 0.0024, weighted value: 0.1214, policy: 0.9572, weighted policy: 0.9572), Train Mean Max: 0.6489
Epoch 7/10, Train Loss: 1.0764 (value: 0.0024, weighted value: 0.1204, policy: 0.9560, weighted policy: 0.9560), Train Mean Max: 0.6490
Epoch 8/10, Train Loss: 1.0685 (value: 0.0023, weighted value: 0.1132, policy: 0.9553, weighted policy: 0.9553), Train Mean Max: 0.6499
Epoch 9/10, Train Loss: 1.0688 (value: 0.0023, weighted value: 0.1148, policy: 0.9540, weighted policy: 0.9540), Train Mean Max: 0.6498
Epoch 10/10, Train Loss: 1.0556 (value: 0.0020, weighted value: 0.1018, policy: 0.9538, weighted policy: 0.9538), Train Mean Max: 0.6510
..training done in 56.63 seconds
..evaluation done in 16.01 seconds
Old network+MCTS average reward: 0.5226, min: -0.0833, max: 1.2037, stdev: 0.2226
New network+MCTS average reward: 0.5295, min: 0.0185, max: 1.2222, stdev: 0.2260
Old bare network average reward: 0.3614, min: -0.2037, max: 1.0463, stdev: 0.2203
New bare network average reward: 0.3619, min: -0.1852, max: 1.0463, stdev: 0.2257
External policy "random" average reward: 0.2569, min: -0.2963, max: 0.9167, stdev: 0.2163
External policy "individual greedy" average reward: 0.5460, min: 0.0370, max: 1.3426, stdev: 0.2241
External policy "total greedy" average reward: 0.6576, min: 0.1019, max: 1.2870, stdev: 0.2187
New network won 143 and tied 36 out of 300 games (53.67% wins where ties are half wins)
Keeping the new network

Training iteration 23 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 51.64 seconds
Training examples lengths: [64566, 64747, 64617, 64661, 64869, 64438, 64737, 64808, 64726, 64650]
Total value: 337691.53
Training on 646819 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 1.0745 (value: 0.0029, weighted value: 0.1426, policy: 0.9319, weighted policy: 0.9319), Train Mean Max: 0.6539
Epoch 2/10, Train Loss: 1.0557 (value: 0.0025, weighted value: 0.1262, policy: 0.9295, weighted policy: 0.9295), Train Mean Max: 0.6562
Epoch 3/10, Train Loss: 1.0516 (value: 0.0024, weighted value: 0.1217, policy: 0.9299, weighted policy: 0.9299), Train Mean Max: 0.6572
Epoch 4/10, Train Loss: 1.0403 (value: 0.0022, weighted value: 0.1121, policy: 0.9282, weighted policy: 0.9282), Train Mean Max: 0.6582
Epoch 5/10, Train Loss: 1.0354 (value: 0.0022, weighted value: 0.1077, policy: 0.9277, weighted policy: 0.9277), Train Mean Max: 0.6588
Epoch 6/10, Train Loss: 1.0326 (value: 0.0021, weighted value: 0.1052, policy: 0.9273, weighted policy: 0.9273), Train Mean Max: 0.6592
Epoch 7/10, Train Loss: 1.0246 (value: 0.0020, weighted value: 0.0987, policy: 0.9259, weighted policy: 0.9259), Train Mean Max: 0.6599
Epoch 8/10, Train Loss: 1.0251 (value: 0.0020, weighted value: 0.0994, policy: 0.9257, weighted policy: 0.9257), Train Mean Max: 0.6600
Epoch 9/10, Train Loss: 1.0168 (value: 0.0018, weighted value: 0.0917, policy: 0.9251, weighted policy: 0.9251), Train Mean Max: 0.6607
Epoch 10/10, Train Loss: 1.0152 (value: 0.0018, weighted value: 0.0905, policy: 0.9247, weighted policy: 0.9247), Train Mean Max: 0.6610
..training done in 63.41 seconds
..evaluation done in 16.78 seconds
Old network+MCTS average reward: 0.5351, min: -0.3056, max: 1.3148, stdev: 0.2273
New network+MCTS average reward: 0.5323, min: -0.1019, max: 1.3148, stdev: 0.2302
Old bare network average reward: 0.3605, min: -0.3241, max: 1.0370, stdev: 0.2358
New bare network average reward: 0.3635, min: -0.5370, max: 1.1204, stdev: 0.2406
External policy "random" average reward: 0.2628, min: -0.3889, max: 0.9722, stdev: 0.2325
External policy "individual greedy" average reward: 0.5502, min: -0.0093, max: 1.2778, stdev: 0.2232
External policy "total greedy" average reward: 0.6534, min: 0.0926, max: 1.3796, stdev: 0.2170
New network won 133 and tied 37 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 24 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 52.11 seconds
Training examples lengths: [64747, 64617, 64661, 64869, 64438, 64737, 64808, 64726, 64650, 64798]
Total value: 339400.75
Training on 647051 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 1.0278 (value: 0.0026, weighted value: 0.1292, policy: 0.8985, weighted policy: 0.8985), Train Mean Max: 0.6646
Epoch 2/10, Train Loss: 1.0099 (value: 0.0022, weighted value: 0.1121, policy: 0.8978, weighted policy: 0.8978), Train Mean Max: 0.6673
Epoch 3/10, Train Loss: 1.0041 (value: 0.0021, weighted value: 0.1072, policy: 0.8970, weighted policy: 0.8970), Train Mean Max: 0.6683
Epoch 4/10, Train Loss: 1.0007 (value: 0.0021, weighted value: 0.1048, policy: 0.8959, weighted policy: 0.8959), Train Mean Max: 0.6690
Epoch 5/10, Train Loss: 0.9918 (value: 0.0019, weighted value: 0.0968, policy: 0.8950, weighted policy: 0.8950), Train Mean Max: 0.6698
Epoch 6/10, Train Loss: 0.9893 (value: 0.0019, weighted value: 0.0946, policy: 0.8946, weighted policy: 0.8946), Train Mean Max: 0.6702
Epoch 7/10, Train Loss: 0.9833 (value: 0.0018, weighted value: 0.0892, policy: 0.8941, weighted policy: 0.8941), Train Mean Max: 0.6709
Epoch 8/10, Train Loss: 0.9837 (value: 0.0018, weighted value: 0.0903, policy: 0.8934, weighted policy: 0.8934), Train Mean Max: 0.6710
Epoch 9/10, Train Loss: 0.9778 (value: 0.0017, weighted value: 0.0850, policy: 0.8928, weighted policy: 0.8928), Train Mean Max: 0.6713
Epoch 10/10, Train Loss: 0.9752 (value: 0.0017, weighted value: 0.0829, policy: 0.8923, weighted policy: 0.8923), Train Mean Max: 0.6719
..training done in 64.84 seconds
..evaluation done in 16.86 seconds
Old network+MCTS average reward: 0.5111, min: -0.2315, max: 1.1389, stdev: 0.2509
New network+MCTS average reward: 0.5101, min: -0.0926, max: 1.2130, stdev: 0.2565
Old bare network average reward: 0.3589, min: -0.1944, max: 1.0556, stdev: 0.2478
New bare network average reward: 0.3587, min: -0.2593, max: 1.1389, stdev: 0.2516
External policy "random" average reward: 0.2600, min: -0.4074, max: 1.0556, stdev: 0.2419
External policy "individual greedy" average reward: 0.5402, min: 0.0185, max: 1.3611, stdev: 0.2419
External policy "total greedy" average reward: 0.6472, min: 0.1019, max: 1.3426, stdev: 0.2360
New network won 125 and tied 40 out of 300 games (48.33% wins where ties are half wins)
Reverting to the old network

Training iteration 25 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 51.24 seconds
Training examples lengths: [64617, 64661, 64869, 64438, 64737, 64808, 64726, 64650, 64798, 64618]
Total value: 340675.47
Training on 646922 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 1.0387 (value: 0.0033, weighted value: 0.1659, policy: 0.8728, weighted policy: 0.8728), Train Mean Max: 0.6683
Epoch 2/10, Train Loss: 1.0143 (value: 0.0029, weighted value: 0.1436, policy: 0.8707, weighted policy: 0.8707), Train Mean Max: 0.6726
Epoch 3/10, Train Loss: 0.9956 (value: 0.0025, weighted value: 0.1268, policy: 0.8688, weighted policy: 0.8688), Train Mean Max: 0.6751
Epoch 4/10, Train Loss: 0.9886 (value: 0.0024, weighted value: 0.1201, policy: 0.8685, weighted policy: 0.8685), Train Mean Max: 0.6764
Epoch 5/10, Train Loss: 0.9788 (value: 0.0022, weighted value: 0.1111, policy: 0.8676, weighted policy: 0.8676), Train Mean Max: 0.6778
Epoch 6/10, Train Loss: 0.9742 (value: 0.0022, weighted value: 0.1075, policy: 0.8667, weighted policy: 0.8667), Train Mean Max: 0.6786
Epoch 7/10, Train Loss: 0.9669 (value: 0.0020, weighted value: 0.1018, policy: 0.8652, weighted policy: 0.8652), Train Mean Max: 0.6793
Epoch 8/10, Train Loss: 0.9667 (value: 0.0020, weighted value: 0.1014, policy: 0.8652, weighted policy: 0.8652), Train Mean Max: 0.6795
Epoch 9/10, Train Loss: 0.9574 (value: 0.0019, weighted value: 0.0930, policy: 0.8644, weighted policy: 0.8644), Train Mean Max: 0.6805
Epoch 10/10, Train Loss: 0.9556 (value: 0.0018, weighted value: 0.0919, policy: 0.8636, weighted policy: 0.8636), Train Mean Max: 0.6807
..training done in 67.05 seconds
..evaluation done in 16.34 seconds
Old network+MCTS average reward: 0.5330, min: -0.2778, max: 1.1111, stdev: 0.2425
New network+MCTS average reward: 0.5433, min: -0.2130, max: 1.4444, stdev: 0.2550
Old bare network average reward: 0.3744, min: -0.3889, max: 1.0926, stdev: 0.2451
New bare network average reward: 0.3909, min: -0.3148, max: 1.0926, stdev: 0.2486
External policy "random" average reward: 0.2623, min: -0.2778, max: 1.0463, stdev: 0.2364
External policy "individual greedy" average reward: 0.5435, min: -0.0833, max: 1.2222, stdev: 0.2337
External policy "total greedy" average reward: 0.6651, min: -0.0741, max: 1.3519, stdev: 0.2291
New network won 135 and tied 41 out of 300 games (51.83% wins where ties are half wins)
Keeping the new network

Training iteration 26 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 51.83 seconds
Training examples lengths: [64661, 64869, 64438, 64737, 64808, 64726, 64650, 64798, 64618, 64720]
Total value: 341942.87
Training on 647025 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.9619 (value: 0.0025, weighted value: 0.1275, policy: 0.8344, weighted policy: 0.8344), Train Mean Max: 0.6849
Epoch 2/10, Train Loss: 0.9469 (value: 0.0023, weighted value: 0.1139, policy: 0.8330, weighted policy: 0.8330), Train Mean Max: 0.6876
Epoch 3/10, Train Loss: 0.9369 (value: 0.0021, weighted value: 0.1049, policy: 0.8320, weighted policy: 0.8320), Train Mean Max: 0.6889
Epoch 4/10, Train Loss: 0.9309 (value: 0.0020, weighted value: 0.1000, policy: 0.8310, weighted policy: 0.8310), Train Mean Max: 0.6902
Epoch 5/10, Train Loss: 0.9278 (value: 0.0019, weighted value: 0.0971, policy: 0.8307, weighted policy: 0.8307), Train Mean Max: 0.6907
Epoch 6/10, Train Loss: 0.9204 (value: 0.0018, weighted value: 0.0909, policy: 0.8295, weighted policy: 0.8295), Train Mean Max: 0.6916
Epoch 7/10, Train Loss: 0.9171 (value: 0.0018, weighted value: 0.0880, policy: 0.8292, weighted policy: 0.8292), Train Mean Max: 0.6921
Epoch 8/10, Train Loss: 0.9139 (value: 0.0017, weighted value: 0.0856, policy: 0.8283, weighted policy: 0.8283), Train Mean Max: 0.6925
Epoch 9/10, Train Loss: 0.9129 (value: 0.0017, weighted value: 0.0849, policy: 0.8280, weighted policy: 0.8280), Train Mean Max: 0.6929
Epoch 10/10, Train Loss: 0.9084 (value: 0.0016, weighted value: 0.0811, policy: 0.8274, weighted policy: 0.8274), Train Mean Max: 0.6931
..training done in 57.73 seconds
..evaluation done in 15.82 seconds
Old network+MCTS average reward: 0.5257, min: -0.2037, max: 1.1574, stdev: 0.2340
New network+MCTS average reward: 0.5419, min: -0.1944, max: 1.1667, stdev: 0.2305
Old bare network average reward: 0.3785, min: -0.3426, max: 0.9722, stdev: 0.2316
New bare network average reward: 0.3932, min: -0.4537, max: 0.9815, stdev: 0.2304
External policy "random" average reward: 0.2651, min: -0.3333, max: 0.8333, stdev: 0.2239
External policy "individual greedy" average reward: 0.5407, min: -0.1204, max: 1.0741, stdev: 0.2148
External policy "total greedy" average reward: 0.6449, min: 0.0741, max: 1.2222, stdev: 0.2053
New network won 147 and tied 42 out of 300 games (56.00% wins where ties are half wins)
Keeping the new network

Training iteration 27 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 51.84 seconds
Training examples lengths: [64869, 64438, 64737, 64808, 64726, 64650, 64798, 64618, 64720, 64668]
Total value: 343307.09
Training on 647032 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.9120 (value: 0.0023, weighted value: 0.1166, policy: 0.7955, weighted policy: 0.7955), Train Mean Max: 0.6975
Epoch 2/10, Train Loss: 0.8988 (value: 0.0021, weighted value: 0.1042, policy: 0.7945, weighted policy: 0.7945), Train Mean Max: 0.7002
Epoch 3/10, Train Loss: 0.8907 (value: 0.0020, weighted value: 0.0975, policy: 0.7931, weighted policy: 0.7931), Train Mean Max: 0.7018
Epoch 4/10, Train Loss: 0.8852 (value: 0.0019, weighted value: 0.0929, policy: 0.7924, weighted policy: 0.7924), Train Mean Max: 0.7029
Epoch 5/10, Train Loss: 0.8794 (value: 0.0018, weighted value: 0.0878, policy: 0.7916, weighted policy: 0.7916), Train Mean Max: 0.7036
Epoch 6/10, Train Loss: 0.8797 (value: 0.0018, weighted value: 0.0881, policy: 0.7915, weighted policy: 0.7915), Train Mean Max: 0.7041
Epoch 7/10, Train Loss: 0.8717 (value: 0.0016, weighted value: 0.0813, policy: 0.7904, weighted policy: 0.7904), Train Mean Max: 0.7049
Epoch 8/10, Train Loss: 0.8706 (value: 0.0016, weighted value: 0.0810, policy: 0.7896, weighted policy: 0.7896), Train Mean Max: 0.7052
Epoch 9/10, Train Loss: 0.8667 (value: 0.0016, weighted value: 0.0776, policy: 0.7891, weighted policy: 0.7891), Train Mean Max: 0.7057
Epoch 10/10, Train Loss: 0.8635 (value: 0.0015, weighted value: 0.0749, policy: 0.7886, weighted policy: 0.7886), Train Mean Max: 0.7060
..training done in 57.90 seconds
..evaluation done in 16.32 seconds
Old network+MCTS average reward: 0.5526, min: -0.0463, max: 1.3519, stdev: 0.2389
New network+MCTS average reward: 0.5494, min: -0.0556, max: 1.3426, stdev: 0.2302
Old bare network average reward: 0.3882, min: -0.2407, max: 1.2500, stdev: 0.2488
New bare network average reward: 0.3885, min: -0.3056, max: 1.1944, stdev: 0.2468
External policy "random" average reward: 0.2659, min: -0.2593, max: 1.0741, stdev: 0.2270
External policy "individual greedy" average reward: 0.5466, min: 0.0556, max: 1.3796, stdev: 0.2316
External policy "total greedy" average reward: 0.6579, min: 0.0741, max: 1.3981, stdev: 0.2194
New network won 131 and tied 43 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 28 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 53.13 seconds
Training examples lengths: [64438, 64737, 64808, 64726, 64650, 64798, 64618, 64720, 64668, 64896]
Total value: 345689.73
Training on 647059 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.8675 (value: 0.0023, weighted value: 0.1157, policy: 0.7518, weighted policy: 0.7518), Train Mean Max: 0.7116
Epoch 2/10, Train Loss: 0.8517 (value: 0.0020, weighted value: 0.1013, policy: 0.7505, weighted policy: 0.7505), Train Mean Max: 0.7146
Epoch 3/10, Train Loss: 0.8455 (value: 0.0019, weighted value: 0.0962, policy: 0.7494, weighted policy: 0.7494), Train Mean Max: 0.7158
Epoch 4/10, Train Loss: 0.8354 (value: 0.0017, weighted value: 0.0870, policy: 0.7484, weighted policy: 0.7484), Train Mean Max: 0.7174
Epoch 5/10, Train Loss: 0.8342 (value: 0.0017, weighted value: 0.0863, policy: 0.7479, weighted policy: 0.7479), Train Mean Max: 0.7182
Epoch 6/10, Train Loss: 0.8297 (value: 0.0017, weighted value: 0.0826, policy: 0.7471, weighted policy: 0.7471), Train Mean Max: 0.7189
Epoch 7/10, Train Loss: 0.8264 (value: 0.0016, weighted value: 0.0797, policy: 0.7467, weighted policy: 0.7467), Train Mean Max: 0.7195
Epoch 8/10, Train Loss: 0.8249 (value: 0.0016, weighted value: 0.0786, policy: 0.7462, weighted policy: 0.7462), Train Mean Max: 0.7199
Epoch 9/10, Train Loss: 0.8210 (value: 0.0015, weighted value: 0.0759, policy: 0.7451, weighted policy: 0.7451), Train Mean Max: 0.7204
Epoch 10/10, Train Loss: 0.8170 (value: 0.0014, weighted value: 0.0721, policy: 0.7449, weighted policy: 0.7449), Train Mean Max: 0.7209
..training done in 57.07 seconds
..evaluation done in 16.12 seconds
Old network+MCTS average reward: 0.5603, min: -0.0463, max: 1.2963, stdev: 0.2354
New network+MCTS average reward: 0.5533, min: -0.1944, max: 1.3148, stdev: 0.2381
Old bare network average reward: 0.3971, min: -0.2037, max: 1.0833, stdev: 0.2385
New bare network average reward: 0.4129, min: -0.1852, max: 1.0833, stdev: 0.2301
External policy "random" average reward: 0.2717, min: -0.4074, max: 0.9074, stdev: 0.2179
External policy "individual greedy" average reward: 0.5423, min: 0.0278, max: 1.3241, stdev: 0.2232
External policy "total greedy" average reward: 0.6554, min: 0.0741, max: 1.3704, stdev: 0.2140
New network won 124 and tied 47 out of 300 games (49.17% wins where ties are half wins)
Reverting to the old network

Training iteration 29 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 52.63 seconds
Training examples lengths: [64737, 64808, 64726, 64650, 64798, 64618, 64720, 64668, 64896, 64550]
Total value: 348205.43
Training on 647171 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.8626 (value: 0.0030, weighted value: 0.1495, policy: 0.7132, weighted policy: 0.7132), Train Mean Max: 0.7172
Epoch 2/10, Train Loss: 0.8465 (value: 0.0027, weighted value: 0.1330, policy: 0.7135, weighted policy: 0.7135), Train Mean Max: 0.7228
Epoch 3/10, Train Loss: 0.8542 (value: 0.0026, weighted value: 0.1281, policy: 0.7260, weighted policy: 0.7260), Train Mean Max: 0.7251
Epoch 4/10, Train Loss: 0.8692 (value: 0.0028, weighted value: 0.1425, policy: 0.7267, weighted policy: 0.7267), Train Mean Max: 0.7251
Epoch 5/10, Train Loss: 0.8532 (value: 0.0026, weighted value: 0.1286, policy: 0.7245, weighted policy: 0.7245), Train Mean Max: 0.7280
Epoch 6/10, Train Loss: 0.9198 (value: 0.0036, weighted value: 0.1798, policy: 0.7399, weighted policy: 0.7399), Train Mean Max: 0.7253
Epoch 7/10, Train Loss: 0.8287 (value: 0.0023, weighted value: 0.1165, policy: 0.7121, weighted policy: 0.7121), Train Mean Max: 0.7307
Epoch 8/10, Train Loss: 0.8126 (value: 0.0022, weighted value: 0.1085, policy: 0.7041, weighted policy: 0.7041), Train Mean Max: 0.7315
Epoch 9/10, Train Loss: 0.8621 (value: 0.0022, weighted value: 0.1087, policy: 0.7534, weighted policy: 0.7534), Train Mean Max: 0.7308
Epoch 10/10, Train Loss: 0.8279 (value: 0.0024, weighted value: 0.1219, policy: 0.7061, weighted policy: 0.7061), Train Mean Max: 0.7303
..training done in 59.32 seconds
..evaluation done in 15.87 seconds
Old network+MCTS average reward: 0.5734, min: -0.0926, max: 1.1204, stdev: 0.2350
New network+MCTS average reward: 0.5560, min: -0.1019, max: 1.2315, stdev: 0.2365
Old bare network average reward: 0.4213, min: -0.2130, max: 1.0741, stdev: 0.2423
New bare network average reward: 0.4319, min: -0.2037, max: 1.0278, stdev: 0.2364
External policy "random" average reward: 0.2839, min: -0.2130, max: 0.8981, stdev: 0.2160
External policy "individual greedy" average reward: 0.5653, min: -0.0370, max: 1.3241, stdev: 0.2272
External policy "total greedy" average reward: 0.6827, min: 0.1481, max: 1.3426, stdev: 0.2198
New network won 107 and tied 34 out of 300 games (41.33% wins where ties are half wins)
Reverting to the old network

Training iteration 30 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 53.06 seconds
Training examples lengths: [64808, 64726, 64650, 64798, 64618, 64720, 64668, 64896, 64550, 65106]
Total value: 350255.81
Training on 647540 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.8821 (value: 0.0037, weighted value: 0.1840, policy: 0.6981, weighted policy: 0.6981), Train Mean Max: 0.7186
Epoch 2/10, Train Loss: 0.8469 (value: 0.0030, weighted value: 0.1516, policy: 0.6953, weighted policy: 0.6953), Train Mean Max: 0.7255
Epoch 3/10, Train Loss: 0.8281 (value: 0.0027, weighted value: 0.1351, policy: 0.6930, weighted policy: 0.6930), Train Mean Max: 0.7292
Epoch 4/10, Train Loss: 0.8135 (value: 0.0025, weighted value: 0.1225, policy: 0.6910, weighted policy: 0.6910), Train Mean Max: 0.7319
Epoch 5/10, Train Loss: 0.8041 (value: 0.0023, weighted value: 0.1147, policy: 0.6894, weighted policy: 0.6894), Train Mean Max: 0.7341
Epoch 6/10, Train Loss: 0.7965 (value: 0.0022, weighted value: 0.1077, policy: 0.6888, weighted policy: 0.6888), Train Mean Max: 0.7357
Epoch 7/10, Train Loss: 0.7889 (value: 0.0020, weighted value: 0.1017, policy: 0.6872, weighted policy: 0.6872), Train Mean Max: 0.7368
Epoch 8/10, Train Loss: 0.7844 (value: 0.0020, weighted value: 0.0981, policy: 0.6862, weighted policy: 0.6862), Train Mean Max: 0.7380
Epoch 9/10, Train Loss: 0.7791 (value: 0.0019, weighted value: 0.0935, policy: 0.6857, weighted policy: 0.6857), Train Mean Max: 0.7388
Epoch 10/10, Train Loss: 0.7761 (value: 0.0018, weighted value: 0.0910, policy: 0.6851, weighted policy: 0.6851), Train Mean Max: 0.7393
..training done in 63.75 seconds
..evaluation done in 16.59 seconds
Old network+MCTS average reward: 0.5411, min: -0.1111, max: 1.4815, stdev: 0.2223
New network+MCTS average reward: 0.5451, min: 0.0000, max: 1.4444, stdev: 0.2165
Old bare network average reward: 0.3986, min: -0.1852, max: 1.1481, stdev: 0.2245
New bare network average reward: 0.4048, min: -0.2778, max: 1.2500, stdev: 0.2258
External policy "random" average reward: 0.2658, min: -0.1944, max: 0.9630, stdev: 0.2071
External policy "individual greedy" average reward: 0.5317, min: -0.0926, max: 1.3981, stdev: 0.2209
External policy "total greedy" average reward: 0.6423, min: 0.1296, max: 1.6296, stdev: 0.2095
New network won 137 and tied 50 out of 300 games (54.00% wins where ties are half wins)
Keeping the new network

Training iteration 31 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 52.73 seconds
Training examples lengths: [64726, 64650, 64798, 64618, 64720, 64668, 64896, 64550, 65106, 64500]
Total value: 351753.31
Training on 647232 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.7917 (value: 0.0025, weighted value: 0.1230, policy: 0.6687, weighted policy: 0.6687), Train Mean Max: 0.7415
Epoch 2/10, Train Loss: 0.7770 (value: 0.0022, weighted value: 0.1086, policy: 0.6683, weighted policy: 0.6683), Train Mean Max: 0.7434
Epoch 3/10, Train Loss: 0.7720 (value: 0.0021, weighted value: 0.1049, policy: 0.6671, weighted policy: 0.6671), Train Mean Max: 0.7444
Epoch 4/10, Train Loss: 0.7617 (value: 0.0019, weighted value: 0.0954, policy: 0.6663, weighted policy: 0.6663), Train Mean Max: 0.7457
Epoch 5/10, Train Loss: 0.7578 (value: 0.0018, weighted value: 0.0924, policy: 0.6654, weighted policy: 0.6654), Train Mean Max: 0.7463
Epoch 6/10, Train Loss: 0.7572 (value: 0.0018, weighted value: 0.0923, policy: 0.6649, weighted policy: 0.6649), Train Mean Max: 0.7468
Epoch 7/10, Train Loss: 0.7500 (value: 0.0017, weighted value: 0.0859, policy: 0.6642, weighted policy: 0.6642), Train Mean Max: 0.7475
Epoch 8/10, Train Loss: 0.7476 (value: 0.0017, weighted value: 0.0837, policy: 0.6638, weighted policy: 0.6638), Train Mean Max: 0.7479
Epoch 9/10, Train Loss: 0.7449 (value: 0.0016, weighted value: 0.0816, policy: 0.6634, weighted policy: 0.6634), Train Mean Max: 0.7484
Epoch 10/10, Train Loss: 0.7399 (value: 0.0016, weighted value: 0.0777, policy: 0.6622, weighted policy: 0.6622), Train Mean Max: 0.7490
..training done in 62.94 seconds
..evaluation done in 16.44 seconds
Old network+MCTS average reward: 0.5396, min: -0.1574, max: 1.3796, stdev: 0.2454
New network+MCTS average reward: 0.5477, min: -0.1574, max: 1.3426, stdev: 0.2432
Old bare network average reward: 0.4001, min: -0.2407, max: 1.0093, stdev: 0.2328
New bare network average reward: 0.3992, min: -0.2963, max: 0.9907, stdev: 0.2336
External policy "random" average reward: 0.2595, min: -0.3148, max: 0.9167, stdev: 0.2204
External policy "individual greedy" average reward: 0.5313, min: -0.1574, max: 1.2778, stdev: 0.2249
External policy "total greedy" average reward: 0.6404, min: 0.0926, max: 1.4074, stdev: 0.2252
New network won 134 and tied 54 out of 300 games (53.67% wins where ties are half wins)
Keeping the new network

Training iteration 32 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 54.14 seconds
Training examples lengths: [64650, 64798, 64618, 64720, 64668, 64896, 64550, 65106, 64500, 65449]
Total value: 354802.60
Training on 647955 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.7565 (value: 0.0023, weighted value: 0.1134, policy: 0.6431, weighted policy: 0.6431), Train Mean Max: 0.7514
Epoch 2/10, Train Loss: 0.7422 (value: 0.0020, weighted value: 0.1005, policy: 0.6417, weighted policy: 0.6417), Train Mean Max: 0.7531
Epoch 3/10, Train Loss: 0.7344 (value: 0.0019, weighted value: 0.0949, policy: 0.6395, weighted policy: 0.6395), Train Mean Max: 0.7541
Epoch 4/10, Train Loss: 0.7296 (value: 0.0018, weighted value: 0.0902, policy: 0.6394, weighted policy: 0.6394), Train Mean Max: 0.7549
Epoch 5/10, Train Loss: 0.7233 (value: 0.0017, weighted value: 0.0844, policy: 0.6389, weighted policy: 0.6389), Train Mean Max: 0.7558
Epoch 6/10, Train Loss: 0.7193 (value: 0.0016, weighted value: 0.0810, policy: 0.6383, weighted policy: 0.6383), Train Mean Max: 0.7564
Epoch 7/10, Train Loss: 0.7165 (value: 0.0016, weighted value: 0.0794, policy: 0.6372, weighted policy: 0.6372), Train Mean Max: 0.7569
Epoch 8/10, Train Loss: 0.7148 (value: 0.0016, weighted value: 0.0786, policy: 0.6362, weighted policy: 0.6362), Train Mean Max: 0.7574
Epoch 9/10, Train Loss: 0.7120 (value: 0.0015, weighted value: 0.0758, policy: 0.6362, weighted policy: 0.6362), Train Mean Max: 0.7576
Epoch 10/10, Train Loss: 0.7075 (value: 0.0014, weighted value: 0.0722, policy: 0.6353, weighted policy: 0.6353), Train Mean Max: 0.7582
..training done in 63.04 seconds
..evaluation done in 16.56 seconds
Old network+MCTS average reward: 0.5698, min: -0.0648, max: 1.3148, stdev: 0.2382
New network+MCTS average reward: 0.5623, min: -0.1204, max: 1.3241, stdev: 0.2447
Old bare network average reward: 0.4199, min: -0.3241, max: 1.3704, stdev: 0.2415
New bare network average reward: 0.4216, min: -0.2222, max: 1.3056, stdev: 0.2377
External policy "random" average reward: 0.2686, min: -0.3148, max: 1.0000, stdev: 0.2282
External policy "individual greedy" average reward: 0.5527, min: -0.0741, max: 1.3704, stdev: 0.2296
External policy "total greedy" average reward: 0.6658, min: -0.0463, max: 1.4444, stdev: 0.2288
New network won 116 and tied 52 out of 300 games (47.33% wins where ties are half wins)
Reverting to the old network

Training iteration 33 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 54.37 seconds
Training examples lengths: [64798, 64618, 64720, 64668, 64896, 64550, 65106, 64500, 65449, 65046]
Total value: 357267.66
Training on 648351 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.7804 (value: 0.0030, weighted value: 0.1524, policy: 0.6281, weighted policy: 0.6281), Train Mean Max: 0.7528
Epoch 2/10, Train Loss: 0.7500 (value: 0.0025, weighted value: 0.1248, policy: 0.6252, weighted policy: 0.6252), Train Mean Max: 0.7558
Epoch 3/10, Train Loss: 0.7398 (value: 0.0023, weighted value: 0.1154, policy: 0.6243, weighted policy: 0.6243), Train Mean Max: 0.7575
Epoch 4/10, Train Loss: 0.7302 (value: 0.0021, weighted value: 0.1072, policy: 0.6230, weighted policy: 0.6230), Train Mean Max: 0.7590
Epoch 5/10, Train Loss: 0.7209 (value: 0.0020, weighted value: 0.0994, policy: 0.6215, weighted policy: 0.6215), Train Mean Max: 0.7602
Epoch 6/10, Train Loss: 0.7152 (value: 0.0019, weighted value: 0.0939, policy: 0.6213, weighted policy: 0.6213), Train Mean Max: 0.7611
Epoch 7/10, Train Loss: 0.7115 (value: 0.0018, weighted value: 0.0918, policy: 0.6197, weighted policy: 0.6197), Train Mean Max: 0.7618
Epoch 8/10, Train Loss: 0.7074 (value: 0.0018, weighted value: 0.0881, policy: 0.6193, weighted policy: 0.6193), Train Mean Max: 0.7625
Epoch 9/10, Train Loss: 0.7054 (value: 0.0017, weighted value: 0.0875, policy: 0.6179, weighted policy: 0.6179), Train Mean Max: 0.7630
Epoch 10/10, Train Loss: 0.6993 (value: 0.0016, weighted value: 0.0811, policy: 0.6182, weighted policy: 0.6182), Train Mean Max: 0.7637
..training done in 72.15 seconds
..evaluation done in 16.67 seconds
Old network+MCTS average reward: 0.5475, min: -0.0556, max: 1.2685, stdev: 0.2181
New network+MCTS average reward: 0.5495, min: -0.0741, max: 1.2685, stdev: 0.2061
Old bare network average reward: 0.4027, min: -0.2407, max: 1.1667, stdev: 0.2161
New bare network average reward: 0.4155, min: -0.1852, max: 1.1019, stdev: 0.2135
External policy "random" average reward: 0.2549, min: -0.2963, max: 0.9722, stdev: 0.2058
External policy "individual greedy" average reward: 0.5297, min: 0.0278, max: 1.4630, stdev: 0.2174
External policy "total greedy" average reward: 0.6392, min: 0.0370, max: 1.5370, stdev: 0.2180
New network won 122 and tied 65 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 34 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 52.80 seconds
Training examples lengths: [64618, 64720, 64668, 64896, 64550, 65106, 64500, 65449, 65046, 64871]
Total value: 359078.87
Training on 648424 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.7177 (value: 0.0023, weighted value: 0.1145, policy: 0.6033, weighted policy: 0.6033), Train Mean Max: 0.7652
Epoch 2/10, Train Loss: 0.7035 (value: 0.0020, weighted value: 0.1018, policy: 0.6017, weighted policy: 0.6017), Train Mean Max: 0.7668
Epoch 3/10, Train Loss: 0.6962 (value: 0.0019, weighted value: 0.0958, policy: 0.6003, weighted policy: 0.6003), Train Mean Max: 0.7679
Epoch 4/10, Train Loss: 0.6923 (value: 0.0018, weighted value: 0.0921, policy: 0.6002, weighted policy: 0.6002), Train Mean Max: 0.7686
Epoch 5/10, Train Loss: 0.6856 (value: 0.0017, weighted value: 0.0866, policy: 0.5991, weighted policy: 0.5991), Train Mean Max: 0.7694
Epoch 6/10, Train Loss: 0.6797 (value: 0.0016, weighted value: 0.0815, policy: 0.5982, weighted policy: 0.5982), Train Mean Max: 0.7701
Epoch 7/10, Train Loss: 0.6808 (value: 0.0017, weighted value: 0.0833, policy: 0.5975, weighted policy: 0.5975), Train Mean Max: 0.7704
Epoch 8/10, Train Loss: 0.6746 (value: 0.0015, weighted value: 0.0769, policy: 0.5977, weighted policy: 0.5977), Train Mean Max: 0.7710
Epoch 9/10, Train Loss: 0.6718 (value: 0.0015, weighted value: 0.0752, policy: 0.5967, weighted policy: 0.5967), Train Mean Max: 0.7714
Epoch 10/10, Train Loss: 0.6693 (value: 0.0015, weighted value: 0.0737, policy: 0.5955, weighted policy: 0.5955), Train Mean Max: 0.7718
..training done in 62.16 seconds
..evaluation done in 24.44 seconds
Old network+MCTS average reward: 0.5436, min: -0.1204, max: 1.2685, stdev: 0.2255
New network+MCTS average reward: 0.5550, min: -0.1481, max: 1.2963, stdev: 0.2224
Old bare network average reward: 0.4167, min: -0.1944, max: 1.1481, stdev: 0.2382
New bare network average reward: 0.4184, min: -0.2130, max: 1.0741, stdev: 0.2325
External policy "random" average reward: 0.2351, min: -0.3981, max: 0.8426, stdev: 0.2142
External policy "individual greedy" average reward: 0.5235, min: -0.0370, max: 1.3426, stdev: 0.2198
External policy "total greedy" average reward: 0.6374, min: -0.0185, max: 1.3981, stdev: 0.2168
New network won 125 and tied 70 out of 300 games (53.33% wins where ties are half wins)
Keeping the new network

Training iteration 35 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 55.52 seconds
Training examples lengths: [64720, 64668, 64896, 64550, 65106, 64500, 65449, 65046, 64871, 64915]
Total value: 361286.66
Training on 648721 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.6903 (value: 0.0022, weighted value: 0.1100, policy: 0.5802, weighted policy: 0.5802), Train Mean Max: 0.7734
Epoch 2/10, Train Loss: 0.6763 (value: 0.0020, weighted value: 0.0983, policy: 0.5780, weighted policy: 0.5780), Train Mean Max: 0.7749
Epoch 3/10, Train Loss: 0.6667 (value: 0.0018, weighted value: 0.0897, policy: 0.5770, weighted policy: 0.5770), Train Mean Max: 0.7760
Epoch 4/10, Train Loss: 0.6607 (value: 0.0017, weighted value: 0.0844, policy: 0.5763, weighted policy: 0.5763), Train Mean Max: 0.7769
Epoch 5/10, Train Loss: 0.6573 (value: 0.0016, weighted value: 0.0819, policy: 0.5754, weighted policy: 0.5754), Train Mean Max: 0.7775
Epoch 6/10, Train Loss: 0.6546 (value: 0.0016, weighted value: 0.0788, policy: 0.5758, weighted policy: 0.5758), Train Mean Max: 0.7780
Epoch 7/10, Train Loss: 0.6493 (value: 0.0015, weighted value: 0.0745, policy: 0.5747, weighted policy: 0.5747), Train Mean Max: 0.7786
Epoch 8/10, Train Loss: 0.6493 (value: 0.0015, weighted value: 0.0753, policy: 0.5740, weighted policy: 0.5740), Train Mean Max: 0.7789
Epoch 9/10, Train Loss: 0.6445 (value: 0.0014, weighted value: 0.0714, policy: 0.5730, weighted policy: 0.5730), Train Mean Max: 0.7795
Epoch 10/10, Train Loss: 0.6432 (value: 0.0014, weighted value: 0.0702, policy: 0.5731, weighted policy: 0.5731), Train Mean Max: 0.7797
..training done in 70.63 seconds
..evaluation done in 17.76 seconds
Old network+MCTS average reward: 0.5720, min: 0.0463, max: 1.2315, stdev: 0.2170
New network+MCTS average reward: 0.5824, min: -0.0093, max: 1.2593, stdev: 0.2107
Old bare network average reward: 0.4397, min: -0.2407, max: 1.0833, stdev: 0.2264
New bare network average reward: 0.4512, min: -0.1944, max: 1.0833, stdev: 0.2200
External policy "random" average reward: 0.2769, min: -0.3519, max: 0.9722, stdev: 0.2158
External policy "individual greedy" average reward: 0.5568, min: -0.0370, max: 1.1667, stdev: 0.2149
External policy "total greedy" average reward: 0.6617, min: 0.1574, max: 1.3704, stdev: 0.2107
New network won 121 and tied 74 out of 300 games (52.67% wins where ties are half wins)
Keeping the new network

Training iteration 36 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 54.12 seconds
Training examples lengths: [64668, 64896, 64550, 65106, 64500, 65449, 65046, 64871, 64915, 64727]
Total value: 363334.21
Training on 648728 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.6643 (value: 0.0021, weighted value: 0.1061, policy: 0.5582, weighted policy: 0.5582), Train Mean Max: 0.7812
Epoch 2/10, Train Loss: 0.6468 (value: 0.0018, weighted value: 0.0900, policy: 0.5568, weighted policy: 0.5568), Train Mean Max: 0.7827
Epoch 3/10, Train Loss: 0.6418 (value: 0.0017, weighted value: 0.0859, policy: 0.5559, weighted policy: 0.5559), Train Mean Max: 0.7836
Epoch 4/10, Train Loss: 0.6375 (value: 0.0017, weighted value: 0.0831, policy: 0.5544, weighted policy: 0.5544), Train Mean Max: 0.7844
Epoch 5/10, Train Loss: 0.6330 (value: 0.0016, weighted value: 0.0783, policy: 0.5547, weighted policy: 0.5547), Train Mean Max: 0.7851
Epoch 6/10, Train Loss: 0.6293 (value: 0.0015, weighted value: 0.0763, policy: 0.5531, weighted policy: 0.5531), Train Mean Max: 0.7856
Epoch 7/10, Train Loss: 0.6274 (value: 0.0015, weighted value: 0.0745, policy: 0.5529, weighted policy: 0.5529), Train Mean Max: 0.7860
Epoch 8/10, Train Loss: 0.6239 (value: 0.0014, weighted value: 0.0717, policy: 0.5523, weighted policy: 0.5523), Train Mean Max: 0.7864
Epoch 9/10, Train Loss: 0.6195 (value: 0.0014, weighted value: 0.0678, policy: 0.5517, weighted policy: 0.5517), Train Mean Max: 0.7871
Epoch 10/10, Train Loss: 0.6195 (value: 0.0014, weighted value: 0.0685, policy: 0.5510, weighted policy: 0.5510), Train Mean Max: 0.7872
..training done in 64.08 seconds
..evaluation done in 16.56 seconds
Old network+MCTS average reward: 0.5456, min: -0.1296, max: 1.2315, stdev: 0.2370
New network+MCTS average reward: 0.5413, min: -0.0093, max: 1.2778, stdev: 0.2325
Old bare network average reward: 0.4123, min: -0.2500, max: 1.1667, stdev: 0.2470
New bare network average reward: 0.4132, min: -0.1852, max: 1.1944, stdev: 0.2419
External policy "random" average reward: 0.2421, min: -0.3796, max: 1.0000, stdev: 0.2236
External policy "individual greedy" average reward: 0.5061, min: -0.1667, max: 1.1944, stdev: 0.2307
External policy "total greedy" average reward: 0.6284, min: -0.0278, max: 1.4352, stdev: 0.2350
New network won 114 and tied 74 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 37 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 54.11 seconds
Training examples lengths: [64896, 64550, 65106, 64500, 65449, 65046, 64871, 64915, 64727, 64758]
Total value: 364987.02
Training on 648818 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.6393 (value: 0.0020, weighted value: 0.1018, policy: 0.5375, weighted policy: 0.5375), Train Mean Max: 0.7891
Epoch 2/10, Train Loss: 0.6256 (value: 0.0018, weighted value: 0.0895, policy: 0.5361, weighted policy: 0.5361), Train Mean Max: 0.7903
Epoch 3/10, Train Loss: 0.6190 (value: 0.0017, weighted value: 0.0840, policy: 0.5349, weighted policy: 0.5349), Train Mean Max: 0.7912
Epoch 4/10, Train Loss: 0.6123 (value: 0.0016, weighted value: 0.0783, policy: 0.5340, weighted policy: 0.5340), Train Mean Max: 0.7919
Epoch 5/10, Train Loss: 0.6103 (value: 0.0015, weighted value: 0.0769, policy: 0.5333, weighted policy: 0.5333), Train Mean Max: 0.7925
Epoch 6/10, Train Loss: 0.6080 (value: 0.0015, weighted value: 0.0748, policy: 0.5332, weighted policy: 0.5332), Train Mean Max: 0.7929
Epoch 7/10, Train Loss: 0.6057 (value: 0.0015, weighted value: 0.0736, policy: 0.5321, weighted policy: 0.5321), Train Mean Max: 0.7934
Epoch 8/10, Train Loss: 0.6014 (value: 0.0014, weighted value: 0.0693, policy: 0.5320, weighted policy: 0.5320), Train Mean Max: 0.7937
Epoch 9/10, Train Loss: 0.5967 (value: 0.0013, weighted value: 0.0659, policy: 0.5308, weighted policy: 0.5308), Train Mean Max: 0.7943
Epoch 10/10, Train Loss: 0.5973 (value: 0.0013, weighted value: 0.0666, policy: 0.5307, weighted policy: 0.5307), Train Mean Max: 0.7944
..training done in 69.73 seconds
..evaluation done in 18.08 seconds
Old network+MCTS average reward: 0.5789, min: -0.0926, max: 1.1759, stdev: 0.2210
New network+MCTS average reward: 0.5780, min: -0.1389, max: 1.1481, stdev: 0.2160
Old bare network average reward: 0.4484, min: -0.1944, max: 1.1481, stdev: 0.2251
New bare network average reward: 0.4455, min: -0.2130, max: 1.1574, stdev: 0.2387
External policy "random" average reward: 0.2703, min: -0.3333, max: 1.1759, stdev: 0.2155
External policy "individual greedy" average reward: 0.5427, min: -0.0370, max: 1.5463, stdev: 0.2386
External policy "total greedy" average reward: 0.6664, min: 0.0278, max: 1.6389, stdev: 0.2324
New network won 102 and tied 90 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 38 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 53.81 seconds
Training examples lengths: [64550, 65106, 64500, 65449, 65046, 64871, 64915, 64727, 64758, 64765]
Total value: 365691.74
Training on 648687 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.6636 (value: 0.0027, weighted value: 0.1349, policy: 0.5287, weighted policy: 0.5287), Train Mean Max: 0.7897
Epoch 2/10, Train Loss: 0.6393 (value: 0.0023, weighted value: 0.1127, policy: 0.5267, weighted policy: 0.5267), Train Mean Max: 0.7918
Epoch 3/10, Train Loss: 0.6274 (value: 0.0021, weighted value: 0.1029, policy: 0.5245, weighted policy: 0.5245), Train Mean Max: 0.7931
Epoch 4/10, Train Loss: 0.6209 (value: 0.0020, weighted value: 0.0975, policy: 0.5234, weighted policy: 0.5234), Train Mean Max: 0.7943
Epoch 5/10, Train Loss: 0.6128 (value: 0.0018, weighted value: 0.0909, policy: 0.5219, weighted policy: 0.5219), Train Mean Max: 0.7949
Epoch 6/10, Train Loss: 0.6063 (value: 0.0017, weighted value: 0.0853, policy: 0.5210, weighted policy: 0.5210), Train Mean Max: 0.7961
Epoch 7/10, Train Loss: 0.6039 (value: 0.0017, weighted value: 0.0837, policy: 0.5202, weighted policy: 0.5202), Train Mean Max: 0.7964
Epoch 8/10, Train Loss: 0.5987 (value: 0.0016, weighted value: 0.0789, policy: 0.5199, weighted policy: 0.5199), Train Mean Max: 0.7972
Epoch 9/10, Train Loss: 0.5972 (value: 0.0016, weighted value: 0.0788, policy: 0.5184, weighted policy: 0.5184), Train Mean Max: 0.7975
Epoch 10/10, Train Loss: 0.5916 (value: 0.0015, weighted value: 0.0732, policy: 0.5184, weighted policy: 0.5184), Train Mean Max: 0.7982
..training done in 63.74 seconds
..evaluation done in 16.26 seconds
Old network+MCTS average reward: 0.5780, min: -0.0833, max: 1.2222, stdev: 0.2179
New network+MCTS average reward: 0.5811, min: -0.0093, max: 1.2037, stdev: 0.2199
Old bare network average reward: 0.4455, min: -0.1667, max: 1.2222, stdev: 0.2227
New bare network average reward: 0.4524, min: -0.2778, max: 1.1574, stdev: 0.2230
External policy "random" average reward: 0.2509, min: -0.3426, max: 0.8333, stdev: 0.2135
External policy "individual greedy" average reward: 0.5457, min: 0.0000, max: 1.1574, stdev: 0.2225
External policy "total greedy" average reward: 0.6553, min: 0.1296, max: 1.3333, stdev: 0.2139
New network won 109 and tied 79 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 39 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 55.66 seconds
Training examples lengths: [65106, 64500, 65449, 65046, 64871, 64915, 64727, 64758, 64765, 64950]
Total value: 367188.32
Training on 649087 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.6826 (value: 0.0033, weighted value: 0.1644, policy: 0.5182, weighted policy: 0.5182), Train Mean Max: 0.7907
Epoch 2/10, Train Loss: 0.6508 (value: 0.0027, weighted value: 0.1357, policy: 0.5151, weighted policy: 0.5151), Train Mean Max: 0.7934
Epoch 3/10, Train Loss: 0.6317 (value: 0.0024, weighted value: 0.1186, policy: 0.5131, weighted policy: 0.5131), Train Mean Max: 0.7956
Epoch 4/10, Train Loss: 0.6246 (value: 0.0023, weighted value: 0.1132, policy: 0.5114, weighted policy: 0.5114), Train Mean Max: 0.7967
Epoch 5/10, Train Loss: 0.6134 (value: 0.0021, weighted value: 0.1034, policy: 0.5100, weighted policy: 0.5100), Train Mean Max: 0.7979
Epoch 6/10, Train Loss: 0.6055 (value: 0.0019, weighted value: 0.0968, policy: 0.5087, weighted policy: 0.5087), Train Mean Max: 0.7991
Epoch 7/10, Train Loss: 0.6004 (value: 0.0019, weighted value: 0.0932, policy: 0.5072, weighted policy: 0.5072), Train Mean Max: 0.8000
Epoch 8/10, Train Loss: 0.5949 (value: 0.0018, weighted value: 0.0880, policy: 0.5069, weighted policy: 0.5069), Train Mean Max: 0.8008
Epoch 9/10, Train Loss: 0.5910 (value: 0.0017, weighted value: 0.0852, policy: 0.5058, weighted policy: 0.5058), Train Mean Max: 0.8014
Epoch 10/10, Train Loss: 0.5868 (value: 0.0016, weighted value: 0.0822, policy: 0.5046, weighted policy: 0.5046), Train Mean Max: 0.8020
..training done in 64.20 seconds
..evaluation done in 17.03 seconds
Old network+MCTS average reward: 0.5678, min: 0.0833, max: 1.2963, stdev: 0.2210
New network+MCTS average reward: 0.5754, min: -0.1111, max: 1.3241, stdev: 0.2274
Old bare network average reward: 0.4494, min: -0.2222, max: 1.1759, stdev: 0.2349
New bare network average reward: 0.4490, min: -0.2222, max: 1.1389, stdev: 0.2322
External policy "random" average reward: 0.2663, min: -0.3704, max: 1.2130, stdev: 0.2385
External policy "individual greedy" average reward: 0.5410, min: 0.0463, max: 1.4352, stdev: 0.2330
External policy "total greedy" average reward: 0.6458, min: 0.1296, max: 1.5278, stdev: 0.2268
New network won 119 and tied 82 out of 300 games (53.33% wins where ties are half wins)
Keeping the new network

Training iteration 40 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 54.59 seconds
Training examples lengths: [64500, 65449, 65046, 64871, 64915, 64727, 64758, 64765, 64950, 65062]
Total value: 369099.54
Training on 649043 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.6025 (value: 0.0022, weighted value: 0.1112, policy: 0.4912, weighted policy: 0.4912), Train Mean Max: 0.8039
Epoch 2/10, Train Loss: 0.5901 (value: 0.0020, weighted value: 0.0998, policy: 0.4903, weighted policy: 0.4903), Train Mean Max: 0.8054
Epoch 3/10, Train Loss: 0.5812 (value: 0.0019, weighted value: 0.0931, policy: 0.4881, weighted policy: 0.4881), Train Mean Max: 0.8066
Epoch 4/10, Train Loss: 0.5749 (value: 0.0018, weighted value: 0.0880, policy: 0.4869, weighted policy: 0.4869), Train Mean Max: 0.8073
Epoch 5/10, Train Loss: 0.5739 (value: 0.0017, weighted value: 0.0867, policy: 0.4872, weighted policy: 0.4872), Train Mean Max: 0.8081
Epoch 6/10, Train Loss: 0.5656 (value: 0.0016, weighted value: 0.0807, policy: 0.4850, weighted policy: 0.4850), Train Mean Max: 0.8087
Epoch 7/10, Train Loss: 0.5625 (value: 0.0016, weighted value: 0.0779, policy: 0.4846, weighted policy: 0.4846), Train Mean Max: 0.8093
Epoch 8/10, Train Loss: 0.5623 (value: 0.0016, weighted value: 0.0781, policy: 0.4842, weighted policy: 0.4842), Train Mean Max: 0.8098
Epoch 9/10, Train Loss: 0.5563 (value: 0.0015, weighted value: 0.0731, policy: 0.4832, weighted policy: 0.4832), Train Mean Max: 0.8102
Epoch 10/10, Train Loss: 0.5531 (value: 0.0014, weighted value: 0.0702, policy: 0.4829, weighted policy: 0.4829), Train Mean Max: 0.8107
..training done in 62.30 seconds
..evaluation done in 16.40 seconds
Old network+MCTS average reward: 0.5837, min: 0.0648, max: 1.2500, stdev: 0.2111
New network+MCTS average reward: 0.5845, min: 0.0556, max: 1.1574, stdev: 0.2095
Old bare network average reward: 0.4584, min: -0.1019, max: 1.0185, stdev: 0.2161
New bare network average reward: 0.4678, min: -0.1111, max: 1.0556, stdev: 0.2117
External policy "random" average reward: 0.2584, min: -0.3426, max: 0.7685, stdev: 0.2089
External policy "individual greedy" average reward: 0.5358, min: -0.0278, max: 1.1481, stdev: 0.2261
External policy "total greedy" average reward: 0.6546, min: 0.0648, max: 1.2037, stdev: 0.2241
New network won 118 and tied 77 out of 300 games (52.17% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_40

Training iteration 41 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 61.77 seconds
Training examples lengths: [65449, 65046, 64871, 64915, 64727, 64758, 64765, 64950, 65062, 64685]
Total value: 370534.29
Training on 649228 examples
Training with 318 batches of size 2048
Epoch 1/10, Train Loss: 0.5781 (value: 0.0021, weighted value: 0.1028, policy: 0.4754, weighted policy: 0.4754), Train Mean Max: 0.8119
Epoch 2/10, Train Loss: 0.5925 (value: 0.0021, weighted value: 0.1057, policy: 0.4868, weighted policy: 0.4868), Train Mean Max: 0.8114
Epoch 3/10, Train Loss: 0.5650 (value: 0.0018, weighted value: 0.0902, policy: 0.4748, weighted policy: 0.4748), Train Mean Max: 0.8131
Epoch 4/10, Train Loss: 0.5649 (value: 0.0017, weighted value: 0.0870, policy: 0.4779, weighted policy: 0.4779), Train Mean Max: 0.8133
Epoch 5/10, Train Loss: 0.5583 (value: 0.0016, weighted value: 0.0822, policy: 0.4762, weighted policy: 0.4762), Train Mean Max: 0.8140
Epoch 6/10, Train Loss: 0.5567 (value: 0.0016, weighted value: 0.0780, policy: 0.4787, weighted policy: 0.4787), Train Mean Max: 0.8146
Epoch 7/10, Train Loss: 0.5512 (value: 0.0016, weighted value: 0.0807, policy: 0.4705, weighted policy: 0.4705), Train Mean Max: 0.8148
Epoch 8/10, Train Loss: 0.5429 (value: 0.0015, weighted value: 0.0738, policy: 0.4691, weighted policy: 0.4691), Train Mean Max: 0.8156
Epoch 9/10, Train Loss: 0.5467 (value: 0.0014, weighted value: 0.0723, policy: 0.4744, weighted policy: 0.4744), Train Mean Max: 0.8156
Epoch 10/10, Train Loss: 0.5474 (value: 0.0015, weighted value: 0.0731, policy: 0.4742, weighted policy: 0.4742), Train Mean Max: 0.8155
..training done in 75.74 seconds
..evaluation done in 19.17 seconds
Old network+MCTS average reward: 0.5649, min: -0.0648, max: 1.3889, stdev: 0.2241
New network+MCTS average reward: 0.5640, min: -0.0556, max: 1.3889, stdev: 0.2196
Old bare network average reward: 0.4462, min: -0.1944, max: 1.2037, stdev: 0.2339
New bare network average reward: 0.4531, min: -0.2037, max: 1.3148, stdev: 0.2369
External policy "random" average reward: 0.2705, min: -0.2593, max: 0.8704, stdev: 0.2164
External policy "individual greedy" average reward: 0.5427, min: -0.0648, max: 1.3611, stdev: 0.2137
External policy "total greedy" average reward: 0.6538, min: 0.0926, max: 1.4722, stdev: 0.2109
New network won 128 and tied 45 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 42 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 54.26 seconds
Training examples lengths: [65046, 64871, 64915, 64727, 64758, 64765, 64950, 65062, 64685, 64920]
Total value: 371354.45
Training on 648699 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.5736 (value: 0.0021, weighted value: 0.1046, policy: 0.4690, weighted policy: 0.4690), Train Mean Max: 0.8161
Epoch 2/10, Train Loss: 0.5522 (value: 0.0018, weighted value: 0.0883, policy: 0.4639, weighted policy: 0.4639), Train Mean Max: 0.8168
Epoch 3/10, Train Loss: 0.5486 (value: 0.0017, weighted value: 0.0860, policy: 0.4626, weighted policy: 0.4626), Train Mean Max: 0.8173
Epoch 4/10, Train Loss: 0.5406 (value: 0.0016, weighted value: 0.0791, policy: 0.4615, weighted policy: 0.4615), Train Mean Max: 0.8178
Epoch 5/10, Train Loss: 0.5388 (value: 0.0016, weighted value: 0.0778, policy: 0.4610, weighted policy: 0.4610), Train Mean Max: 0.8182
Epoch 6/10, Train Loss: 0.5349 (value: 0.0015, weighted value: 0.0744, policy: 0.4604, weighted policy: 0.4604), Train Mean Max: 0.8187
Epoch 7/10, Train Loss: 0.5302 (value: 0.0014, weighted value: 0.0705, policy: 0.4598, weighted policy: 0.4598), Train Mean Max: 0.8192
Epoch 8/10, Train Loss: 0.5297 (value: 0.0014, weighted value: 0.0702, policy: 0.4596, weighted policy: 0.4596), Train Mean Max: 0.8194
Epoch 9/10, Train Loss: 0.5281 (value: 0.0014, weighted value: 0.0694, policy: 0.4587, weighted policy: 0.4587), Train Mean Max: 0.8198
Epoch 10/10, Train Loss: 0.5243 (value: 0.0013, weighted value: 0.0658, policy: 0.4585, weighted policy: 0.4585), Train Mean Max: 0.8201
..training done in 61.34 seconds
..evaluation done in 17.73 seconds
Old network+MCTS average reward: 0.5848, min: 0.0000, max: 1.2870, stdev: 0.2154
New network+MCTS average reward: 0.5964, min: 0.0741, max: 1.2870, stdev: 0.2128
Old bare network average reward: 0.4672, min: -0.1389, max: 1.1667, stdev: 0.2263
New bare network average reward: 0.4647, min: -0.0741, max: 1.0833, stdev: 0.2241
External policy "random" average reward: 0.2590, min: -0.2778, max: 0.8981, stdev: 0.2239
External policy "individual greedy" average reward: 0.5483, min: -0.0093, max: 1.1574, stdev: 0.2120
External policy "total greedy" average reward: 0.6627, min: 0.0833, max: 1.2500, stdev: 0.2074
New network won 134 and tied 61 out of 300 games (54.83% wins where ties are half wins)
Keeping the new network

Training iteration 43 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 55.90 seconds
Training examples lengths: [64871, 64915, 64727, 64758, 64765, 64950, 65062, 64685, 64920, 64696]
Total value: 372843.41
Training on 648349 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.5491 (value: 0.0020, weighted value: 0.0988, policy: 0.4503, weighted policy: 0.4503), Train Mean Max: 0.8208
Epoch 2/10, Train Loss: 0.5339 (value: 0.0017, weighted value: 0.0860, policy: 0.4479, weighted policy: 0.4479), Train Mean Max: 0.8219
Epoch 3/10, Train Loss: 0.5287 (value: 0.0016, weighted value: 0.0814, policy: 0.4473, weighted policy: 0.4473), Train Mean Max: 0.8225
Epoch 4/10, Train Loss: 0.5233 (value: 0.0015, weighted value: 0.0774, policy: 0.4459, weighted policy: 0.4459), Train Mean Max: 0.8231
Epoch 5/10, Train Loss: 0.5210 (value: 0.0015, weighted value: 0.0757, policy: 0.4453, weighted policy: 0.4453), Train Mean Max: 0.8234
Epoch 6/10, Train Loss: 0.5155 (value: 0.0014, weighted value: 0.0711, policy: 0.4444, weighted policy: 0.4444), Train Mean Max: 0.8242
Epoch 7/10, Train Loss: 0.5143 (value: 0.0014, weighted value: 0.0704, policy: 0.4440, weighted policy: 0.4440), Train Mean Max: 0.8245
Epoch 8/10, Train Loss: 0.5116 (value: 0.0014, weighted value: 0.0684, policy: 0.4432, weighted policy: 0.4432), Train Mean Max: 0.8247
Epoch 9/10, Train Loss: 0.5096 (value: 0.0013, weighted value: 0.0667, policy: 0.4429, weighted policy: 0.4429), Train Mean Max: 0.8252
Epoch 10/10, Train Loss: 0.5073 (value: 0.0013, weighted value: 0.0652, policy: 0.4421, weighted policy: 0.4421), Train Mean Max: 0.8256
..training done in 61.72 seconds
..evaluation done in 17.24 seconds
Old network+MCTS average reward: 0.5869, min: -0.1204, max: 1.3333, stdev: 0.2272
New network+MCTS average reward: 0.5951, min: -0.0463, max: 1.2963, stdev: 0.2235
Old bare network average reward: 0.4612, min: -0.0741, max: 1.2778, stdev: 0.2348
New bare network average reward: 0.4645, min: -0.3796, max: 1.3056, stdev: 0.2290
External policy "random" average reward: 0.2584, min: -0.4444, max: 1.0278, stdev: 0.2306
External policy "individual greedy" average reward: 0.5482, min: -0.0278, max: 1.4352, stdev: 0.2230
External policy "total greedy" average reward: 0.6691, min: 0.0463, max: 1.4815, stdev: 0.2296
New network won 123 and tied 80 out of 300 games (54.33% wins where ties are half wins)
Keeping the new network

Training iteration 44 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 54.50 seconds
Training examples lengths: [64915, 64727, 64758, 64765, 64950, 65062, 64685, 64920, 64696, 64824]
Total value: 373980.22
Training on 648302 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.5294 (value: 0.0019, weighted value: 0.0935, policy: 0.4359, weighted policy: 0.4359), Train Mean Max: 0.8263
Epoch 2/10, Train Loss: 0.5188 (value: 0.0017, weighted value: 0.0848, policy: 0.4340, weighted policy: 0.4340), Train Mean Max: 0.8270
Epoch 3/10, Train Loss: 0.5138 (value: 0.0016, weighted value: 0.0810, policy: 0.4328, weighted policy: 0.4328), Train Mean Max: 0.8275
Epoch 4/10, Train Loss: 0.5068 (value: 0.0015, weighted value: 0.0749, policy: 0.4319, weighted policy: 0.4319), Train Mean Max: 0.8281
Epoch 5/10, Train Loss: 0.5031 (value: 0.0015, weighted value: 0.0730, policy: 0.4300, weighted policy: 0.4300), Train Mean Max: 0.8288
Epoch 6/10, Train Loss: 0.5005 (value: 0.0014, weighted value: 0.0706, policy: 0.4299, weighted policy: 0.4299), Train Mean Max: 0.8291
Epoch 7/10, Train Loss: 0.4977 (value: 0.0014, weighted value: 0.0678, policy: 0.4299, weighted policy: 0.4299), Train Mean Max: 0.8296
Epoch 8/10, Train Loss: 0.4959 (value: 0.0013, weighted value: 0.0670, policy: 0.4289, weighted policy: 0.4289), Train Mean Max: 0.8298
Epoch 9/10, Train Loss: 0.4916 (value: 0.0013, weighted value: 0.0638, policy: 0.4278, weighted policy: 0.4278), Train Mean Max: 0.8304
Epoch 10/10, Train Loss: 0.4922 (value: 0.0013, weighted value: 0.0637, policy: 0.4286, weighted policy: 0.4286), Train Mean Max: 0.8306
..training done in 59.91 seconds
..evaluation done in 16.40 seconds
Old network+MCTS average reward: 0.5751, min: -0.0648, max: 1.2315, stdev: 0.2135
New network+MCTS average reward: 0.5745, min: -0.1574, max: 1.2315, stdev: 0.2174
Old bare network average reward: 0.4531, min: -0.2037, max: 1.2037, stdev: 0.2237
New bare network average reward: 0.4558, min: -0.1759, max: 1.1574, stdev: 0.2294
External policy "random" average reward: 0.2601, min: -0.5093, max: 0.9722, stdev: 0.2253
External policy "individual greedy" average reward: 0.5296, min: -0.1019, max: 1.2037, stdev: 0.2287
External policy "total greedy" average reward: 0.6408, min: -0.0648, max: 1.3519, stdev: 0.2182
New network won 104 and tied 98 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 45 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 54.79 seconds
Training examples lengths: [64727, 64758, 64765, 64950, 65062, 64685, 64920, 64696, 64824, 64785]
Total value: 375029.99
Training on 648172 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.5189 (value: 0.0019, weighted value: 0.0967, policy: 0.4223, weighted policy: 0.4223), Train Mean Max: 0.8311
Epoch 2/10, Train Loss: 0.5057 (value: 0.0017, weighted value: 0.0848, policy: 0.4209, weighted policy: 0.4209), Train Mean Max: 0.8320
Epoch 3/10, Train Loss: 0.4987 (value: 0.0016, weighted value: 0.0793, policy: 0.4194, weighted policy: 0.4194), Train Mean Max: 0.8327
Epoch 4/10, Train Loss: 0.4947 (value: 0.0015, weighted value: 0.0767, policy: 0.4180, weighted policy: 0.4180), Train Mean Max: 0.8331
Epoch 5/10, Train Loss: 0.4908 (value: 0.0015, weighted value: 0.0735, policy: 0.4174, weighted policy: 0.4174), Train Mean Max: 0.8336
Epoch 6/10, Train Loss: 0.4880 (value: 0.0014, weighted value: 0.0713, policy: 0.4168, weighted policy: 0.4168), Train Mean Max: 0.8339
Epoch 7/10, Train Loss: 0.4850 (value: 0.0014, weighted value: 0.0678, policy: 0.4172, weighted policy: 0.4172), Train Mean Max: 0.8344
Epoch 8/10, Train Loss: 0.4801 (value: 0.0013, weighted value: 0.0650, policy: 0.4150, weighted policy: 0.4150), Train Mean Max: 0.8347
Epoch 9/10, Train Loss: 0.4795 (value: 0.0013, weighted value: 0.0639, policy: 0.4157, weighted policy: 0.4157), Train Mean Max: 0.8352
Epoch 10/10, Train Loss: 0.4791 (value: 0.0013, weighted value: 0.0646, policy: 0.4146, weighted policy: 0.4146), Train Mean Max: 0.8353
..training done in 60.12 seconds
..evaluation done in 17.19 seconds
Old network+MCTS average reward: 0.5636, min: 0.0093, max: 1.5648, stdev: 0.2260
New network+MCTS average reward: 0.5665, min: 0.0556, max: 1.5185, stdev: 0.2194
Old bare network average reward: 0.4445, min: -0.1667, max: 1.4722, stdev: 0.2257
New bare network average reward: 0.4400, min: -0.1759, max: 1.4722, stdev: 0.2291
External policy "random" average reward: 0.2447, min: -0.2870, max: 0.9907, stdev: 0.2245
External policy "individual greedy" average reward: 0.5251, min: -0.0185, max: 1.4074, stdev: 0.2334
External policy "total greedy" average reward: 0.6304, min: 0.1019, max: 1.5370, stdev: 0.2237
New network won 111 and tied 83 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 46 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 54.97 seconds
Training examples lengths: [64758, 64765, 64950, 65062, 64685, 64920, 64696, 64824, 64785, 64695]
Total value: 376561.78
Training on 648140 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.5006 (value: 0.0018, weighted value: 0.0918, policy: 0.4088, weighted policy: 0.4088), Train Mean Max: 0.8360
Epoch 2/10, Train Loss: 0.4911 (value: 0.0017, weighted value: 0.0839, policy: 0.4072, weighted policy: 0.4072), Train Mean Max: 0.8368
Epoch 3/10, Train Loss: 0.4839 (value: 0.0015, weighted value: 0.0775, policy: 0.4064, weighted policy: 0.4064), Train Mean Max: 0.8373
Epoch 4/10, Train Loss: 0.4781 (value: 0.0015, weighted value: 0.0728, policy: 0.4053, weighted policy: 0.4053), Train Mean Max: 0.8379
Epoch 5/10, Train Loss: 0.4751 (value: 0.0014, weighted value: 0.0705, policy: 0.4046, weighted policy: 0.4046), Train Mean Max: 0.8383
Epoch 6/10, Train Loss: 0.4747 (value: 0.0014, weighted value: 0.0702, policy: 0.4045, weighted policy: 0.4045), Train Mean Max: 0.8387
Epoch 7/10, Train Loss: 0.4677 (value: 0.0013, weighted value: 0.0644, policy: 0.4033, weighted policy: 0.4033), Train Mean Max: 0.8391
Epoch 8/10, Train Loss: 0.4676 (value: 0.0013, weighted value: 0.0648, policy: 0.4027, weighted policy: 0.4027), Train Mean Max: 0.8394
Epoch 9/10, Train Loss: 0.4660 (value: 0.0013, weighted value: 0.0636, policy: 0.4024, weighted policy: 0.4024), Train Mean Max: 0.8398
Epoch 10/10, Train Loss: 0.4644 (value: 0.0012, weighted value: 0.0618, policy: 0.4025, weighted policy: 0.4025), Train Mean Max: 0.8401
..training done in 69.09 seconds
..evaluation done in 17.87 seconds
Old network+MCTS average reward: 0.5904, min: 0.0093, max: 1.6111, stdev: 0.2195
New network+MCTS average reward: 0.5928, min: 0.1019, max: 1.5741, stdev: 0.2216
Old bare network average reward: 0.4829, min: -0.2037, max: 1.2407, stdev: 0.2281
New bare network average reward: 0.4845, min: -0.1852, max: 1.2407, stdev: 0.2338
External policy "random" average reward: 0.2732, min: -0.4074, max: 0.9722, stdev: 0.2069
External policy "individual greedy" average reward: 0.5512, min: -0.1852, max: 1.3981, stdev: 0.2273
External policy "total greedy" average reward: 0.6710, min: 0.0741, max: 1.6204, stdev: 0.2217
New network won 96 and tied 103 out of 300 games (49.17% wins where ties are half wins)
Reverting to the old network

Training iteration 47 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 55.51 seconds
Training examples lengths: [64765, 64950, 65062, 64685, 64920, 64696, 64824, 64785, 64695, 64726]
Total value: 378400.41
Training on 648108 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.5266 (value: 0.0024, weighted value: 0.1214, policy: 0.4052, weighted policy: 0.4052), Train Mean Max: 0.8366
Epoch 2/10, Train Loss: 0.5043 (value: 0.0020, weighted value: 0.1020, policy: 0.4023, weighted policy: 0.4023), Train Mean Max: 0.8377
Epoch 3/10, Train Loss: 0.4954 (value: 0.0019, weighted value: 0.0944, policy: 0.4010, weighted policy: 0.4010), Train Mean Max: 0.8385
Epoch 4/10, Train Loss: 0.4884 (value: 0.0018, weighted value: 0.0888, policy: 0.3996, weighted policy: 0.3996), Train Mean Max: 0.8393
Epoch 5/10, Train Loss: 0.4812 (value: 0.0017, weighted value: 0.0832, policy: 0.3981, weighted policy: 0.3981), Train Mean Max: 0.8399
Epoch 6/10, Train Loss: 0.4765 (value: 0.0016, weighted value: 0.0792, policy: 0.3974, weighted policy: 0.3974), Train Mean Max: 0.8406
Epoch 7/10, Train Loss: 0.4717 (value: 0.0015, weighted value: 0.0762, policy: 0.3955, weighted policy: 0.3955), Train Mean Max: 0.8410
Epoch 8/10, Train Loss: 0.4688 (value: 0.0015, weighted value: 0.0737, policy: 0.3951, weighted policy: 0.3951), Train Mean Max: 0.8413
Epoch 9/10, Train Loss: 0.4662 (value: 0.0014, weighted value: 0.0715, policy: 0.3948, weighted policy: 0.3948), Train Mean Max: 0.8419
Epoch 10/10, Train Loss: 0.4640 (value: 0.0014, weighted value: 0.0698, policy: 0.3942, weighted policy: 0.3942), Train Mean Max: 0.8422
..training done in 63.38 seconds
..evaluation done in 17.84 seconds
Old network+MCTS average reward: 0.5655, min: -0.1852, max: 1.1204, stdev: 0.2201
New network+MCTS average reward: 0.5730, min: -0.1111, max: 1.1204, stdev: 0.2178
Old bare network average reward: 0.4488, min: -0.1852, max: 1.1204, stdev: 0.2312
New bare network average reward: 0.4606, min: -0.1944, max: 1.0556, stdev: 0.2280
External policy "random" average reward: 0.2507, min: -0.2778, max: 0.7963, stdev: 0.2166
External policy "individual greedy" average reward: 0.5188, min: -0.2500, max: 1.0926, stdev: 0.2193
External policy "total greedy" average reward: 0.6377, min: 0.0093, max: 1.3241, stdev: 0.2216
New network won 112 and tied 83 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 48 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 55.53 seconds
Training examples lengths: [64950, 65062, 64685, 64920, 64696, 64824, 64785, 64695, 64726, 64801]
Total value: 380056.25
Training on 648144 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.4857 (value: 0.0019, weighted value: 0.0969, policy: 0.3889, weighted policy: 0.3889), Train Mean Max: 0.8432
Epoch 2/10, Train Loss: 0.4738 (value: 0.0017, weighted value: 0.0871, policy: 0.3867, weighted policy: 0.3867), Train Mean Max: 0.8440
Epoch 3/10, Train Loss: 0.4671 (value: 0.0016, weighted value: 0.0815, policy: 0.3856, weighted policy: 0.3856), Train Mean Max: 0.8445
Epoch 4/10, Train Loss: 0.4636 (value: 0.0016, weighted value: 0.0793, policy: 0.3843, weighted policy: 0.3843), Train Mean Max: 0.8451
Epoch 5/10, Train Loss: 0.4575 (value: 0.0015, weighted value: 0.0738, policy: 0.3837, weighted policy: 0.3837), Train Mean Max: 0.8456
Epoch 6/10, Train Loss: 0.4547 (value: 0.0014, weighted value: 0.0721, policy: 0.3826, weighted policy: 0.3826), Train Mean Max: 0.8462
Epoch 7/10, Train Loss: 0.4526 (value: 0.0014, weighted value: 0.0705, policy: 0.3821, weighted policy: 0.3821), Train Mean Max: 0.8464
Epoch 8/10, Train Loss: 0.4486 (value: 0.0013, weighted value: 0.0667, policy: 0.3819, weighted policy: 0.3819), Train Mean Max: 0.8469
Epoch 9/10, Train Loss: 0.4476 (value: 0.0013, weighted value: 0.0664, policy: 0.3812, weighted policy: 0.3812), Train Mean Max: 0.8473
Epoch 10/10, Train Loss: 0.4451 (value: 0.0013, weighted value: 0.0648, policy: 0.3803, weighted policy: 0.3803), Train Mean Max: 0.8476
..training done in 60.32 seconds
..evaluation done in 17.65 seconds
Old network+MCTS average reward: 0.5981, min: 0.0556, max: 1.7222, stdev: 0.2132
New network+MCTS average reward: 0.5917, min: 0.0833, max: 1.4907, stdev: 0.2085
Old bare network average reward: 0.4871, min: -0.0556, max: 1.3241, stdev: 0.2100
New bare network average reward: 0.4873, min: -0.2130, max: 1.6852, stdev: 0.2176
External policy "random" average reward: 0.2670, min: -0.4537, max: 1.2407, stdev: 0.2142
External policy "individual greedy" average reward: 0.5627, min: -0.0185, max: 1.5185, stdev: 0.2211
External policy "total greedy" average reward: 0.6679, min: 0.1296, max: 1.5926, stdev: 0.2179
New network won 81 and tied 111 out of 300 games (45.50% wins where ties are half wins)
Reverting to the old network

Training iteration 49 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.39 seconds
Training examples lengths: [65062, 64685, 64920, 64696, 64824, 64785, 64695, 64726, 64801, 64668]
Total value: 380635.02
Training on 647862 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.5136 (value: 0.0026, weighted value: 0.1297, policy: 0.3839, weighted policy: 0.3839), Train Mean Max: 0.8433
Epoch 2/10, Train Loss: 0.4907 (value: 0.0022, weighted value: 0.1094, policy: 0.3813, weighted policy: 0.3813), Train Mean Max: 0.8445
Epoch 3/10, Train Loss: 0.4786 (value: 0.0020, weighted value: 0.0995, policy: 0.3791, weighted policy: 0.3791), Train Mean Max: 0.8455
Epoch 4/10, Train Loss: 0.4709 (value: 0.0019, weighted value: 0.0930, policy: 0.3779, weighted policy: 0.3779), Train Mean Max: 0.8463
Epoch 5/10, Train Loss: 0.4653 (value: 0.0018, weighted value: 0.0885, policy: 0.3769, weighted policy: 0.3769), Train Mean Max: 0.8468
Epoch 6/10, Train Loss: 0.4599 (value: 0.0017, weighted value: 0.0850, policy: 0.3749, weighted policy: 0.3749), Train Mean Max: 0.8475
Epoch 7/10, Train Loss: 0.4549 (value: 0.0016, weighted value: 0.0803, policy: 0.3746, weighted policy: 0.3746), Train Mean Max: 0.8482
Epoch 8/10, Train Loss: 0.4507 (value: 0.0015, weighted value: 0.0770, policy: 0.3738, weighted policy: 0.3738), Train Mean Max: 0.8486
Epoch 9/10, Train Loss: 0.4493 (value: 0.0015, weighted value: 0.0753, policy: 0.3740, weighted policy: 0.3740), Train Mean Max: 0.8490
Epoch 10/10, Train Loss: 0.4449 (value: 0.0014, weighted value: 0.0724, policy: 0.3724, weighted policy: 0.3724), Train Mean Max: 0.8497
..training done in 59.11 seconds
..evaluation done in 17.77 seconds
Old network+MCTS average reward: 0.5877, min: 0.0556, max: 1.1759, stdev: 0.2222
New network+MCTS average reward: 0.5935, min: -0.0648, max: 1.2037, stdev: 0.2204
Old bare network average reward: 0.4819, min: -0.1296, max: 1.0556, stdev: 0.2235
New bare network average reward: 0.4880, min: -0.0370, max: 1.0556, stdev: 0.2314
External policy "random" average reward: 0.2630, min: -0.2593, max: 0.8981, stdev: 0.2238
External policy "individual greedy" average reward: 0.5528, min: 0.0370, max: 1.2685, stdev: 0.2200
External policy "total greedy" average reward: 0.6640, min: 0.1481, max: 1.4352, stdev: 0.2143
New network won 110 and tied 87 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 50 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 55.67 seconds
Training examples lengths: [64685, 64920, 64696, 64824, 64785, 64695, 64726, 64801, 64668, 64761]
Total value: 381582.27
Training on 647561 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.4698 (value: 0.0020, weighted value: 0.0997, policy: 0.3700, weighted policy: 0.3700), Train Mean Max: 0.8503
Epoch 2/10, Train Loss: 0.4567 (value: 0.0018, weighted value: 0.0890, policy: 0.3677, weighted policy: 0.3677), Train Mean Max: 0.8510
Epoch 3/10, Train Loss: 0.4517 (value: 0.0017, weighted value: 0.0851, policy: 0.3666, weighted policy: 0.3666), Train Mean Max: 0.8513
Epoch 4/10, Train Loss: 0.4450 (value: 0.0016, weighted value: 0.0792, policy: 0.3658, weighted policy: 0.3658), Train Mean Max: 0.8520
Epoch 5/10, Train Loss: 0.4430 (value: 0.0016, weighted value: 0.0786, policy: 0.3644, weighted policy: 0.3644), Train Mean Max: 0.8522
Epoch 6/10, Train Loss: 0.4380 (value: 0.0015, weighted value: 0.0750, policy: 0.3630, weighted policy: 0.3630), Train Mean Max: 0.8529
Epoch 7/10, Train Loss: 0.4319 (value: 0.0014, weighted value: 0.0703, policy: 0.3616, weighted policy: 0.3616), Train Mean Max: 0.8534
Epoch 8/10, Train Loss: 0.4314 (value: 0.0014, weighted value: 0.0693, policy: 0.3621, weighted policy: 0.3621), Train Mean Max: 0.8538
Epoch 9/10, Train Loss: 0.4302 (value: 0.0014, weighted value: 0.0688, policy: 0.3614, weighted policy: 0.3614), Train Mean Max: 0.8541
Epoch 10/10, Train Loss: 0.4276 (value: 0.0013, weighted value: 0.0667, policy: 0.3608, weighted policy: 0.3608), Train Mean Max: 0.8546
..training done in 68.29 seconds
..evaluation done in 16.93 seconds
Old network+MCTS average reward: 0.5975, min: -0.0926, max: 1.2870, stdev: 0.2272
New network+MCTS average reward: 0.6036, min: -0.0185, max: 1.2593, stdev: 0.2161
Old bare network average reward: 0.4893, min: -0.0926, max: 1.1852, stdev: 0.2255
New bare network average reward: 0.4911, min: -0.1389, max: 1.1944, stdev: 0.2273
External policy "random" average reward: 0.2700, min: -0.2407, max: 1.0185, stdev: 0.2100
External policy "individual greedy" average reward: 0.5490, min: 0.0278, max: 1.3056, stdev: 0.2208
External policy "total greedy" average reward: 0.6656, min: 0.1389, max: 1.3148, stdev: 0.2156
New network won 114 and tied 93 out of 300 games (53.50% wins where ties are half wins)
Keeping the new network

Training iteration 51 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 55.35 seconds
Training examples lengths: [64920, 64696, 64824, 64785, 64695, 64726, 64801, 64668, 64761, 64993]
Total value: 382368.31
Training on 647869 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.4543 (value: 0.0019, weighted value: 0.0961, policy: 0.3583, weighted policy: 0.3583), Train Mean Max: 0.8546
Epoch 2/10, Train Loss: 0.4427 (value: 0.0017, weighted value: 0.0862, policy: 0.3565, weighted policy: 0.3565), Train Mean Max: 0.8554
Epoch 3/10, Train Loss: 0.4366 (value: 0.0016, weighted value: 0.0817, policy: 0.3549, weighted policy: 0.3549), Train Mean Max: 0.8559
Epoch 4/10, Train Loss: 0.4324 (value: 0.0016, weighted value: 0.0781, policy: 0.3544, weighted policy: 0.3544), Train Mean Max: 0.8562
Epoch 5/10, Train Loss: 0.4251 (value: 0.0014, weighted value: 0.0725, policy: 0.3526, weighted policy: 0.3526), Train Mean Max: 0.8568
Epoch 6/10, Train Loss: 0.4250 (value: 0.0015, weighted value: 0.0726, policy: 0.3524, weighted policy: 0.3524), Train Mean Max: 0.8572
Epoch 7/10, Train Loss: 0.4207 (value: 0.0014, weighted value: 0.0689, policy: 0.3518, weighted policy: 0.3518), Train Mean Max: 0.8575
Epoch 8/10, Train Loss: 0.4168 (value: 0.0013, weighted value: 0.0659, policy: 0.3509, weighted policy: 0.3509), Train Mean Max: 0.8580
Epoch 9/10, Train Loss: 0.4171 (value: 0.0013, weighted value: 0.0658, policy: 0.3513, weighted policy: 0.3513), Train Mean Max: 0.8583
Epoch 10/10, Train Loss: 0.4153 (value: 0.0013, weighted value: 0.0646, policy: 0.3508, weighted policy: 0.3508), Train Mean Max: 0.8586
..training done in 70.12 seconds
..evaluation done in 17.03 seconds
Old network+MCTS average reward: 0.5810, min: 0.0185, max: 1.3056, stdev: 0.2225
New network+MCTS average reward: 0.5802, min: -0.0093, max: 1.3056, stdev: 0.2174
Old bare network average reward: 0.4725, min: -0.0741, max: 1.3056, stdev: 0.2256
New bare network average reward: 0.4726, min: -0.1389, max: 1.3056, stdev: 0.2265
External policy "random" average reward: 0.2352, min: -0.4630, max: 0.9074, stdev: 0.2293
External policy "individual greedy" average reward: 0.5146, min: -0.1481, max: 1.2685, stdev: 0.2254
External policy "total greedy" average reward: 0.6433, min: 0.0278, max: 1.5370, stdev: 0.2279
New network won 99 and tied 104 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 52 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.26 seconds
Training examples lengths: [64696, 64824, 64785, 64695, 64726, 64801, 64668, 64761, 64993, 65089]
Total value: 383497.69
Training on 648038 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.4385 (value: 0.0018, weighted value: 0.0919, policy: 0.3466, weighted policy: 0.3466), Train Mean Max: 0.8593
Epoch 2/10, Train Loss: 0.4268 (value: 0.0016, weighted value: 0.0821, policy: 0.3447, weighted policy: 0.3447), Train Mean Max: 0.8599
Epoch 3/10, Train Loss: 0.4206 (value: 0.0016, weighted value: 0.0779, policy: 0.3427, weighted policy: 0.3427), Train Mean Max: 0.8604
Epoch 4/10, Train Loss: 0.4156 (value: 0.0015, weighted value: 0.0735, policy: 0.3421, weighted policy: 0.3421), Train Mean Max: 0.8609
Epoch 5/10, Train Loss: 0.4133 (value: 0.0014, weighted value: 0.0714, policy: 0.3419, weighted policy: 0.3419), Train Mean Max: 0.8613
Epoch 6/10, Train Loss: 0.4090 (value: 0.0014, weighted value: 0.0688, policy: 0.3402, weighted policy: 0.3402), Train Mean Max: 0.8617
Epoch 7/10, Train Loss: 0.4066 (value: 0.0013, weighted value: 0.0665, policy: 0.3401, weighted policy: 0.3401), Train Mean Max: 0.8621
Epoch 8/10, Train Loss: 0.4033 (value: 0.0013, weighted value: 0.0640, policy: 0.3393, weighted policy: 0.3393), Train Mean Max: 0.8623
Epoch 9/10, Train Loss: 0.4014 (value: 0.0013, weighted value: 0.0630, policy: 0.3383, weighted policy: 0.3383), Train Mean Max: 0.8629
Epoch 10/10, Train Loss: 0.4001 (value: 0.0012, weighted value: 0.0624, policy: 0.3378, weighted policy: 0.3378), Train Mean Max: 0.8629
..training done in 60.50 seconds
..evaluation done in 17.12 seconds
Old network+MCTS average reward: 0.6050, min: -0.0093, max: 1.5093, stdev: 0.2276
New network+MCTS average reward: 0.6053, min: -0.0648, max: 1.5093, stdev: 0.2279
Old bare network average reward: 0.4908, min: -0.1944, max: 1.2963, stdev: 0.2405
New bare network average reward: 0.4965, min: -0.1481, max: 1.4537, stdev: 0.2467
External policy "random" average reward: 0.2713, min: -0.3333, max: 1.1574, stdev: 0.2255
External policy "individual greedy" average reward: 0.5559, min: 0.0185, max: 1.5926, stdev: 0.2245
External policy "total greedy" average reward: 0.6757, min: 0.1204, max: 1.4907, stdev: 0.2187
New network won 98 and tied 105 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 53 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 55.77 seconds
Training examples lengths: [64824, 64785, 64695, 64726, 64801, 64668, 64761, 64993, 65089, 64882]
Total value: 384111.49
Training on 648224 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.4267 (value: 0.0018, weighted value: 0.0903, policy: 0.3364, weighted policy: 0.3364), Train Mean Max: 0.8632
Epoch 2/10, Train Loss: 0.4145 (value: 0.0016, weighted value: 0.0798, policy: 0.3348, weighted policy: 0.3348), Train Mean Max: 0.8639
Epoch 3/10, Train Loss: 0.4086 (value: 0.0015, weighted value: 0.0755, policy: 0.3331, weighted policy: 0.3331), Train Mean Max: 0.8643
Epoch 4/10, Train Loss: 0.4032 (value: 0.0014, weighted value: 0.0707, policy: 0.3325, weighted policy: 0.3325), Train Mean Max: 0.8648
Epoch 5/10, Train Loss: 0.4013 (value: 0.0014, weighted value: 0.0696, policy: 0.3316, weighted policy: 0.3316), Train Mean Max: 0.8651
Epoch 6/10, Train Loss: 0.3960 (value: 0.0013, weighted value: 0.0659, policy: 0.3301, weighted policy: 0.3301), Train Mean Max: 0.8656
Epoch 7/10, Train Loss: 0.3965 (value: 0.0013, weighted value: 0.0670, policy: 0.3295, weighted policy: 0.3295), Train Mean Max: 0.8659
Epoch 8/10, Train Loss: 0.3926 (value: 0.0013, weighted value: 0.0642, policy: 0.3284, weighted policy: 0.3284), Train Mean Max: 0.8663
Epoch 9/10, Train Loss: 0.3891 (value: 0.0012, weighted value: 0.0603, policy: 0.3288, weighted policy: 0.3288), Train Mean Max: 0.8667
Epoch 10/10, Train Loss: 0.3899 (value: 0.0012, weighted value: 0.0611, policy: 0.3288, weighted policy: 0.3288), Train Mean Max: 0.8668
..training done in 63.89 seconds
..evaluation done in 17.25 seconds
Old network+MCTS average reward: 0.6024, min: -0.0278, max: 1.2222, stdev: 0.2049
New network+MCTS average reward: 0.6096, min: 0.0648, max: 1.2222, stdev: 0.2109
Old bare network average reward: 0.5003, min: -0.0278, max: 1.1574, stdev: 0.2141
New bare network average reward: 0.5019, min: -0.0370, max: 1.2222, stdev: 0.2145
External policy "random" average reward: 0.2645, min: -0.2963, max: 0.9537, stdev: 0.2281
External policy "individual greedy" average reward: 0.5439, min: 0.0093, max: 1.1944, stdev: 0.2191
External policy "total greedy" average reward: 0.6628, min: 0.1389, max: 1.2685, stdev: 0.2204
New network won 102 and tied 106 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 54 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.11 seconds
Training examples lengths: [64785, 64695, 64726, 64801, 64668, 64761, 64993, 65089, 64882, 64872]
Total value: 384743.41
Training on 648272 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.4180 (value: 0.0018, weighted value: 0.0881, policy: 0.3299, weighted policy: 0.3299), Train Mean Max: 0.8665
Epoch 2/10, Train Loss: 0.4108 (value: 0.0017, weighted value: 0.0833, policy: 0.3276, weighted policy: 0.3276), Train Mean Max: 0.8668
Epoch 3/10, Train Loss: 0.4001 (value: 0.0015, weighted value: 0.0736, policy: 0.3265, weighted policy: 0.3265), Train Mean Max: 0.8673
Epoch 4/10, Train Loss: 0.3983 (value: 0.0014, weighted value: 0.0724, policy: 0.3259, weighted policy: 0.3259), Train Mean Max: 0.8676
Epoch 5/10, Train Loss: 0.3933 (value: 0.0014, weighted value: 0.0687, policy: 0.3245, weighted policy: 0.3245), Train Mean Max: 0.8680
Epoch 6/10, Train Loss: 0.3902 (value: 0.0013, weighted value: 0.0665, policy: 0.3236, weighted policy: 0.3236), Train Mean Max: 0.8683
Epoch 7/10, Train Loss: 0.3875 (value: 0.0013, weighted value: 0.0639, policy: 0.3236, weighted policy: 0.3236), Train Mean Max: 0.8688
Epoch 8/10, Train Loss: 0.3863 (value: 0.0013, weighted value: 0.0631, policy: 0.3232, weighted policy: 0.3232), Train Mean Max: 0.8689
Epoch 9/10, Train Loss: 0.3826 (value: 0.0012, weighted value: 0.0609, policy: 0.3218, weighted policy: 0.3218), Train Mean Max: 0.8692
Epoch 10/10, Train Loss: 0.3816 (value: 0.0012, weighted value: 0.0603, policy: 0.3213, weighted policy: 0.3213), Train Mean Max: 0.8694
..training done in 64.25 seconds
..evaluation done in 16.97 seconds
Old network+MCTS average reward: 0.5838, min: 0.0463, max: 1.1667, stdev: 0.2208
New network+MCTS average reward: 0.5837, min: -0.0093, max: 1.1667, stdev: 0.2220
Old bare network average reward: 0.4822, min: -0.0833, max: 1.1389, stdev: 0.2293
New bare network average reward: 0.4915, min: -0.0833, max: 1.1389, stdev: 0.2271
External policy "random" average reward: 0.2666, min: -0.4444, max: 0.8333, stdev: 0.2123
External policy "individual greedy" average reward: 0.5352, min: 0.0093, max: 1.2593, stdev: 0.2091
External policy "total greedy" average reward: 0.6468, min: 0.1204, max: 1.2778, stdev: 0.2094
New network won 96 and tied 103 out of 300 games (49.17% wins where ties are half wins)
Reverting to the old network

Training iteration 55 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 55.94 seconds
Training examples lengths: [64695, 64726, 64801, 64668, 64761, 64993, 65089, 64882, 64872, 64903]
Total value: 385677.19
Training on 648390 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.4512 (value: 0.0024, weighted value: 0.1196, policy: 0.3316, weighted policy: 0.3316), Train Mean Max: 0.8661
Epoch 2/10, Train Loss: 0.4304 (value: 0.0020, weighted value: 0.1021, policy: 0.3284, weighted policy: 0.3284), Train Mean Max: 0.8669
Epoch 3/10, Train Loss: 0.4189 (value: 0.0019, weighted value: 0.0935, policy: 0.3254, weighted policy: 0.3254), Train Mean Max: 0.8673
Epoch 4/10, Train Loss: 0.4108 (value: 0.0017, weighted value: 0.0872, policy: 0.3236, weighted policy: 0.3236), Train Mean Max: 0.8679
Epoch 5/10, Train Loss: 0.4052 (value: 0.0017, weighted value: 0.0832, policy: 0.3220, weighted policy: 0.3220), Train Mean Max: 0.8683
Epoch 6/10, Train Loss: 0.4000 (value: 0.0016, weighted value: 0.0779, policy: 0.3222, weighted policy: 0.3222), Train Mean Max: 0.8687
Epoch 7/10, Train Loss: 0.3981 (value: 0.0015, weighted value: 0.0769, policy: 0.3212, weighted policy: 0.3212), Train Mean Max: 0.8689
Epoch 8/10, Train Loss: 0.3921 (value: 0.0015, weighted value: 0.0726, policy: 0.3195, weighted policy: 0.3195), Train Mean Max: 0.8693
Epoch 9/10, Train Loss: 0.3903 (value: 0.0014, weighted value: 0.0707, policy: 0.3196, weighted policy: 0.3196), Train Mean Max: 0.8697
Epoch 10/10, Train Loss: 0.3864 (value: 0.0014, weighted value: 0.0687, policy: 0.3177, weighted policy: 0.3177), Train Mean Max: 0.8699
..training done in 63.97 seconds
..evaluation done in 17.23 seconds
Old network+MCTS average reward: 0.5822, min: -0.1019, max: 1.3056, stdev: 0.2120
New network+MCTS average reward: 0.5905, min: -0.0370, max: 1.1852, stdev: 0.2131
Old bare network average reward: 0.4913, min: -0.2130, max: 1.1389, stdev: 0.2222
New bare network average reward: 0.4948, min: -0.1481, max: 1.1389, stdev: 0.2208
External policy "random" average reward: 0.2552, min: -0.2593, max: 0.8519, stdev: 0.2198
External policy "individual greedy" average reward: 0.5300, min: -0.0278, max: 1.2500, stdev: 0.2217
External policy "total greedy" average reward: 0.6507, min: 0.1481, max: 1.4167, stdev: 0.2118
New network won 109 and tied 91 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 56 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.81 seconds
Training examples lengths: [64726, 64801, 64668, 64761, 64993, 65089, 64882, 64872, 64903, 64854]
Total value: 386404.54
Training on 648549 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.4148 (value: 0.0019, weighted value: 0.0951, policy: 0.3197, weighted policy: 0.3197), Train Mean Max: 0.8699
Epoch 2/10, Train Loss: 0.4020 (value: 0.0017, weighted value: 0.0841, policy: 0.3179, weighted policy: 0.3179), Train Mean Max: 0.8706
Epoch 3/10, Train Loss: 0.3966 (value: 0.0016, weighted value: 0.0809, policy: 0.3157, weighted policy: 0.3157), Train Mean Max: 0.8710
Epoch 4/10, Train Loss: 0.3931 (value: 0.0016, weighted value: 0.0779, policy: 0.3152, weighted policy: 0.3152), Train Mean Max: 0.8713
Epoch 5/10, Train Loss: 0.3868 (value: 0.0015, weighted value: 0.0727, policy: 0.3141, weighted policy: 0.3141), Train Mean Max: 0.8717
Epoch 6/10, Train Loss: 0.3861 (value: 0.0015, weighted value: 0.0727, policy: 0.3134, weighted policy: 0.3134), Train Mean Max: 0.8719
Epoch 7/10, Train Loss: 0.3801 (value: 0.0014, weighted value: 0.0679, policy: 0.3122, weighted policy: 0.3122), Train Mean Max: 0.8724
Epoch 8/10, Train Loss: 0.3779 (value: 0.0013, weighted value: 0.0672, policy: 0.3106, weighted policy: 0.3106), Train Mean Max: 0.8725
Epoch 9/10, Train Loss: 0.3759 (value: 0.0013, weighted value: 0.0651, policy: 0.3107, weighted policy: 0.3107), Train Mean Max: 0.8729
Epoch 10/10, Train Loss: 0.3722 (value: 0.0012, weighted value: 0.0615, policy: 0.3106, weighted policy: 0.3106), Train Mean Max: 0.8732
..training done in 63.32 seconds
..evaluation done in 19.37 seconds
Old network+MCTS average reward: 0.6185, min: -0.2222, max: 1.2037, stdev: 0.2315
New network+MCTS average reward: 0.6209, min: -0.1111, max: 1.1852, stdev: 0.2226
Old bare network average reward: 0.5305, min: -0.0926, max: 1.1759, stdev: 0.2308
New bare network average reward: 0.5294, min: -0.1389, max: 1.1574, stdev: 0.2362
External policy "random" average reward: 0.2972, min: -0.3889, max: 0.9907, stdev: 0.2459
External policy "individual greedy" average reward: 0.5619, min: -0.1389, max: 1.4074, stdev: 0.2349
External policy "total greedy" average reward: 0.6785, min: 0.0463, max: 1.2870, stdev: 0.2240
New network won 101 and tied 101 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 57 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.12 seconds
Training examples lengths: [64801, 64668, 64761, 64993, 65089, 64882, 64872, 64903, 64854, 64761]
Total value: 387130.52
Training on 648584 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.4032 (value: 0.0018, weighted value: 0.0911, policy: 0.3121, weighted policy: 0.3121), Train Mean Max: 0.8731
Epoch 2/10, Train Loss: 0.3914 (value: 0.0016, weighted value: 0.0819, policy: 0.3096, weighted policy: 0.3096), Train Mean Max: 0.8736
Epoch 3/10, Train Loss: 0.3867 (value: 0.0016, weighted value: 0.0786, policy: 0.3081, weighted policy: 0.3081), Train Mean Max: 0.8739
Epoch 4/10, Train Loss: 0.3810 (value: 0.0015, weighted value: 0.0743, policy: 0.3068, weighted policy: 0.3068), Train Mean Max: 0.8742
Epoch 5/10, Train Loss: 0.3770 (value: 0.0014, weighted value: 0.0700, policy: 0.3070, weighted policy: 0.3070), Train Mean Max: 0.8747
Epoch 6/10, Train Loss: 0.3732 (value: 0.0014, weighted value: 0.0685, policy: 0.3046, weighted policy: 0.3046), Train Mean Max: 0.8751
Epoch 7/10, Train Loss: 0.3714 (value: 0.0013, weighted value: 0.0660, policy: 0.3054, weighted policy: 0.3054), Train Mean Max: 0.8752
Epoch 8/10, Train Loss: 0.3682 (value: 0.0013, weighted value: 0.0646, policy: 0.3036, weighted policy: 0.3036), Train Mean Max: 0.8757
Epoch 9/10, Train Loss: 0.3658 (value: 0.0012, weighted value: 0.0621, policy: 0.3038, weighted policy: 0.3038), Train Mean Max: 0.8758
Epoch 10/10, Train Loss: 0.3641 (value: 0.0012, weighted value: 0.0615, policy: 0.3026, weighted policy: 0.3026), Train Mean Max: 0.8762
..training done in 67.43 seconds
..evaluation done in 18.17 seconds
Old network+MCTS average reward: 0.6198, min: 0.0833, max: 1.2407, stdev: 0.2195
New network+MCTS average reward: 0.6246, min: 0.1111, max: 1.3426, stdev: 0.2218
Old bare network average reward: 0.5223, min: -0.0093, max: 1.2222, stdev: 0.2267
New bare network average reward: 0.5329, min: -0.1204, max: 1.2222, stdev: 0.2352
External policy "random" average reward: 0.2718, min: -0.2130, max: 1.0185, stdev: 0.2165
External policy "individual greedy" average reward: 0.5628, min: 0.0000, max: 1.3333, stdev: 0.2238
External policy "total greedy" average reward: 0.6688, min: 0.1574, max: 1.3889, stdev: 0.2170
New network won 104 and tied 102 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 58 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.38 seconds
Training examples lengths: [64668, 64761, 64993, 65089, 64882, 64872, 64903, 64854, 64761, 65127]
Total value: 388923.47
Training on 648910 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3934 (value: 0.0018, weighted value: 0.0900, policy: 0.3034, weighted policy: 0.3034), Train Mean Max: 0.8764
Epoch 2/10, Train Loss: 0.3822 (value: 0.0016, weighted value: 0.0802, policy: 0.3020, weighted policy: 0.3020), Train Mean Max: 0.8766
Epoch 3/10, Train Loss: 0.3773 (value: 0.0016, weighted value: 0.0778, policy: 0.2995, weighted policy: 0.2995), Train Mean Max: 0.8770
Epoch 4/10, Train Loss: 0.3699 (value: 0.0014, weighted value: 0.0710, policy: 0.2989, weighted policy: 0.2989), Train Mean Max: 0.8774
Epoch 5/10, Train Loss: 0.3670 (value: 0.0014, weighted value: 0.0690, policy: 0.2979, weighted policy: 0.2979), Train Mean Max: 0.8778
Epoch 6/10, Train Loss: 0.3655 (value: 0.0014, weighted value: 0.0686, policy: 0.2969, weighted policy: 0.2969), Train Mean Max: 0.8779
Epoch 7/10, Train Loss: 0.3613 (value: 0.0013, weighted value: 0.0648, policy: 0.2965, weighted policy: 0.2965), Train Mean Max: 0.8784
Epoch 8/10, Train Loss: 0.3586 (value: 0.0013, weighted value: 0.0633, policy: 0.2953, weighted policy: 0.2953), Train Mean Max: 0.8787
Epoch 9/10, Train Loss: 0.3597 (value: 0.0013, weighted value: 0.0638, policy: 0.2959, weighted policy: 0.2959), Train Mean Max: 0.8791
Epoch 10/10, Train Loss: 0.3551 (value: 0.0012, weighted value: 0.0597, policy: 0.2954, weighted policy: 0.2954), Train Mean Max: 0.8793
..training done in 60.57 seconds
..evaluation done in 17.05 seconds
Old network+MCTS average reward: 0.5936, min: -0.1852, max: 1.3333, stdev: 0.2262
New network+MCTS average reward: 0.5960, min: -0.0185, max: 1.3333, stdev: 0.2223
Old bare network average reward: 0.5067, min: -0.2963, max: 1.1481, stdev: 0.2334
New bare network average reward: 0.5020, min: -0.2963, max: 1.1481, stdev: 0.2359
External policy "random" average reward: 0.2639, min: -0.3241, max: 1.0648, stdev: 0.2362
External policy "individual greedy" average reward: 0.5423, min: -0.1574, max: 1.3333, stdev: 0.2267
External policy "total greedy" average reward: 0.6557, min: -0.0093, max: 1.3611, stdev: 0.2288
New network won 105 and tied 99 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 59 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 55.19 seconds
Training examples lengths: [64761, 64993, 65089, 64882, 64872, 64903, 64854, 64761, 65127, 64774]
Total value: 390314.05
Training on 649016 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3817 (value: 0.0018, weighted value: 0.0875, policy: 0.2942, weighted policy: 0.2942), Train Mean Max: 0.8794
Epoch 2/10, Train Loss: 0.3719 (value: 0.0016, weighted value: 0.0790, policy: 0.2929, weighted policy: 0.2929), Train Mean Max: 0.8799
Epoch 3/10, Train Loss: 0.3656 (value: 0.0015, weighted value: 0.0741, policy: 0.2915, weighted policy: 0.2915), Train Mean Max: 0.8803
Epoch 4/10, Train Loss: 0.3614 (value: 0.0014, weighted value: 0.0714, policy: 0.2900, weighted policy: 0.2900), Train Mean Max: 0.8806
Epoch 5/10, Train Loss: 0.3578 (value: 0.0014, weighted value: 0.0683, policy: 0.2896, weighted policy: 0.2896), Train Mean Max: 0.8809
Epoch 6/10, Train Loss: 0.3559 (value: 0.0013, weighted value: 0.0667, policy: 0.2892, weighted policy: 0.2892), Train Mean Max: 0.8813
Epoch 7/10, Train Loss: 0.3511 (value: 0.0013, weighted value: 0.0631, policy: 0.2880, weighted policy: 0.2880), Train Mean Max: 0.8816
Epoch 8/10, Train Loss: 0.3512 (value: 0.0013, weighted value: 0.0642, policy: 0.2869, weighted policy: 0.2869), Train Mean Max: 0.8819
Epoch 9/10, Train Loss: 0.3489 (value: 0.0012, weighted value: 0.0610, policy: 0.2878, weighted policy: 0.2878), Train Mean Max: 0.8821
Epoch 10/10, Train Loss: 0.3445 (value: 0.0012, weighted value: 0.0586, policy: 0.2859, weighted policy: 0.2859), Train Mean Max: 0.8825
..training done in 58.73 seconds
..evaluation done in 20.64 seconds
Old network+MCTS average reward: 0.5740, min: -0.1389, max: 1.2963, stdev: 0.2256
New network+MCTS average reward: 0.5719, min: -0.1019, max: 1.2963, stdev: 0.2308
Old bare network average reward: 0.4781, min: -0.0833, max: 1.2963, stdev: 0.2306
New bare network average reward: 0.4781, min: -0.1481, max: 1.2963, stdev: 0.2344
External policy "random" average reward: 0.2347, min: -0.3611, max: 0.9352, stdev: 0.2313
External policy "individual greedy" average reward: 0.5276, min: -0.1111, max: 1.3333, stdev: 0.2327
External policy "total greedy" average reward: 0.6319, min: 0.0278, max: 1.3704, stdev: 0.2272
New network won 94 and tied 126 out of 300 games (52.33% wins where ties are half wins)
Keeping the new network

Training iteration 60 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 55.50 seconds
Training examples lengths: [64993, 65089, 64882, 64872, 64903, 64854, 64761, 65127, 64774, 64803]
Total value: 390277.88
Training on 649058 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3747 (value: 0.0017, weighted value: 0.0866, policy: 0.2881, weighted policy: 0.2881), Train Mean Max: 0.8820
Epoch 2/10, Train Loss: 0.3656 (value: 0.0016, weighted value: 0.0783, policy: 0.2872, weighted policy: 0.2872), Train Mean Max: 0.8825
Epoch 3/10, Train Loss: 0.3602 (value: 0.0015, weighted value: 0.0751, policy: 0.2851, weighted policy: 0.2851), Train Mean Max: 0.8829
Epoch 4/10, Train Loss: 0.3534 (value: 0.0014, weighted value: 0.0691, policy: 0.2843, weighted policy: 0.2843), Train Mean Max: 0.8831
Epoch 5/10, Train Loss: 0.3518 (value: 0.0014, weighted value: 0.0676, policy: 0.2842, weighted policy: 0.2842), Train Mean Max: 0.8833
Epoch 6/10, Train Loss: 0.3475 (value: 0.0013, weighted value: 0.0649, policy: 0.2825, weighted policy: 0.2825), Train Mean Max: 0.8837
Epoch 7/10, Train Loss: 0.3444 (value: 0.0013, weighted value: 0.0627, policy: 0.2817, weighted policy: 0.2817), Train Mean Max: 0.8840
Epoch 8/10, Train Loss: 0.3420 (value: 0.0012, weighted value: 0.0614, policy: 0.2805, weighted policy: 0.2805), Train Mean Max: 0.8843
Epoch 9/10, Train Loss: 0.3414 (value: 0.0012, weighted value: 0.0605, policy: 0.2809, weighted policy: 0.2809), Train Mean Max: 0.8845
Epoch 10/10, Train Loss: 0.3405 (value: 0.0012, weighted value: 0.0594, policy: 0.2811, weighted policy: 0.2811), Train Mean Max: 0.8846
..training done in 72.17 seconds
..evaluation done in 17.58 seconds
Old network+MCTS average reward: 0.6081, min: 0.0648, max: 1.3426, stdev: 0.2205
New network+MCTS average reward: 0.6040, min: 0.0093, max: 1.3148, stdev: 0.2171
Old bare network average reward: 0.5032, min: -0.0926, max: 1.2778, stdev: 0.2249
New bare network average reward: 0.5110, min: -0.0185, max: 1.3241, stdev: 0.2290
External policy "random" average reward: 0.2603, min: -0.2685, max: 1.0833, stdev: 0.2234
External policy "individual greedy" average reward: 0.5448, min: -0.0833, max: 1.4352, stdev: 0.2344
External policy "total greedy" average reward: 0.6502, min: 0.1389, max: 1.4167, stdev: 0.2199
New network won 85 and tied 115 out of 300 games (47.50% wins where ties are half wins)
Reverting to the old network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_60

Training iteration 61 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 55.67 seconds
Training examples lengths: [65089, 64882, 64872, 64903, 64854, 64761, 65127, 64774, 64803, 64597]
Total value: 391156.11
Training on 648662 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.4066 (value: 0.0023, weighted value: 0.1154, policy: 0.2912, weighted policy: 0.2912), Train Mean Max: 0.8817
Epoch 2/10, Train Loss: 0.3882 (value: 0.0020, weighted value: 0.0996, policy: 0.2886, weighted policy: 0.2886), Train Mean Max: 0.8822
Epoch 3/10, Train Loss: 0.3769 (value: 0.0018, weighted value: 0.0915, policy: 0.2854, weighted policy: 0.2854), Train Mean Max: 0.8827
Epoch 4/10, Train Loss: 0.3692 (value: 0.0017, weighted value: 0.0855, policy: 0.2836, weighted policy: 0.2836), Train Mean Max: 0.8829
Epoch 5/10, Train Loss: 0.3634 (value: 0.0016, weighted value: 0.0808, policy: 0.2826, weighted policy: 0.2826), Train Mean Max: 0.8834
Epoch 6/10, Train Loss: 0.3584 (value: 0.0015, weighted value: 0.0766, policy: 0.2818, weighted policy: 0.2818), Train Mean Max: 0.8839
Epoch 7/10, Train Loss: 0.3562 (value: 0.0015, weighted value: 0.0753, policy: 0.2808, weighted policy: 0.2808), Train Mean Max: 0.8839
Epoch 8/10, Train Loss: 0.3508 (value: 0.0014, weighted value: 0.0708, policy: 0.2800, weighted policy: 0.2800), Train Mean Max: 0.8844
Epoch 9/10, Train Loss: 0.3493 (value: 0.0014, weighted value: 0.0698, policy: 0.2795, weighted policy: 0.2795), Train Mean Max: 0.8847
Epoch 10/10, Train Loss: 0.3456 (value: 0.0013, weighted value: 0.0670, policy: 0.2786, weighted policy: 0.2786), Train Mean Max: 0.8851
..training done in 64.15 seconds
..evaluation done in 17.34 seconds
Old network+MCTS average reward: 0.5954, min: -0.0556, max: 1.3889, stdev: 0.2318
New network+MCTS average reward: 0.6041, min: 0.0093, max: 1.3889, stdev: 0.2249
Old bare network average reward: 0.5102, min: -0.0926, max: 1.2685, stdev: 0.2355
New bare network average reward: 0.5148, min: -0.0463, max: 1.2130, stdev: 0.2322
External policy "random" average reward: 0.2596, min: -0.3704, max: 0.9259, stdev: 0.2280
External policy "individual greedy" average reward: 0.5320, min: -0.0741, max: 1.4444, stdev: 0.2347
External policy "total greedy" average reward: 0.6620, min: 0.0370, max: 1.5463, stdev: 0.2336
New network won 107 and tied 113 out of 300 games (54.50% wins where ties are half wins)
Keeping the new network

Training iteration 62 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.48 seconds
Training examples lengths: [64882, 64872, 64903, 64854, 64761, 65127, 64774, 64803, 64597, 64880]
Total value: 391318.61
Training on 648453 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3763 (value: 0.0019, weighted value: 0.0937, policy: 0.2826, weighted policy: 0.2826), Train Mean Max: 0.8845
Epoch 2/10, Train Loss: 0.3666 (value: 0.0017, weighted value: 0.0857, policy: 0.2808, weighted policy: 0.2808), Train Mean Max: 0.8848
Epoch 3/10, Train Loss: 0.3590 (value: 0.0016, weighted value: 0.0798, policy: 0.2791, weighted policy: 0.2791), Train Mean Max: 0.8852
Epoch 4/10, Train Loss: 0.3531 (value: 0.0015, weighted value: 0.0760, policy: 0.2772, weighted policy: 0.2772), Train Mean Max: 0.8855
Epoch 5/10, Train Loss: 0.3489 (value: 0.0014, weighted value: 0.0725, policy: 0.2764, weighted policy: 0.2764), Train Mean Max: 0.8859
Epoch 6/10, Train Loss: 0.3458 (value: 0.0014, weighted value: 0.0699, policy: 0.2759, weighted policy: 0.2759), Train Mean Max: 0.8862
Epoch 7/10, Train Loss: 0.3451 (value: 0.0014, weighted value: 0.0693, policy: 0.2758, weighted policy: 0.2758), Train Mean Max: 0.8864
Epoch 8/10, Train Loss: 0.3389 (value: 0.0013, weighted value: 0.0650, policy: 0.2739, weighted policy: 0.2739), Train Mean Max: 0.8867
Epoch 9/10, Train Loss: 0.3392 (value: 0.0013, weighted value: 0.0658, policy: 0.2734, weighted policy: 0.2734), Train Mean Max: 0.8868
Epoch 10/10, Train Loss: 0.3356 (value: 0.0013, weighted value: 0.0626, policy: 0.2730, weighted policy: 0.2730), Train Mean Max: 0.8873
..training done in 64.97 seconds
..evaluation done in 18.00 seconds
Old network+MCTS average reward: 0.6117, min: 0.0648, max: 1.3148, stdev: 0.2183
New network+MCTS average reward: 0.6060, min: 0.0185, max: 1.3981, stdev: 0.2213
Old bare network average reward: 0.5185, min: 0.0185, max: 1.3889, stdev: 0.2261
New bare network average reward: 0.5232, min: -0.0833, max: 1.2963, stdev: 0.2299
External policy "random" average reward: 0.2731, min: -0.3796, max: 0.9630, stdev: 0.2319
External policy "individual greedy" average reward: 0.5485, min: -0.0556, max: 1.2037, stdev: 0.2316
External policy "total greedy" average reward: 0.6654, min: 0.1481, max: 1.4074, stdev: 0.2300
New network won 87 and tied 119 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 63 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 55.95 seconds
Training examples lengths: [64872, 64903, 64854, 64761, 65127, 64774, 64803, 64597, 64880, 64964]
Total value: 391418.18
Training on 648535 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.4102 (value: 0.0025, weighted value: 0.1235, policy: 0.2867, weighted policy: 0.2867), Train Mean Max: 0.8838
Epoch 2/10, Train Loss: 0.3887 (value: 0.0021, weighted value: 0.1060, policy: 0.2826, weighted policy: 0.2826), Train Mean Max: 0.8843
Epoch 3/10, Train Loss: 0.3770 (value: 0.0019, weighted value: 0.0969, policy: 0.2802, weighted policy: 0.2802), Train Mean Max: 0.8846
Epoch 4/10, Train Loss: 0.3712 (value: 0.0018, weighted value: 0.0917, policy: 0.2795, weighted policy: 0.2795), Train Mean Max: 0.8849
Epoch 5/10, Train Loss: 0.3649 (value: 0.0017, weighted value: 0.0872, policy: 0.2777, weighted policy: 0.2777), Train Mean Max: 0.8853
Epoch 6/10, Train Loss: 0.3583 (value: 0.0016, weighted value: 0.0813, policy: 0.2770, weighted policy: 0.2770), Train Mean Max: 0.8858
Epoch 7/10, Train Loss: 0.3543 (value: 0.0016, weighted value: 0.0783, policy: 0.2760, weighted policy: 0.2760), Train Mean Max: 0.8862
Epoch 8/10, Train Loss: 0.3515 (value: 0.0015, weighted value: 0.0765, policy: 0.2749, weighted policy: 0.2749), Train Mean Max: 0.8864
Epoch 9/10, Train Loss: 0.3469 (value: 0.0015, weighted value: 0.0739, policy: 0.2731, weighted policy: 0.2731), Train Mean Max: 0.8866
Epoch 10/10, Train Loss: 0.3442 (value: 0.0014, weighted value: 0.0709, policy: 0.2733, weighted policy: 0.2733), Train Mean Max: 0.8871
..training done in 58.86 seconds
..evaluation done in 18.68 seconds
Old network+MCTS average reward: 0.6077, min: -0.0185, max: 1.2500, stdev: 0.2324
New network+MCTS average reward: 0.6118, min: 0.0833, max: 1.2500, stdev: 0.2268
Old bare network average reward: 0.5219, min: -0.1111, max: 1.1759, stdev: 0.2294
New bare network average reward: 0.5193, min: -0.1111, max: 1.2407, stdev: 0.2274
External policy "random" average reward: 0.2696, min: -0.3056, max: 0.8981, stdev: 0.2241
External policy "individual greedy" average reward: 0.5619, min: -0.0093, max: 1.3981, stdev: 0.2256
External policy "total greedy" average reward: 0.6700, min: 0.0648, max: 1.3704, stdev: 0.2265
New network won 99 and tied 113 out of 300 games (51.83% wins where ties are half wins)
Keeping the new network

Training iteration 64 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.87 seconds
Training examples lengths: [64903, 64854, 64761, 65127, 64774, 64803, 64597, 64880, 64964, 64617]
Total value: 391911.04
Training on 648280 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3714 (value: 0.0019, weighted value: 0.0951, policy: 0.2763, weighted policy: 0.2763), Train Mean Max: 0.8868
Epoch 2/10, Train Loss: 0.3627 (value: 0.0018, weighted value: 0.0884, policy: 0.2743, weighted policy: 0.2743), Train Mean Max: 0.8872
Epoch 3/10, Train Loss: 0.3547 (value: 0.0016, weighted value: 0.0822, policy: 0.2725, weighted policy: 0.2725), Train Mean Max: 0.8874
Epoch 4/10, Train Loss: 0.3482 (value: 0.0015, weighted value: 0.0773, policy: 0.2709, weighted policy: 0.2709), Train Mean Max: 0.8879
Epoch 5/10, Train Loss: 0.3458 (value: 0.0015, weighted value: 0.0759, policy: 0.2699, weighted policy: 0.2699), Train Mean Max: 0.8882
Epoch 6/10, Train Loss: 0.3416 (value: 0.0014, weighted value: 0.0722, policy: 0.2694, weighted policy: 0.2694), Train Mean Max: 0.8885
Epoch 7/10, Train Loss: 0.3388 (value: 0.0014, weighted value: 0.0698, policy: 0.2690, weighted policy: 0.2690), Train Mean Max: 0.8888
Epoch 8/10, Train Loss: 0.3358 (value: 0.0014, weighted value: 0.0684, policy: 0.2674, weighted policy: 0.2674), Train Mean Max: 0.8890
Epoch 9/10, Train Loss: 0.3335 (value: 0.0013, weighted value: 0.0661, policy: 0.2674, weighted policy: 0.2674), Train Mean Max: 0.8893
Epoch 10/10, Train Loss: 0.3307 (value: 0.0013, weighted value: 0.0640, policy: 0.2666, weighted policy: 0.2666), Train Mean Max: 0.8897
..training done in 67.87 seconds
..evaluation done in 17.77 seconds
Old network+MCTS average reward: 0.6198, min: -0.1204, max: 1.4074, stdev: 0.2454
New network+MCTS average reward: 0.6217, min: -0.0741, max: 1.4352, stdev: 0.2419
Old bare network average reward: 0.5327, min: -0.1111, max: 1.3241, stdev: 0.2511
New bare network average reward: 0.5303, min: -0.2315, max: 1.3241, stdev: 0.2429
External policy "random" average reward: 0.2715, min: -0.5556, max: 1.0556, stdev: 0.2446
External policy "individual greedy" average reward: 0.5458, min: -0.1389, max: 1.2500, stdev: 0.2556
External policy "total greedy" average reward: 0.6660, min: -0.1852, max: 1.3889, stdev: 0.2497
New network won 97 and tied 105 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 65 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.42 seconds
Training examples lengths: [64854, 64761, 65127, 64774, 64803, 64597, 64880, 64964, 64617, 64702]
Total value: 392222.07
Training on 648079 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.4003 (value: 0.0024, weighted value: 0.1213, policy: 0.2790, weighted policy: 0.2790), Train Mean Max: 0.8864
Epoch 2/10, Train Loss: 0.3847 (value: 0.0022, weighted value: 0.1084, policy: 0.2763, weighted policy: 0.2763), Train Mean Max: 0.8869
Epoch 3/10, Train Loss: 0.3694 (value: 0.0019, weighted value: 0.0966, policy: 0.2728, weighted policy: 0.2728), Train Mean Max: 0.8875
Epoch 4/10, Train Loss: 0.3626 (value: 0.0018, weighted value: 0.0917, policy: 0.2709, weighted policy: 0.2709), Train Mean Max: 0.8877
Epoch 5/10, Train Loss: 0.3550 (value: 0.0017, weighted value: 0.0855, policy: 0.2694, weighted policy: 0.2694), Train Mean Max: 0.8883
Epoch 6/10, Train Loss: 0.3517 (value: 0.0017, weighted value: 0.0829, policy: 0.2688, weighted policy: 0.2688), Train Mean Max: 0.8885
Epoch 7/10, Train Loss: 0.3480 (value: 0.0016, weighted value: 0.0806, policy: 0.2675, weighted policy: 0.2675), Train Mean Max: 0.8887
Epoch 8/10, Train Loss: 0.3429 (value: 0.0015, weighted value: 0.0757, policy: 0.2671, weighted policy: 0.2671), Train Mean Max: 0.8892
Epoch 9/10, Train Loss: 0.3400 (value: 0.0015, weighted value: 0.0743, policy: 0.2657, weighted policy: 0.2657), Train Mean Max: 0.8895
Epoch 10/10, Train Loss: 0.3351 (value: 0.0014, weighted value: 0.0706, policy: 0.2645, weighted policy: 0.2645), Train Mean Max: 0.8898
..training done in 58.39 seconds
..evaluation done in 16.76 seconds
Old network+MCTS average reward: 0.5949, min: 0.0556, max: 1.2407, stdev: 0.2200
New network+MCTS average reward: 0.5952, min: 0.0463, max: 1.2778, stdev: 0.2225
Old bare network average reward: 0.5050, min: -0.0370, max: 1.2222, stdev: 0.2458
New bare network average reward: 0.5091, min: -0.1111, max: 1.2222, stdev: 0.2384
External policy "random" average reward: 0.2652, min: -0.3611, max: 0.8889, stdev: 0.2324
External policy "individual greedy" average reward: 0.5399, min: -0.0185, max: 1.3796, stdev: 0.2360
External policy "total greedy" average reward: 0.6514, min: 0.0926, max: 1.4167, stdev: 0.2366
New network won 99 and tied 109 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 66 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.22 seconds
Training examples lengths: [64761, 65127, 64774, 64803, 64597, 64880, 64964, 64617, 64702, 64689]
Total value: 392387.81
Training on 647914 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3656 (value: 0.0019, weighted value: 0.0969, policy: 0.2687, weighted policy: 0.2687), Train Mean Max: 0.8894
Epoch 2/10, Train Loss: 0.3540 (value: 0.0017, weighted value: 0.0864, policy: 0.2676, weighted policy: 0.2676), Train Mean Max: 0.8897
Epoch 3/10, Train Loss: 0.3490 (value: 0.0017, weighted value: 0.0837, policy: 0.2653, weighted policy: 0.2653), Train Mean Max: 0.8901
Epoch 4/10, Train Loss: 0.3417 (value: 0.0016, weighted value: 0.0775, policy: 0.2642, weighted policy: 0.2642), Train Mean Max: 0.8905
Epoch 5/10, Train Loss: 0.3383 (value: 0.0015, weighted value: 0.0757, policy: 0.2626, weighted policy: 0.2626), Train Mean Max: 0.8908
Epoch 6/10, Train Loss: 0.3347 (value: 0.0015, weighted value: 0.0733, policy: 0.2614, weighted policy: 0.2614), Train Mean Max: 0.8913
Epoch 7/10, Train Loss: 0.3306 (value: 0.0014, weighted value: 0.0692, policy: 0.2614, weighted policy: 0.2614), Train Mean Max: 0.8915
Epoch 8/10, Train Loss: 0.3287 (value: 0.0014, weighted value: 0.0689, policy: 0.2598, weighted policy: 0.2598), Train Mean Max: 0.8918
Epoch 9/10, Train Loss: 0.3253 (value: 0.0013, weighted value: 0.0653, policy: 0.2600, weighted policy: 0.2600), Train Mean Max: 0.8922
Epoch 10/10, Train Loss: 0.3255 (value: 0.0013, weighted value: 0.0664, policy: 0.2592, weighted policy: 0.2592), Train Mean Max: 0.8922
..training done in 58.94 seconds
..evaluation done in 17.16 seconds
Old network+MCTS average reward: 0.6240, min: 0.0278, max: 1.4074, stdev: 0.2418
New network+MCTS average reward: 0.6242, min: 0.1019, max: 1.3796, stdev: 0.2341
Old bare network average reward: 0.5345, min: -0.0556, max: 1.4537, stdev: 0.2411
New bare network average reward: 0.5364, min: -0.0833, max: 1.3148, stdev: 0.2428
External policy "random" average reward: 0.2728, min: -0.3241, max: 1.0463, stdev: 0.2398
External policy "individual greedy" average reward: 0.5616, min: -0.1019, max: 1.2130, stdev: 0.2388
External policy "total greedy" average reward: 0.6736, min: 0.1111, max: 1.3148, stdev: 0.2297
New network won 89 and tied 124 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 67 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 55.79 seconds
Training examples lengths: [65127, 64774, 64803, 64597, 64880, 64964, 64617, 64702, 64689, 65171]
Total value: 393287.65
Training on 648324 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3573 (value: 0.0019, weighted value: 0.0927, policy: 0.2647, weighted policy: 0.2647), Train Mean Max: 0.8916
Epoch 2/10, Train Loss: 0.3434 (value: 0.0017, weighted value: 0.0829, policy: 0.2605, weighted policy: 0.2605), Train Mean Max: 0.8922
Epoch 3/10, Train Loss: 0.3367 (value: 0.0015, weighted value: 0.0767, policy: 0.2600, weighted policy: 0.2600), Train Mean Max: 0.8925
Epoch 4/10, Train Loss: 0.3333 (value: 0.0015, weighted value: 0.0746, policy: 0.2587, weighted policy: 0.2587), Train Mean Max: 0.8928
Epoch 5/10, Train Loss: 0.3300 (value: 0.0014, weighted value: 0.0720, policy: 0.2581, weighted policy: 0.2581), Train Mean Max: 0.8931
Epoch 6/10, Train Loss: 0.3255 (value: 0.0014, weighted value: 0.0687, policy: 0.2568, weighted policy: 0.2568), Train Mean Max: 0.8934
Epoch 7/10, Train Loss: 0.3223 (value: 0.0013, weighted value: 0.0659, policy: 0.2565, weighted policy: 0.2565), Train Mean Max: 0.8936
Epoch 8/10, Train Loss: 0.3218 (value: 0.0013, weighted value: 0.0661, policy: 0.2557, weighted policy: 0.2557), Train Mean Max: 0.8939
Epoch 9/10, Train Loss: 0.3189 (value: 0.0013, weighted value: 0.0641, policy: 0.2547, weighted policy: 0.2547), Train Mean Max: 0.8940
Epoch 10/10, Train Loss: 0.3165 (value: 0.0012, weighted value: 0.0618, policy: 0.2546, weighted policy: 0.2546), Train Mean Max: 0.8942
..training done in 58.47 seconds
..evaluation done in 16.75 seconds
Old network+MCTS average reward: 0.5940, min: -0.0370, max: 1.3056, stdev: 0.2238
New network+MCTS average reward: 0.6000, min: 0.1111, max: 1.3333, stdev: 0.2234
Old bare network average reward: 0.5117, min: -0.0648, max: 1.3333, stdev: 0.2297
New bare network average reward: 0.5080, min: -0.0926, max: 1.3333, stdev: 0.2395
External policy "random" average reward: 0.2568, min: -0.3333, max: 0.8519, stdev: 0.2223
External policy "individual greedy" average reward: 0.5250, min: -0.0463, max: 1.1574, stdev: 0.2304
External policy "total greedy" average reward: 0.6444, min: 0.1019, max: 1.3519, stdev: 0.2251
New network won 96 and tied 121 out of 300 games (52.17% wins where ties are half wins)
Keeping the new network

Training iteration 68 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 63.03 seconds
Training examples lengths: [64774, 64803, 64597, 64880, 64964, 64617, 64702, 64689, 65171, 64832]
Total value: 393320.98
Training on 648029 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3499 (value: 0.0018, weighted value: 0.0905, policy: 0.2594, weighted policy: 0.2594), Train Mean Max: 0.8936
Epoch 2/10, Train Loss: 0.3358 (value: 0.0016, weighted value: 0.0781, policy: 0.2576, weighted policy: 0.2576), Train Mean Max: 0.8941
Epoch 3/10, Train Loss: 0.3309 (value: 0.0015, weighted value: 0.0754, policy: 0.2555, weighted policy: 0.2555), Train Mean Max: 0.8943
Epoch 4/10, Train Loss: 0.3278 (value: 0.0015, weighted value: 0.0731, policy: 0.2548, weighted policy: 0.2548), Train Mean Max: 0.8946
Epoch 5/10, Train Loss: 0.3220 (value: 0.0014, weighted value: 0.0688, policy: 0.2533, weighted policy: 0.2533), Train Mean Max: 0.8948
Epoch 6/10, Train Loss: 0.3194 (value: 0.0013, weighted value: 0.0667, policy: 0.2527, weighted policy: 0.2527), Train Mean Max: 0.8951
Epoch 7/10, Train Loss: 0.3170 (value: 0.0013, weighted value: 0.0653, policy: 0.2517, weighted policy: 0.2517), Train Mean Max: 0.8955
Epoch 8/10, Train Loss: 0.3157 (value: 0.0013, weighted value: 0.0640, policy: 0.2517, weighted policy: 0.2517), Train Mean Max: 0.8955
Epoch 9/10, Train Loss: 0.3118 (value: 0.0012, weighted value: 0.0604, policy: 0.2514, weighted policy: 0.2514), Train Mean Max: 0.8960
Epoch 10/10, Train Loss: 0.3106 (value: 0.0012, weighted value: 0.0605, policy: 0.2501, weighted policy: 0.2501), Train Mean Max: 0.8961
..training done in 67.56 seconds
..evaluation done in 18.10 seconds
Old network+MCTS average reward: 0.6041, min: 0.0185, max: 1.4722, stdev: 0.2303
New network+MCTS average reward: 0.6000, min: 0.0000, max: 1.4630, stdev: 0.2267
Old bare network average reward: 0.5171, min: -0.1481, max: 1.3241, stdev: 0.2377
New bare network average reward: 0.5213, min: -0.1481, max: 1.3056, stdev: 0.2303
External policy "random" average reward: 0.2624, min: -0.3056, max: 1.0556, stdev: 0.2221
External policy "individual greedy" average reward: 0.5311, min: -0.0648, max: 1.4630, stdev: 0.2356
External policy "total greedy" average reward: 0.6563, min: 0.0741, max: 1.5741, stdev: 0.2313
New network won 84 and tied 118 out of 300 games (47.67% wins where ties are half wins)
Reverting to the old network

Training iteration 69 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 55.88 seconds
Training examples lengths: [64803, 64597, 64880, 64964, 64617, 64702, 64689, 65171, 64832, 64497]
Total value: 393909.36
Training on 647752 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3809 (value: 0.0023, weighted value: 0.1165, policy: 0.2645, weighted policy: 0.2645), Train Mean Max: 0.8930
Epoch 2/10, Train Loss: 0.3602 (value: 0.0020, weighted value: 0.0989, policy: 0.2613, weighted policy: 0.2613), Train Mean Max: 0.8935
Epoch 3/10, Train Loss: 0.3486 (value: 0.0018, weighted value: 0.0917, policy: 0.2569, weighted policy: 0.2569), Train Mean Max: 0.8939
Epoch 4/10, Train Loss: 0.3415 (value: 0.0017, weighted value: 0.0860, policy: 0.2555, weighted policy: 0.2555), Train Mean Max: 0.8942
Epoch 5/10, Train Loss: 0.3354 (value: 0.0016, weighted value: 0.0811, policy: 0.2543, weighted policy: 0.2543), Train Mean Max: 0.8945
Epoch 6/10, Train Loss: 0.3333 (value: 0.0016, weighted value: 0.0784, policy: 0.2549, weighted policy: 0.2549), Train Mean Max: 0.8946
Epoch 7/10, Train Loss: 0.3283 (value: 0.0015, weighted value: 0.0753, policy: 0.2529, weighted policy: 0.2529), Train Mean Max: 0.8951
Epoch 8/10, Train Loss: 0.3243 (value: 0.0015, weighted value: 0.0732, policy: 0.2512, weighted policy: 0.2512), Train Mean Max: 0.8952
Epoch 9/10, Train Loss: 0.3186 (value: 0.0014, weighted value: 0.0685, policy: 0.2501, weighted policy: 0.2501), Train Mean Max: 0.8956
Epoch 10/10, Train Loss: 0.3191 (value: 0.0014, weighted value: 0.0684, policy: 0.2507, weighted policy: 0.2507), Train Mean Max: 0.8958
..training done in 58.91 seconds
..evaluation done in 17.61 seconds
Old network+MCTS average reward: 0.6044, min: -0.0278, max: 1.6111, stdev: 0.2271
New network+MCTS average reward: 0.6079, min: -0.0278, max: 1.5556, stdev: 0.2239
Old bare network average reward: 0.5187, min: -0.1296, max: 1.4907, stdev: 0.2275
New bare network average reward: 0.5242, min: -0.0278, max: 1.3889, stdev: 0.2272
External policy "random" average reward: 0.2812, min: -0.2685, max: 1.2870, stdev: 0.2188
External policy "individual greedy" average reward: 0.5548, min: 0.0000, max: 1.7037, stdev: 0.2230
External policy "total greedy" average reward: 0.6614, min: 0.1574, max: 1.6852, stdev: 0.2263
New network won 105 and tied 113 out of 300 games (53.83% wins where ties are half wins)
Keeping the new network

Training iteration 70 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.87 seconds
Training examples lengths: [64597, 64880, 64964, 64617, 64702, 64689, 65171, 64832, 64497, 65060]
Total value: 394893.38
Training on 648009 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3470 (value: 0.0018, weighted value: 0.0921, policy: 0.2549, weighted policy: 0.2549), Train Mean Max: 0.8956
Epoch 2/10, Train Loss: 0.3361 (value: 0.0017, weighted value: 0.0838, policy: 0.2523, weighted policy: 0.2523), Train Mean Max: 0.8959
Epoch 3/10, Train Loss: 0.3293 (value: 0.0016, weighted value: 0.0785, policy: 0.2508, weighted policy: 0.2508), Train Mean Max: 0.8963
Epoch 4/10, Train Loss: 0.3278 (value: 0.0015, weighted value: 0.0771, policy: 0.2507, weighted policy: 0.2507), Train Mean Max: 0.8964
Epoch 5/10, Train Loss: 0.3212 (value: 0.0015, weighted value: 0.0729, policy: 0.2483, weighted policy: 0.2483), Train Mean Max: 0.8967
Epoch 6/10, Train Loss: 0.3157 (value: 0.0014, weighted value: 0.0688, policy: 0.2469, weighted policy: 0.2469), Train Mean Max: 0.8971
Epoch 7/10, Train Loss: 0.3129 (value: 0.0013, weighted value: 0.0665, policy: 0.2464, weighted policy: 0.2464), Train Mean Max: 0.8974
Epoch 8/10, Train Loss: 0.3116 (value: 0.0013, weighted value: 0.0649, policy: 0.2467, weighted policy: 0.2467), Train Mean Max: 0.8976
Epoch 9/10, Train Loss: 0.3106 (value: 0.0013, weighted value: 0.0648, policy: 0.2458, weighted policy: 0.2458), Train Mean Max: 0.8978
Epoch 10/10, Train Loss: 0.3070 (value: 0.0013, weighted value: 0.0625, policy: 0.2445, weighted policy: 0.2445), Train Mean Max: 0.8982
..training done in 70.34 seconds
..evaluation done in 18.19 seconds
Old network+MCTS average reward: 0.6202, min: -0.1481, max: 1.4815, stdev: 0.2330
New network+MCTS average reward: 0.6217, min: -0.1019, max: 1.4815, stdev: 0.2342
Old bare network average reward: 0.5373, min: -0.1481, max: 1.4352, stdev: 0.2434
New bare network average reward: 0.5394, min: -0.1481, max: 1.4352, stdev: 0.2376
External policy "random" average reward: 0.2800, min: -0.3333, max: 1.1019, stdev: 0.2391
External policy "individual greedy" average reward: 0.5569, min: -0.0185, max: 1.5556, stdev: 0.2303
External policy "total greedy" average reward: 0.6652, min: 0.0093, max: 1.4537, stdev: 0.2270
New network won 85 and tied 122 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 71 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.83 seconds
Training examples lengths: [64880, 64964, 64617, 64702, 64689, 65171, 64832, 64497, 65060, 64767]
Total value: 395517.94
Training on 648179 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3791 (value: 0.0024, weighted value: 0.1188, policy: 0.2604, weighted policy: 0.2604), Train Mean Max: 0.8950
Epoch 2/10, Train Loss: 0.3592 (value: 0.0021, weighted value: 0.1040, policy: 0.2552, weighted policy: 0.2552), Train Mean Max: 0.8955
Epoch 3/10, Train Loss: 0.3475 (value: 0.0019, weighted value: 0.0954, policy: 0.2521, weighted policy: 0.2521), Train Mean Max: 0.8959
Epoch 4/10, Train Loss: 0.3404 (value: 0.0018, weighted value: 0.0892, policy: 0.2512, weighted policy: 0.2512), Train Mean Max: 0.8962
Epoch 5/10, Train Loss: 0.3331 (value: 0.0017, weighted value: 0.0838, policy: 0.2493, weighted policy: 0.2493), Train Mean Max: 0.8966
Epoch 6/10, Train Loss: 0.3284 (value: 0.0016, weighted value: 0.0802, policy: 0.2482, weighted policy: 0.2482), Train Mean Max: 0.8968
Epoch 7/10, Train Loss: 0.3257 (value: 0.0016, weighted value: 0.0785, policy: 0.2472, weighted policy: 0.2472), Train Mean Max: 0.8971
Epoch 8/10, Train Loss: 0.3208 (value: 0.0015, weighted value: 0.0753, policy: 0.2454, weighted policy: 0.2454), Train Mean Max: 0.8974
Epoch 9/10, Train Loss: 0.3167 (value: 0.0014, weighted value: 0.0712, policy: 0.2455, weighted policy: 0.2455), Train Mean Max: 0.8978
Epoch 10/10, Train Loss: 0.3150 (value: 0.0014, weighted value: 0.0699, policy: 0.2451, weighted policy: 0.2451), Train Mean Max: 0.8979
..training done in 61.50 seconds
..evaluation done in 17.87 seconds
Old network+MCTS average reward: 0.5980, min: -0.0278, max: 1.2685, stdev: 0.2162
New network+MCTS average reward: 0.5968, min: 0.0463, max: 1.2593, stdev: 0.2149
Old bare network average reward: 0.5134, min: -0.0556, max: 1.2315, stdev: 0.2206
New bare network average reward: 0.5132, min: -0.0556, max: 1.2593, stdev: 0.2242
External policy "random" average reward: 0.2567, min: -0.2315, max: 0.9907, stdev: 0.2077
External policy "individual greedy" average reward: 0.5417, min: 0.0278, max: 1.2685, stdev: 0.2152
External policy "total greedy" average reward: 0.6432, min: -0.0741, max: 1.3148, stdev: 0.2234
New network won 93 and tied 113 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 72 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 54.79 seconds
Training examples lengths: [64964, 64617, 64702, 64689, 65171, 64832, 64497, 65060, 64767, 64650]
Total value: 396037.91
Training on 647949 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.4097 (value: 0.0029, weighted value: 0.1455, policy: 0.2641, weighted policy: 0.2641), Train Mean Max: 0.8944
Epoch 2/10, Train Loss: 0.3786 (value: 0.0024, weighted value: 0.1203, policy: 0.2583, weighted policy: 0.2583), Train Mean Max: 0.8950
Epoch 3/10, Train Loss: 0.3661 (value: 0.0022, weighted value: 0.1116, policy: 0.2546, weighted policy: 0.2546), Train Mean Max: 0.8955
Epoch 4/10, Train Loss: 0.3544 (value: 0.0020, weighted value: 0.1024, policy: 0.2520, weighted policy: 0.2520), Train Mean Max: 0.8959
Epoch 5/10, Train Loss: 0.3477 (value: 0.0019, weighted value: 0.0967, policy: 0.2510, weighted policy: 0.2510), Train Mean Max: 0.8961
Epoch 6/10, Train Loss: 0.3399 (value: 0.0018, weighted value: 0.0897, policy: 0.2501, weighted policy: 0.2501), Train Mean Max: 0.8965
Epoch 7/10, Train Loss: 0.3348 (value: 0.0017, weighted value: 0.0868, policy: 0.2480, weighted policy: 0.2480), Train Mean Max: 0.8970
Epoch 8/10, Train Loss: 0.3303 (value: 0.0017, weighted value: 0.0837, policy: 0.2466, weighted policy: 0.2466), Train Mean Max: 0.8970
Epoch 9/10, Train Loss: 0.3258 (value: 0.0016, weighted value: 0.0787, policy: 0.2471, weighted policy: 0.2471), Train Mean Max: 0.8975
Epoch 10/10, Train Loss: 0.3230 (value: 0.0015, weighted value: 0.0775, policy: 0.2456, weighted policy: 0.2456), Train Mean Max: 0.8979
..training done in 59.91 seconds
..evaluation done in 17.34 seconds
Old network+MCTS average reward: 0.6425, min: 0.1111, max: 1.2037, stdev: 0.2204
New network+MCTS average reward: 0.6432, min: 0.0926, max: 1.1759, stdev: 0.2134
Old bare network average reward: 0.5604, min: -0.0926, max: 1.1204, stdev: 0.2293
New bare network average reward: 0.5610, min: -0.0833, max: 1.1574, stdev: 0.2312
External policy "random" average reward: 0.2856, min: -0.2963, max: 0.9630, stdev: 0.2357
External policy "individual greedy" average reward: 0.5758, min: 0.0556, max: 1.3611, stdev: 0.2318
External policy "total greedy" average reward: 0.6781, min: 0.1759, max: 1.3333, stdev: 0.2185
New network won 99 and tied 112 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 73 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.50 seconds
Training examples lengths: [64617, 64702, 64689, 65171, 64832, 64497, 65060, 64767, 64650, 64722]
Total value: 397170.77
Training on 647707 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3508 (value: 0.0020, weighted value: 0.1007, policy: 0.2501, weighted policy: 0.2501), Train Mean Max: 0.8974
Epoch 2/10, Train Loss: 0.3375 (value: 0.0018, weighted value: 0.0909, policy: 0.2467, weighted policy: 0.2467), Train Mean Max: 0.8978
Epoch 3/10, Train Loss: 0.3304 (value: 0.0017, weighted value: 0.0857, policy: 0.2448, weighted policy: 0.2448), Train Mean Max: 0.8981
Epoch 4/10, Train Loss: 0.3247 (value: 0.0016, weighted value: 0.0809, policy: 0.2437, weighted policy: 0.2437), Train Mean Max: 0.8985
Epoch 5/10, Train Loss: 0.3199 (value: 0.0016, weighted value: 0.0780, policy: 0.2418, weighted policy: 0.2418), Train Mean Max: 0.8989
Epoch 6/10, Train Loss: 0.3167 (value: 0.0015, weighted value: 0.0741, policy: 0.2426, weighted policy: 0.2426), Train Mean Max: 0.8992
Epoch 7/10, Train Loss: 0.3146 (value: 0.0015, weighted value: 0.0739, policy: 0.2407, weighted policy: 0.2407), Train Mean Max: 0.8995
Epoch 8/10, Train Loss: 0.3105 (value: 0.0014, weighted value: 0.0704, policy: 0.2401, weighted policy: 0.2401), Train Mean Max: 0.8996
Epoch 9/10, Train Loss: 0.3073 (value: 0.0014, weighted value: 0.0685, policy: 0.2388, weighted policy: 0.2388), Train Mean Max: 0.9001
Epoch 10/10, Train Loss: 0.3059 (value: 0.0013, weighted value: 0.0667, policy: 0.2392, weighted policy: 0.2392), Train Mean Max: 0.9004
..training done in 59.22 seconds
..evaluation done in 17.53 seconds
Old network+MCTS average reward: 0.6188, min: 0.1389, max: 1.2685, stdev: 0.2153
New network+MCTS average reward: 0.6216, min: 0.1019, max: 1.2407, stdev: 0.2148
Old bare network average reward: 0.5315, min: -0.0185, max: 1.1481, stdev: 0.2257
New bare network average reward: 0.5303, min: 0.0185, max: 1.0463, stdev: 0.2218
External policy "random" average reward: 0.2890, min: -0.2593, max: 1.0093, stdev: 0.2206
External policy "individual greedy" average reward: 0.5461, min: 0.0278, max: 1.1574, stdev: 0.2297
External policy "total greedy" average reward: 0.6479, min: 0.0741, max: 1.2407, stdev: 0.2276
New network won 104 and tied 105 out of 300 games (52.17% wins where ties are half wins)
Keeping the new network

Training iteration 74 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.41 seconds
Training examples lengths: [64702, 64689, 65171, 64832, 64497, 65060, 64767, 64650, 64722, 64750]
Total value: 398392.34
Training on 647840 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3355 (value: 0.0018, weighted value: 0.0906, policy: 0.2450, weighted policy: 0.2450), Train Mean Max: 0.8997
Epoch 2/10, Train Loss: 0.3280 (value: 0.0017, weighted value: 0.0853, policy: 0.2426, weighted policy: 0.2426), Train Mean Max: 0.9000
Epoch 3/10, Train Loss: 0.3188 (value: 0.0016, weighted value: 0.0781, policy: 0.2407, weighted policy: 0.2407), Train Mean Max: 0.9004
Epoch 4/10, Train Loss: 0.3134 (value: 0.0015, weighted value: 0.0741, policy: 0.2392, weighted policy: 0.2392), Train Mean Max: 0.9007
Epoch 5/10, Train Loss: 0.3103 (value: 0.0015, weighted value: 0.0733, policy: 0.2370, weighted policy: 0.2370), Train Mean Max: 0.9009
Epoch 6/10, Train Loss: 0.3077 (value: 0.0014, weighted value: 0.0700, policy: 0.2377, weighted policy: 0.2377), Train Mean Max: 0.9012
Epoch 7/10, Train Loss: 0.3038 (value: 0.0013, weighted value: 0.0673, policy: 0.2365, weighted policy: 0.2365), Train Mean Max: 0.9016
Epoch 8/10, Train Loss: 0.3020 (value: 0.0013, weighted value: 0.0666, policy: 0.2354, weighted policy: 0.2354), Train Mean Max: 0.9017
Epoch 9/10, Train Loss: 0.2993 (value: 0.0013, weighted value: 0.0643, policy: 0.2350, weighted policy: 0.2350), Train Mean Max: 0.9020
Epoch 10/10, Train Loss: 0.2978 (value: 0.0013, weighted value: 0.0626, policy: 0.2352, weighted policy: 0.2352), Train Mean Max: 0.9022
..training done in 59.49 seconds
..evaluation done in 17.16 seconds
Old network+MCTS average reward: 0.6026, min: 0.1204, max: 1.2407, stdev: 0.1962
New network+MCTS average reward: 0.6059, min: 0.0648, max: 1.4444, stdev: 0.1966
Old bare network average reward: 0.5144, min: 0.0000, max: 1.2407, stdev: 0.2025
New bare network average reward: 0.5184, min: -0.0833, max: 1.4167, stdev: 0.2010
External policy "random" average reward: 0.2704, min: -0.1944, max: 1.0741, stdev: 0.2068
External policy "individual greedy" average reward: 0.5364, min: 0.0926, max: 1.6667, stdev: 0.2179
External policy "total greedy" average reward: 0.6438, min: 0.1204, max: 1.6759, stdev: 0.2081
New network won 98 and tied 115 out of 300 games (51.83% wins where ties are half wins)
Keeping the new network

Training iteration 75 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.22 seconds
Training examples lengths: [64689, 65171, 64832, 64497, 65060, 64767, 64650, 64722, 64750, 64942]
Total value: 399126.40
Training on 648080 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3274 (value: 0.0018, weighted value: 0.0875, policy: 0.2399, weighted policy: 0.2399), Train Mean Max: 0.9015
Epoch 2/10, Train Loss: 0.3176 (value: 0.0016, weighted value: 0.0795, policy: 0.2381, weighted policy: 0.2381), Train Mean Max: 0.9021
Epoch 3/10, Train Loss: 0.3112 (value: 0.0015, weighted value: 0.0753, policy: 0.2359, weighted policy: 0.2359), Train Mean Max: 0.9022
Epoch 4/10, Train Loss: 0.3089 (value: 0.0015, weighted value: 0.0741, policy: 0.2349, weighted policy: 0.2349), Train Mean Max: 0.9023
Epoch 5/10, Train Loss: 0.3023 (value: 0.0014, weighted value: 0.0687, policy: 0.2336, weighted policy: 0.2336), Train Mean Max: 0.9028
Epoch 6/10, Train Loss: 0.3004 (value: 0.0013, weighted value: 0.0671, policy: 0.2332, weighted policy: 0.2332), Train Mean Max: 0.9030
Epoch 7/10, Train Loss: 0.2980 (value: 0.0013, weighted value: 0.0650, policy: 0.2330, weighted policy: 0.2330), Train Mean Max: 0.9031
Epoch 8/10, Train Loss: 0.2957 (value: 0.0013, weighted value: 0.0634, policy: 0.2323, weighted policy: 0.2323), Train Mean Max: 0.9034
Epoch 9/10, Train Loss: 0.2922 (value: 0.0012, weighted value: 0.0622, policy: 0.2300, weighted policy: 0.2300), Train Mean Max: 0.9037
Epoch 10/10, Train Loss: 0.2900 (value: 0.0012, weighted value: 0.0601, policy: 0.2299, weighted policy: 0.2299), Train Mean Max: 0.9038
..training done in 59.61 seconds
..evaluation done in 16.90 seconds
Old network+MCTS average reward: 0.6047, min: 0.0556, max: 1.2500, stdev: 0.2132
New network+MCTS average reward: 0.6039, min: 0.0926, max: 1.2500, stdev: 0.2128
Old bare network average reward: 0.5141, min: -0.0926, max: 1.1667, stdev: 0.2274
New bare network average reward: 0.5310, min: -0.0370, max: 1.2500, stdev: 0.2297
External policy "random" average reward: 0.2549, min: -0.2315, max: 0.8889, stdev: 0.2104
External policy "individual greedy" average reward: 0.5325, min: 0.0093, max: 1.1481, stdev: 0.2022
External policy "total greedy" average reward: 0.6467, min: 0.0370, max: 1.2778, stdev: 0.2022
New network won 85 and tied 119 out of 300 games (48.17% wins where ties are half wins)
Reverting to the old network

Training iteration 76 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.84 seconds
Training examples lengths: [65171, 64832, 64497, 65060, 64767, 64650, 64722, 64750, 64942, 64710]
Total value: 399939.58
Training on 648101 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3575 (value: 0.0022, weighted value: 0.1121, policy: 0.2454, weighted policy: 0.2454), Train Mean Max: 0.9009
Epoch 2/10, Train Loss: 0.3393 (value: 0.0020, weighted value: 0.0978, policy: 0.2416, weighted policy: 0.2416), Train Mean Max: 0.9013
Epoch 3/10, Train Loss: 0.3293 (value: 0.0018, weighted value: 0.0906, policy: 0.2387, weighted policy: 0.2387), Train Mean Max: 0.9017
Epoch 4/10, Train Loss: 0.3229 (value: 0.0017, weighted value: 0.0865, policy: 0.2364, weighted policy: 0.2364), Train Mean Max: 0.9018
Epoch 5/10, Train Loss: 0.3171 (value: 0.0016, weighted value: 0.0817, policy: 0.2354, weighted policy: 0.2354), Train Mean Max: 0.9021
Epoch 6/10, Train Loss: 0.3111 (value: 0.0015, weighted value: 0.0774, policy: 0.2337, weighted policy: 0.2337), Train Mean Max: 0.9025
Epoch 7/10, Train Loss: 0.3084 (value: 0.0015, weighted value: 0.0742, policy: 0.2341, weighted policy: 0.2341), Train Mean Max: 0.9028
Epoch 8/10, Train Loss: 0.3044 (value: 0.0014, weighted value: 0.0717, policy: 0.2327, weighted policy: 0.2327), Train Mean Max: 0.9032
Epoch 9/10, Train Loss: 0.3013 (value: 0.0014, weighted value: 0.0699, policy: 0.2315, weighted policy: 0.2315), Train Mean Max: 0.9034
Epoch 10/10, Train Loss: 0.2978 (value: 0.0014, weighted value: 0.0676, policy: 0.2302, weighted policy: 0.2302), Train Mean Max: 0.9036
..training done in 58.98 seconds
..evaluation done in 17.63 seconds
Old network+MCTS average reward: 0.6368, min: 0.1204, max: 1.3796, stdev: 0.2253
New network+MCTS average reward: 0.6394, min: 0.1574, max: 1.2778, stdev: 0.2169
Old bare network average reward: 0.5529, min: 0.0000, max: 1.3056, stdev: 0.2405
New bare network average reward: 0.5525, min: -0.0185, max: 1.3148, stdev: 0.2411
External policy "random" average reward: 0.2928, min: -0.3796, max: 1.0000, stdev: 0.2376
External policy "individual greedy" average reward: 0.5609, min: -0.0833, max: 1.3611, stdev: 0.2364
External policy "total greedy" average reward: 0.6749, min: 0.0926, max: 1.4352, stdev: 0.2241
New network won 89 and tied 135 out of 300 games (52.17% wins where ties are half wins)
Keeping the new network

Training iteration 77 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.61 seconds
Training examples lengths: [64832, 64497, 65060, 64767, 64650, 64722, 64750, 64942, 64710, 64900]
Total value: 400218.84
Training on 647830 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3309 (value: 0.0019, weighted value: 0.0948, policy: 0.2362, weighted policy: 0.2362), Train Mean Max: 0.9029
Epoch 2/10, Train Loss: 0.3208 (value: 0.0017, weighted value: 0.0862, policy: 0.2346, weighted policy: 0.2346), Train Mean Max: 0.9031
Epoch 3/10, Train Loss: 0.3111 (value: 0.0016, weighted value: 0.0784, policy: 0.2327, weighted policy: 0.2327), Train Mean Max: 0.9036
Epoch 4/10, Train Loss: 0.3079 (value: 0.0015, weighted value: 0.0763, policy: 0.2316, weighted policy: 0.2316), Train Mean Max: 0.9037
Epoch 5/10, Train Loss: 0.3033 (value: 0.0015, weighted value: 0.0733, policy: 0.2300, weighted policy: 0.2300), Train Mean Max: 0.9041
Epoch 6/10, Train Loss: 0.3022 (value: 0.0015, weighted value: 0.0726, policy: 0.2296, weighted policy: 0.2296), Train Mean Max: 0.9044
Epoch 7/10, Train Loss: 0.2972 (value: 0.0014, weighted value: 0.0683, policy: 0.2289, weighted policy: 0.2289), Train Mean Max: 0.9046
Epoch 8/10, Train Loss: 0.2933 (value: 0.0013, weighted value: 0.0654, policy: 0.2279, weighted policy: 0.2279), Train Mean Max: 0.9049
Epoch 9/10, Train Loss: 0.2915 (value: 0.0013, weighted value: 0.0650, policy: 0.2265, weighted policy: 0.2265), Train Mean Max: 0.9051
Epoch 10/10, Train Loss: 0.2901 (value: 0.0013, weighted value: 0.0626, policy: 0.2274, weighted policy: 0.2274), Train Mean Max: 0.9052
..training done in 64.20 seconds
..evaluation done in 17.42 seconds
Old network+MCTS average reward: 0.6107, min: -0.0926, max: 1.2037, stdev: 0.2231
New network+MCTS average reward: 0.6086, min: -0.1667, max: 1.3611, stdev: 0.2218
Old bare network average reward: 0.5300, min: -0.2130, max: 1.1574, stdev: 0.2322
New bare network average reward: 0.5237, min: -0.2130, max: 1.1481, stdev: 0.2344
External policy "random" average reward: 0.2629, min: -0.3056, max: 0.8796, stdev: 0.2324
External policy "individual greedy" average reward: 0.5297, min: 0.0833, max: 1.3611, stdev: 0.2263
External policy "total greedy" average reward: 0.6505, min: 0.0926, max: 1.4259, stdev: 0.2242
New network won 74 and tied 135 out of 300 games (47.17% wins where ties are half wins)
Reverting to the old network

Training iteration 78 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.80 seconds
Training examples lengths: [64497, 65060, 64767, 64650, 64722, 64750, 64942, 64710, 64900, 64765]
Total value: 400175.81
Training on 647763 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3635 (value: 0.0024, weighted value: 0.1203, policy: 0.2433, weighted policy: 0.2433), Train Mean Max: 0.9019
Epoch 2/10, Train Loss: 0.3437 (value: 0.0021, weighted value: 0.1036, policy: 0.2401, weighted policy: 0.2401), Train Mean Max: 0.9024
Epoch 3/10, Train Loss: 0.3295 (value: 0.0019, weighted value: 0.0928, policy: 0.2367, weighted policy: 0.2367), Train Mean Max: 0.9028
Epoch 4/10, Train Loss: 0.3233 (value: 0.0018, weighted value: 0.0893, policy: 0.2341, weighted policy: 0.2341), Train Mean Max: 0.9032
Epoch 5/10, Train Loss: 0.3175 (value: 0.0017, weighted value: 0.0850, policy: 0.2325, weighted policy: 0.2325), Train Mean Max: 0.9033
Epoch 6/10, Train Loss: 0.3119 (value: 0.0016, weighted value: 0.0804, policy: 0.2315, weighted policy: 0.2315), Train Mean Max: 0.9036
Epoch 7/10, Train Loss: 0.3078 (value: 0.0016, weighted value: 0.0777, policy: 0.2301, weighted policy: 0.2301), Train Mean Max: 0.9037
Epoch 8/10, Train Loss: 0.3055 (value: 0.0015, weighted value: 0.0751, policy: 0.2304, weighted policy: 0.2304), Train Mean Max: 0.9041
Epoch 9/10, Train Loss: 0.3007 (value: 0.0014, weighted value: 0.0720, policy: 0.2287, weighted policy: 0.2287), Train Mean Max: 0.9045
Epoch 10/10, Train Loss: 0.2967 (value: 0.0014, weighted value: 0.0695, policy: 0.2272, weighted policy: 0.2272), Train Mean Max: 0.9047
..training done in 58.39 seconds
..evaluation done in 16.70 seconds
Old network+MCTS average reward: 0.6102, min: 0.0185, max: 1.3426, stdev: 0.2037
New network+MCTS average reward: 0.6152, min: 0.0648, max: 1.2963, stdev: 0.2026
Old bare network average reward: 0.5356, min: 0.0093, max: 1.2593, stdev: 0.2125
New bare network average reward: 0.5368, min: 0.0093, max: 1.1944, stdev: 0.2131
External policy "random" average reward: 0.2562, min: -0.2407, max: 0.9074, stdev: 0.2101
External policy "individual greedy" average reward: 0.5417, min: 0.0463, max: 1.2778, stdev: 0.2050
External policy "total greedy" average reward: 0.6576, min: 0.1574, max: 1.3241, stdev: 0.2034
New network won 95 and tied 119 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 79 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.73 seconds
Training examples lengths: [65060, 64767, 64650, 64722, 64750, 64942, 64710, 64900, 64765, 64890]
Total value: 400253.44
Training on 648156 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3268 (value: 0.0019, weighted value: 0.0928, policy: 0.2340, weighted policy: 0.2340), Train Mean Max: 0.9040
Epoch 2/10, Train Loss: 0.3156 (value: 0.0017, weighted value: 0.0826, policy: 0.2331, weighted policy: 0.2331), Train Mean Max: 0.9044
Epoch 3/10, Train Loss: 0.3081 (value: 0.0016, weighted value: 0.0789, policy: 0.2292, weighted policy: 0.2292), Train Mean Max: 0.9047
Epoch 4/10, Train Loss: 0.3056 (value: 0.0015, weighted value: 0.0766, policy: 0.2289, weighted policy: 0.2289), Train Mean Max: 0.9047
Epoch 5/10, Train Loss: 0.2996 (value: 0.0014, weighted value: 0.0721, policy: 0.2275, weighted policy: 0.2275), Train Mean Max: 0.9052
Epoch 6/10, Train Loss: 0.2972 (value: 0.0014, weighted value: 0.0710, policy: 0.2262, weighted policy: 0.2262), Train Mean Max: 0.9055
Epoch 7/10, Train Loss: 0.2937 (value: 0.0014, weighted value: 0.0685, policy: 0.2252, weighted policy: 0.2252), Train Mean Max: 0.9058
Epoch 8/10, Train Loss: 0.2914 (value: 0.0013, weighted value: 0.0663, policy: 0.2251, weighted policy: 0.2251), Train Mean Max: 0.9061
Epoch 9/10, Train Loss: 0.2895 (value: 0.0013, weighted value: 0.0654, policy: 0.2241, weighted policy: 0.2241), Train Mean Max: 0.9064
Epoch 10/10, Train Loss: 0.2860 (value: 0.0013, weighted value: 0.0625, policy: 0.2235, weighted policy: 0.2235), Train Mean Max: 0.9064
..training done in 58.43 seconds
..evaluation done in 17.40 seconds
Old network+MCTS average reward: 0.6194, min: -0.1296, max: 1.2685, stdev: 0.2147
New network+MCTS average reward: 0.6156, min: -0.0278, max: 1.2870, stdev: 0.2133
Old bare network average reward: 0.5355, min: -0.2130, max: 1.1667, stdev: 0.2237
New bare network average reward: 0.5406, min: -0.1204, max: 1.1852, stdev: 0.2230
External policy "random" average reward: 0.2586, min: -0.3056, max: 0.9815, stdev: 0.2219
External policy "individual greedy" average reward: 0.5417, min: -0.0278, max: 1.2037, stdev: 0.2288
External policy "total greedy" average reward: 0.6572, min: 0.0556, max: 1.2870, stdev: 0.2210
New network won 78 and tied 130 out of 300 games (47.67% wins where ties are half wins)
Reverting to the old network

Training iteration 80 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.20 seconds
Training examples lengths: [64767, 64650, 64722, 64750, 64942, 64710, 64900, 64765, 64890, 64852]
Total value: 400769.57
Training on 647948 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3560 (value: 0.0023, weighted value: 0.1160, policy: 0.2400, weighted policy: 0.2400), Train Mean Max: 0.9033
Epoch 2/10, Train Loss: 0.3396 (value: 0.0021, weighted value: 0.1030, policy: 0.2365, weighted policy: 0.2365), Train Mean Max: 0.9036
Epoch 3/10, Train Loss: 0.3268 (value: 0.0019, weighted value: 0.0939, policy: 0.2329, weighted policy: 0.2329), Train Mean Max: 0.9040
Epoch 4/10, Train Loss: 0.3191 (value: 0.0018, weighted value: 0.0882, policy: 0.2308, weighted policy: 0.2308), Train Mean Max: 0.9044
Epoch 5/10, Train Loss: 0.3139 (value: 0.0017, weighted value: 0.0846, policy: 0.2293, weighted policy: 0.2293), Train Mean Max: 0.9046
Epoch 6/10, Train Loss: 0.3082 (value: 0.0016, weighted value: 0.0798, policy: 0.2285, weighted policy: 0.2285), Train Mean Max: 0.9049
Epoch 7/10, Train Loss: 0.3047 (value: 0.0015, weighted value: 0.0773, policy: 0.2274, weighted policy: 0.2274), Train Mean Max: 0.9051
Epoch 8/10, Train Loss: 0.3012 (value: 0.0015, weighted value: 0.0747, policy: 0.2265, weighted policy: 0.2265), Train Mean Max: 0.9054
Epoch 9/10, Train Loss: 0.2977 (value: 0.0014, weighted value: 0.0724, policy: 0.2254, weighted policy: 0.2254), Train Mean Max: 0.9057
Epoch 10/10, Train Loss: 0.2957 (value: 0.0014, weighted value: 0.0707, policy: 0.2250, weighted policy: 0.2250), Train Mean Max: 0.9059
..training done in 66.57 seconds
..evaluation done in 19.35 seconds
Old network+MCTS average reward: 0.6243, min: -0.1204, max: 1.4167, stdev: 0.2177
New network+MCTS average reward: 0.6197, min: -0.0463, max: 1.4167, stdev: 0.2147
Old bare network average reward: 0.5335, min: -0.1574, max: 1.4167, stdev: 0.2212
New bare network average reward: 0.5456, min: -0.0278, max: 1.4259, stdev: 0.2201
External policy "random" average reward: 0.2533, min: -0.2315, max: 0.8519, stdev: 0.2103
External policy "individual greedy" average reward: 0.5477, min: -0.0370, max: 1.2593, stdev: 0.2148
External policy "total greedy" average reward: 0.6514, min: 0.0093, max: 1.2593, stdev: 0.2172
New network won 78 and tied 124 out of 300 games (46.67% wins where ties are half wins)
Reverting to the old network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_80

Training iteration 81 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.37 seconds
Training examples lengths: [64650, 64722, 64750, 64942, 64710, 64900, 64765, 64890, 64852, 64931]
Total value: 401170.76
Training on 648112 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3848 (value: 0.0028, weighted value: 0.1383, policy: 0.2465, weighted policy: 0.2465), Train Mean Max: 0.9023
Epoch 2/10, Train Loss: 0.3599 (value: 0.0024, weighted value: 0.1190, policy: 0.2410, weighted policy: 0.2410), Train Mean Max: 0.9029
Epoch 3/10, Train Loss: 0.3419 (value: 0.0021, weighted value: 0.1068, policy: 0.2351, weighted policy: 0.2351), Train Mean Max: 0.9034
Epoch 4/10, Train Loss: 0.3357 (value: 0.0020, weighted value: 0.1014, policy: 0.2343, weighted policy: 0.2343), Train Mean Max: 0.9034
Epoch 5/10, Train Loss: 0.3275 (value: 0.0019, weighted value: 0.0947, policy: 0.2328, weighted policy: 0.2328), Train Mean Max: 0.9036
Epoch 6/10, Train Loss: 0.3189 (value: 0.0018, weighted value: 0.0889, policy: 0.2300, weighted policy: 0.2300), Train Mean Max: 0.9042
Epoch 7/10, Train Loss: 0.3153 (value: 0.0017, weighted value: 0.0852, policy: 0.2301, weighted policy: 0.2301), Train Mean Max: 0.9044
Epoch 8/10, Train Loss: 0.3107 (value: 0.0016, weighted value: 0.0823, policy: 0.2283, weighted policy: 0.2283), Train Mean Max: 0.9047
Epoch 9/10, Train Loss: 0.3059 (value: 0.0016, weighted value: 0.0787, policy: 0.2271, weighted policy: 0.2271), Train Mean Max: 0.9049
Epoch 10/10, Train Loss: 0.3026 (value: 0.0015, weighted value: 0.0763, policy: 0.2263, weighted policy: 0.2263), Train Mean Max: 0.9052
..training done in 73.33 seconds
..evaluation done in 17.11 seconds
Old network+MCTS average reward: 0.5868, min: 0.0000, max: 1.2222, stdev: 0.2222
New network+MCTS average reward: 0.5947, min: 0.0370, max: 1.2222, stdev: 0.2221
Old bare network average reward: 0.5079, min: -0.0370, max: 1.1111, stdev: 0.2321
New bare network average reward: 0.5241, min: -0.0278, max: 1.1019, stdev: 0.2291
External policy "random" average reward: 0.2594, min: -0.2500, max: 0.8889, stdev: 0.2248
External policy "individual greedy" average reward: 0.5247, min: 0.0278, max: 1.2222, stdev: 0.2323
External policy "total greedy" average reward: 0.6344, min: 0.1481, max: 1.2500, stdev: 0.2242
New network won 105 and tied 104 out of 300 games (52.33% wins where ties are half wins)
Keeping the new network

Training iteration 82 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.10 seconds
Training examples lengths: [64722, 64750, 64942, 64710, 64900, 64765, 64890, 64852, 64931, 64551]
Total value: 401528.81
Training on 648013 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3319 (value: 0.0020, weighted value: 0.0993, policy: 0.2326, weighted policy: 0.2326), Train Mean Max: 0.9046
Epoch 2/10, Train Loss: 0.3187 (value: 0.0018, weighted value: 0.0890, policy: 0.2297, weighted policy: 0.2297), Train Mean Max: 0.9051
Epoch 3/10, Train Loss: 0.3157 (value: 0.0017, weighted value: 0.0874, policy: 0.2283, weighted policy: 0.2283), Train Mean Max: 0.9052
Epoch 4/10, Train Loss: 0.3068 (value: 0.0016, weighted value: 0.0805, policy: 0.2264, weighted policy: 0.2264), Train Mean Max: 0.9058
Epoch 5/10, Train Loss: 0.3012 (value: 0.0015, weighted value: 0.0767, policy: 0.2245, weighted policy: 0.2245), Train Mean Max: 0.9060
Epoch 6/10, Train Loss: 0.2982 (value: 0.0015, weighted value: 0.0741, policy: 0.2242, weighted policy: 0.2242), Train Mean Max: 0.9062
Epoch 7/10, Train Loss: 0.2977 (value: 0.0015, weighted value: 0.0746, policy: 0.2231, weighted policy: 0.2231), Train Mean Max: 0.9064
Epoch 8/10, Train Loss: 0.2916 (value: 0.0014, weighted value: 0.0696, policy: 0.2220, weighted policy: 0.2220), Train Mean Max: 0.9067
Epoch 9/10, Train Loss: 0.2904 (value: 0.0014, weighted value: 0.0679, policy: 0.2225, weighted policy: 0.2225), Train Mean Max: 0.9069
Epoch 10/10, Train Loss: 0.2882 (value: 0.0013, weighted value: 0.0671, policy: 0.2211, weighted policy: 0.2211), Train Mean Max: 0.9072
..training done in 64.55 seconds
..evaluation done in 17.80 seconds
Old network+MCTS average reward: 0.6252, min: 0.1296, max: 1.3333, stdev: 0.2215
New network+MCTS average reward: 0.6192, min: 0.0370, max: 1.3333, stdev: 0.2184
Old bare network average reward: 0.5462, min: -0.0093, max: 1.3333, stdev: 0.2345
New bare network average reward: 0.5447, min: -0.1111, max: 1.3333, stdev: 0.2283
External policy "random" average reward: 0.2740, min: -0.3148, max: 1.2222, stdev: 0.2377
External policy "individual greedy" average reward: 0.5475, min: -0.0833, max: 1.5648, stdev: 0.2375
External policy "total greedy" average reward: 0.6631, min: -0.0556, max: 1.5741, stdev: 0.2337
New network won 93 and tied 115 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 83 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.43 seconds
Training examples lengths: [64750, 64942, 64710, 64900, 64765, 64890, 64852, 64931, 64551, 65113]
Total value: 401770.32
Training on 648404 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3178 (value: 0.0018, weighted value: 0.0903, policy: 0.2275, weighted policy: 0.2275), Train Mean Max: 0.9065
Epoch 2/10, Train Loss: 0.3062 (value: 0.0016, weighted value: 0.0812, policy: 0.2250, weighted policy: 0.2250), Train Mean Max: 0.9069
Epoch 3/10, Train Loss: 0.3020 (value: 0.0016, weighted value: 0.0784, policy: 0.2236, weighted policy: 0.2236), Train Mean Max: 0.9072
Epoch 4/10, Train Loss: 0.2976 (value: 0.0015, weighted value: 0.0752, policy: 0.2224, weighted policy: 0.2224), Train Mean Max: 0.9074
Epoch 5/10, Train Loss: 0.2919 (value: 0.0014, weighted value: 0.0712, policy: 0.2207, weighted policy: 0.2207), Train Mean Max: 0.9078
Epoch 6/10, Train Loss: 0.2887 (value: 0.0014, weighted value: 0.0681, policy: 0.2207, weighted policy: 0.2207), Train Mean Max: 0.9079
Epoch 7/10, Train Loss: 0.2861 (value: 0.0013, weighted value: 0.0671, policy: 0.2190, weighted policy: 0.2190), Train Mean Max: 0.9083
Epoch 8/10, Train Loss: 0.2843 (value: 0.0013, weighted value: 0.0658, policy: 0.2185, weighted policy: 0.2185), Train Mean Max: 0.9084
Epoch 9/10, Train Loss: 0.2813 (value: 0.0013, weighted value: 0.0640, policy: 0.2173, weighted policy: 0.2173), Train Mean Max: 0.9086
Epoch 10/10, Train Loss: 0.2785 (value: 0.0012, weighted value: 0.0613, policy: 0.2173, weighted policy: 0.2173), Train Mean Max: 0.9090
..training done in 61.97 seconds
..evaluation done in 18.20 seconds
Old network+MCTS average reward: 0.6103, min: -0.0463, max: 1.7778, stdev: 0.2151
New network+MCTS average reward: 0.6090, min: 0.0556, max: 1.7778, stdev: 0.2145
Old bare network average reward: 0.5346, min: -0.0370, max: 1.7778, stdev: 0.2223
New bare network average reward: 0.5363, min: -0.0370, max: 1.4444, stdev: 0.2146
External policy "random" average reward: 0.2570, min: -0.2500, max: 1.4444, stdev: 0.2065
External policy "individual greedy" average reward: 0.5374, min: -0.0556, max: 1.5370, stdev: 0.2099
External policy "total greedy" average reward: 0.6451, min: 0.0926, max: 1.7037, stdev: 0.2198
New network won 88 and tied 119 out of 300 games (49.17% wins where ties are half wins)
Reverting to the old network

Training iteration 84 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.23 seconds
Training examples lengths: [64942, 64710, 64900, 64765, 64890, 64852, 64931, 64551, 65113, 64762]
Total value: 402267.38
Training on 648416 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3473 (value: 0.0022, weighted value: 0.1120, policy: 0.2353, weighted policy: 0.2353), Train Mean Max: 0.9058
Epoch 2/10, Train Loss: 0.3292 (value: 0.0020, weighted value: 0.0995, policy: 0.2297, weighted policy: 0.2297), Train Mean Max: 0.9063
Epoch 3/10, Train Loss: 0.3180 (value: 0.0018, weighted value: 0.0918, policy: 0.2261, weighted policy: 0.2261), Train Mean Max: 0.9065
Epoch 4/10, Train Loss: 0.3101 (value: 0.0017, weighted value: 0.0861, policy: 0.2239, weighted policy: 0.2239), Train Mean Max: 0.9070
Epoch 5/10, Train Loss: 0.3048 (value: 0.0016, weighted value: 0.0821, policy: 0.2227, weighted policy: 0.2227), Train Mean Max: 0.9072
Epoch 6/10, Train Loss: 0.3006 (value: 0.0016, weighted value: 0.0791, policy: 0.2215, weighted policy: 0.2215), Train Mean Max: 0.9075
Epoch 7/10, Train Loss: 0.2981 (value: 0.0015, weighted value: 0.0771, policy: 0.2210, weighted policy: 0.2210), Train Mean Max: 0.9076
Epoch 8/10, Train Loss: 0.2904 (value: 0.0014, weighted value: 0.0710, policy: 0.2195, weighted policy: 0.2195), Train Mean Max: 0.9080
Epoch 9/10, Train Loss: 0.2895 (value: 0.0014, weighted value: 0.0714, policy: 0.2180, weighted policy: 0.2180), Train Mean Max: 0.9082
Epoch 10/10, Train Loss: 0.2860 (value: 0.0013, weighted value: 0.0671, policy: 0.2188, weighted policy: 0.2188), Train Mean Max: 0.9085
..training done in 59.22 seconds
..evaluation done in 17.72 seconds
Old network+MCTS average reward: 0.5993, min: 0.0556, max: 1.3333, stdev: 0.2105
New network+MCTS average reward: 0.6053, min: 0.0926, max: 1.3981, stdev: 0.2149
Old bare network average reward: 0.5293, min: -0.0093, max: 1.3333, stdev: 0.2254
New bare network average reward: 0.5303, min: -0.1111, max: 1.3333, stdev: 0.2244
External policy "random" average reward: 0.2566, min: -0.4722, max: 0.9167, stdev: 0.2329
External policy "individual greedy" average reward: 0.5341, min: 0.0278, max: 1.3426, stdev: 0.2258
External policy "total greedy" average reward: 0.6465, min: 0.1296, max: 1.4259, stdev: 0.2123
New network won 94 and tied 133 out of 300 games (53.50% wins where ties are half wins)
Keeping the new network

Training iteration 85 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.44 seconds
Training examples lengths: [64710, 64900, 64765, 64890, 64852, 64931, 64551, 65113, 64762, 64615]
Total value: 402445.99
Training on 648089 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3168 (value: 0.0018, weighted value: 0.0917, policy: 0.2251, weighted policy: 0.2251), Train Mean Max: 0.9077
Epoch 2/10, Train Loss: 0.3048 (value: 0.0017, weighted value: 0.0828, policy: 0.2220, weighted policy: 0.2220), Train Mean Max: 0.9082
Epoch 3/10, Train Loss: 0.2984 (value: 0.0016, weighted value: 0.0775, policy: 0.2209, weighted policy: 0.2209), Train Mean Max: 0.9085
Epoch 4/10, Train Loss: 0.2947 (value: 0.0015, weighted value: 0.0763, policy: 0.2184, weighted policy: 0.2184), Train Mean Max: 0.9086
Epoch 5/10, Train Loss: 0.2907 (value: 0.0015, weighted value: 0.0725, policy: 0.2182, weighted policy: 0.2182), Train Mean Max: 0.9090
Epoch 6/10, Train Loss: 0.2862 (value: 0.0014, weighted value: 0.0686, policy: 0.2175, weighted policy: 0.2175), Train Mean Max: 0.9091
Epoch 7/10, Train Loss: 0.2844 (value: 0.0014, weighted value: 0.0681, policy: 0.2164, weighted policy: 0.2164), Train Mean Max: 0.9094
Epoch 8/10, Train Loss: 0.2818 (value: 0.0013, weighted value: 0.0665, policy: 0.2153, weighted policy: 0.2153), Train Mean Max: 0.9095
Epoch 9/10, Train Loss: 0.2789 (value: 0.0013, weighted value: 0.0646, policy: 0.2142, weighted policy: 0.2142), Train Mean Max: 0.9098
Epoch 10/10, Train Loss: 0.2770 (value: 0.0012, weighted value: 0.0621, policy: 0.2149, weighted policy: 0.2149), Train Mean Max: 0.9101
..training done in 58.98 seconds
..evaluation done in 17.43 seconds
Old network+MCTS average reward: 0.6173, min: 0.0648, max: 1.3148, stdev: 0.2233
New network+MCTS average reward: 0.6224, min: 0.0000, max: 1.3148, stdev: 0.2191
Old bare network average reward: 0.5358, min: 0.0278, max: 1.2778, stdev: 0.2296
New bare network average reward: 0.5419, min: -0.0833, max: 1.2593, stdev: 0.2296
External policy "random" average reward: 0.2661, min: -0.3426, max: 0.9907, stdev: 0.2187
External policy "individual greedy" average reward: 0.5369, min: -0.0463, max: 1.2963, stdev: 0.2338
External policy "total greedy" average reward: 0.6434, min: 0.0741, max: 1.2870, stdev: 0.2188
New network won 99 and tied 116 out of 300 games (52.33% wins where ties are half wins)
Keeping the new network

Training iteration 86 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.24 seconds
Training examples lengths: [64900, 64765, 64890, 64852, 64931, 64551, 65113, 64762, 64615, 64862]
Total value: 402728.77
Training on 648241 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3076 (value: 0.0017, weighted value: 0.0859, policy: 0.2217, weighted policy: 0.2217), Train Mean Max: 0.9092
Epoch 2/10, Train Loss: 0.2973 (value: 0.0016, weighted value: 0.0797, policy: 0.2177, weighted policy: 0.2177), Train Mean Max: 0.9095
Epoch 3/10, Train Loss: 0.2924 (value: 0.0015, weighted value: 0.0742, policy: 0.2182, weighted policy: 0.2182), Train Mean Max: 0.9097
Epoch 4/10, Train Loss: 0.2863 (value: 0.0014, weighted value: 0.0704, policy: 0.2159, weighted policy: 0.2159), Train Mean Max: 0.9101
Epoch 5/10, Train Loss: 0.2821 (value: 0.0013, weighted value: 0.0672, policy: 0.2149, weighted policy: 0.2149), Train Mean Max: 0.9103
Epoch 6/10, Train Loss: 0.2816 (value: 0.0014, weighted value: 0.0675, policy: 0.2141, weighted policy: 0.2141), Train Mean Max: 0.9105
Epoch 7/10, Train Loss: 0.2766 (value: 0.0013, weighted value: 0.0638, policy: 0.2129, weighted policy: 0.2129), Train Mean Max: 0.9108
Epoch 8/10, Train Loss: 0.2755 (value: 0.0013, weighted value: 0.0633, policy: 0.2122, weighted policy: 0.2122), Train Mean Max: 0.9110
Epoch 9/10, Train Loss: 0.2735 (value: 0.0012, weighted value: 0.0616, policy: 0.2119, weighted policy: 0.2119), Train Mean Max: 0.9111
Epoch 10/10, Train Loss: 0.2706 (value: 0.0012, weighted value: 0.0587, policy: 0.2120, weighted policy: 0.2120), Train Mean Max: 0.9114
..training done in 62.97 seconds
..evaluation done in 17.74 seconds
Old network+MCTS average reward: 0.6143, min: -0.0093, max: 1.1667, stdev: 0.2160
New network+MCTS average reward: 0.6211, min: -0.0093, max: 1.1574, stdev: 0.2150
Old bare network average reward: 0.5346, min: -0.0463, max: 1.0926, stdev: 0.2236
New bare network average reward: 0.5403, min: -0.0463, max: 1.1204, stdev: 0.2265
External policy "random" average reward: 0.2636, min: -0.2593, max: 0.8611, stdev: 0.2114
External policy "individual greedy" average reward: 0.5377, min: 0.0370, max: 1.2500, stdev: 0.2253
External policy "total greedy" average reward: 0.6475, min: 0.1111, max: 1.2685, stdev: 0.2208
New network won 94 and tied 131 out of 300 games (53.17% wins where ties are half wins)
Keeping the new network

Training iteration 87 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.62 seconds
Training examples lengths: [64765, 64890, 64852, 64931, 64551, 65113, 64762, 64615, 64862, 64866]
Total value: 403357.06
Training on 648207 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3007 (value: 0.0017, weighted value: 0.0830, policy: 0.2178, weighted policy: 0.2178), Train Mean Max: 0.9106
Epoch 2/10, Train Loss: 0.2910 (value: 0.0015, weighted value: 0.0753, policy: 0.2157, weighted policy: 0.2157), Train Mean Max: 0.9108
Epoch 3/10, Train Loss: 0.2866 (value: 0.0014, weighted value: 0.0722, policy: 0.2144, weighted policy: 0.2144), Train Mean Max: 0.9111
Epoch 4/10, Train Loss: 0.2805 (value: 0.0013, weighted value: 0.0673, policy: 0.2132, weighted policy: 0.2132), Train Mean Max: 0.9114
Epoch 5/10, Train Loss: 0.2779 (value: 0.0013, weighted value: 0.0672, policy: 0.2108, weighted policy: 0.2108), Train Mean Max: 0.9118
Epoch 6/10, Train Loss: 0.2742 (value: 0.0013, weighted value: 0.0637, policy: 0.2105, weighted policy: 0.2105), Train Mean Max: 0.9120
Epoch 7/10, Train Loss: 0.2718 (value: 0.0012, weighted value: 0.0614, policy: 0.2104, weighted policy: 0.2104), Train Mean Max: 0.9122
Epoch 8/10, Train Loss: 0.2699 (value: 0.0012, weighted value: 0.0606, policy: 0.2093, weighted policy: 0.2093), Train Mean Max: 0.9123
Epoch 9/10, Train Loss: 0.2687 (value: 0.0012, weighted value: 0.0598, policy: 0.2088, weighted policy: 0.2088), Train Mean Max: 0.9126
Epoch 10/10, Train Loss: 0.2663 (value: 0.0012, weighted value: 0.0579, policy: 0.2084, weighted policy: 0.2084), Train Mean Max: 0.9127
..training done in 63.89 seconds
..evaluation done in 17.37 seconds
Old network+MCTS average reward: 0.6185, min: -0.1759, max: 1.5093, stdev: 0.2328
New network+MCTS average reward: 0.6190, min: -0.1759, max: 1.5093, stdev: 0.2295
Old bare network average reward: 0.5365, min: -0.1944, max: 1.5093, stdev: 0.2305
New bare network average reward: 0.5426, min: -0.1111, max: 1.5093, stdev: 0.2322
External policy "random" average reward: 0.2562, min: -0.2407, max: 0.8889, stdev: 0.2146
External policy "individual greedy" average reward: 0.5357, min: -0.0833, max: 1.2778, stdev: 0.2223
External policy "total greedy" average reward: 0.6488, min: 0.0648, max: 1.4259, stdev: 0.2229
New network won 86 and tied 135 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 88 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.15 seconds
Training examples lengths: [64890, 64852, 64931, 64551, 65113, 64762, 64615, 64862, 64866, 64922]
Total value: 404087.59
Training on 648364 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2971 (value: 0.0016, weighted value: 0.0814, policy: 0.2157, weighted policy: 0.2157), Train Mean Max: 0.9119
Epoch 2/10, Train Loss: 0.2885 (value: 0.0015, weighted value: 0.0740, policy: 0.2145, weighted policy: 0.2145), Train Mean Max: 0.9121
Epoch 3/10, Train Loss: 0.2834 (value: 0.0014, weighted value: 0.0720, policy: 0.2114, weighted policy: 0.2114), Train Mean Max: 0.9124
Epoch 4/10, Train Loss: 0.2754 (value: 0.0013, weighted value: 0.0659, policy: 0.2095, weighted policy: 0.2095), Train Mean Max: 0.9127
Epoch 5/10, Train Loss: 0.2751 (value: 0.0013, weighted value: 0.0657, policy: 0.2094, weighted policy: 0.2094), Train Mean Max: 0.9128
Epoch 6/10, Train Loss: 0.2716 (value: 0.0013, weighted value: 0.0641, policy: 0.2075, weighted policy: 0.2075), Train Mean Max: 0.9130
Epoch 7/10, Train Loss: 0.2685 (value: 0.0012, weighted value: 0.0598, policy: 0.2086, weighted policy: 0.2086), Train Mean Max: 0.9132
Epoch 8/10, Train Loss: 0.2661 (value: 0.0012, weighted value: 0.0599, policy: 0.2062, weighted policy: 0.2062), Train Mean Max: 0.9135
Epoch 9/10, Train Loss: 0.2642 (value: 0.0012, weighted value: 0.0578, policy: 0.2064, weighted policy: 0.2064), Train Mean Max: 0.9138
Epoch 10/10, Train Loss: 0.2631 (value: 0.0011, weighted value: 0.0569, policy: 0.2062, weighted policy: 0.2062), Train Mean Max: 0.9138
..training done in 58.36 seconds
..evaluation done in 17.88 seconds
Old network+MCTS average reward: 0.6201, min: -0.0370, max: 1.6296, stdev: 0.2378
New network+MCTS average reward: 0.6222, min: -0.0278, max: 1.6296, stdev: 0.2358
Old bare network average reward: 0.5463, min: -0.0741, max: 1.6296, stdev: 0.2473
New bare network average reward: 0.5531, min: -0.1389, max: 1.6296, stdev: 0.2520
External policy "random" average reward: 0.2655, min: -0.2778, max: 1.2963, stdev: 0.2477
External policy "individual greedy" average reward: 0.5435, min: -0.1204, max: 1.6667, stdev: 0.2352
External policy "total greedy" average reward: 0.6548, min: 0.0556, max: 1.4537, stdev: 0.2352
New network won 100 and tied 120 out of 300 games (53.33% wins where ties are half wins)
Keeping the new network

Training iteration 89 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.85 seconds
Training examples lengths: [64852, 64931, 64551, 65113, 64762, 64615, 64862, 64866, 64922, 64767]
Total value: 404680.26
Training on 648241 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2988 (value: 0.0017, weighted value: 0.0850, policy: 0.2138, weighted policy: 0.2138), Train Mean Max: 0.9129
Epoch 2/10, Train Loss: 0.2830 (value: 0.0014, weighted value: 0.0722, policy: 0.2108, weighted policy: 0.2108), Train Mean Max: 0.9135
Epoch 3/10, Train Loss: 0.2781 (value: 0.0014, weighted value: 0.0699, policy: 0.2082, weighted policy: 0.2082), Train Mean Max: 0.9137
Epoch 4/10, Train Loss: 0.2745 (value: 0.0013, weighted value: 0.0664, policy: 0.2081, weighted policy: 0.2081), Train Mean Max: 0.9138
Epoch 5/10, Train Loss: 0.2718 (value: 0.0013, weighted value: 0.0649, policy: 0.2068, weighted policy: 0.2068), Train Mean Max: 0.9141
Epoch 6/10, Train Loss: 0.2688 (value: 0.0013, weighted value: 0.0630, policy: 0.2058, weighted policy: 0.2058), Train Mean Max: 0.9142
Epoch 7/10, Train Loss: 0.2666 (value: 0.0012, weighted value: 0.0613, policy: 0.2053, weighted policy: 0.2053), Train Mean Max: 0.9144
Epoch 8/10, Train Loss: 0.2634 (value: 0.0012, weighted value: 0.0580, policy: 0.2053, weighted policy: 0.2053), Train Mean Max: 0.9146
Epoch 9/10, Train Loss: 0.2633 (value: 0.0012, weighted value: 0.0590, policy: 0.2043, weighted policy: 0.2043), Train Mean Max: 0.9149
Epoch 10/10, Train Loss: 0.2587 (value: 0.0011, weighted value: 0.0560, policy: 0.2026, weighted policy: 0.2026), Train Mean Max: 0.9151
..training done in 66.36 seconds
..evaluation done in 20.99 seconds
Old network+MCTS average reward: 0.5957, min: 0.0463, max: 1.3148, stdev: 0.2224
New network+MCTS average reward: 0.6034, min: 0.0463, max: 1.3426, stdev: 0.2243
Old bare network average reward: 0.5240, min: -0.1296, max: 1.1944, stdev: 0.2357
New bare network average reward: 0.5294, min: -0.1204, max: 1.2315, stdev: 0.2321
External policy "random" average reward: 0.2400, min: -0.4167, max: 0.8981, stdev: 0.2269
External policy "individual greedy" average reward: 0.5212, min: 0.0463, max: 1.3241, stdev: 0.2212
External policy "total greedy" average reward: 0.6350, min: 0.1667, max: 1.2778, stdev: 0.2220
New network won 98 and tied 135 out of 300 games (55.17% wins where ties are half wins)
Keeping the new network

Training iteration 90 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.13 seconds
Training examples lengths: [64931, 64551, 65113, 64762, 64615, 64862, 64866, 64922, 64767, 64971]
Total value: 405196.41
Training on 648360 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2921 (value: 0.0016, weighted value: 0.0806, policy: 0.2114, weighted policy: 0.2114), Train Mean Max: 0.9139
Epoch 2/10, Train Loss: 0.2828 (value: 0.0015, weighted value: 0.0743, policy: 0.2085, weighted policy: 0.2085), Train Mean Max: 0.9144
Epoch 3/10, Train Loss: 0.2755 (value: 0.0014, weighted value: 0.0689, policy: 0.2067, weighted policy: 0.2067), Train Mean Max: 0.9148
Epoch 4/10, Train Loss: 0.2722 (value: 0.0013, weighted value: 0.0663, policy: 0.2059, weighted policy: 0.2059), Train Mean Max: 0.9149
Epoch 5/10, Train Loss: 0.2683 (value: 0.0013, weighted value: 0.0646, policy: 0.2038, weighted policy: 0.2038), Train Mean Max: 0.9153
Epoch 6/10, Train Loss: 0.2655 (value: 0.0012, weighted value: 0.0618, policy: 0.2037, weighted policy: 0.2037), Train Mean Max: 0.9153
Epoch 7/10, Train Loss: 0.2640 (value: 0.0012, weighted value: 0.0606, policy: 0.2034, weighted policy: 0.2034), Train Mean Max: 0.9154
Epoch 8/10, Train Loss: 0.2602 (value: 0.0012, weighted value: 0.0584, policy: 0.2018, weighted policy: 0.2018), Train Mean Max: 0.9156
Epoch 9/10, Train Loss: 0.2601 (value: 0.0012, weighted value: 0.0580, policy: 0.2021, weighted policy: 0.2021), Train Mean Max: 0.9158
Epoch 10/10, Train Loss: 0.2577 (value: 0.0011, weighted value: 0.0561, policy: 0.2017, weighted policy: 0.2017), Train Mean Max: 0.9160
..training done in 71.37 seconds
..evaluation done in 18.00 seconds
Old network+MCTS average reward: 0.6097, min: -0.0278, max: 1.3704, stdev: 0.2079
New network+MCTS average reward: 0.6145, min: 0.0278, max: 1.3704, stdev: 0.2071
Old bare network average reward: 0.5383, min: -0.0278, max: 1.2315, stdev: 0.2186
New bare network average reward: 0.5382, min: -0.0278, max: 1.3148, stdev: 0.2160
External policy "random" average reward: 0.2708, min: -0.3519, max: 0.8796, stdev: 0.2220
External policy "individual greedy" average reward: 0.5364, min: -0.0093, max: 1.1389, stdev: 0.2079
External policy "total greedy" average reward: 0.6525, min: 0.0000, max: 1.3148, stdev: 0.2101
New network won 95 and tied 135 out of 300 games (54.17% wins where ties are half wins)
Keeping the new network

Training iteration 91 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.57 seconds
Training examples lengths: [64551, 65113, 64762, 64615, 64862, 64866, 64922, 64767, 64971, 64699]
Total value: 405628.51
Training on 648128 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2915 (value: 0.0016, weighted value: 0.0823, policy: 0.2092, weighted policy: 0.2092), Train Mean Max: 0.9153
Epoch 2/10, Train Loss: 0.2795 (value: 0.0015, weighted value: 0.0737, policy: 0.2058, weighted policy: 0.2058), Train Mean Max: 0.9155
Epoch 3/10, Train Loss: 0.2727 (value: 0.0014, weighted value: 0.0696, policy: 0.2031, weighted policy: 0.2031), Train Mean Max: 0.9159
Epoch 4/10, Train Loss: 0.2707 (value: 0.0014, weighted value: 0.0686, policy: 0.2021, weighted policy: 0.2021), Train Mean Max: 0.9160
Epoch 5/10, Train Loss: 0.2673 (value: 0.0013, weighted value: 0.0650, policy: 0.2023, weighted policy: 0.2023), Train Mean Max: 0.9161
Epoch 6/10, Train Loss: 0.2624 (value: 0.0012, weighted value: 0.0615, policy: 0.2009, weighted policy: 0.2009), Train Mean Max: 0.9165
Epoch 7/10, Train Loss: 0.2616 (value: 0.0012, weighted value: 0.0617, policy: 0.2000, weighted policy: 0.2000), Train Mean Max: 0.9167
Epoch 8/10, Train Loss: 0.2574 (value: 0.0012, weighted value: 0.0579, policy: 0.1995, weighted policy: 0.1995), Train Mean Max: 0.9168
Epoch 9/10, Train Loss: 0.2576 (value: 0.0012, weighted value: 0.0583, policy: 0.1992, weighted policy: 0.1992), Train Mean Max: 0.9170
Epoch 10/10, Train Loss: 0.2553 (value: 0.0011, weighted value: 0.0563, policy: 0.1990, weighted policy: 0.1990), Train Mean Max: 0.9171
..training done in 63.94 seconds
..evaluation done in 18.76 seconds
Old network+MCTS average reward: 0.6306, min: 0.0648, max: 1.4167, stdev: 0.2242
New network+MCTS average reward: 0.6308, min: 0.0648, max: 1.4167, stdev: 0.2213
Old bare network average reward: 0.5576, min: -0.1204, max: 1.4167, stdev: 0.2288
New bare network average reward: 0.5553, min: -0.1204, max: 1.4167, stdev: 0.2317
External policy "random" average reward: 0.2886, min: -0.3056, max: 1.0093, stdev: 0.2340
External policy "individual greedy" average reward: 0.5500, min: -0.0926, max: 1.3611, stdev: 0.2285
External policy "total greedy" average reward: 0.6636, min: 0.1296, max: 1.6204, stdev: 0.2325
New network won 86 and tied 133 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 92 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.02 seconds
Training examples lengths: [65113, 64762, 64615, 64862, 64866, 64922, 64767, 64971, 64699, 64741]
Total value: 406057.23
Training on 648318 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2855 (value: 0.0016, weighted value: 0.0808, policy: 0.2047, weighted policy: 0.2047), Train Mean Max: 0.9161
Epoch 2/10, Train Loss: 0.2746 (value: 0.0014, weighted value: 0.0719, policy: 0.2027, weighted policy: 0.2027), Train Mean Max: 0.9165
Epoch 3/10, Train Loss: 0.2727 (value: 0.0014, weighted value: 0.0710, policy: 0.2016, weighted policy: 0.2016), Train Mean Max: 0.9167
Epoch 4/10, Train Loss: 0.2653 (value: 0.0013, weighted value: 0.0656, policy: 0.1997, weighted policy: 0.1997), Train Mean Max: 0.9170
Epoch 5/10, Train Loss: 0.2633 (value: 0.0013, weighted value: 0.0640, policy: 0.1993, weighted policy: 0.1993), Train Mean Max: 0.9171
Epoch 6/10, Train Loss: 0.2604 (value: 0.0012, weighted value: 0.0624, policy: 0.1979, weighted policy: 0.1979), Train Mean Max: 0.9173
Epoch 7/10, Train Loss: 0.2571 (value: 0.0012, weighted value: 0.0596, policy: 0.1975, weighted policy: 0.1975), Train Mean Max: 0.9175
Epoch 8/10, Train Loss: 0.2553 (value: 0.0012, weighted value: 0.0588, policy: 0.1965, weighted policy: 0.1965), Train Mean Max: 0.9179
Epoch 9/10, Train Loss: 0.2534 (value: 0.0011, weighted value: 0.0572, policy: 0.1962, weighted policy: 0.1962), Train Mean Max: 0.9178
Epoch 10/10, Train Loss: 0.2516 (value: 0.0011, weighted value: 0.0563, policy: 0.1953, weighted policy: 0.1953), Train Mean Max: 0.9182
..training done in 73.23 seconds
..evaluation done in 21.53 seconds
Old network+MCTS average reward: 0.6119, min: 0.0648, max: 1.1759, stdev: 0.2062
New network+MCTS average reward: 0.6106, min: 0.0093, max: 1.3056, stdev: 0.2085
Old bare network average reward: 0.5385, min: -0.0093, max: 1.1574, stdev: 0.2157
New bare network average reward: 0.5387, min: -0.0463, max: 1.1574, stdev: 0.2111
External policy "random" average reward: 0.2313, min: -0.5185, max: 0.8519, stdev: 0.2253
External policy "individual greedy" average reward: 0.5252, min: -0.0093, max: 1.3241, stdev: 0.2107
External policy "total greedy" average reward: 0.6363, min: 0.0185, max: 1.4444, stdev: 0.2108
New network won 77 and tied 146 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 93 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.82 seconds
Training examples lengths: [64762, 64615, 64862, 64866, 64922, 64767, 64971, 64699, 64741, 64583]
Total value: 406284.09
Training on 647788 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2858 (value: 0.0016, weighted value: 0.0806, policy: 0.2052, weighted policy: 0.2052), Train Mean Max: 0.9170
Epoch 2/10, Train Loss: 0.2759 (value: 0.0015, weighted value: 0.0735, policy: 0.2024, weighted policy: 0.2024), Train Mean Max: 0.9174
Epoch 3/10, Train Loss: 0.2683 (value: 0.0014, weighted value: 0.0685, policy: 0.1998, weighted policy: 0.1998), Train Mean Max: 0.9176
Epoch 4/10, Train Loss: 0.2635 (value: 0.0013, weighted value: 0.0654, policy: 0.1981, weighted policy: 0.1981), Train Mean Max: 0.9179
Epoch 5/10, Train Loss: 0.2619 (value: 0.0013, weighted value: 0.0646, policy: 0.1973, weighted policy: 0.1973), Train Mean Max: 0.9180
Epoch 6/10, Train Loss: 0.2580 (value: 0.0012, weighted value: 0.0616, policy: 0.1964, weighted policy: 0.1964), Train Mean Max: 0.9183
Epoch 7/10, Train Loss: 0.2576 (value: 0.0012, weighted value: 0.0607, policy: 0.1970, weighted policy: 0.1970), Train Mean Max: 0.9183
Epoch 8/10, Train Loss: 0.2531 (value: 0.0012, weighted value: 0.0576, policy: 0.1955, weighted policy: 0.1955), Train Mean Max: 0.9187
Epoch 9/10, Train Loss: 0.2526 (value: 0.0011, weighted value: 0.0571, policy: 0.1956, weighted policy: 0.1956), Train Mean Max: 0.9187
Epoch 10/10, Train Loss: 0.2505 (value: 0.0011, weighted value: 0.0558, policy: 0.1947, weighted policy: 0.1947), Train Mean Max: 0.9189
..training done in 63.12 seconds
..evaluation done in 17.53 seconds
Old network+MCTS average reward: 0.6039, min: -0.0833, max: 1.4167, stdev: 0.2310
New network+MCTS average reward: 0.6057, min: -0.2222, max: 1.3889, stdev: 0.2302
Old bare network average reward: 0.5314, min: -0.1111, max: 1.2963, stdev: 0.2420
New bare network average reward: 0.5321, min: -0.1019, max: 1.3796, stdev: 0.2372
External policy "random" average reward: 0.2372, min: -0.3981, max: 0.9630, stdev: 0.2325
External policy "individual greedy" average reward: 0.5209, min: -0.1389, max: 1.3519, stdev: 0.2346
External policy "total greedy" average reward: 0.6484, min: -0.1296, max: 1.4167, stdev: 0.2208
New network won 87 and tied 125 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 94 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.23 seconds
Training examples lengths: [64615, 64862, 64866, 64922, 64767, 64971, 64699, 64741, 64583, 64416]
Total value: 405371.44
Training on 647442 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3177 (value: 0.0021, weighted value: 0.1038, policy: 0.2139, weighted policy: 0.2139), Train Mean Max: 0.9157
Epoch 2/10, Train Loss: 0.3015 (value: 0.0019, weighted value: 0.0929, policy: 0.2086, weighted policy: 0.2086), Train Mean Max: 0.9161
Epoch 3/10, Train Loss: 0.2891 (value: 0.0017, weighted value: 0.0847, policy: 0.2045, weighted policy: 0.2045), Train Mean Max: 0.9163
Epoch 4/10, Train Loss: 0.2819 (value: 0.0016, weighted value: 0.0800, policy: 0.2019, weighted policy: 0.2019), Train Mean Max: 0.9167
Epoch 5/10, Train Loss: 0.2774 (value: 0.0015, weighted value: 0.0760, policy: 0.2014, weighted policy: 0.2014), Train Mean Max: 0.9169
Epoch 6/10, Train Loss: 0.2733 (value: 0.0015, weighted value: 0.0733, policy: 0.2000, weighted policy: 0.2000), Train Mean Max: 0.9170
Epoch 7/10, Train Loss: 0.2692 (value: 0.0014, weighted value: 0.0697, policy: 0.1995, weighted policy: 0.1995), Train Mean Max: 0.9171
Epoch 8/10, Train Loss: 0.2657 (value: 0.0013, weighted value: 0.0671, policy: 0.1986, weighted policy: 0.1986), Train Mean Max: 0.9174
Epoch 9/10, Train Loss: 0.2646 (value: 0.0013, weighted value: 0.0658, policy: 0.1987, weighted policy: 0.1987), Train Mean Max: 0.9176
Epoch 10/10, Train Loss: 0.2606 (value: 0.0013, weighted value: 0.0632, policy: 0.1974, weighted policy: 0.1974), Train Mean Max: 0.9177
..training done in 59.66 seconds
..evaluation done in 17.47 seconds
Old network+MCTS average reward: 0.6190, min: 0.0463, max: 1.2500, stdev: 0.2139
New network+MCTS average reward: 0.6221, min: 0.1019, max: 1.2500, stdev: 0.2088
Old bare network average reward: 0.5444, min: -0.1019, max: 1.2500, stdev: 0.2233
New bare network average reward: 0.5497, min: -0.1019, max: 1.2222, stdev: 0.2267
External policy "random" average reward: 0.2584, min: -0.2593, max: 0.9722, stdev: 0.2173
External policy "individual greedy" average reward: 0.5249, min: -0.0833, max: 1.1944, stdev: 0.2238
External policy "total greedy" average reward: 0.6410, min: 0.0370, max: 1.2500, stdev: 0.2100
New network won 85 and tied 131 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 95 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.79 seconds
Training examples lengths: [64862, 64866, 64922, 64767, 64971, 64699, 64741, 64583, 64416, 64619]
Total value: 405982.75
Training on 647446 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2932 (value: 0.0017, weighted value: 0.0870, policy: 0.2062, weighted policy: 0.2062), Train Mean Max: 0.9167
Epoch 2/10, Train Loss: 0.2809 (value: 0.0016, weighted value: 0.0787, policy: 0.2021, weighted policy: 0.2021), Train Mean Max: 0.9172
Epoch 3/10, Train Loss: 0.2735 (value: 0.0015, weighted value: 0.0743, policy: 0.1993, weighted policy: 0.1993), Train Mean Max: 0.9175
Epoch 4/10, Train Loss: 0.2716 (value: 0.0015, weighted value: 0.0735, policy: 0.1981, weighted policy: 0.1981), Train Mean Max: 0.9176
Epoch 5/10, Train Loss: 0.2652 (value: 0.0014, weighted value: 0.0679, policy: 0.1973, weighted policy: 0.1973), Train Mean Max: 0.9179
Epoch 6/10, Train Loss: 0.2622 (value: 0.0013, weighted value: 0.0660, policy: 0.1962, weighted policy: 0.1962), Train Mean Max: 0.9181
Epoch 7/10, Train Loss: 0.2595 (value: 0.0013, weighted value: 0.0638, policy: 0.1957, weighted policy: 0.1957), Train Mean Max: 0.9182
Epoch 8/10, Train Loss: 0.2586 (value: 0.0013, weighted value: 0.0635, policy: 0.1951, weighted policy: 0.1951), Train Mean Max: 0.9184
Epoch 9/10, Train Loss: 0.2561 (value: 0.0012, weighted value: 0.0602, policy: 0.1959, weighted policy: 0.1959), Train Mean Max: 0.9186
Epoch 10/10, Train Loss: 0.2542 (value: 0.0012, weighted value: 0.0598, policy: 0.1943, weighted policy: 0.1943), Train Mean Max: 0.9188
..training done in 59.29 seconds
..evaluation done in 17.48 seconds
Old network+MCTS average reward: 0.6262, min: 0.0648, max: 1.5556, stdev: 0.2230
New network+MCTS average reward: 0.6322, min: 0.0556, max: 1.6296, stdev: 0.2266
Old bare network average reward: 0.5597, min: 0.0093, max: 1.4722, stdev: 0.2329
New bare network average reward: 0.5573, min: -0.0926, max: 1.5741, stdev: 0.2408
External policy "random" average reward: 0.2679, min: -0.2685, max: 1.0185, stdev: 0.2357
External policy "individual greedy" average reward: 0.5385, min: -0.0648, max: 1.6389, stdev: 0.2385
External policy "total greedy" average reward: 0.6580, min: 0.0833, max: 1.5556, stdev: 0.2258
New network won 78 and tied 149 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 96 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.71 seconds
Training examples lengths: [64866, 64922, 64767, 64971, 64699, 64741, 64583, 64416, 64619, 64494]
Total value: 406040.56
Training on 647078 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2849 (value: 0.0017, weighted value: 0.0829, policy: 0.2020, weighted policy: 0.2020), Train Mean Max: 0.9179
Epoch 2/10, Train Loss: 0.2740 (value: 0.0015, weighted value: 0.0757, policy: 0.1983, weighted policy: 0.1983), Train Mean Max: 0.9182
Epoch 3/10, Train Loss: 0.2666 (value: 0.0014, weighted value: 0.0705, policy: 0.1962, weighted policy: 0.1962), Train Mean Max: 0.9186
Epoch 4/10, Train Loss: 0.2649 (value: 0.0014, weighted value: 0.0691, policy: 0.1958, weighted policy: 0.1958), Train Mean Max: 0.9186
Epoch 5/10, Train Loss: 0.2604 (value: 0.0013, weighted value: 0.0660, policy: 0.1944, weighted policy: 0.1944), Train Mean Max: 0.9189
Epoch 6/10, Train Loss: 0.2578 (value: 0.0013, weighted value: 0.0642, policy: 0.1935, weighted policy: 0.1935), Train Mean Max: 0.9191
Epoch 7/10, Train Loss: 0.2542 (value: 0.0012, weighted value: 0.0618, policy: 0.1925, weighted policy: 0.1925), Train Mean Max: 0.9193
Epoch 8/10, Train Loss: 0.2519 (value: 0.0012, weighted value: 0.0591, policy: 0.1928, weighted policy: 0.1928), Train Mean Max: 0.9194
Epoch 9/10, Train Loss: 0.2509 (value: 0.0012, weighted value: 0.0592, policy: 0.1918, weighted policy: 0.1918), Train Mean Max: 0.9197
Epoch 10/10, Train Loss: 0.2496 (value: 0.0011, weighted value: 0.0574, policy: 0.1923, weighted policy: 0.1923), Train Mean Max: 0.9197
..training done in 65.61 seconds
..evaluation done in 18.34 seconds
Old network+MCTS average reward: 0.6191, min: -0.0093, max: 1.8704, stdev: 0.2426
New network+MCTS average reward: 0.6229, min: 0.0556, max: 1.8704, stdev: 0.2432
Old bare network average reward: 0.5488, min: -0.0926, max: 1.5648, stdev: 0.2503
New bare network average reward: 0.5473, min: 0.0093, max: 1.5278, stdev: 0.2453
External policy "random" average reward: 0.2661, min: -0.2500, max: 1.3981, stdev: 0.2339
External policy "individual greedy" average reward: 0.5479, min: 0.0556, max: 1.5093, stdev: 0.2346
External policy "total greedy" average reward: 0.6580, min: 0.1296, max: 1.7500, stdev: 0.2296
New network won 85 and tied 145 out of 300 games (52.50% wins where ties are half wins)
Keeping the new network

Training iteration 97 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.81 seconds
Training examples lengths: [64922, 64767, 64971, 64699, 64741, 64583, 64416, 64619, 64494, 64740]
Total value: 405848.72
Training on 646952 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2813 (value: 0.0016, weighted value: 0.0807, policy: 0.2006, weighted policy: 0.2006), Train Mean Max: 0.9187
Epoch 2/10, Train Loss: 0.2744 (value: 0.0015, weighted value: 0.0763, policy: 0.1981, weighted policy: 0.1981), Train Mean Max: 0.9189
Epoch 3/10, Train Loss: 0.2659 (value: 0.0014, weighted value: 0.0710, policy: 0.1949, weighted policy: 0.1949), Train Mean Max: 0.9192
Epoch 4/10, Train Loss: 0.2605 (value: 0.0013, weighted value: 0.0661, policy: 0.1944, weighted policy: 0.1944), Train Mean Max: 0.9195
Epoch 5/10, Train Loss: 0.2584 (value: 0.0013, weighted value: 0.0655, policy: 0.1929, weighted policy: 0.1929), Train Mean Max: 0.9197
Epoch 6/10, Train Loss: 0.2555 (value: 0.0013, weighted value: 0.0643, policy: 0.1912, weighted policy: 0.1912), Train Mean Max: 0.9198
Epoch 7/10, Train Loss: 0.2515 (value: 0.0012, weighted value: 0.0602, policy: 0.1913, weighted policy: 0.1913), Train Mean Max: 0.9200
Epoch 8/10, Train Loss: 0.2483 (value: 0.0011, weighted value: 0.0575, policy: 0.1908, weighted policy: 0.1908), Train Mean Max: 0.9203
Epoch 9/10, Train Loss: 0.2498 (value: 0.0012, weighted value: 0.0591, policy: 0.1907, weighted policy: 0.1907), Train Mean Max: 0.9203
Epoch 10/10, Train Loss: 0.2478 (value: 0.0012, weighted value: 0.0580, policy: 0.1898, weighted policy: 0.1898), Train Mean Max: 0.9206
..training done in 59.14 seconds
..evaluation done in 17.97 seconds
Old network+MCTS average reward: 0.6335, min: -0.0926, max: 1.2963, stdev: 0.2140
New network+MCTS average reward: 0.6295, min: -0.1389, max: 1.2778, stdev: 0.2072
Old bare network average reward: 0.5659, min: -0.0648, max: 1.2963, stdev: 0.2172
New bare network average reward: 0.5600, min: -0.0648, max: 1.1759, stdev: 0.2111
External policy "random" average reward: 0.2553, min: -0.5000, max: 0.9907, stdev: 0.2270
External policy "individual greedy" average reward: 0.5527, min: -0.1296, max: 1.2407, stdev: 0.2249
External policy "total greedy" average reward: 0.6666, min: -0.0278, max: 1.3796, stdev: 0.2229
New network won 77 and tied 139 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 98 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.87 seconds
Training examples lengths: [64767, 64971, 64699, 64741, 64583, 64416, 64619, 64494, 64740, 64839]
Total value: 406106.13
Training on 646869 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.3151 (value: 0.0021, weighted value: 0.1066, policy: 0.2085, weighted policy: 0.2085), Train Mean Max: 0.9176
Epoch 2/10, Train Loss: 0.2950 (value: 0.0019, weighted value: 0.0927, policy: 0.2023, weighted policy: 0.2023), Train Mean Max: 0.9180
Epoch 3/10, Train Loss: 0.2834 (value: 0.0017, weighted value: 0.0839, policy: 0.1995, weighted policy: 0.1995), Train Mean Max: 0.9184
Epoch 4/10, Train Loss: 0.2771 (value: 0.0016, weighted value: 0.0800, policy: 0.1971, weighted policy: 0.1971), Train Mean Max: 0.9186
Epoch 5/10, Train Loss: 0.2711 (value: 0.0015, weighted value: 0.0755, policy: 0.1956, weighted policy: 0.1956), Train Mean Max: 0.9189
Epoch 6/10, Train Loss: 0.2681 (value: 0.0015, weighted value: 0.0734, policy: 0.1947, weighted policy: 0.1947), Train Mean Max: 0.9189
Epoch 7/10, Train Loss: 0.2638 (value: 0.0014, weighted value: 0.0699, policy: 0.1939, weighted policy: 0.1939), Train Mean Max: 0.9192
Epoch 8/10, Train Loss: 0.2637 (value: 0.0014, weighted value: 0.0705, policy: 0.1933, weighted policy: 0.1933), Train Mean Max: 0.9193
Epoch 9/10, Train Loss: 0.2556 (value: 0.0013, weighted value: 0.0641, policy: 0.1915, weighted policy: 0.1915), Train Mean Max: 0.9197
Epoch 10/10, Train Loss: 0.2574 (value: 0.0013, weighted value: 0.0653, policy: 0.1921, weighted policy: 0.1921), Train Mean Max: 0.9197
..training done in 59.27 seconds
..evaluation done in 17.68 seconds
Old network+MCTS average reward: 0.6431, min: -0.1296, max: 1.2963, stdev: 0.2268
New network+MCTS average reward: 0.6484, min: -0.1204, max: 1.2963, stdev: 0.2251
Old bare network average reward: 0.5766, min: -0.5185, max: 1.2593, stdev: 0.2397
New bare network average reward: 0.5803, min: -0.3148, max: 1.2963, stdev: 0.2369
External policy "random" average reward: 0.2777, min: -0.4259, max: 0.9444, stdev: 0.2311
External policy "individual greedy" average reward: 0.5535, min: -0.2037, max: 1.2407, stdev: 0.2448
External policy "total greedy" average reward: 0.6647, min: 0.0000, max: 1.2870, stdev: 0.2335
New network won 83 and tied 136 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 99 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.44 seconds
Training examples lengths: [64971, 64699, 64741, 64583, 64416, 64619, 64494, 64740, 64839, 64692]
Total value: 406813.79
Training on 646794 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2844 (value: 0.0017, weighted value: 0.0866, policy: 0.1979, weighted policy: 0.1979), Train Mean Max: 0.9190
Epoch 2/10, Train Loss: 0.2765 (value: 0.0016, weighted value: 0.0796, policy: 0.1969, weighted policy: 0.1969), Train Mean Max: 0.9192
Epoch 3/10, Train Loss: 0.2686 (value: 0.0015, weighted value: 0.0746, policy: 0.1940, weighted policy: 0.1940), Train Mean Max: 0.9195
Epoch 4/10, Train Loss: 0.2632 (value: 0.0014, weighted value: 0.0712, policy: 0.1920, weighted policy: 0.1920), Train Mean Max: 0.9198
Epoch 5/10, Train Loss: 0.2590 (value: 0.0014, weighted value: 0.0683, policy: 0.1907, weighted policy: 0.1907), Train Mean Max: 0.9201
Epoch 6/10, Train Loss: 0.2565 (value: 0.0013, weighted value: 0.0660, policy: 0.1905, weighted policy: 0.1905), Train Mean Max: 0.9202
Epoch 7/10, Train Loss: 0.2536 (value: 0.0013, weighted value: 0.0646, policy: 0.1891, weighted policy: 0.1891), Train Mean Max: 0.9205
Epoch 8/10, Train Loss: 0.2549 (value: 0.0013, weighted value: 0.0645, policy: 0.1904, weighted policy: 0.1904), Train Mean Max: 0.9206
Epoch 9/10, Train Loss: 0.2483 (value: 0.0012, weighted value: 0.0600, policy: 0.1883, weighted policy: 0.1883), Train Mean Max: 0.9210
Epoch 10/10, Train Loss: 0.2494 (value: 0.0012, weighted value: 0.0607, policy: 0.1887, weighted policy: 0.1887), Train Mean Max: 0.9210
..training done in 64.37 seconds
..evaluation done in 17.60 seconds
Old network+MCTS average reward: 0.6187, min: 0.0185, max: 1.5648, stdev: 0.2355
New network+MCTS average reward: 0.6194, min: 0.1019, max: 1.7222, stdev: 0.2382
Old bare network average reward: 0.5477, min: -0.1481, max: 1.5185, stdev: 0.2487
New bare network average reward: 0.5456, min: -0.0185, max: 1.7222, stdev: 0.2491
External policy "random" average reward: 0.2560, min: -0.2315, max: 1.2593, stdev: 0.2318
External policy "individual greedy" average reward: 0.5414, min: -0.0185, max: 1.3519, stdev: 0.2373
External policy "total greedy" average reward: 0.6446, min: 0.0370, max: 1.6019, stdev: 0.2341
New network won 79 and tied 139 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 100 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.73 seconds
Training examples lengths: [64699, 64741, 64583, 64416, 64619, 64494, 64740, 64839, 64692, 64845]
Total value: 406488.32
Training on 646668 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.3140 (value: 0.0022, weighted value: 0.1077, policy: 0.2063, weighted policy: 0.2063), Train Mean Max: 0.9177
Epoch 2/10, Train Loss: 0.2969 (value: 0.0019, weighted value: 0.0959, policy: 0.2010, weighted policy: 0.2010), Train Mean Max: 0.9183
Epoch 3/10, Train Loss: 0.2894 (value: 0.0018, weighted value: 0.0908, policy: 0.1986, weighted policy: 0.1986), Train Mean Max: 0.9186
Epoch 4/10, Train Loss: 0.2798 (value: 0.0017, weighted value: 0.0840, policy: 0.1958, weighted policy: 0.1958), Train Mean Max: 0.9189
Epoch 5/10, Train Loss: 0.2750 (value: 0.0016, weighted value: 0.0808, policy: 0.1943, weighted policy: 0.1943), Train Mean Max: 0.9191
Epoch 6/10, Train Loss: 0.2689 (value: 0.0015, weighted value: 0.0757, policy: 0.1931, weighted policy: 0.1931), Train Mean Max: 0.9193
Epoch 7/10, Train Loss: 0.2667 (value: 0.0015, weighted value: 0.0740, policy: 0.1927, weighted policy: 0.1927), Train Mean Max: 0.9196
Epoch 8/10, Train Loss: 0.2633 (value: 0.0014, weighted value: 0.0709, policy: 0.1923, weighted policy: 0.1923), Train Mean Max: 0.9198
Epoch 9/10, Train Loss: 0.2611 (value: 0.0014, weighted value: 0.0707, policy: 0.1903, weighted policy: 0.1903), Train Mean Max: 0.9200
Epoch 10/10, Train Loss: 0.2558 (value: 0.0013, weighted value: 0.0662, policy: 0.1896, weighted policy: 0.1896), Train Mean Max: 0.9202
..training done in 62.88 seconds
..evaluation done in 17.88 seconds
Old network+MCTS average reward: 0.6068, min: 0.0093, max: 1.1944, stdev: 0.2162
New network+MCTS average reward: 0.6102, min: 0.0093, max: 1.1852, stdev: 0.2225
Old bare network average reward: 0.5391, min: -0.0093, max: 1.1852, stdev: 0.2218
New bare network average reward: 0.5417, min: -0.0093, max: 1.1852, stdev: 0.2224
External policy "random" average reward: 0.2402, min: -0.3426, max: 0.9259, stdev: 0.2254
External policy "individual greedy" average reward: 0.5278, min: -0.1944, max: 1.1574, stdev: 0.2172
External policy "total greedy" average reward: 0.6401, min: 0.1111, max: 1.2130, stdev: 0.2158
New network won 98 and tied 128 out of 300 games (54.00% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_100

Training iteration 101 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.09 seconds
Training examples lengths: [64741, 64583, 64416, 64619, 64494, 64740, 64839, 64692, 64845, 64857]
Total value: 407160.11
Training on 646826 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2859 (value: 0.0018, weighted value: 0.0883, policy: 0.1976, weighted policy: 0.1976), Train Mean Max: 0.9192
Epoch 2/10, Train Loss: 0.2774 (value: 0.0016, weighted value: 0.0823, policy: 0.1951, weighted policy: 0.1951), Train Mean Max: 0.9195
Epoch 3/10, Train Loss: 0.2695 (value: 0.0015, weighted value: 0.0765, policy: 0.1930, weighted policy: 0.1930), Train Mean Max: 0.9198
Epoch 4/10, Train Loss: 0.2641 (value: 0.0015, weighted value: 0.0727, policy: 0.1913, weighted policy: 0.1913), Train Mean Max: 0.9202
Epoch 5/10, Train Loss: 0.2614 (value: 0.0014, weighted value: 0.0715, policy: 0.1899, weighted policy: 0.1899), Train Mean Max: 0.9204
Epoch 6/10, Train Loss: 0.2563 (value: 0.0013, weighted value: 0.0669, policy: 0.1894, weighted policy: 0.1894), Train Mean Max: 0.9206
Epoch 7/10, Train Loss: 0.2549 (value: 0.0013, weighted value: 0.0662, policy: 0.1887, weighted policy: 0.1887), Train Mean Max: 0.9208
Epoch 8/10, Train Loss: 0.2546 (value: 0.0013, weighted value: 0.0662, policy: 0.1884, weighted policy: 0.1884), Train Mean Max: 0.9208
Epoch 9/10, Train Loss: 0.2473 (value: 0.0012, weighted value: 0.0605, policy: 0.1868, weighted policy: 0.1868), Train Mean Max: 0.9215
Epoch 10/10, Train Loss: 0.2477 (value: 0.0012, weighted value: 0.0605, policy: 0.1871, weighted policy: 0.1871), Train Mean Max: 0.9214
..training done in 68.10 seconds
..evaluation done in 18.72 seconds
Old network+MCTS average reward: 0.6204, min: -0.0833, max: 1.2037, stdev: 0.2305
New network+MCTS average reward: 0.6198, min: -0.1667, max: 1.2037, stdev: 0.2265
Old bare network average reward: 0.5540, min: -0.1111, max: 1.1759, stdev: 0.2360
New bare network average reward: 0.5635, min: -0.1667, max: 1.1759, stdev: 0.2310
External policy "random" average reward: 0.2520, min: -0.4722, max: 0.8611, stdev: 0.2255
External policy "individual greedy" average reward: 0.5333, min: -0.0741, max: 1.1574, stdev: 0.2221
External policy "total greedy" average reward: 0.6452, min: -0.0093, max: 1.2685, stdev: 0.2220
New network won 73 and tied 147 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 102 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.69 seconds
Training examples lengths: [64583, 64416, 64619, 64494, 64740, 64839, 64692, 64845, 64857, 64646]
Total value: 407706.51
Training on 646731 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.3164 (value: 0.0022, weighted value: 0.1102, policy: 0.2061, weighted policy: 0.2061), Train Mean Max: 0.9181
Epoch 2/10, Train Loss: 0.2996 (value: 0.0020, weighted value: 0.0995, policy: 0.2001, weighted policy: 0.2001), Train Mean Max: 0.9186
Epoch 3/10, Train Loss: 0.2880 (value: 0.0018, weighted value: 0.0907, policy: 0.1973, weighted policy: 0.1973), Train Mean Max: 0.9190
Epoch 4/10, Train Loss: 0.2809 (value: 0.0017, weighted value: 0.0867, policy: 0.1941, weighted policy: 0.1941), Train Mean Max: 0.9193
Epoch 5/10, Train Loss: 0.2741 (value: 0.0016, weighted value: 0.0815, policy: 0.1926, weighted policy: 0.1926), Train Mean Max: 0.9194
Epoch 6/10, Train Loss: 0.2703 (value: 0.0016, weighted value: 0.0784, policy: 0.1919, weighted policy: 0.1919), Train Mean Max: 0.9197
Epoch 7/10, Train Loss: 0.2668 (value: 0.0015, weighted value: 0.0751, policy: 0.1917, weighted policy: 0.1917), Train Mean Max: 0.9199
Epoch 8/10, Train Loss: 0.2616 (value: 0.0014, weighted value: 0.0706, policy: 0.1910, weighted policy: 0.1910), Train Mean Max: 0.9202
Epoch 9/10, Train Loss: 0.2608 (value: 0.0014, weighted value: 0.0718, policy: 0.1890, weighted policy: 0.1890), Train Mean Max: 0.9205
Epoch 10/10, Train Loss: 0.2547 (value: 0.0013, weighted value: 0.0663, policy: 0.1884, weighted policy: 0.1884), Train Mean Max: 0.9206
..training done in 64.60 seconds
..evaluation done in 18.05 seconds
Old network+MCTS average reward: 0.6428, min: 0.1944, max: 1.4259, stdev: 0.2088
New network+MCTS average reward: 0.6432, min: 0.1481, max: 1.4259, stdev: 0.2140
Old bare network average reward: 0.5738, min: -0.0093, max: 1.4259, stdev: 0.2290
New bare network average reward: 0.5745, min: 0.0741, max: 1.3426, stdev: 0.2248
External policy "random" average reward: 0.2761, min: -0.3148, max: 1.1574, stdev: 0.2253
External policy "individual greedy" average reward: 0.5535, min: -0.0093, max: 1.4074, stdev: 0.2295
External policy "total greedy" average reward: 0.6793, min: 0.1204, max: 1.4537, stdev: 0.2217
New network won 91 and tied 130 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network

Training iteration 103 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.57 seconds
Training examples lengths: [64416, 64619, 64494, 64740, 64839, 64692, 64845, 64857, 64646, 64532]
Total value: 408189.06
Training on 646680 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2878 (value: 0.0018, weighted value: 0.0902, policy: 0.1976, weighted policy: 0.1976), Train Mean Max: 0.9195
Epoch 2/10, Train Loss: 0.2734 (value: 0.0016, weighted value: 0.0799, policy: 0.1935, weighted policy: 0.1935), Train Mean Max: 0.9202
Epoch 3/10, Train Loss: 0.2687 (value: 0.0015, weighted value: 0.0762, policy: 0.1925, weighted policy: 0.1925), Train Mean Max: 0.9204
Epoch 4/10, Train Loss: 0.2636 (value: 0.0015, weighted value: 0.0740, policy: 0.1896, weighted policy: 0.1896), Train Mean Max: 0.9208
Epoch 5/10, Train Loss: 0.2589 (value: 0.0014, weighted value: 0.0695, policy: 0.1894, weighted policy: 0.1894), Train Mean Max: 0.9209
Epoch 6/10, Train Loss: 0.2576 (value: 0.0014, weighted value: 0.0691, policy: 0.1885, weighted policy: 0.1885), Train Mean Max: 0.9210
Epoch 7/10, Train Loss: 0.2536 (value: 0.0013, weighted value: 0.0659, policy: 0.1877, weighted policy: 0.1877), Train Mean Max: 0.9213
Epoch 8/10, Train Loss: 0.2503 (value: 0.0012, weighted value: 0.0625, policy: 0.1879, weighted policy: 0.1879), Train Mean Max: 0.9215
Epoch 9/10, Train Loss: 0.2496 (value: 0.0013, weighted value: 0.0627, policy: 0.1869, weighted policy: 0.1869), Train Mean Max: 0.9215
Epoch 10/10, Train Loss: 0.2480 (value: 0.0012, weighted value: 0.0622, policy: 0.1857, weighted policy: 0.1857), Train Mean Max: 0.9221
..training done in 59.35 seconds
..evaluation done in 17.71 seconds
Old network+MCTS average reward: 0.6467, min: 0.1111, max: 1.3241, stdev: 0.2121
New network+MCTS average reward: 0.6433, min: -0.0648, max: 1.3241, stdev: 0.2154
Old bare network average reward: 0.5704, min: -0.1944, max: 1.3241, stdev: 0.2221
New bare network average reward: 0.5713, min: -0.1759, max: 1.3241, stdev: 0.2215
External policy "random" average reward: 0.2612, min: -0.2870, max: 0.9722, stdev: 0.2251
External policy "individual greedy" average reward: 0.5577, min: -0.0185, max: 1.2130, stdev: 0.2255
External policy "total greedy" average reward: 0.6715, min: 0.0185, max: 1.2130, stdev: 0.2150
New network won 82 and tied 129 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 104 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.10 seconds
Training examples lengths: [64619, 64494, 64740, 64839, 64692, 64845, 64857, 64646, 64532, 64589]
Total value: 409097.36
Training on 646853 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.3148 (value: 0.0022, weighted value: 0.1100, policy: 0.2047, weighted policy: 0.2047), Train Mean Max: 0.9187
Epoch 2/10, Train Loss: 0.2959 (value: 0.0019, weighted value: 0.0965, policy: 0.1995, weighted policy: 0.1995), Train Mean Max: 0.9193
Epoch 3/10, Train Loss: 0.2869 (value: 0.0018, weighted value: 0.0908, policy: 0.1961, weighted policy: 0.1961), Train Mean Max: 0.9196
Epoch 4/10, Train Loss: 0.2788 (value: 0.0017, weighted value: 0.0853, policy: 0.1934, weighted policy: 0.1934), Train Mean Max: 0.9198
Epoch 5/10, Train Loss: 0.2726 (value: 0.0016, weighted value: 0.0802, policy: 0.1924, weighted policy: 0.1924), Train Mean Max: 0.9202
Epoch 6/10, Train Loss: 0.2684 (value: 0.0016, weighted value: 0.0780, policy: 0.1904, weighted policy: 0.1904), Train Mean Max: 0.9204
Epoch 7/10, Train Loss: 0.2657 (value: 0.0015, weighted value: 0.0753, policy: 0.1904, weighted policy: 0.1904), Train Mean Max: 0.9206
Epoch 8/10, Train Loss: 0.2612 (value: 0.0014, weighted value: 0.0716, policy: 0.1897, weighted policy: 0.1897), Train Mean Max: 0.9208
Epoch 9/10, Train Loss: 0.2575 (value: 0.0014, weighted value: 0.0691, policy: 0.1884, weighted policy: 0.1884), Train Mean Max: 0.9210
Epoch 10/10, Train Loss: 0.2542 (value: 0.0013, weighted value: 0.0674, policy: 0.1868, weighted policy: 0.1868), Train Mean Max: 0.9213
..training done in 59.83 seconds
..evaluation done in 19.24 seconds
Old network+MCTS average reward: 0.6022, min: 0.0370, max: 1.3333, stdev: 0.2218
New network+MCTS average reward: 0.6067, min: 0.0648, max: 1.3704, stdev: 0.2191
Old bare network average reward: 0.5356, min: -0.0556, max: 1.3333, stdev: 0.2291
New bare network average reward: 0.5325, min: -0.0556, max: 1.3704, stdev: 0.2303
External policy "random" average reward: 0.2340, min: -0.4074, max: 1.1204, stdev: 0.2207
External policy "individual greedy" average reward: 0.5131, min: -0.1204, max: 1.3704, stdev: 0.2193
External policy "total greedy" average reward: 0.6249, min: 0.1019, max: 1.3611, stdev: 0.2041
New network won 88 and tied 134 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 105 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.52 seconds
Training examples lengths: [64494, 64740, 64839, 64692, 64845, 64857, 64646, 64532, 64589, 65056]
Total value: 409753.05
Training on 647290 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2851 (value: 0.0018, weighted value: 0.0898, policy: 0.1954, weighted policy: 0.1954), Train Mean Max: 0.9204
Epoch 2/10, Train Loss: 0.2745 (value: 0.0016, weighted value: 0.0800, policy: 0.1945, weighted policy: 0.1945), Train Mean Max: 0.9207
Epoch 3/10, Train Loss: 0.2661 (value: 0.0015, weighted value: 0.0757, policy: 0.1905, weighted policy: 0.1905), Train Mean Max: 0.9211
Epoch 4/10, Train Loss: 0.2660 (value: 0.0015, weighted value: 0.0759, policy: 0.1901, weighted policy: 0.1901), Train Mean Max: 0.9213
Epoch 5/10, Train Loss: 0.2593 (value: 0.0014, weighted value: 0.0709, policy: 0.1884, weighted policy: 0.1884), Train Mean Max: 0.9217
Epoch 6/10, Train Loss: 0.2554 (value: 0.0014, weighted value: 0.0683, policy: 0.1870, weighted policy: 0.1870), Train Mean Max: 0.9218
Epoch 7/10, Train Loss: 0.2525 (value: 0.0013, weighted value: 0.0660, policy: 0.1865, weighted policy: 0.1865), Train Mean Max: 0.9220
Epoch 8/10, Train Loss: 0.2509 (value: 0.0013, weighted value: 0.0651, policy: 0.1858, weighted policy: 0.1858), Train Mean Max: 0.9223
Epoch 9/10, Train Loss: 0.2489 (value: 0.0013, weighted value: 0.0631, policy: 0.1858, weighted policy: 0.1858), Train Mean Max: 0.9223
Epoch 10/10, Train Loss: 0.2462 (value: 0.0012, weighted value: 0.0620, policy: 0.1843, weighted policy: 0.1843), Train Mean Max: 0.9226
..training done in 65.72 seconds
..evaluation done in 17.68 seconds
Old network+MCTS average reward: 0.6411, min: 0.0370, max: 1.5185, stdev: 0.2239
New network+MCTS average reward: 0.6386, min: 0.0185, max: 1.3519, stdev: 0.2212
Old bare network average reward: 0.5731, min: -0.0370, max: 1.3519, stdev: 0.2268
New bare network average reward: 0.5723, min: -0.0463, max: 1.2685, stdev: 0.2285
External policy "random" average reward: 0.2580, min: -0.2963, max: 1.1389, stdev: 0.2191
External policy "individual greedy" average reward: 0.5433, min: -0.1389, max: 1.3519, stdev: 0.2331
External policy "total greedy" average reward: 0.6555, min: 0.0648, max: 1.4352, stdev: 0.2345
New network won 75 and tied 122 out of 300 games (45.33% wins where ties are half wins)
Reverting to the old network

Training iteration 106 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.07 seconds
Training examples lengths: [64740, 64839, 64692, 64845, 64857, 64646, 64532, 64589, 65056, 64750]
Total value: 410722.06
Training on 647546 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3132 (value: 0.0022, weighted value: 0.1105, policy: 0.2027, weighted policy: 0.2027), Train Mean Max: 0.9196
Epoch 2/10, Train Loss: 0.2958 (value: 0.0020, weighted value: 0.0978, policy: 0.1981, weighted policy: 0.1981), Train Mean Max: 0.9201
Epoch 3/10, Train Loss: 0.2844 (value: 0.0018, weighted value: 0.0907, policy: 0.1937, weighted policy: 0.1937), Train Mean Max: 0.9204
Epoch 4/10, Train Loss: 0.2776 (value: 0.0017, weighted value: 0.0858, policy: 0.1918, weighted policy: 0.1918), Train Mean Max: 0.9207
Epoch 5/10, Train Loss: 0.2714 (value: 0.0016, weighted value: 0.0808, policy: 0.1906, weighted policy: 0.1906), Train Mean Max: 0.9210
Epoch 6/10, Train Loss: 0.2654 (value: 0.0016, weighted value: 0.0776, policy: 0.1878, weighted policy: 0.1878), Train Mean Max: 0.9212
Epoch 7/10, Train Loss: 0.2625 (value: 0.0015, weighted value: 0.0752, policy: 0.1873, weighted policy: 0.1873), Train Mean Max: 0.9215
Epoch 8/10, Train Loss: 0.2595 (value: 0.0014, weighted value: 0.0721, policy: 0.1873, weighted policy: 0.1873), Train Mean Max: 0.9216
Epoch 9/10, Train Loss: 0.2554 (value: 0.0014, weighted value: 0.0691, policy: 0.1863, weighted policy: 0.1863), Train Mean Max: 0.9219
Epoch 10/10, Train Loss: 0.2534 (value: 0.0014, weighted value: 0.0684, policy: 0.1850, weighted policy: 0.1850), Train Mean Max: 0.9222
..training done in 66.39 seconds
..evaluation done in 17.79 seconds
Old network+MCTS average reward: 0.6619, min: -0.0741, max: 1.3519, stdev: 0.2350
New network+MCTS average reward: 0.6629, min: -0.0185, max: 1.4074, stdev: 0.2312
Old bare network average reward: 0.5855, min: -0.0741, max: 1.3333, stdev: 0.2337
New bare network average reward: 0.5944, min: -0.0741, max: 1.4074, stdev: 0.2347
External policy "random" average reward: 0.2804, min: -0.4815, max: 0.9167, stdev: 0.2410
External policy "individual greedy" average reward: 0.5737, min: 0.0093, max: 1.3241, stdev: 0.2347
External policy "total greedy" average reward: 0.6871, min: 0.0370, max: 1.4907, stdev: 0.2319
New network won 84 and tied 139 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 107 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.61 seconds
Training examples lengths: [64839, 64692, 64845, 64857, 64646, 64532, 64589, 65056, 64750, 64718]
Total value: 411206.89
Training on 647524 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2825 (value: 0.0018, weighted value: 0.0882, policy: 0.1943, weighted policy: 0.1943), Train Mean Max: 0.9213
Epoch 2/10, Train Loss: 0.2716 (value: 0.0016, weighted value: 0.0806, policy: 0.1910, weighted policy: 0.1910), Train Mean Max: 0.9215
Epoch 3/10, Train Loss: 0.2651 (value: 0.0015, weighted value: 0.0763, policy: 0.1888, weighted policy: 0.1888), Train Mean Max: 0.9220
Epoch 4/10, Train Loss: 0.2609 (value: 0.0015, weighted value: 0.0732, policy: 0.1877, weighted policy: 0.1877), Train Mean Max: 0.9221
Epoch 5/10, Train Loss: 0.2585 (value: 0.0014, weighted value: 0.0715, policy: 0.1870, weighted policy: 0.1870), Train Mean Max: 0.9223
Epoch 6/10, Train Loss: 0.2528 (value: 0.0014, weighted value: 0.0675, policy: 0.1853, weighted policy: 0.1853), Train Mean Max: 0.9228
Epoch 7/10, Train Loss: 0.2502 (value: 0.0013, weighted value: 0.0655, policy: 0.1847, weighted policy: 0.1847), Train Mean Max: 0.9228
Epoch 8/10, Train Loss: 0.2479 (value: 0.0013, weighted value: 0.0640, policy: 0.1839, weighted policy: 0.1839), Train Mean Max: 0.9231
Epoch 9/10, Train Loss: 0.2467 (value: 0.0013, weighted value: 0.0634, policy: 0.1833, weighted policy: 0.1833), Train Mean Max: 0.9232
Epoch 10/10, Train Loss: 0.2434 (value: 0.0012, weighted value: 0.0605, policy: 0.1829, weighted policy: 0.1829), Train Mean Max: 0.9235
..training done in 65.13 seconds
..evaluation done in 18.24 seconds
Old network+MCTS average reward: 0.6516, min: 0.1296, max: 1.4444, stdev: 0.2058
New network+MCTS average reward: 0.6518, min: 0.0833, max: 1.4444, stdev: 0.2106
Old bare network average reward: 0.5945, min: -0.1019, max: 1.4444, stdev: 0.2200
New bare network average reward: 0.5828, min: -0.0370, max: 1.4444, stdev: 0.2205
External policy "random" average reward: 0.2905, min: -0.3519, max: 1.0370, stdev: 0.2330
External policy "individual greedy" average reward: 0.5594, min: 0.0741, max: 1.3611, stdev: 0.2168
External policy "total greedy" average reward: 0.6777, min: 0.2222, max: 1.4722, stdev: 0.2116
New network won 97 and tied 118 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network

Training iteration 108 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.05 seconds
Training examples lengths: [64692, 64845, 64857, 64646, 64532, 64589, 65056, 64750, 64718, 64857]
Total value: 411862.31
Training on 647542 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2742 (value: 0.0016, weighted value: 0.0822, policy: 0.1920, weighted policy: 0.1920), Train Mean Max: 0.9225
Epoch 2/10, Train Loss: 0.2650 (value: 0.0015, weighted value: 0.0753, policy: 0.1897, weighted policy: 0.1897), Train Mean Max: 0.9227
Epoch 3/10, Train Loss: 0.2579 (value: 0.0014, weighted value: 0.0724, policy: 0.1855, weighted policy: 0.1855), Train Mean Max: 0.9231
Epoch 4/10, Train Loss: 0.2538 (value: 0.0014, weighted value: 0.0692, policy: 0.1846, weighted policy: 0.1846), Train Mean Max: 0.9233
Epoch 5/10, Train Loss: 0.2491 (value: 0.0013, weighted value: 0.0657, policy: 0.1834, weighted policy: 0.1834), Train Mean Max: 0.9235
Epoch 6/10, Train Loss: 0.2476 (value: 0.0013, weighted value: 0.0641, policy: 0.1835, weighted policy: 0.1835), Train Mean Max: 0.9236
Epoch 7/10, Train Loss: 0.2452 (value: 0.0013, weighted value: 0.0635, policy: 0.1817, weighted policy: 0.1817), Train Mean Max: 0.9238
Epoch 8/10, Train Loss: 0.2425 (value: 0.0012, weighted value: 0.0609, policy: 0.1816, weighted policy: 0.1816), Train Mean Max: 0.9240
Epoch 9/10, Train Loss: 0.2399 (value: 0.0012, weighted value: 0.0588, policy: 0.1812, weighted policy: 0.1812), Train Mean Max: 0.9240
Epoch 10/10, Train Loss: 0.2396 (value: 0.0012, weighted value: 0.0587, policy: 0.1809, weighted policy: 0.1809), Train Mean Max: 0.9244
..training done in 63.61 seconds
..evaluation done in 17.85 seconds
Old network+MCTS average reward: 0.6370, min: 0.0648, max: 1.5000, stdev: 0.2255
New network+MCTS average reward: 0.6367, min: 0.0185, max: 1.5000, stdev: 0.2264
Old bare network average reward: 0.5642, min: 0.0093, max: 1.5000, stdev: 0.2328
New bare network average reward: 0.5624, min: -0.0463, max: 1.3241, stdev: 0.2354
External policy "random" average reward: 0.2630, min: -0.3148, max: 1.0278, stdev: 0.2497
External policy "individual greedy" average reward: 0.5439, min: -0.0185, max: 1.3426, stdev: 0.2250
External policy "total greedy" average reward: 0.6544, min: 0.1389, max: 1.4537, stdev: 0.2157
New network won 73 and tied 132 out of 300 games (46.33% wins where ties are half wins)
Reverting to the old network

Training iteration 109 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.39 seconds
Training examples lengths: [64845, 64857, 64646, 64532, 64589, 65056, 64750, 64718, 64857, 64797]
Total value: 411931.98
Training on 647647 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3092 (value: 0.0021, weighted value: 0.1066, policy: 0.2025, weighted policy: 0.2025), Train Mean Max: 0.9209
Epoch 2/10, Train Loss: 0.2870 (value: 0.0018, weighted value: 0.0920, policy: 0.1950, weighted policy: 0.1950), Train Mean Max: 0.9215
Epoch 3/10, Train Loss: 0.2777 (value: 0.0017, weighted value: 0.0856, policy: 0.1921, weighted policy: 0.1921), Train Mean Max: 0.9218
Epoch 4/10, Train Loss: 0.2710 (value: 0.0016, weighted value: 0.0822, policy: 0.1887, weighted policy: 0.1887), Train Mean Max: 0.9222
Epoch 5/10, Train Loss: 0.2653 (value: 0.0016, weighted value: 0.0784, policy: 0.1869, weighted policy: 0.1869), Train Mean Max: 0.9222
Epoch 6/10, Train Loss: 0.2615 (value: 0.0015, weighted value: 0.0746, policy: 0.1869, weighted policy: 0.1869), Train Mean Max: 0.9224
Epoch 7/10, Train Loss: 0.2558 (value: 0.0014, weighted value: 0.0706, policy: 0.1851, weighted policy: 0.1851), Train Mean Max: 0.9228
Epoch 8/10, Train Loss: 0.2543 (value: 0.0014, weighted value: 0.0694, policy: 0.1849, weighted policy: 0.1849), Train Mean Max: 0.9229
Epoch 9/10, Train Loss: 0.2506 (value: 0.0013, weighted value: 0.0669, policy: 0.1837, weighted policy: 0.1837), Train Mean Max: 0.9231
Epoch 10/10, Train Loss: 0.2454 (value: 0.0013, weighted value: 0.0631, policy: 0.1823, weighted policy: 0.1823), Train Mean Max: 0.9235
..training done in 63.55 seconds
..evaluation done in 17.88 seconds
Old network+MCTS average reward: 0.6447, min: -0.0463, max: 1.3611, stdev: 0.2189
New network+MCTS average reward: 0.6437, min: -0.0463, max: 1.2778, stdev: 0.2184
Old bare network average reward: 0.5763, min: -0.1111, max: 1.2685, stdev: 0.2310
New bare network average reward: 0.5717, min: -0.1111, max: 1.2778, stdev: 0.2283
External policy "random" average reward: 0.2810, min: -0.2593, max: 0.8426, stdev: 0.2278
External policy "individual greedy" average reward: 0.5465, min: -0.0463, max: 1.2315, stdev: 0.2259
External policy "total greedy" average reward: 0.6637, min: 0.1111, max: 1.4444, stdev: 0.2123
New network won 84 and tied 127 out of 300 games (49.17% wins where ties are half wins)
Reverting to the old network

Training iteration 110 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.08 seconds
Training examples lengths: [64857, 64646, 64532, 64589, 65056, 64750, 64718, 64857, 64797, 64715]
Total value: 412910.14
Training on 647517 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3392 (value: 0.0026, weighted value: 0.1290, policy: 0.2102, weighted policy: 0.2102), Train Mean Max: 0.9197
Epoch 2/10, Train Loss: 0.3112 (value: 0.0022, weighted value: 0.1091, policy: 0.2022, weighted policy: 0.2022), Train Mean Max: 0.9203
Epoch 3/10, Train Loss: 0.2949 (value: 0.0020, weighted value: 0.0986, policy: 0.1963, weighted policy: 0.1963), Train Mean Max: 0.9208
Epoch 4/10, Train Loss: 0.2871 (value: 0.0019, weighted value: 0.0939, policy: 0.1932, weighted policy: 0.1932), Train Mean Max: 0.9209
Epoch 5/10, Train Loss: 0.2809 (value: 0.0018, weighted value: 0.0896, policy: 0.1913, weighted policy: 0.1913), Train Mean Max: 0.9211
Epoch 6/10, Train Loss: 0.2730 (value: 0.0017, weighted value: 0.0826, policy: 0.1904, weighted policy: 0.1904), Train Mean Max: 0.9213
Epoch 7/10, Train Loss: 0.2689 (value: 0.0016, weighted value: 0.0803, policy: 0.1885, weighted policy: 0.1885), Train Mean Max: 0.9216
Epoch 8/10, Train Loss: 0.2658 (value: 0.0015, weighted value: 0.0772, policy: 0.1886, weighted policy: 0.1886), Train Mean Max: 0.9216
Epoch 9/10, Train Loss: 0.2585 (value: 0.0015, weighted value: 0.0726, policy: 0.1859, weighted policy: 0.1859), Train Mean Max: 0.9222
Epoch 10/10, Train Loss: 0.2590 (value: 0.0014, weighted value: 0.0725, policy: 0.1865, weighted policy: 0.1865), Train Mean Max: 0.9222
..training done in 68.20 seconds
..evaluation done in 22.09 seconds
Old network+MCTS average reward: 0.6201, min: 0.0370, max: 1.2315, stdev: 0.2217
New network+MCTS average reward: 0.6269, min: 0.0741, max: 1.2315, stdev: 0.2197
Old bare network average reward: 0.5481, min: 0.0093, max: 1.2037, stdev: 0.2258
New bare network average reward: 0.5556, min: 0.0093, max: 1.1667, stdev: 0.2295
External policy "random" average reward: 0.2441, min: -0.4259, max: 0.9444, stdev: 0.2247
External policy "individual greedy" average reward: 0.5340, min: -0.1296, max: 1.2407, stdev: 0.2331
External policy "total greedy" average reward: 0.6375, min: 0.1019, max: 1.3426, stdev: 0.2298
New network won 104 and tied 125 out of 300 games (55.50% wins where ties are half wins)
Keeping the new network

Training iteration 111 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.60 seconds
Training examples lengths: [64646, 64532, 64589, 65056, 64750, 64718, 64857, 64797, 64715, 64759]
Total value: 412833.40
Training on 647419 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2862 (value: 0.0018, weighted value: 0.0908, policy: 0.1954, weighted policy: 0.1954), Train Mean Max: 0.9211
Epoch 2/10, Train Loss: 0.2740 (value: 0.0016, weighted value: 0.0823, policy: 0.1917, weighted policy: 0.1917), Train Mean Max: 0.9216
Epoch 3/10, Train Loss: 0.2688 (value: 0.0016, weighted value: 0.0794, policy: 0.1894, weighted policy: 0.1894), Train Mean Max: 0.9219
Epoch 4/10, Train Loss: 0.2634 (value: 0.0015, weighted value: 0.0764, policy: 0.1870, weighted policy: 0.1870), Train Mean Max: 0.9221
Epoch 5/10, Train Loss: 0.2577 (value: 0.0014, weighted value: 0.0714, policy: 0.1863, weighted policy: 0.1863), Train Mean Max: 0.9224
Epoch 6/10, Train Loss: 0.2559 (value: 0.0014, weighted value: 0.0699, policy: 0.1861, weighted policy: 0.1861), Train Mean Max: 0.9225
Epoch 7/10, Train Loss: 0.2547 (value: 0.0014, weighted value: 0.0688, policy: 0.1859, weighted policy: 0.1859), Train Mean Max: 0.9227
Epoch 8/10, Train Loss: 0.2484 (value: 0.0013, weighted value: 0.0648, policy: 0.1836, weighted policy: 0.1836), Train Mean Max: 0.9230
Epoch 9/10, Train Loss: 0.2476 (value: 0.0013, weighted value: 0.0646, policy: 0.1830, weighted policy: 0.1830), Train Mean Max: 0.9231
Epoch 10/10, Train Loss: 0.2458 (value: 0.0012, weighted value: 0.0624, policy: 0.1834, weighted policy: 0.1834), Train Mean Max: 0.9233
..training done in 77.84 seconds
..evaluation done in 31.32 seconds
Old network+MCTS average reward: 0.6257, min: 0.0833, max: 1.2685, stdev: 0.2294
New network+MCTS average reward: 0.6266, min: 0.0093, max: 1.2500, stdev: 0.2255
Old bare network average reward: 0.5586, min: 0.0000, max: 1.1944, stdev: 0.2365
New bare network average reward: 0.5522, min: -0.0370, max: 1.2778, stdev: 0.2355
External policy "random" average reward: 0.2699, min: -0.4352, max: 0.9444, stdev: 0.2250
External policy "individual greedy" average reward: 0.5417, min: -0.0370, max: 1.2407, stdev: 0.2256
External policy "total greedy" average reward: 0.6525, min: 0.1667, max: 1.2685, stdev: 0.2192
New network won 93 and tied 109 out of 300 games (49.17% wins where ties are half wins)
Reverting to the old network

Training iteration 112 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 60.48 seconds
Training examples lengths: [64532, 64589, 65056, 64750, 64718, 64857, 64797, 64715, 64759, 65010]
Total value: 413592.31
Training on 647783 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3147 (value: 0.0022, weighted value: 0.1112, policy: 0.2035, weighted policy: 0.2035), Train Mean Max: 0.9199
Epoch 2/10, Train Loss: 0.2974 (value: 0.0020, weighted value: 0.1003, policy: 0.1971, weighted policy: 0.1971), Train Mean Max: 0.9206
Epoch 3/10, Train Loss: 0.2844 (value: 0.0018, weighted value: 0.0915, policy: 0.1929, weighted policy: 0.1929), Train Mean Max: 0.9209
Epoch 4/10, Train Loss: 0.2797 (value: 0.0018, weighted value: 0.0882, policy: 0.1915, weighted policy: 0.1915), Train Mean Max: 0.9210
Epoch 5/10, Train Loss: 0.2728 (value: 0.0016, weighted value: 0.0824, policy: 0.1903, weighted policy: 0.1903), Train Mean Max: 0.9214
Epoch 6/10, Train Loss: 0.2673 (value: 0.0016, weighted value: 0.0791, policy: 0.1881, weighted policy: 0.1881), Train Mean Max: 0.9216
Epoch 7/10, Train Loss: 0.2632 (value: 0.0015, weighted value: 0.0765, policy: 0.1867, weighted policy: 0.1867), Train Mean Max: 0.9218
Epoch 8/10, Train Loss: 0.2600 (value: 0.0015, weighted value: 0.0725, policy: 0.1875, weighted policy: 0.1875), Train Mean Max: 0.9221
Epoch 9/10, Train Loss: 0.2550 (value: 0.0014, weighted value: 0.0702, policy: 0.1848, weighted policy: 0.1848), Train Mean Max: 0.9224
Epoch 10/10, Train Loss: 0.2553 (value: 0.0014, weighted value: 0.0700, policy: 0.1853, weighted policy: 0.1853), Train Mean Max: 0.9224
..training done in 60.71 seconds
..evaluation done in 17.69 seconds
Old network+MCTS average reward: 0.6165, min: 0.0370, max: 1.2037, stdev: 0.2109
New network+MCTS average reward: 0.6165, min: 0.0370, max: 1.2037, stdev: 0.2116
Old bare network average reward: 0.5577, min: 0.0093, max: 1.2037, stdev: 0.2172
New bare network average reward: 0.5577, min: -0.0093, max: 1.1667, stdev: 0.2230
External policy "random" average reward: 0.2667, min: -0.2778, max: 1.0370, stdev: 0.2200
External policy "individual greedy" average reward: 0.5239, min: -0.0278, max: 1.2778, stdev: 0.2220
External policy "total greedy" average reward: 0.6501, min: 0.0648, max: 1.3056, stdev: 0.2118
New network won 88 and tied 125 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 113 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.10 seconds
Training examples lengths: [64589, 65056, 64750, 64718, 64857, 64797, 64715, 64759, 65010, 64561]
Total value: 413936.90
Training on 647812 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2813 (value: 0.0018, weighted value: 0.0882, policy: 0.1931, weighted policy: 0.1931), Train Mean Max: 0.9213
Epoch 2/10, Train Loss: 0.2710 (value: 0.0016, weighted value: 0.0800, policy: 0.1910, weighted policy: 0.1910), Train Mean Max: 0.9217
Epoch 3/10, Train Loss: 0.2639 (value: 0.0015, weighted value: 0.0763, policy: 0.1877, weighted policy: 0.1877), Train Mean Max: 0.9222
Epoch 4/10, Train Loss: 0.2574 (value: 0.0014, weighted value: 0.0717, policy: 0.1857, weighted policy: 0.1857), Train Mean Max: 0.9224
Epoch 5/10, Train Loss: 0.2571 (value: 0.0014, weighted value: 0.0718, policy: 0.1853, weighted policy: 0.1853), Train Mean Max: 0.9225
Epoch 6/10, Train Loss: 0.2532 (value: 0.0014, weighted value: 0.0682, policy: 0.1851, weighted policy: 0.1851), Train Mean Max: 0.9227
Epoch 7/10, Train Loss: 0.2497 (value: 0.0013, weighted value: 0.0668, policy: 0.1830, weighted policy: 0.1830), Train Mean Max: 0.9229
Epoch 8/10, Train Loss: 0.2464 (value: 0.0013, weighted value: 0.0635, policy: 0.1829, weighted policy: 0.1829), Train Mean Max: 0.9232
Epoch 9/10, Train Loss: 0.2453 (value: 0.0012, weighted value: 0.0620, policy: 0.1832, weighted policy: 0.1832), Train Mean Max: 0.9234
Epoch 10/10, Train Loss: 0.2442 (value: 0.0012, weighted value: 0.0620, policy: 0.1822, weighted policy: 0.1822), Train Mean Max: 0.9236
..training done in 67.26 seconds
..evaluation done in 17.80 seconds
Old network+MCTS average reward: 0.6562, min: 0.0648, max: 1.4167, stdev: 0.2233
New network+MCTS average reward: 0.6598, min: 0.0648, max: 1.3889, stdev: 0.2258
Old bare network average reward: 0.5850, min: 0.0463, max: 1.4167, stdev: 0.2294
New bare network average reward: 0.5923, min: -0.0741, max: 1.4167, stdev: 0.2333
External policy "random" average reward: 0.2812, min: -0.2685, max: 0.9352, stdev: 0.2289
External policy "individual greedy" average reward: 0.5637, min: 0.0093, max: 1.4537, stdev: 0.2295
External policy "total greedy" average reward: 0.6731, min: 0.1759, max: 1.4167, stdev: 0.2202
New network won 85 and tied 143 out of 300 games (52.17% wins where ties are half wins)
Keeping the new network

Training iteration 114 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.32 seconds
Training examples lengths: [65056, 64750, 64718, 64857, 64797, 64715, 64759, 65010, 64561, 64854]
Total value: 414568.20
Training on 648077 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2717 (value: 0.0017, weighted value: 0.0834, policy: 0.1883, weighted policy: 0.1883), Train Mean Max: 0.9224
Epoch 2/10, Train Loss: 0.2615 (value: 0.0015, weighted value: 0.0744, policy: 0.1871, weighted policy: 0.1871), Train Mean Max: 0.9230
Epoch 3/10, Train Loss: 0.2569 (value: 0.0014, weighted value: 0.0723, policy: 0.1846, weighted policy: 0.1846), Train Mean Max: 0.9234
Epoch 4/10, Train Loss: 0.2505 (value: 0.0013, weighted value: 0.0672, policy: 0.1833, weighted policy: 0.1833), Train Mean Max: 0.9235
Epoch 5/10, Train Loss: 0.2477 (value: 0.0013, weighted value: 0.0661, policy: 0.1816, weighted policy: 0.1816), Train Mean Max: 0.9239
Epoch 6/10, Train Loss: 0.2443 (value: 0.0013, weighted value: 0.0629, policy: 0.1814, weighted policy: 0.1814), Train Mean Max: 0.9241
Epoch 7/10, Train Loss: 0.2422 (value: 0.0012, weighted value: 0.0623, policy: 0.1800, weighted policy: 0.1800), Train Mean Max: 0.9242
Epoch 8/10, Train Loss: 0.2414 (value: 0.0012, weighted value: 0.0612, policy: 0.1802, weighted policy: 0.1802), Train Mean Max: 0.9244
Epoch 9/10, Train Loss: 0.2378 (value: 0.0012, weighted value: 0.0589, policy: 0.1790, weighted policy: 0.1790), Train Mean Max: 0.9246
Epoch 10/10, Train Loss: 0.2352 (value: 0.0011, weighted value: 0.0565, policy: 0.1787, weighted policy: 0.1787), Train Mean Max: 0.9248
..training done in 60.00 seconds
..evaluation done in 18.74 seconds
Old network+MCTS average reward: 0.6540, min: 0.0185, max: 1.3796, stdev: 0.2227
New network+MCTS average reward: 0.6540, min: -0.0093, max: 1.3519, stdev: 0.2217
Old bare network average reward: 0.5921, min: 0.0278, max: 1.3796, stdev: 0.2312
New bare network average reward: 0.5971, min: -0.0556, max: 1.3519, stdev: 0.2261
External policy "random" average reward: 0.2681, min: -0.3241, max: 1.1019, stdev: 0.2312
External policy "individual greedy" average reward: 0.5560, min: -0.0648, max: 1.2870, stdev: 0.2351
External policy "total greedy" average reward: 0.6686, min: 0.1759, max: 1.3056, stdev: 0.2271
New network won 77 and tied 131 out of 300 games (47.50% wins where ties are half wins)
Reverting to the old network

Training iteration 115 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.58 seconds
Training examples lengths: [64750, 64718, 64857, 64797, 64715, 64759, 65010, 64561, 64854, 65032]
Total value: 414974.06
Training on 648053 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3026 (value: 0.0021, weighted value: 0.1051, policy: 0.1976, weighted policy: 0.1976), Train Mean Max: 0.9213
Epoch 2/10, Train Loss: 0.2853 (value: 0.0019, weighted value: 0.0927, policy: 0.1926, weighted policy: 0.1926), Train Mean Max: 0.9219
Epoch 3/10, Train Loss: 0.2742 (value: 0.0017, weighted value: 0.0852, policy: 0.1890, weighted policy: 0.1890), Train Mean Max: 0.9223
Epoch 4/10, Train Loss: 0.2681 (value: 0.0016, weighted value: 0.0815, policy: 0.1866, weighted policy: 0.1866), Train Mean Max: 0.9224
Epoch 5/10, Train Loss: 0.2618 (value: 0.0015, weighted value: 0.0768, policy: 0.1849, weighted policy: 0.1849), Train Mean Max: 0.9228
Epoch 6/10, Train Loss: 0.2573 (value: 0.0015, weighted value: 0.0733, policy: 0.1840, weighted policy: 0.1840), Train Mean Max: 0.9231
Epoch 7/10, Train Loss: 0.2547 (value: 0.0014, weighted value: 0.0705, policy: 0.1842, weighted policy: 0.1842), Train Mean Max: 0.9231
Epoch 8/10, Train Loss: 0.2521 (value: 0.0014, weighted value: 0.0694, policy: 0.1827, weighted policy: 0.1827), Train Mean Max: 0.9235
Epoch 9/10, Train Loss: 0.2484 (value: 0.0013, weighted value: 0.0669, policy: 0.1814, weighted policy: 0.1814), Train Mean Max: 0.9237
Epoch 10/10, Train Loss: 0.2449 (value: 0.0013, weighted value: 0.0641, policy: 0.1808, weighted policy: 0.1808), Train Mean Max: 0.9238
..training done in 58.46 seconds
..evaluation done in 19.27 seconds
Old network+MCTS average reward: 0.6429, min: 0.1481, max: 1.2407, stdev: 0.2175
New network+MCTS average reward: 0.6481, min: 0.1481, max: 1.3148, stdev: 0.2181
Old bare network average reward: 0.5674, min: 0.1019, max: 1.2315, stdev: 0.2256
New bare network average reward: 0.5754, min: 0.1019, max: 1.2315, stdev: 0.2248
External policy "random" average reward: 0.2644, min: -0.4630, max: 0.9352, stdev: 0.2069
External policy "individual greedy" average reward: 0.5539, min: 0.0370, max: 1.3056, stdev: 0.2204
External policy "total greedy" average reward: 0.6658, min: 0.1759, max: 1.3981, stdev: 0.2220
New network won 84 and tied 150 out of 300 games (53.00% wins where ties are half wins)
Keeping the new network

Training iteration 116 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.32 seconds
Training examples lengths: [64718, 64857, 64797, 64715, 64759, 65010, 64561, 64854, 65032, 65186]
Total value: 415631.02
Training on 648489 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2756 (value: 0.0017, weighted value: 0.0857, policy: 0.1899, weighted policy: 0.1899), Train Mean Max: 0.9227
Epoch 2/10, Train Loss: 0.2627 (value: 0.0015, weighted value: 0.0767, policy: 0.1860, weighted policy: 0.1860), Train Mean Max: 0.9233
Epoch 3/10, Train Loss: 0.2570 (value: 0.0015, weighted value: 0.0734, policy: 0.1836, weighted policy: 0.1836), Train Mean Max: 0.9234
Epoch 4/10, Train Loss: 0.2536 (value: 0.0014, weighted value: 0.0710, policy: 0.1827, weighted policy: 0.1827), Train Mean Max: 0.9236
Epoch 5/10, Train Loss: 0.2494 (value: 0.0014, weighted value: 0.0677, policy: 0.1818, weighted policy: 0.1818), Train Mean Max: 0.9239
Epoch 6/10, Train Loss: 0.2475 (value: 0.0013, weighted value: 0.0664, policy: 0.1812, weighted policy: 0.1812), Train Mean Max: 0.9241
Epoch 7/10, Train Loss: 0.2431 (value: 0.0013, weighted value: 0.0634, policy: 0.1796, weighted policy: 0.1796), Train Mean Max: 0.9246
Epoch 8/10, Train Loss: 0.2404 (value: 0.0012, weighted value: 0.0609, policy: 0.1795, weighted policy: 0.1795), Train Mean Max: 0.9244
Epoch 9/10, Train Loss: 0.2395 (value: 0.0012, weighted value: 0.0609, policy: 0.1786, weighted policy: 0.1786), Train Mean Max: 0.9248
Epoch 10/10, Train Loss: 0.2377 (value: 0.0012, weighted value: 0.0586, policy: 0.1791, weighted policy: 0.1791), Train Mean Max: 0.9250
..training done in 59.25 seconds
..evaluation done in 18.38 seconds
Old network+MCTS average reward: 0.6311, min: 0.0000, max: 1.2500, stdev: 0.2265
New network+MCTS average reward: 0.6345, min: -0.1296, max: 1.2500, stdev: 0.2285
Old bare network average reward: 0.5660, min: -0.0926, max: 1.1944, stdev: 0.2326
New bare network average reward: 0.5732, min: -0.0926, max: 1.1944, stdev: 0.2347
External policy "random" average reward: 0.2731, min: -0.2870, max: 0.9630, stdev: 0.2387
External policy "individual greedy" average reward: 0.5312, min: -0.1389, max: 1.2778, stdev: 0.2383
External policy "total greedy" average reward: 0.6519, min: -0.0833, max: 1.4722, stdev: 0.2371
New network won 82 and tied 149 out of 300 games (52.17% wins where ties are half wins)
Keeping the new network

Training iteration 117 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.99 seconds
Training examples lengths: [64857, 64797, 64715, 64759, 65010, 64561, 64854, 65032, 65186, 64781]
Total value: 416151.80
Training on 648552 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2695 (value: 0.0016, weighted value: 0.0822, policy: 0.1873, weighted policy: 0.1873), Train Mean Max: 0.9238
Epoch 2/10, Train Loss: 0.2587 (value: 0.0015, weighted value: 0.0735, policy: 0.1852, weighted policy: 0.1852), Train Mean Max: 0.9243
Epoch 3/10, Train Loss: 0.2512 (value: 0.0014, weighted value: 0.0695, policy: 0.1818, weighted policy: 0.1818), Train Mean Max: 0.9245
Epoch 4/10, Train Loss: 0.2488 (value: 0.0014, weighted value: 0.0676, policy: 0.1812, weighted policy: 0.1812), Train Mean Max: 0.9247
Epoch 5/10, Train Loss: 0.2438 (value: 0.0013, weighted value: 0.0646, policy: 0.1792, weighted policy: 0.1792), Train Mean Max: 0.9251
Epoch 6/10, Train Loss: 0.2418 (value: 0.0013, weighted value: 0.0633, policy: 0.1785, weighted policy: 0.1785), Train Mean Max: 0.9252
Epoch 7/10, Train Loss: 0.2400 (value: 0.0012, weighted value: 0.0615, policy: 0.1785, weighted policy: 0.1785), Train Mean Max: 0.9252
Epoch 8/10, Train Loss: 0.2379 (value: 0.0012, weighted value: 0.0600, policy: 0.1779, weighted policy: 0.1779), Train Mean Max: 0.9255
Epoch 9/10, Train Loss: 0.2349 (value: 0.0012, weighted value: 0.0576, policy: 0.1773, weighted policy: 0.1773), Train Mean Max: 0.9257
Epoch 10/10, Train Loss: 0.2308 (value: 0.0011, weighted value: 0.0555, policy: 0.1753, weighted policy: 0.1753), Train Mean Max: 0.9261
..training done in 59.84 seconds
..evaluation done in 17.81 seconds
Old network+MCTS average reward: 0.6295, min: -0.0463, max: 1.2500, stdev: 0.2221
New network+MCTS average reward: 0.6336, min: -0.0463, max: 1.3241, stdev: 0.2251
Old bare network average reward: 0.5623, min: -0.0833, max: 1.2130, stdev: 0.2352
New bare network average reward: 0.5583, min: -0.2037, max: 1.2130, stdev: 0.2298
External policy "random" average reward: 0.2441, min: -0.4167, max: 0.8889, stdev: 0.2232
External policy "individual greedy" average reward: 0.5337, min: 0.0093, max: 1.1852, stdev: 0.2335
External policy "total greedy" average reward: 0.6518, min: 0.1111, max: 1.3333, stdev: 0.2351
New network won 89 and tied 131 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 118 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.38 seconds
Training examples lengths: [64797, 64715, 64759, 65010, 64561, 64854, 65032, 65186, 64781, 64723]
Total value: 416451.20
Training on 648418 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2664 (value: 0.0016, weighted value: 0.0806, policy: 0.1859, weighted policy: 0.1859), Train Mean Max: 0.9245
Epoch 2/10, Train Loss: 0.2544 (value: 0.0014, weighted value: 0.0711, policy: 0.1833, weighted policy: 0.1833), Train Mean Max: 0.9250
Epoch 3/10, Train Loss: 0.2476 (value: 0.0013, weighted value: 0.0675, policy: 0.1801, weighted policy: 0.1801), Train Mean Max: 0.9253
Epoch 4/10, Train Loss: 0.2446 (value: 0.0013, weighted value: 0.0654, policy: 0.1792, weighted policy: 0.1792), Train Mean Max: 0.9255
Epoch 5/10, Train Loss: 0.2422 (value: 0.0013, weighted value: 0.0644, policy: 0.1778, weighted policy: 0.1778), Train Mean Max: 0.9257
Epoch 6/10, Train Loss: 0.2382 (value: 0.0012, weighted value: 0.0617, policy: 0.1766, weighted policy: 0.1766), Train Mean Max: 0.9260
Epoch 7/10, Train Loss: 0.2345 (value: 0.0012, weighted value: 0.0576, policy: 0.1769, weighted policy: 0.1769), Train Mean Max: 0.9262
Epoch 8/10, Train Loss: 0.2336 (value: 0.0011, weighted value: 0.0571, policy: 0.1764, weighted policy: 0.1764), Train Mean Max: 0.9264
Epoch 9/10, Train Loss: 0.2334 (value: 0.0011, weighted value: 0.0574, policy: 0.1760, weighted policy: 0.1760), Train Mean Max: 0.9264
Epoch 10/10, Train Loss: 0.2308 (value: 0.0011, weighted value: 0.0547, policy: 0.1760, weighted policy: 0.1760), Train Mean Max: 0.9266
..training done in 68.27 seconds
..evaluation done in 18.95 seconds
Old network+MCTS average reward: 0.6548, min: 0.0926, max: 1.2407, stdev: 0.2237
New network+MCTS average reward: 0.6570, min: 0.0926, max: 1.2130, stdev: 0.2196
Old bare network average reward: 0.5988, min: 0.0370, max: 1.2130, stdev: 0.2262
New bare network average reward: 0.5935, min: 0.0278, max: 1.1944, stdev: 0.2350
External policy "random" average reward: 0.2840, min: -0.3611, max: 1.0463, stdev: 0.2220
External policy "individual greedy" average reward: 0.5566, min: 0.0648, max: 1.2685, stdev: 0.2271
External policy "total greedy" average reward: 0.6701, min: 0.1759, max: 1.3241, stdev: 0.2230
New network won 79 and tied 141 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 119 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.92 seconds
Training examples lengths: [64715, 64759, 65010, 64561, 64854, 65032, 65186, 64781, 64723, 64650]
Total value: 416921.92
Training on 648271 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2950 (value: 0.0020, weighted value: 0.1006, policy: 0.1944, weighted policy: 0.1944), Train Mean Max: 0.9236
Epoch 2/10, Train Loss: 0.2763 (value: 0.0018, weighted value: 0.0878, policy: 0.1884, weighted policy: 0.1884), Train Mean Max: 0.9241
Epoch 3/10, Train Loss: 0.2666 (value: 0.0016, weighted value: 0.0824, policy: 0.1842, weighted policy: 0.1842), Train Mean Max: 0.9244
Epoch 4/10, Train Loss: 0.2613 (value: 0.0016, weighted value: 0.0787, policy: 0.1827, weighted policy: 0.1827), Train Mean Max: 0.9244
Epoch 5/10, Train Loss: 0.2550 (value: 0.0015, weighted value: 0.0734, policy: 0.1817, weighted policy: 0.1817), Train Mean Max: 0.9248
Epoch 6/10, Train Loss: 0.2503 (value: 0.0014, weighted value: 0.0713, policy: 0.1791, weighted policy: 0.1791), Train Mean Max: 0.9250
Epoch 7/10, Train Loss: 0.2478 (value: 0.0014, weighted value: 0.0682, policy: 0.1796, weighted policy: 0.1796), Train Mean Max: 0.9253
Epoch 8/10, Train Loss: 0.2438 (value: 0.0013, weighted value: 0.0659, policy: 0.1779, weighted policy: 0.1779), Train Mean Max: 0.9255
Epoch 9/10, Train Loss: 0.2401 (value: 0.0013, weighted value: 0.0638, policy: 0.1763, weighted policy: 0.1763), Train Mean Max: 0.9257
Epoch 10/10, Train Loss: 0.2401 (value: 0.0012, weighted value: 0.0616, policy: 0.1786, weighted policy: 0.1786), Train Mean Max: 0.9256
..training done in 68.00 seconds
..evaluation done in 18.07 seconds
Old network+MCTS average reward: 0.6342, min: -0.0741, max: 1.2685, stdev: 0.2284
New network+MCTS average reward: 0.6343, min: -0.0741, max: 1.3241, stdev: 0.2226
Old bare network average reward: 0.5651, min: -0.0648, max: 1.2685, stdev: 0.2286
New bare network average reward: 0.5647, min: -0.0648, max: 1.2685, stdev: 0.2288
External policy "random" average reward: 0.2477, min: -0.3241, max: 0.9074, stdev: 0.2140
External policy "individual greedy" average reward: 0.5337, min: -0.0926, max: 1.0648, stdev: 0.2242
External policy "total greedy" average reward: 0.6341, min: -0.0926, max: 1.2778, stdev: 0.2266
New network won 87 and tied 129 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 120 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.92 seconds
Training examples lengths: [64759, 65010, 64561, 64854, 65032, 65186, 64781, 64723, 64650, 64785]
Total value: 417331.81
Training on 648341 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2701 (value: 0.0017, weighted value: 0.0834, policy: 0.1867, weighted policy: 0.1867), Train Mean Max: 0.9245
Epoch 2/10, Train Loss: 0.2589 (value: 0.0015, weighted value: 0.0759, policy: 0.1830, weighted policy: 0.1830), Train Mean Max: 0.9251
Epoch 3/10, Train Loss: 0.2527 (value: 0.0014, weighted value: 0.0723, policy: 0.1803, weighted policy: 0.1803), Train Mean Max: 0.9252
Epoch 4/10, Train Loss: 0.2479 (value: 0.0014, weighted value: 0.0687, policy: 0.1792, weighted policy: 0.1792), Train Mean Max: 0.9255
Epoch 5/10, Train Loss: 0.2443 (value: 0.0013, weighted value: 0.0666, policy: 0.1777, weighted policy: 0.1777), Train Mean Max: 0.9258
Epoch 6/10, Train Loss: 0.2415 (value: 0.0013, weighted value: 0.0645, policy: 0.1769, weighted policy: 0.1769), Train Mean Max: 0.9259
Epoch 7/10, Train Loss: 0.2409 (value: 0.0013, weighted value: 0.0638, policy: 0.1771, weighted policy: 0.1771), Train Mean Max: 0.9259
Epoch 8/10, Train Loss: 0.2354 (value: 0.0012, weighted value: 0.0591, policy: 0.1763, weighted policy: 0.1763), Train Mean Max: 0.9262
Epoch 9/10, Train Loss: 0.2357 (value: 0.0012, weighted value: 0.0603, policy: 0.1754, weighted policy: 0.1754), Train Mean Max: 0.9265
Epoch 10/10, Train Loss: 0.2316 (value: 0.0011, weighted value: 0.0566, policy: 0.1750, weighted policy: 0.1750), Train Mean Max: 0.9267
..training done in 68.56 seconds
..evaluation done in 18.58 seconds
Old network+MCTS average reward: 0.6588, min: 0.0741, max: 1.3056, stdev: 0.2187
New network+MCTS average reward: 0.6546, min: -0.0093, max: 1.3056, stdev: 0.2237
Old bare network average reward: 0.5986, min: 0.0185, max: 1.3056, stdev: 0.2270
New bare network average reward: 0.5948, min: -0.0370, max: 1.3056, stdev: 0.2290
External policy "random" average reward: 0.2781, min: -0.3519, max: 1.1574, stdev: 0.2242
External policy "individual greedy" average reward: 0.5595, min: -0.0278, max: 1.3241, stdev: 0.2275
External policy "total greedy" average reward: 0.6652, min: 0.0741, max: 1.3796, stdev: 0.2192
New network won 67 and tied 151 out of 300 games (47.50% wins where ties are half wins)
Reverting to the old network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_120

Training iteration 121 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 62.60 seconds
Training examples lengths: [65010, 64561, 64854, 65032, 65186, 64781, 64723, 64650, 64785, 64487]
Total value: 417104.33
Training on 648069 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2994 (value: 0.0021, weighted value: 0.1049, policy: 0.1945, weighted policy: 0.1945), Train Mean Max: 0.9235
Epoch 2/10, Train Loss: 0.2794 (value: 0.0018, weighted value: 0.0911, policy: 0.1882, weighted policy: 0.1882), Train Mean Max: 0.9239
Epoch 3/10, Train Loss: 0.2711 (value: 0.0017, weighted value: 0.0856, policy: 0.1855, weighted policy: 0.1855), Train Mean Max: 0.9243
Epoch 4/10, Train Loss: 0.2629 (value: 0.0016, weighted value: 0.0809, policy: 0.1820, weighted policy: 0.1820), Train Mean Max: 0.9247
Epoch 5/10, Train Loss: 0.2584 (value: 0.0015, weighted value: 0.0768, policy: 0.1816, weighted policy: 0.1816), Train Mean Max: 0.9248
Epoch 6/10, Train Loss: 0.2533 (value: 0.0015, weighted value: 0.0736, policy: 0.1797, weighted policy: 0.1797), Train Mean Max: 0.9250
Epoch 7/10, Train Loss: 0.2503 (value: 0.0014, weighted value: 0.0708, policy: 0.1796, weighted policy: 0.1796), Train Mean Max: 0.9252
Epoch 8/10, Train Loss: 0.2461 (value: 0.0014, weighted value: 0.0679, policy: 0.1781, weighted policy: 0.1781), Train Mean Max: 0.9255
Epoch 9/10, Train Loss: 0.2419 (value: 0.0013, weighted value: 0.0651, policy: 0.1768, weighted policy: 0.1768), Train Mean Max: 0.9257
Epoch 10/10, Train Loss: 0.2414 (value: 0.0013, weighted value: 0.0645, policy: 0.1769, weighted policy: 0.1769), Train Mean Max: 0.9257
..training done in 73.56 seconds
..evaluation done in 18.65 seconds
Old network+MCTS average reward: 0.6395, min: 0.0278, max: 1.3333, stdev: 0.2267
New network+MCTS average reward: 0.6364, min: 0.0463, max: 1.3704, stdev: 0.2258
Old bare network average reward: 0.5682, min: -0.0278, max: 1.3241, stdev: 0.2369
New bare network average reward: 0.5750, min: 0.0093, max: 1.3241, stdev: 0.2386
External policy "random" average reward: 0.2691, min: -0.3519, max: 0.9722, stdev: 0.2234
External policy "individual greedy" average reward: 0.5530, min: 0.0556, max: 1.3333, stdev: 0.2249
External policy "total greedy" average reward: 0.6580, min: 0.1667, max: 1.3056, stdev: 0.2187
New network won 82 and tied 122 out of 300 games (47.67% wins where ties are half wins)
Reverting to the old network

Training iteration 122 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.72 seconds
Training examples lengths: [64561, 64854, 65032, 65186, 64781, 64723, 64650, 64785, 64487, 64971]
Total value: 417615.59
Training on 648030 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3282 (value: 0.0025, weighted value: 0.1265, policy: 0.2016, weighted policy: 0.2016), Train Mean Max: 0.9223
Epoch 2/10, Train Loss: 0.3016 (value: 0.0022, weighted value: 0.1085, policy: 0.1931, weighted policy: 0.1931), Train Mean Max: 0.9229
Epoch 3/10, Train Loss: 0.2864 (value: 0.0020, weighted value: 0.0986, policy: 0.1878, weighted policy: 0.1878), Train Mean Max: 0.9234
Epoch 4/10, Train Loss: 0.2787 (value: 0.0018, weighted value: 0.0920, policy: 0.1866, weighted policy: 0.1866), Train Mean Max: 0.9236
Epoch 5/10, Train Loss: 0.2717 (value: 0.0017, weighted value: 0.0873, policy: 0.1844, weighted policy: 0.1844), Train Mean Max: 0.9238
Epoch 6/10, Train Loss: 0.2646 (value: 0.0016, weighted value: 0.0818, policy: 0.1828, weighted policy: 0.1828), Train Mean Max: 0.9241
Epoch 7/10, Train Loss: 0.2615 (value: 0.0016, weighted value: 0.0805, policy: 0.1811, weighted policy: 0.1811), Train Mean Max: 0.9244
Epoch 8/10, Train Loss: 0.2555 (value: 0.0015, weighted value: 0.0748, policy: 0.1808, weighted policy: 0.1808), Train Mean Max: 0.9245
Epoch 9/10, Train Loss: 0.2528 (value: 0.0015, weighted value: 0.0730, policy: 0.1798, weighted policy: 0.1798), Train Mean Max: 0.9247
Epoch 10/10, Train Loss: 0.2509 (value: 0.0014, weighted value: 0.0705, policy: 0.1804, weighted policy: 0.1804), Train Mean Max: 0.9249
..training done in 59.21 seconds
..evaluation done in 17.80 seconds
Old network+MCTS average reward: 0.6334, min: 0.0648, max: 1.2500, stdev: 0.2163
New network+MCTS average reward: 0.6299, min: 0.0648, max: 1.2500, stdev: 0.2155
Old bare network average reward: 0.5753, min: -0.1019, max: 1.2500, stdev: 0.2249
New bare network average reward: 0.5723, min: -0.1019, max: 1.2500, stdev: 0.2297
External policy "random" average reward: 0.2629, min: -0.3611, max: 1.0648, stdev: 0.2258
External policy "individual greedy" average reward: 0.5352, min: 0.0463, max: 1.2037, stdev: 0.2191
External policy "total greedy" average reward: 0.6464, min: 0.0093, max: 1.2407, stdev: 0.2152
New network won 86 and tied 130 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 123 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.94 seconds
Training examples lengths: [64854, 65032, 65186, 64781, 64723, 64650, 64785, 64487, 64971, 65233]
Total value: 419011.10
Training on 648702 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2770 (value: 0.0018, weighted value: 0.0891, policy: 0.1878, weighted policy: 0.1878), Train Mean Max: 0.9241
Epoch 2/10, Train Loss: 0.2662 (value: 0.0017, weighted value: 0.0827, policy: 0.1834, weighted policy: 0.1834), Train Mean Max: 0.9245
Epoch 3/10, Train Loss: 0.2592 (value: 0.0016, weighted value: 0.0780, policy: 0.1812, weighted policy: 0.1812), Train Mean Max: 0.9248
Epoch 4/10, Train Loss: 0.2547 (value: 0.0015, weighted value: 0.0746, policy: 0.1801, weighted policy: 0.1801), Train Mean Max: 0.9248
Epoch 5/10, Train Loss: 0.2493 (value: 0.0014, weighted value: 0.0703, policy: 0.1789, weighted policy: 0.1789), Train Mean Max: 0.9253
Epoch 6/10, Train Loss: 0.2466 (value: 0.0014, weighted value: 0.0684, policy: 0.1782, weighted policy: 0.1782), Train Mean Max: 0.9256
Epoch 7/10, Train Loss: 0.2436 (value: 0.0013, weighted value: 0.0667, policy: 0.1770, weighted policy: 0.1770), Train Mean Max: 0.9257
Epoch 8/10, Train Loss: 0.2394 (value: 0.0013, weighted value: 0.0637, policy: 0.1757, weighted policy: 0.1757), Train Mean Max: 0.9260
Epoch 9/10, Train Loss: 0.2385 (value: 0.0013, weighted value: 0.0626, policy: 0.1759, weighted policy: 0.1759), Train Mean Max: 0.9261
Epoch 10/10, Train Loss: 0.2357 (value: 0.0012, weighted value: 0.0609, policy: 0.1748, weighted policy: 0.1748), Train Mean Max: 0.9264
..training done in 59.01 seconds
..evaluation done in 18.04 seconds
Old network+MCTS average reward: 0.6374, min: 0.0648, max: 1.2778, stdev: 0.2229
New network+MCTS average reward: 0.6375, min: 0.1019, max: 1.2593, stdev: 0.2203
Old bare network average reward: 0.5771, min: 0.0370, max: 1.2778, stdev: 0.2375
New bare network average reward: 0.5816, min: 0.0463, max: 1.2222, stdev: 0.2337
External policy "random" average reward: 0.2755, min: -0.3704, max: 1.0833, stdev: 0.2185
External policy "individual greedy" average reward: 0.5439, min: 0.0556, max: 1.1759, stdev: 0.2240
External policy "total greedy" average reward: 0.6628, min: 0.0648, max: 1.3426, stdev: 0.2235
New network won 91 and tied 117 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 124 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.75 seconds
Training examples lengths: [65032, 65186, 64781, 64723, 64650, 64785, 64487, 64971, 65233, 64930]
Total value: 419394.36
Training on 648778 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3066 (value: 0.0022, weighted value: 0.1110, policy: 0.1956, weighted policy: 0.1956), Train Mean Max: 0.9231
Epoch 2/10, Train Loss: 0.2882 (value: 0.0020, weighted value: 0.0986, policy: 0.1896, weighted policy: 0.1896), Train Mean Max: 0.9234
Epoch 3/10, Train Loss: 0.2762 (value: 0.0018, weighted value: 0.0906, policy: 0.1855, weighted policy: 0.1855), Train Mean Max: 0.9238
Epoch 4/10, Train Loss: 0.2692 (value: 0.0017, weighted value: 0.0845, policy: 0.1847, weighted policy: 0.1847), Train Mean Max: 0.9239
Epoch 5/10, Train Loss: 0.2642 (value: 0.0016, weighted value: 0.0805, policy: 0.1836, weighted policy: 0.1836), Train Mean Max: 0.9243
Epoch 6/10, Train Loss: 0.2599 (value: 0.0016, weighted value: 0.0783, policy: 0.1816, weighted policy: 0.1816), Train Mean Max: 0.9245
Epoch 7/10, Train Loss: 0.2532 (value: 0.0015, weighted value: 0.0744, policy: 0.1788, weighted policy: 0.1788), Train Mean Max: 0.9247
Epoch 8/10, Train Loss: 0.2520 (value: 0.0014, weighted value: 0.0724, policy: 0.1796, weighted policy: 0.1796), Train Mean Max: 0.9249
Epoch 9/10, Train Loss: 0.2458 (value: 0.0013, weighted value: 0.0674, policy: 0.1783, weighted policy: 0.1783), Train Mean Max: 0.9252
Epoch 10/10, Train Loss: 0.2449 (value: 0.0013, weighted value: 0.0664, policy: 0.1785, weighted policy: 0.1785), Train Mean Max: 0.9254
..training done in 60.57 seconds
..evaluation done in 19.05 seconds
Old network+MCTS average reward: 0.6310, min: 0.0741, max: 1.4074, stdev: 0.2251
New network+MCTS average reward: 0.6317, min: 0.0185, max: 1.4074, stdev: 0.2225
Old bare network average reward: 0.5623, min: -0.1389, max: 1.4074, stdev: 0.2287
New bare network average reward: 0.5692, min: -0.1204, max: 1.4444, stdev: 0.2390
External policy "random" average reward: 0.2595, min: -0.2778, max: 0.9722, stdev: 0.2214
External policy "individual greedy" average reward: 0.5240, min: -0.0370, max: 1.1296, stdev: 0.2225
External policy "total greedy" average reward: 0.6437, min: 0.0556, max: 1.2593, stdev: 0.2195
New network won 84 and tied 128 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 125 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 55.84 seconds
Training examples lengths: [65186, 64781, 64723, 64650, 64785, 64487, 64971, 65233, 64930, 64793]
Total value: 419644.72
Training on 648539 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3342 (value: 0.0026, weighted value: 0.1308, policy: 0.2034, weighted policy: 0.2034), Train Mean Max: 0.9217
Epoch 2/10, Train Loss: 0.3079 (value: 0.0022, weighted value: 0.1115, policy: 0.1964, weighted policy: 0.1964), Train Mean Max: 0.9224
Epoch 3/10, Train Loss: 0.2946 (value: 0.0021, weighted value: 0.1034, policy: 0.1913, weighted policy: 0.1913), Train Mean Max: 0.9227
Epoch 4/10, Train Loss: 0.2834 (value: 0.0019, weighted value: 0.0954, policy: 0.1880, weighted policy: 0.1880), Train Mean Max: 0.9229
Epoch 5/10, Train Loss: 0.2770 (value: 0.0018, weighted value: 0.0910, policy: 0.1860, weighted policy: 0.1860), Train Mean Max: 0.9232
Epoch 6/10, Train Loss: 0.2705 (value: 0.0017, weighted value: 0.0858, policy: 0.1847, weighted policy: 0.1847), Train Mean Max: 0.9235
Epoch 7/10, Train Loss: 0.2653 (value: 0.0017, weighted value: 0.0827, policy: 0.1826, weighted policy: 0.1826), Train Mean Max: 0.9238
Epoch 8/10, Train Loss: 0.2586 (value: 0.0015, weighted value: 0.0770, policy: 0.1815, weighted policy: 0.1815), Train Mean Max: 0.9240
Epoch 9/10, Train Loss: 0.2589 (value: 0.0015, weighted value: 0.0770, policy: 0.1819, weighted policy: 0.1819), Train Mean Max: 0.9240
Epoch 10/10, Train Loss: 0.2512 (value: 0.0014, weighted value: 0.0717, policy: 0.1795, weighted policy: 0.1795), Train Mean Max: 0.9245
..training done in 64.92 seconds
..evaluation done in 17.61 seconds
Old network+MCTS average reward: 0.6584, min: 0.0648, max: 1.2870, stdev: 0.2157
New network+MCTS average reward: 0.6581, min: 0.1111, max: 1.2685, stdev: 0.2154
Old bare network average reward: 0.5953, min: -0.0370, max: 1.2037, stdev: 0.2256
New bare network average reward: 0.5886, min: -0.0741, max: 1.2037, stdev: 0.2274
External policy "random" average reward: 0.2637, min: -0.2685, max: 0.8426, stdev: 0.2248
External policy "individual greedy" average reward: 0.5510, min: -0.1019, max: 1.1389, stdev: 0.2312
External policy "total greedy" average reward: 0.6699, min: 0.1204, max: 1.1759, stdev: 0.2194
New network won 87 and tied 129 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 126 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.22 seconds
Training examples lengths: [64781, 64723, 64650, 64785, 64487, 64971, 65233, 64930, 64793, 64719]
Total value: 419694.72
Training on 648072 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2819 (value: 0.0019, weighted value: 0.0934, policy: 0.1885, weighted policy: 0.1885), Train Mean Max: 0.9233
Epoch 2/10, Train Loss: 0.2712 (value: 0.0017, weighted value: 0.0860, policy: 0.1852, weighted policy: 0.1852), Train Mean Max: 0.9236
Epoch 3/10, Train Loss: 0.2639 (value: 0.0016, weighted value: 0.0800, policy: 0.1839, weighted policy: 0.1839), Train Mean Max: 0.9237
Epoch 4/10, Train Loss: 0.2589 (value: 0.0015, weighted value: 0.0763, policy: 0.1825, weighted policy: 0.1825), Train Mean Max: 0.9242
Epoch 5/10, Train Loss: 0.2529 (value: 0.0015, weighted value: 0.0735, policy: 0.1794, weighted policy: 0.1794), Train Mean Max: 0.9246
Epoch 6/10, Train Loss: 0.2500 (value: 0.0014, weighted value: 0.0705, policy: 0.1795, weighted policy: 0.1795), Train Mean Max: 0.9248
Epoch 7/10, Train Loss: 0.2479 (value: 0.0014, weighted value: 0.0690, policy: 0.1789, weighted policy: 0.1789), Train Mean Max: 0.9249
Epoch 8/10, Train Loss: 0.2441 (value: 0.0013, weighted value: 0.0655, policy: 0.1787, weighted policy: 0.1787), Train Mean Max: 0.9251
Epoch 9/10, Train Loss: 0.2421 (value: 0.0013, weighted value: 0.0653, policy: 0.1768, weighted policy: 0.1768), Train Mean Max: 0.9254
Epoch 10/10, Train Loss: 0.2395 (value: 0.0013, weighted value: 0.0633, policy: 0.1762, weighted policy: 0.1762), Train Mean Max: 0.9256
..training done in 59.25 seconds
..evaluation done in 17.87 seconds
Old network+MCTS average reward: 0.6436, min: 0.0648, max: 1.2963, stdev: 0.2268
New network+MCTS average reward: 0.6427, min: 0.0648, max: 1.3981, stdev: 0.2254
Old bare network average reward: 0.5776, min: -0.0648, max: 1.2130, stdev: 0.2360
New bare network average reward: 0.5778, min: -0.0648, max: 1.3241, stdev: 0.2365
External policy "random" average reward: 0.2684, min: -0.3611, max: 0.9537, stdev: 0.2344
External policy "individual greedy" average reward: 0.5423, min: -0.1296, max: 1.1204, stdev: 0.2387
External policy "total greedy" average reward: 0.6540, min: 0.0185, max: 1.3241, stdev: 0.2264
New network won 81 and tied 140 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 127 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.43 seconds
Training examples lengths: [64723, 64650, 64785, 64487, 64971, 65233, 64930, 64793, 64719, 64626]
Total value: 420329.15
Training on 647917 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2696 (value: 0.0017, weighted value: 0.0833, policy: 0.1863, weighted policy: 0.1863), Train Mean Max: 0.9243
Epoch 2/10, Train Loss: 0.2593 (value: 0.0015, weighted value: 0.0763, policy: 0.1830, weighted policy: 0.1830), Train Mean Max: 0.9247
Epoch 3/10, Train Loss: 0.2532 (value: 0.0015, weighted value: 0.0728, policy: 0.1804, weighted policy: 0.1804), Train Mean Max: 0.9250
Epoch 4/10, Train Loss: 0.2484 (value: 0.0014, weighted value: 0.0698, policy: 0.1786, weighted policy: 0.1786), Train Mean Max: 0.9252
Epoch 5/10, Train Loss: 0.2461 (value: 0.0014, weighted value: 0.0680, policy: 0.1781, weighted policy: 0.1781), Train Mean Max: 0.9254
Epoch 6/10, Train Loss: 0.2429 (value: 0.0013, weighted value: 0.0666, policy: 0.1763, weighted policy: 0.1763), Train Mean Max: 0.9257
Epoch 7/10, Train Loss: 0.2377 (value: 0.0012, weighted value: 0.0614, policy: 0.1764, weighted policy: 0.1764), Train Mean Max: 0.9259
Epoch 8/10, Train Loss: 0.2356 (value: 0.0012, weighted value: 0.0599, policy: 0.1757, weighted policy: 0.1757), Train Mean Max: 0.9260
Epoch 9/10, Train Loss: 0.2373 (value: 0.0012, weighted value: 0.0618, policy: 0.1755, weighted policy: 0.1755), Train Mean Max: 0.9263
Epoch 10/10, Train Loss: 0.2338 (value: 0.0012, weighted value: 0.0588, policy: 0.1750, weighted policy: 0.1750), Train Mean Max: 0.9265
..training done in 59.66 seconds
..evaluation done in 17.93 seconds
Old network+MCTS average reward: 0.6428, min: 0.1111, max: 1.3148, stdev: 0.2131
New network+MCTS average reward: 0.6489, min: 0.1204, max: 1.5093, stdev: 0.2183
Old bare network average reward: 0.5807, min: -0.0370, max: 1.3148, stdev: 0.2219
New bare network average reward: 0.5827, min: -0.0370, max: 1.5556, stdev: 0.2328
External policy "random" average reward: 0.2688, min: -0.2963, max: 1.2037, stdev: 0.2254
External policy "individual greedy" average reward: 0.5303, min: -0.1204, max: 1.4907, stdev: 0.2318
External policy "total greedy" average reward: 0.6527, min: 0.0278, max: 1.5463, stdev: 0.2192
New network won 94 and tied 127 out of 300 games (52.50% wins where ties are half wins)
Keeping the new network

Training iteration 128 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.50 seconds
Training examples lengths: [64650, 64785, 64487, 64971, 65233, 64930, 64793, 64719, 64626, 64748]
Total value: 420459.33
Training on 647942 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2616 (value: 0.0016, weighted value: 0.0782, policy: 0.1834, weighted policy: 0.1834), Train Mean Max: 0.9253
Epoch 2/10, Train Loss: 0.2514 (value: 0.0014, weighted value: 0.0709, policy: 0.1805, weighted policy: 0.1805), Train Mean Max: 0.9258
Epoch 3/10, Train Loss: 0.2460 (value: 0.0014, weighted value: 0.0682, policy: 0.1778, weighted policy: 0.1778), Train Mean Max: 0.9261
Epoch 4/10, Train Loss: 0.2430 (value: 0.0013, weighted value: 0.0662, policy: 0.1768, weighted policy: 0.1768), Train Mean Max: 0.9262
Epoch 5/10, Train Loss: 0.2380 (value: 0.0013, weighted value: 0.0626, policy: 0.1754, weighted policy: 0.1754), Train Mean Max: 0.9266
Epoch 6/10, Train Loss: 0.2352 (value: 0.0012, weighted value: 0.0610, policy: 0.1742, weighted policy: 0.1742), Train Mean Max: 0.9266
Epoch 7/10, Train Loss: 0.2356 (value: 0.0012, weighted value: 0.0609, policy: 0.1747, weighted policy: 0.1747), Train Mean Max: 0.9267
Epoch 8/10, Train Loss: 0.2315 (value: 0.0012, weighted value: 0.0583, policy: 0.1732, weighted policy: 0.1732), Train Mean Max: 0.9270
Epoch 9/10, Train Loss: 0.2292 (value: 0.0011, weighted value: 0.0558, policy: 0.1734, weighted policy: 0.1734), Train Mean Max: 0.9272
Epoch 10/10, Train Loss: 0.2274 (value: 0.0011, weighted value: 0.0549, policy: 0.1725, weighted policy: 0.1725), Train Mean Max: 0.9273
..training done in 59.44 seconds
..evaluation done in 18.41 seconds
Old network+MCTS average reward: 0.6356, min: -0.1944, max: 1.2593, stdev: 0.2211
New network+MCTS average reward: 0.6380, min: -0.3148, max: 1.3426, stdev: 0.2261
Old bare network average reward: 0.5747, min: -0.1852, max: 1.1481, stdev: 0.2306
New bare network average reward: 0.5744, min: -0.2407, max: 1.1944, stdev: 0.2306
External policy "random" average reward: 0.2637, min: -0.3519, max: 1.0556, stdev: 0.2295
External policy "individual greedy" average reward: 0.5440, min: -0.1019, max: 1.3426, stdev: 0.2360
External policy "total greedy" average reward: 0.6599, min: 0.0000, max: 1.3611, stdev: 0.2293
New network won 93 and tied 134 out of 300 games (53.33% wins where ties are half wins)
Keeping the new network

Training iteration 129 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.72 seconds
Training examples lengths: [64785, 64487, 64971, 65233, 64930, 64793, 64719, 64626, 64748, 64679]
Total value: 421164.00
Training on 647971 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2592 (value: 0.0016, weighted value: 0.0781, policy: 0.1811, weighted policy: 0.1811), Train Mean Max: 0.9263
Epoch 2/10, Train Loss: 0.2496 (value: 0.0014, weighted value: 0.0720, policy: 0.1777, weighted policy: 0.1777), Train Mean Max: 0.9268
Epoch 3/10, Train Loss: 0.2422 (value: 0.0013, weighted value: 0.0659, policy: 0.1763, weighted policy: 0.1763), Train Mean Max: 0.9269
Epoch 4/10, Train Loss: 0.2380 (value: 0.0013, weighted value: 0.0638, policy: 0.1742, weighted policy: 0.1742), Train Mean Max: 0.9272
Epoch 5/10, Train Loss: 0.2362 (value: 0.0012, weighted value: 0.0619, policy: 0.1743, weighted policy: 0.1743), Train Mean Max: 0.9274
Epoch 6/10, Train Loss: 0.2330 (value: 0.0012, weighted value: 0.0602, policy: 0.1728, weighted policy: 0.1728), Train Mean Max: 0.9276
Epoch 7/10, Train Loss: 0.2315 (value: 0.0012, weighted value: 0.0589, policy: 0.1726, weighted policy: 0.1726), Train Mean Max: 0.9277
Epoch 8/10, Train Loss: 0.2294 (value: 0.0011, weighted value: 0.0573, policy: 0.1721, weighted policy: 0.1721), Train Mean Max: 0.9279
Epoch 9/10, Train Loss: 0.2268 (value: 0.0011, weighted value: 0.0548, policy: 0.1719, weighted policy: 0.1719), Train Mean Max: 0.9280
Epoch 10/10, Train Loss: 0.2239 (value: 0.0011, weighted value: 0.0539, policy: 0.1700, weighted policy: 0.1700), Train Mean Max: 0.9282
..training done in 64.87 seconds
..evaluation done in 17.97 seconds
Old network+MCTS average reward: 0.6443, min: 0.1389, max: 1.2593, stdev: 0.2069
New network+MCTS average reward: 0.6442, min: -0.0093, max: 1.2593, stdev: 0.2105
Old bare network average reward: 0.5814, min: 0.0093, max: 1.1944, stdev: 0.2223
New bare network average reward: 0.5790, min: 0.0000, max: 1.1667, stdev: 0.2256
External policy "random" average reward: 0.2559, min: -0.3519, max: 0.9907, stdev: 0.2157
External policy "individual greedy" average reward: 0.5336, min: 0.0093, max: 1.2222, stdev: 0.2211
External policy "total greedy" average reward: 0.6460, min: 0.0185, max: 1.2593, stdev: 0.2098
New network won 83 and tied 135 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 130 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.32 seconds
Training examples lengths: [64487, 64971, 65233, 64930, 64793, 64719, 64626, 64748, 64679, 64778]
Total value: 421533.04
Training on 647964 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2575 (value: 0.0015, weighted value: 0.0762, policy: 0.1813, weighted policy: 0.1813), Train Mean Max: 0.9269
Epoch 2/10, Train Loss: 0.2449 (value: 0.0014, weighted value: 0.0684, policy: 0.1765, weighted policy: 0.1765), Train Mean Max: 0.9276
Epoch 3/10, Train Loss: 0.2399 (value: 0.0013, weighted value: 0.0658, policy: 0.1742, weighted policy: 0.1742), Train Mean Max: 0.9277
Epoch 4/10, Train Loss: 0.2359 (value: 0.0013, weighted value: 0.0631, policy: 0.1728, weighted policy: 0.1728), Train Mean Max: 0.9280
Epoch 5/10, Train Loss: 0.2321 (value: 0.0012, weighted value: 0.0600, policy: 0.1720, weighted policy: 0.1720), Train Mean Max: 0.9281
Epoch 6/10, Train Loss: 0.2304 (value: 0.0012, weighted value: 0.0591, policy: 0.1713, weighted policy: 0.1713), Train Mean Max: 0.9283
Epoch 7/10, Train Loss: 0.2282 (value: 0.0011, weighted value: 0.0566, policy: 0.1717, weighted policy: 0.1717), Train Mean Max: 0.9285
Epoch 8/10, Train Loss: 0.2269 (value: 0.0011, weighted value: 0.0561, policy: 0.1707, weighted policy: 0.1707), Train Mean Max: 0.9286
Epoch 9/10, Train Loss: 0.2239 (value: 0.0011, weighted value: 0.0543, policy: 0.1697, weighted policy: 0.1697), Train Mean Max: 0.9288
Epoch 10/10, Train Loss: 0.2236 (value: 0.0011, weighted value: 0.0528, policy: 0.1709, weighted policy: 0.1709), Train Mean Max: 0.9288
..training done in 64.92 seconds
..evaluation done in 18.29 seconds
Old network+MCTS average reward: 0.6565, min: 0.1204, max: 1.4907, stdev: 0.2242
New network+MCTS average reward: 0.6619, min: 0.0370, max: 1.4907, stdev: 0.2222
Old bare network average reward: 0.6001, min: 0.0926, max: 1.4352, stdev: 0.2310
New bare network average reward: 0.5982, min: 0.0556, max: 1.4352, stdev: 0.2303
External policy "random" average reward: 0.2715, min: -0.2407, max: 0.8796, stdev: 0.2181
External policy "individual greedy" average reward: 0.5581, min: 0.0000, max: 1.3889, stdev: 0.2187
External policy "total greedy" average reward: 0.6699, min: 0.0185, max: 1.4259, stdev: 0.2160
New network won 81 and tied 153 out of 300 games (52.50% wins where ties are half wins)
Keeping the new network

Training iteration 131 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.32 seconds
Training examples lengths: [64971, 65233, 64930, 64793, 64719, 64626, 64748, 64679, 64778, 64698]
Total value: 422232.82
Training on 648175 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2539 (value: 0.0015, weighted value: 0.0756, policy: 0.1783, weighted policy: 0.1783), Train Mean Max: 0.9276
Epoch 2/10, Train Loss: 0.2442 (value: 0.0014, weighted value: 0.0689, policy: 0.1752, weighted policy: 0.1752), Train Mean Max: 0.9281
Epoch 3/10, Train Loss: 0.2378 (value: 0.0013, weighted value: 0.0650, policy: 0.1728, weighted policy: 0.1728), Train Mean Max: 0.9283
Epoch 4/10, Train Loss: 0.2340 (value: 0.0012, weighted value: 0.0615, policy: 0.1725, weighted policy: 0.1725), Train Mean Max: 0.9285
Epoch 5/10, Train Loss: 0.2320 (value: 0.0012, weighted value: 0.0610, policy: 0.1710, weighted policy: 0.1710), Train Mean Max: 0.9286
Epoch 6/10, Train Loss: 0.2265 (value: 0.0012, weighted value: 0.0575, policy: 0.1689, weighted policy: 0.1689), Train Mean Max: 0.9291
Epoch 7/10, Train Loss: 0.2275 (value: 0.0011, weighted value: 0.0574, policy: 0.1701, weighted policy: 0.1701), Train Mean Max: 0.9291
Epoch 8/10, Train Loss: 0.2243 (value: 0.0011, weighted value: 0.0553, policy: 0.1690, weighted policy: 0.1690), Train Mean Max: 0.9292
Epoch 9/10, Train Loss: 0.2227 (value: 0.0011, weighted value: 0.0541, policy: 0.1686, weighted policy: 0.1686), Train Mean Max: 0.9293
Epoch 10/10, Train Loss: 0.2197 (value: 0.0010, weighted value: 0.0521, policy: 0.1676, weighted policy: 0.1676), Train Mean Max: 0.9297
..training done in 64.54 seconds
..evaluation done in 18.73 seconds
Old network+MCTS average reward: 0.6577, min: 0.0370, max: 1.3796, stdev: 0.2264
New network+MCTS average reward: 0.6629, min: -0.0185, max: 1.3611, stdev: 0.2257
Old bare network average reward: 0.6028, min: 0.0741, max: 1.3889, stdev: 0.2338
New bare network average reward: 0.6001, min: 0.0185, max: 1.3611, stdev: 0.2298
External policy "random" average reward: 0.2726, min: -0.2593, max: 0.9722, stdev: 0.2112
External policy "individual greedy" average reward: 0.5470, min: -0.0556, max: 1.1852, stdev: 0.2178
External policy "total greedy" average reward: 0.6712, min: 0.0926, max: 1.3889, stdev: 0.2282
New network won 95 and tied 132 out of 300 games (53.67% wins where ties are half wins)
Keeping the new network

Training iteration 132 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.24 seconds
Training examples lengths: [65233, 64930, 64793, 64719, 64626, 64748, 64679, 64778, 64698, 64846]
Total value: 422334.93
Training on 648050 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2537 (value: 0.0015, weighted value: 0.0740, policy: 0.1797, weighted policy: 0.1797), Train Mean Max: 0.9279
Epoch 2/10, Train Loss: 0.2445 (value: 0.0014, weighted value: 0.0687, policy: 0.1758, weighted policy: 0.1758), Train Mean Max: 0.9285
Epoch 3/10, Train Loss: 0.2366 (value: 0.0013, weighted value: 0.0640, policy: 0.1726, weighted policy: 0.1726), Train Mean Max: 0.9288
Epoch 4/10, Train Loss: 0.2339 (value: 0.0013, weighted value: 0.0625, policy: 0.1714, weighted policy: 0.1714), Train Mean Max: 0.9288
Epoch 5/10, Train Loss: 0.2291 (value: 0.0012, weighted value: 0.0590, policy: 0.1701, weighted policy: 0.1701), Train Mean Max: 0.9291
Epoch 6/10, Train Loss: 0.2282 (value: 0.0012, weighted value: 0.0584, policy: 0.1698, weighted policy: 0.1698), Train Mean Max: 0.9293
Epoch 7/10, Train Loss: 0.2241 (value: 0.0011, weighted value: 0.0556, policy: 0.1685, weighted policy: 0.1685), Train Mean Max: 0.9295
Epoch 8/10, Train Loss: 0.2241 (value: 0.0011, weighted value: 0.0557, policy: 0.1684, weighted policy: 0.1684), Train Mean Max: 0.9296
Epoch 9/10, Train Loss: 0.2222 (value: 0.0011, weighted value: 0.0545, policy: 0.1677, weighted policy: 0.1677), Train Mean Max: 0.9297
Epoch 10/10, Train Loss: 0.2190 (value: 0.0010, weighted value: 0.0518, policy: 0.1672, weighted policy: 0.1672), Train Mean Max: 0.9299
..training done in 63.04 seconds
..evaluation done in 17.89 seconds
Old network+MCTS average reward: 0.6362, min: 0.1389, max: 1.2593, stdev: 0.2157
New network+MCTS average reward: 0.6333, min: 0.1389, max: 1.2593, stdev: 0.2138
Old bare network average reward: 0.5784, min: 0.0556, max: 1.2593, stdev: 0.2133
New bare network average reward: 0.5784, min: 0.0463, max: 1.1944, stdev: 0.2144
External policy "random" average reward: 0.2627, min: -0.3056, max: 0.8056, stdev: 0.2080
External policy "individual greedy" average reward: 0.5319, min: -0.0556, max: 1.2315, stdev: 0.2169
External policy "total greedy" average reward: 0.6508, min: 0.0370, max: 1.2130, stdev: 0.2125
New network won 81 and tied 148 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 133 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 63.22 seconds
Training examples lengths: [64930, 64793, 64719, 64626, 64748, 64679, 64778, 64698, 64846, 64423]
Total value: 421568.81
Training on 647240 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2520 (value: 0.0015, weighted value: 0.0743, policy: 0.1777, weighted policy: 0.1777), Train Mean Max: 0.9282
Epoch 2/10, Train Loss: 0.2417 (value: 0.0013, weighted value: 0.0673, policy: 0.1744, weighted policy: 0.1744), Train Mean Max: 0.9290
Epoch 3/10, Train Loss: 0.2383 (value: 0.0013, weighted value: 0.0655, policy: 0.1728, weighted policy: 0.1728), Train Mean Max: 0.9290
Epoch 4/10, Train Loss: 0.2334 (value: 0.0012, weighted value: 0.0617, policy: 0.1718, weighted policy: 0.1718), Train Mean Max: 0.9292
Epoch 5/10, Train Loss: 0.2319 (value: 0.0012, weighted value: 0.0594, policy: 0.1725, weighted policy: 0.1725), Train Mean Max: 0.9294
Epoch 6/10, Train Loss: 0.2247 (value: 0.0011, weighted value: 0.0564, policy: 0.1682, weighted policy: 0.1682), Train Mean Max: 0.9300
Epoch 7/10, Train Loss: 0.2254 (value: 0.0011, weighted value: 0.0561, policy: 0.1693, weighted policy: 0.1693), Train Mean Max: 0.9298
Epoch 8/10, Train Loss: 0.2230 (value: 0.0011, weighted value: 0.0551, policy: 0.1679, weighted policy: 0.1679), Train Mean Max: 0.9302
Epoch 9/10, Train Loss: 0.2206 (value: 0.0011, weighted value: 0.0529, policy: 0.1677, weighted policy: 0.1677), Train Mean Max: 0.9302
Epoch 10/10, Train Loss: 0.2201 (value: 0.0010, weighted value: 0.0525, policy: 0.1676, weighted policy: 0.1676), Train Mean Max: 0.9303
..training done in 74.44 seconds
..evaluation done in 20.55 seconds
Old network+MCTS average reward: 0.6511, min: -0.1944, max: 1.5833, stdev: 0.2361
New network+MCTS average reward: 0.6481, min: -0.3611, max: 1.4630, stdev: 0.2328
Old bare network average reward: 0.5838, min: -0.2685, max: 1.4630, stdev: 0.2440
New bare network average reward: 0.5868, min: 0.0185, max: 1.4630, stdev: 0.2434
External policy "random" average reward: 0.2619, min: -0.4352, max: 0.8889, stdev: 0.2318
External policy "individual greedy" average reward: 0.5305, min: -0.0278, max: 1.4444, stdev: 0.2405
External policy "total greedy" average reward: 0.6501, min: 0.0926, max: 1.5556, stdev: 0.2344
New network won 78 and tied 143 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 134 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.88 seconds
Training examples lengths: [64793, 64719, 64626, 64748, 64679, 64778, 64698, 64846, 64423, 64831]
Total value: 422349.75
Training on 647141 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2827 (value: 0.0019, weighted value: 0.0961, policy: 0.1866, weighted policy: 0.1866), Train Mean Max: 0.9273
Epoch 2/10, Train Loss: 0.2652 (value: 0.0017, weighted value: 0.0853, policy: 0.1800, weighted policy: 0.1800), Train Mean Max: 0.9278
Epoch 3/10, Train Loss: 0.2532 (value: 0.0015, weighted value: 0.0773, policy: 0.1759, weighted policy: 0.1759), Train Mean Max: 0.9282
Epoch 4/10, Train Loss: 0.2491 (value: 0.0015, weighted value: 0.0744, policy: 0.1747, weighted policy: 0.1747), Train Mean Max: 0.9283
Epoch 5/10, Train Loss: 0.2404 (value: 0.0014, weighted value: 0.0686, policy: 0.1717, weighted policy: 0.1717), Train Mean Max: 0.9287
Epoch 6/10, Train Loss: 0.2377 (value: 0.0013, weighted value: 0.0668, policy: 0.1709, weighted policy: 0.1709), Train Mean Max: 0.9289
Epoch 7/10, Train Loss: 0.2365 (value: 0.0013, weighted value: 0.0660, policy: 0.1704, weighted policy: 0.1704), Train Mean Max: 0.9289
Epoch 8/10, Train Loss: 0.2320 (value: 0.0013, weighted value: 0.0628, policy: 0.1692, weighted policy: 0.1692), Train Mean Max: 0.9292
Epoch 9/10, Train Loss: 0.2287 (value: 0.0012, weighted value: 0.0596, policy: 0.1691, weighted policy: 0.1691), Train Mean Max: 0.9294
Epoch 10/10, Train Loss: 0.2285 (value: 0.0012, weighted value: 0.0599, policy: 0.1685, weighted policy: 0.1685), Train Mean Max: 0.9296
..training done in 60.26 seconds
..evaluation done in 18.37 seconds
Old network+MCTS average reward: 0.6359, min: 0.0648, max: 1.2963, stdev: 0.2169
New network+MCTS average reward: 0.6415, min: 0.0278, max: 1.3148, stdev: 0.2200
Old bare network average reward: 0.5729, min: 0.0278, max: 1.2500, stdev: 0.2247
New bare network average reward: 0.5740, min: -0.0556, max: 1.2037, stdev: 0.2251
External policy "random" average reward: 0.2578, min: -0.2870, max: 0.7963, stdev: 0.2129
External policy "individual greedy" average reward: 0.5298, min: -0.0741, max: 1.1944, stdev: 0.2148
External policy "total greedy" average reward: 0.6487, min: 0.1481, max: 1.3426, stdev: 0.2157
New network won 92 and tied 141 out of 300 games (54.17% wins where ties are half wins)
Keeping the new network

Training iteration 135 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.68 seconds
Training examples lengths: [64719, 64626, 64748, 64679, 64778, 64698, 64846, 64423, 64831, 64729]
Total value: 422737.99
Training on 647077 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2579 (value: 0.0016, weighted value: 0.0798, policy: 0.1780, weighted policy: 0.1780), Train Mean Max: 0.9283
Epoch 2/10, Train Loss: 0.2493 (value: 0.0015, weighted value: 0.0741, policy: 0.1752, weighted policy: 0.1752), Train Mean Max: 0.9285
Epoch 3/10, Train Loss: 0.2430 (value: 0.0014, weighted value: 0.0698, policy: 0.1731, weighted policy: 0.1731), Train Mean Max: 0.9288
Epoch 4/10, Train Loss: 0.2359 (value: 0.0013, weighted value: 0.0651, policy: 0.1708, weighted policy: 0.1708), Train Mean Max: 0.9291
Epoch 5/10, Train Loss: 0.2339 (value: 0.0013, weighted value: 0.0635, policy: 0.1704, weighted policy: 0.1704), Train Mean Max: 0.9291
Epoch 6/10, Train Loss: 0.2302 (value: 0.0012, weighted value: 0.0608, policy: 0.1694, weighted policy: 0.1694), Train Mean Max: 0.9295
Epoch 7/10, Train Loss: 0.2301 (value: 0.0012, weighted value: 0.0618, policy: 0.1683, weighted policy: 0.1683), Train Mean Max: 0.9294
Epoch 8/10, Train Loss: 0.2255 (value: 0.0011, weighted value: 0.0566, policy: 0.1689, weighted policy: 0.1689), Train Mean Max: 0.9297
Epoch 9/10, Train Loss: 0.2250 (value: 0.0011, weighted value: 0.0574, policy: 0.1676, weighted policy: 0.1676), Train Mean Max: 0.9299
Epoch 10/10, Train Loss: 0.2205 (value: 0.0011, weighted value: 0.0539, policy: 0.1666, weighted policy: 0.1666), Train Mean Max: 0.9302
..training done in 58.90 seconds
..evaluation done in 18.26 seconds
Old network+MCTS average reward: 0.6466, min: 0.0463, max: 1.3056, stdev: 0.2356
New network+MCTS average reward: 0.6407, min: 0.0093, max: 1.2963, stdev: 0.2359
Old bare network average reward: 0.5880, min: -0.0278, max: 1.2407, stdev: 0.2430
New bare network average reward: 0.5811, min: -0.0278, max: 1.2222, stdev: 0.2392
External policy "random" average reward: 0.2653, min: -0.2593, max: 0.8426, stdev: 0.2312
External policy "individual greedy" average reward: 0.5379, min: -0.0278, max: 1.1667, stdev: 0.2385
External policy "total greedy" average reward: 0.6531, min: 0.0370, max: 1.3056, stdev: 0.2329
New network won 68 and tied 142 out of 300 games (46.33% wins where ties are half wins)
Reverting to the old network

Training iteration 136 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.51 seconds
Training examples lengths: [64626, 64748, 64679, 64778, 64698, 64846, 64423, 64831, 64729, 64879]
Total value: 423035.09
Training on 647237 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2924 (value: 0.0021, weighted value: 0.1033, policy: 0.1890, weighted policy: 0.1890), Train Mean Max: 0.9266
Epoch 2/10, Train Loss: 0.2744 (value: 0.0018, weighted value: 0.0897, policy: 0.1847, weighted policy: 0.1847), Train Mean Max: 0.9271
Epoch 3/10, Train Loss: 0.2630 (value: 0.0017, weighted value: 0.0826, policy: 0.1804, weighted policy: 0.1804), Train Mean Max: 0.9275
Epoch 4/10, Train Loss: 0.2561 (value: 0.0016, weighted value: 0.0790, policy: 0.1770, weighted policy: 0.1770), Train Mean Max: 0.9278
Epoch 5/10, Train Loss: 0.2511 (value: 0.0015, weighted value: 0.0745, policy: 0.1766, weighted policy: 0.1766), Train Mean Max: 0.9279
Epoch 6/10, Train Loss: 0.2446 (value: 0.0014, weighted value: 0.0703, policy: 0.1742, weighted policy: 0.1742), Train Mean Max: 0.9281
Epoch 7/10, Train Loss: 0.2424 (value: 0.0014, weighted value: 0.0680, policy: 0.1743, weighted policy: 0.1743), Train Mean Max: 0.9284
Epoch 8/10, Train Loss: 0.2366 (value: 0.0013, weighted value: 0.0656, policy: 0.1709, weighted policy: 0.1709), Train Mean Max: 0.9288
Epoch 9/10, Train Loss: 0.2371 (value: 0.0013, weighted value: 0.0653, policy: 0.1719, weighted policy: 0.1719), Train Mean Max: 0.9288
Epoch 10/10, Train Loss: 0.2314 (value: 0.0012, weighted value: 0.0608, policy: 0.1706, weighted policy: 0.1706), Train Mean Max: 0.9289
..training done in 67.16 seconds
..evaluation done in 21.30 seconds
Old network+MCTS average reward: 0.6481, min: 0.0185, max: 1.2593, stdev: 0.2175
New network+MCTS average reward: 0.6469, min: 0.0833, max: 1.2593, stdev: 0.2122
Old bare network average reward: 0.5870, min: -0.0185, max: 1.2593, stdev: 0.2210
New bare network average reward: 0.5843, min: -0.0185, max: 1.1759, stdev: 0.2252
External policy "random" average reward: 0.2619, min: -0.3704, max: 0.8889, stdev: 0.2229
External policy "individual greedy" average reward: 0.5358, min: -0.0185, max: 1.2500, stdev: 0.2216
External policy "total greedy" average reward: 0.6586, min: 0.1204, max: 1.2593, stdev: 0.2123
New network won 87 and tied 109 out of 300 games (47.17% wins where ties are half wins)
Reverting to the old network

Training iteration 137 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.57 seconds
Training examples lengths: [64748, 64679, 64778, 64698, 64846, 64423, 64831, 64729, 64879, 64845]
Total value: 422933.72
Training on 647456 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3222 (value: 0.0025, weighted value: 0.1240, policy: 0.1982, weighted policy: 0.1982), Train Mean Max: 0.9253
Epoch 2/10, Train Loss: 0.2934 (value: 0.0021, weighted value: 0.1051, policy: 0.1883, weighted policy: 0.1883), Train Mean Max: 0.9260
Epoch 3/10, Train Loss: 0.2805 (value: 0.0019, weighted value: 0.0967, policy: 0.1838, weighted policy: 0.1838), Train Mean Max: 0.9264
Epoch 4/10, Train Loss: 0.2686 (value: 0.0018, weighted value: 0.0890, policy: 0.1795, weighted policy: 0.1795), Train Mean Max: 0.9267
Epoch 5/10, Train Loss: 0.2635 (value: 0.0017, weighted value: 0.0848, policy: 0.1787, weighted policy: 0.1787), Train Mean Max: 0.9268
Epoch 6/10, Train Loss: 0.2575 (value: 0.0016, weighted value: 0.0805, policy: 0.1770, weighted policy: 0.1770), Train Mean Max: 0.9270
Epoch 7/10, Train Loss: 0.2504 (value: 0.0015, weighted value: 0.0756, policy: 0.1748, weighted policy: 0.1748), Train Mean Max: 0.9274
Epoch 8/10, Train Loss: 0.2491 (value: 0.0015, weighted value: 0.0739, policy: 0.1752, weighted policy: 0.1752), Train Mean Max: 0.9273
Epoch 9/10, Train Loss: 0.2442 (value: 0.0014, weighted value: 0.0707, policy: 0.1735, weighted policy: 0.1735), Train Mean Max: 0.9276
Epoch 10/10, Train Loss: 0.2411 (value: 0.0014, weighted value: 0.0682, policy: 0.1728, weighted policy: 0.1728), Train Mean Max: 0.9279
..training done in 67.59 seconds
..evaluation done in 19.35 seconds
Old network+MCTS average reward: 0.6152, min: 0.0463, max: 1.2963, stdev: 0.2356
New network+MCTS average reward: 0.6272, min: 0.0741, max: 1.3056, stdev: 0.2364
Old bare network average reward: 0.5648, min: -0.0185, max: 1.2870, stdev: 0.2443
New bare network average reward: 0.5680, min: -0.0556, max: 1.3056, stdev: 0.2450
External policy "random" average reward: 0.2362, min: -0.3889, max: 0.8611, stdev: 0.2243
External policy "individual greedy" average reward: 0.5142, min: -0.0741, max: 1.1759, stdev: 0.2331
External policy "total greedy" average reward: 0.6308, min: 0.0926, max: 1.3333, stdev: 0.2302
New network won 111 and tied 122 out of 300 games (57.33% wins where ties are half wins)
Keeping the new network

Training iteration 138 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.00 seconds
Training examples lengths: [64679, 64778, 64698, 64846, 64423, 64831, 64729, 64879, 64845, 64603]
Total value: 423291.53
Training on 647311 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2709 (value: 0.0018, weighted value: 0.0879, policy: 0.1831, weighted policy: 0.1831), Train Mean Max: 0.9267
Epoch 2/10, Train Loss: 0.2596 (value: 0.0016, weighted value: 0.0795, policy: 0.1801, weighted policy: 0.1801), Train Mean Max: 0.9269
Epoch 3/10, Train Loss: 0.2523 (value: 0.0015, weighted value: 0.0745, policy: 0.1778, weighted policy: 0.1778), Train Mean Max: 0.9273
Epoch 4/10, Train Loss: 0.2500 (value: 0.0015, weighted value: 0.0732, policy: 0.1768, weighted policy: 0.1768), Train Mean Max: 0.9273
Epoch 5/10, Train Loss: 0.2445 (value: 0.0014, weighted value: 0.0690, policy: 0.1755, weighted policy: 0.1755), Train Mean Max: 0.9274
Epoch 6/10, Train Loss: 0.2416 (value: 0.0014, weighted value: 0.0677, policy: 0.1739, weighted policy: 0.1739), Train Mean Max: 0.9278
Epoch 7/10, Train Loss: 0.2364 (value: 0.0013, weighted value: 0.0637, policy: 0.1727, weighted policy: 0.1727), Train Mean Max: 0.9279
Epoch 8/10, Train Loss: 0.2363 (value: 0.0013, weighted value: 0.0630, policy: 0.1733, weighted policy: 0.1733), Train Mean Max: 0.9281
Epoch 9/10, Train Loss: 0.2323 (value: 0.0012, weighted value: 0.0614, policy: 0.1709, weighted policy: 0.1709), Train Mean Max: 0.9285
Epoch 10/10, Train Loss: 0.2309 (value: 0.0012, weighted value: 0.0589, policy: 0.1721, weighted policy: 0.1721), Train Mean Max: 0.9284
..training done in 64.33 seconds
..evaluation done in 20.60 seconds
Old network+MCTS average reward: 0.6614, min: 0.0833, max: 1.5370, stdev: 0.2172
New network+MCTS average reward: 0.6617, min: 0.1389, max: 1.5000, stdev: 0.2172
Old bare network average reward: 0.5966, min: 0.0741, max: 1.4722, stdev: 0.2243
New bare network average reward: 0.5960, min: 0.0000, max: 1.5000, stdev: 0.2253
External policy "random" average reward: 0.2649, min: -0.2685, max: 0.8333, stdev: 0.2186
External policy "individual greedy" average reward: 0.5448, min: 0.0185, max: 1.1852, stdev: 0.2146
External policy "total greedy" average reward: 0.6705, min: 0.1296, max: 1.3148, stdev: 0.2126
New network won 90 and tied 125 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 139 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.82 seconds
Training examples lengths: [64778, 64698, 64846, 64423, 64831, 64729, 64879, 64845, 64603, 64759]
Total value: 423380.79
Training on 647391 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2627 (value: 0.0016, weighted value: 0.0809, policy: 0.1818, weighted policy: 0.1818), Train Mean Max: 0.9269
Epoch 2/10, Train Loss: 0.2517 (value: 0.0015, weighted value: 0.0737, policy: 0.1780, weighted policy: 0.1780), Train Mean Max: 0.9274
Epoch 3/10, Train Loss: 0.2462 (value: 0.0014, weighted value: 0.0703, policy: 0.1759, weighted policy: 0.1759), Train Mean Max: 0.9276
Epoch 4/10, Train Loss: 0.2402 (value: 0.0013, weighted value: 0.0665, policy: 0.1737, weighted policy: 0.1737), Train Mean Max: 0.9279
Epoch 5/10, Train Loss: 0.2364 (value: 0.0013, weighted value: 0.0634, policy: 0.1730, weighted policy: 0.1730), Train Mean Max: 0.9279
Epoch 6/10, Train Loss: 0.2340 (value: 0.0012, weighted value: 0.0620, policy: 0.1720, weighted policy: 0.1720), Train Mean Max: 0.9282
Epoch 7/10, Train Loss: 0.2321 (value: 0.0012, weighted value: 0.0616, policy: 0.1706, weighted policy: 0.1706), Train Mean Max: 0.9284
Epoch 8/10, Train Loss: 0.2289 (value: 0.0012, weighted value: 0.0581, policy: 0.1708, weighted policy: 0.1708), Train Mean Max: 0.9284
Epoch 9/10, Train Loss: 0.2272 (value: 0.0011, weighted value: 0.0571, policy: 0.1701, weighted policy: 0.1701), Train Mean Max: 0.9287
Epoch 10/10, Train Loss: 0.2265 (value: 0.0011, weighted value: 0.0571, policy: 0.1693, weighted policy: 0.1693), Train Mean Max: 0.9288
..training done in 64.75 seconds
..evaluation done in 18.36 seconds
Old network+MCTS average reward: 0.6496, min: -0.1944, max: 1.4352, stdev: 0.2317
New network+MCTS average reward: 0.6536, min: -0.0185, max: 1.4537, stdev: 0.2262
Old bare network average reward: 0.5951, min: -0.1759, max: 1.4352, stdev: 0.2309
New bare network average reward: 0.5932, min: -0.1574, max: 1.4352, stdev: 0.2360
External policy "random" average reward: 0.2754, min: -0.3241, max: 1.0556, stdev: 0.2362
External policy "individual greedy" average reward: 0.5505, min: -0.0370, max: 1.3426, stdev: 0.2278
External policy "total greedy" average reward: 0.6613, min: 0.0278, max: 1.4537, stdev: 0.2179
New network won 94 and tied 133 out of 300 games (53.50% wins where ties are half wins)
Keeping the new network

Training iteration 140 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.84 seconds
Training examples lengths: [64698, 64846, 64423, 64831, 64729, 64879, 64845, 64603, 64759, 64781]
Total value: 423852.51
Training on 647394 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2564 (value: 0.0015, weighted value: 0.0772, policy: 0.1793, weighted policy: 0.1793), Train Mean Max: 0.9276
Epoch 2/10, Train Loss: 0.2454 (value: 0.0014, weighted value: 0.0689, policy: 0.1765, weighted policy: 0.1765), Train Mean Max: 0.9280
Epoch 3/10, Train Loss: 0.2393 (value: 0.0013, weighted value: 0.0660, policy: 0.1733, weighted policy: 0.1733), Train Mean Max: 0.9284
Epoch 4/10, Train Loss: 0.2348 (value: 0.0013, weighted value: 0.0634, policy: 0.1714, weighted policy: 0.1714), Train Mean Max: 0.9286
Epoch 5/10, Train Loss: 0.2334 (value: 0.0012, weighted value: 0.0614, policy: 0.1719, weighted policy: 0.1719), Train Mean Max: 0.9286
Epoch 6/10, Train Loss: 0.2314 (value: 0.0012, weighted value: 0.0606, policy: 0.1708, weighted policy: 0.1708), Train Mean Max: 0.9288
Epoch 7/10, Train Loss: 0.2255 (value: 0.0011, weighted value: 0.0563, policy: 0.1692, weighted policy: 0.1692), Train Mean Max: 0.9291
Epoch 8/10, Train Loss: 0.2269 (value: 0.0011, weighted value: 0.0570, policy: 0.1699, weighted policy: 0.1699), Train Mean Max: 0.9292
Epoch 9/10, Train Loss: 0.2237 (value: 0.0011, weighted value: 0.0555, policy: 0.1683, weighted policy: 0.1683), Train Mean Max: 0.9295
Epoch 10/10, Train Loss: 0.2208 (value: 0.0010, weighted value: 0.0522, policy: 0.1686, weighted policy: 0.1686), Train Mean Max: 0.9296
..training done in 63.88 seconds
..evaluation done in 18.30 seconds
Old network+MCTS average reward: 0.6492, min: -0.1019, max: 1.3426, stdev: 0.2234
New network+MCTS average reward: 0.6433, min: -0.0093, max: 1.3426, stdev: 0.2337
Old bare network average reward: 0.5902, min: -0.0741, max: 1.1944, stdev: 0.2325
New bare network average reward: 0.5862, min: -0.3148, max: 1.2315, stdev: 0.2336
External policy "random" average reward: 0.2575, min: -0.4167, max: 0.9074, stdev: 0.2325
External policy "individual greedy" average reward: 0.5433, min: -0.0370, max: 1.1759, stdev: 0.2270
External policy "total greedy" average reward: 0.6484, min: -0.1204, max: 1.1667, stdev: 0.2216
New network won 74 and tied 129 out of 300 games (46.17% wins where ties are half wins)
Reverting to the old network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_140

Training iteration 141 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.19 seconds
Training examples lengths: [64846, 64423, 64831, 64729, 64879, 64845, 64603, 64759, 64781, 64832]
Total value: 424367.06
Training on 647528 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2875 (value: 0.0019, weighted value: 0.0971, policy: 0.1904, weighted policy: 0.1904), Train Mean Max: 0.9262
Epoch 2/10, Train Loss: 0.2700 (value: 0.0018, weighted value: 0.0877, policy: 0.1823, weighted policy: 0.1823), Train Mean Max: 0.9267
Epoch 3/10, Train Loss: 0.2574 (value: 0.0016, weighted value: 0.0795, policy: 0.1779, weighted policy: 0.1779), Train Mean Max: 0.9271
Epoch 4/10, Train Loss: 0.2521 (value: 0.0015, weighted value: 0.0754, policy: 0.1768, weighted policy: 0.1768), Train Mean Max: 0.9274
Epoch 5/10, Train Loss: 0.2467 (value: 0.0014, weighted value: 0.0719, policy: 0.1749, weighted policy: 0.1749), Train Mean Max: 0.9275
Epoch 6/10, Train Loss: 0.2429 (value: 0.0014, weighted value: 0.0697, policy: 0.1732, weighted policy: 0.1732), Train Mean Max: 0.9279
Epoch 7/10, Train Loss: 0.2387 (value: 0.0013, weighted value: 0.0658, policy: 0.1729, weighted policy: 0.1729), Train Mean Max: 0.9279
Epoch 8/10, Train Loss: 0.2365 (value: 0.0013, weighted value: 0.0639, policy: 0.1726, weighted policy: 0.1726), Train Mean Max: 0.9280
Epoch 9/10, Train Loss: 0.2334 (value: 0.0012, weighted value: 0.0622, policy: 0.1712, weighted policy: 0.1712), Train Mean Max: 0.9284
Epoch 10/10, Train Loss: 0.2320 (value: 0.0012, weighted value: 0.0616, policy: 0.1703, weighted policy: 0.1703), Train Mean Max: 0.9285
..training done in 60.94 seconds
..evaluation done in 17.62 seconds
Old network+MCTS average reward: 0.6541, min: 0.0833, max: 1.3333, stdev: 0.2241
New network+MCTS average reward: 0.6480, min: 0.0463, max: 1.4074, stdev: 0.2262
Old bare network average reward: 0.5932, min: -0.0648, max: 1.3333, stdev: 0.2288
New bare network average reward: 0.5913, min: -0.0556, max: 1.3333, stdev: 0.2307
External policy "random" average reward: 0.2631, min: -0.3611, max: 0.9167, stdev: 0.2126
External policy "individual greedy" average reward: 0.5302, min: 0.0185, max: 1.3426, stdev: 0.2159
External policy "total greedy" average reward: 0.6466, min: 0.0926, max: 1.3611, stdev: 0.2135
New network won 69 and tied 128 out of 300 games (44.33% wins where ties are half wins)
Reverting to the old network

Training iteration 142 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 61.19 seconds
Training examples lengths: [64423, 64831, 64729, 64879, 64845, 64603, 64759, 64781, 64832, 64857]
Total value: 425047.31
Training on 647539 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3162 (value: 0.0024, weighted value: 0.1179, policy: 0.1983, weighted policy: 0.1983), Train Mean Max: 0.9250
Epoch 2/10, Train Loss: 0.2893 (value: 0.0020, weighted value: 0.1014, policy: 0.1879, weighted policy: 0.1879), Train Mean Max: 0.9257
Epoch 3/10, Train Loss: 0.2758 (value: 0.0018, weighted value: 0.0921, policy: 0.1837, weighted policy: 0.1837), Train Mean Max: 0.9260
Epoch 4/10, Train Loss: 0.2667 (value: 0.0017, weighted value: 0.0865, policy: 0.1801, weighted policy: 0.1801), Train Mean Max: 0.9263
Epoch 5/10, Train Loss: 0.2596 (value: 0.0016, weighted value: 0.0810, policy: 0.1786, weighted policy: 0.1786), Train Mean Max: 0.9265
Epoch 6/10, Train Loss: 0.2527 (value: 0.0015, weighted value: 0.0768, policy: 0.1760, weighted policy: 0.1760), Train Mean Max: 0.9268
Epoch 7/10, Train Loss: 0.2492 (value: 0.0015, weighted value: 0.0744, policy: 0.1748, weighted policy: 0.1748), Train Mean Max: 0.9272
Epoch 8/10, Train Loss: 0.2446 (value: 0.0014, weighted value: 0.0703, policy: 0.1743, weighted policy: 0.1743), Train Mean Max: 0.9273
Epoch 9/10, Train Loss: 0.2431 (value: 0.0014, weighted value: 0.0701, policy: 0.1730, weighted policy: 0.1730), Train Mean Max: 0.9274
Epoch 10/10, Train Loss: 0.2395 (value: 0.0013, weighted value: 0.0669, policy: 0.1726, weighted policy: 0.1726), Train Mean Max: 0.9275
..training done in 64.77 seconds
..evaluation done in 17.69 seconds
Old network+MCTS average reward: 0.6360, min: 0.0926, max: 1.3426, stdev: 0.2168
New network+MCTS average reward: 0.6333, min: 0.0926, max: 1.3426, stdev: 0.2184
Old bare network average reward: 0.5708, min: 0.0278, max: 1.2500, stdev: 0.2162
New bare network average reward: 0.5779, min: 0.0556, max: 1.2500, stdev: 0.2235
External policy "random" average reward: 0.2571, min: -0.3796, max: 1.0556, stdev: 0.2297
External policy "individual greedy" average reward: 0.5237, min: 0.0185, max: 1.1667, stdev: 0.2138
External policy "total greedy" average reward: 0.6347, min: 0.2037, max: 1.3333, stdev: 0.2132
New network won 72 and tied 133 out of 300 games (46.17% wins where ties are half wins)
Reverting to the old network

Training iteration 143 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.02 seconds
Training examples lengths: [64831, 64729, 64879, 64845, 64603, 64759, 64781, 64832, 64857, 64545]
Total value: 425716.76
Training on 647661 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3427 (value: 0.0027, weighted value: 0.1370, policy: 0.2057, weighted policy: 0.2057), Train Mean Max: 0.9239
Epoch 2/10, Train Loss: 0.3079 (value: 0.0023, weighted value: 0.1148, policy: 0.1931, weighted policy: 0.1931), Train Mean Max: 0.9248
Epoch 3/10, Train Loss: 0.2924 (value: 0.0021, weighted value: 0.1053, policy: 0.1871, weighted policy: 0.1871), Train Mean Max: 0.9251
Epoch 4/10, Train Loss: 0.2800 (value: 0.0019, weighted value: 0.0959, policy: 0.1842, weighted policy: 0.1842), Train Mean Max: 0.9255
Epoch 5/10, Train Loss: 0.2712 (value: 0.0018, weighted value: 0.0902, policy: 0.1811, weighted policy: 0.1811), Train Mean Max: 0.9257
Epoch 6/10, Train Loss: 0.2661 (value: 0.0017, weighted value: 0.0869, policy: 0.1792, weighted policy: 0.1792), Train Mean Max: 0.9259
Epoch 7/10, Train Loss: 0.2586 (value: 0.0016, weighted value: 0.0807, policy: 0.1779, weighted policy: 0.1779), Train Mean Max: 0.9261
Epoch 8/10, Train Loss: 0.2561 (value: 0.0016, weighted value: 0.0779, policy: 0.1782, weighted policy: 0.1782), Train Mean Max: 0.9261
Epoch 9/10, Train Loss: 0.2505 (value: 0.0015, weighted value: 0.0753, policy: 0.1753, weighted policy: 0.1753), Train Mean Max: 0.9267
Epoch 10/10, Train Loss: 0.2469 (value: 0.0014, weighted value: 0.0722, policy: 0.1747, weighted policy: 0.1747), Train Mean Max: 0.9268
..training done in 59.90 seconds
..evaluation done in 17.58 seconds
Old network+MCTS average reward: 0.6551, min: -0.0278, max: 1.3333, stdev: 0.2252
New network+MCTS average reward: 0.6584, min: -0.0556, max: 1.3519, stdev: 0.2197
Old bare network average reward: 0.5979, min: -0.0741, max: 1.2963, stdev: 0.2380
New bare network average reward: 0.5994, min: -0.0278, max: 1.2778, stdev: 0.2266
External policy "random" average reward: 0.2555, min: -0.3241, max: 0.9259, stdev: 0.2283
External policy "individual greedy" average reward: 0.5404, min: -0.0185, max: 1.0926, stdev: 0.2272
External policy "total greedy" average reward: 0.6637, min: 0.1204, max: 1.2685, stdev: 0.2230
New network won 92 and tied 130 out of 300 games (52.33% wins where ties are half wins)
Keeping the new network

Training iteration 144 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.31 seconds
Training examples lengths: [64729, 64879, 64845, 64603, 64759, 64781, 64832, 64857, 64545, 64636]
Total value: 425561.20
Training on 647466 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2735 (value: 0.0018, weighted value: 0.0897, policy: 0.1838, weighted policy: 0.1838), Train Mean Max: 0.9254
Epoch 2/10, Train Loss: 0.2648 (value: 0.0017, weighted value: 0.0842, policy: 0.1806, weighted policy: 0.1806), Train Mean Max: 0.9257
Epoch 3/10, Train Loss: 0.2573 (value: 0.0016, weighted value: 0.0785, policy: 0.1788, weighted policy: 0.1788), Train Mean Max: 0.9261
Epoch 4/10, Train Loss: 0.2488 (value: 0.0015, weighted value: 0.0728, policy: 0.1761, weighted policy: 0.1761), Train Mean Max: 0.9264
Epoch 5/10, Train Loss: 0.2474 (value: 0.0014, weighted value: 0.0722, policy: 0.1752, weighted policy: 0.1752), Train Mean Max: 0.9265
Epoch 6/10, Train Loss: 0.2425 (value: 0.0014, weighted value: 0.0681, policy: 0.1745, weighted policy: 0.1745), Train Mean Max: 0.9269
Epoch 7/10, Train Loss: 0.2408 (value: 0.0013, weighted value: 0.0666, policy: 0.1742, weighted policy: 0.1742), Train Mean Max: 0.9271
Epoch 8/10, Train Loss: 0.2398 (value: 0.0013, weighted value: 0.0666, policy: 0.1732, weighted policy: 0.1732), Train Mean Max: 0.9270
Epoch 9/10, Train Loss: 0.2356 (value: 0.0013, weighted value: 0.0628, policy: 0.1728, weighted policy: 0.1728), Train Mean Max: 0.9275
Epoch 10/10, Train Loss: 0.2310 (value: 0.0012, weighted value: 0.0601, policy: 0.1709, weighted policy: 0.1709), Train Mean Max: 0.9277
..training done in 63.60 seconds
..evaluation done in 17.34 seconds
Old network+MCTS average reward: 0.6487, min: 0.1111, max: 1.3981, stdev: 0.2125
New network+MCTS average reward: 0.6454, min: 0.1019, max: 1.3981, stdev: 0.2154
Old bare network average reward: 0.5884, min: -0.0278, max: 1.3611, stdev: 0.2247
New bare network average reward: 0.5825, min: 0.0370, max: 1.3981, stdev: 0.2230
External policy "random" average reward: 0.2660, min: -0.3241, max: 1.0000, stdev: 0.2119
External policy "individual greedy" average reward: 0.5384, min: 0.0648, max: 1.3796, stdev: 0.2112
External policy "total greedy" average reward: 0.6560, min: 0.1574, max: 1.3704, stdev: 0.2131
New network won 76 and tied 146 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 145 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 69.72 seconds
Training examples lengths: [64879, 64845, 64603, 64759, 64781, 64832, 64857, 64545, 64636, 65016]
Total value: 426210.10
Training on 647753 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3018 (value: 0.0022, weighted value: 0.1089, policy: 0.1928, weighted policy: 0.1928), Train Mean Max: 0.9243
Epoch 2/10, Train Loss: 0.2835 (value: 0.0020, weighted value: 0.0991, policy: 0.1844, weighted policy: 0.1844), Train Mean Max: 0.9251
Epoch 3/10, Train Loss: 0.2699 (value: 0.0018, weighted value: 0.0879, policy: 0.1820, weighted policy: 0.1820), Train Mean Max: 0.9253
Epoch 4/10, Train Loss: 0.2643 (value: 0.0017, weighted value: 0.0846, policy: 0.1797, weighted policy: 0.1797), Train Mean Max: 0.9256
Epoch 5/10, Train Loss: 0.2577 (value: 0.0016, weighted value: 0.0801, policy: 0.1776, weighted policy: 0.1776), Train Mean Max: 0.9259
Epoch 6/10, Train Loss: 0.2543 (value: 0.0015, weighted value: 0.0769, policy: 0.1773, weighted policy: 0.1773), Train Mean Max: 0.9261
Epoch 7/10, Train Loss: 0.2487 (value: 0.0015, weighted value: 0.0729, policy: 0.1758, weighted policy: 0.1758), Train Mean Max: 0.9265
Epoch 8/10, Train Loss: 0.2455 (value: 0.0014, weighted value: 0.0709, policy: 0.1746, weighted policy: 0.1746), Train Mean Max: 0.9267
Epoch 9/10, Train Loss: 0.2436 (value: 0.0014, weighted value: 0.0694, policy: 0.1742, weighted policy: 0.1742), Train Mean Max: 0.9269
Epoch 10/10, Train Loss: 0.2379 (value: 0.0013, weighted value: 0.0646, policy: 0.1733, weighted policy: 0.1733), Train Mean Max: 0.9271
..training done in 64.95 seconds
..evaluation done in 17.62 seconds
Old network+MCTS average reward: 0.6402, min: 0.0648, max: 1.3889, stdev: 0.2340
New network+MCTS average reward: 0.6427, min: 0.1389, max: 1.5463, stdev: 0.2342
Old bare network average reward: 0.5797, min: 0.0926, max: 1.2500, stdev: 0.2322
New bare network average reward: 0.5845, min: 0.0370, max: 1.3889, stdev: 0.2385
External policy "random" average reward: 0.2671, min: -0.3796, max: 1.0185, stdev: 0.2313
External policy "individual greedy" average reward: 0.5367, min: -0.0185, max: 1.3148, stdev: 0.2379
External policy "total greedy" average reward: 0.6575, min: 0.0741, max: 1.5278, stdev: 0.2335
New network won 90 and tied 140 out of 300 games (53.33% wins where ties are half wins)
Keeping the new network

Training iteration 146 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.16 seconds
Training examples lengths: [64845, 64603, 64759, 64781, 64832, 64857, 64545, 64636, 65016, 64683]
Total value: 426531.66
Training on 647557 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2671 (value: 0.0017, weighted value: 0.0846, policy: 0.1825, weighted policy: 0.1825), Train Mean Max: 0.9262
Epoch 2/10, Train Loss: 0.2573 (value: 0.0016, weighted value: 0.0789, policy: 0.1784, weighted policy: 0.1784), Train Mean Max: 0.9266
Epoch 3/10, Train Loss: 0.2501 (value: 0.0015, weighted value: 0.0742, policy: 0.1760, weighted policy: 0.1760), Train Mean Max: 0.9269
Epoch 4/10, Train Loss: 0.2439 (value: 0.0014, weighted value: 0.0700, policy: 0.1739, weighted policy: 0.1739), Train Mean Max: 0.9273
Epoch 5/10, Train Loss: 0.2415 (value: 0.0014, weighted value: 0.0679, policy: 0.1736, weighted policy: 0.1736), Train Mean Max: 0.9274
Epoch 6/10, Train Loss: 0.2371 (value: 0.0013, weighted value: 0.0654, policy: 0.1717, weighted policy: 0.1717), Train Mean Max: 0.9277
Epoch 7/10, Train Loss: 0.2347 (value: 0.0013, weighted value: 0.0634, policy: 0.1713, weighted policy: 0.1713), Train Mean Max: 0.9279
Epoch 8/10, Train Loss: 0.2329 (value: 0.0012, weighted value: 0.0619, policy: 0.1711, weighted policy: 0.1711), Train Mean Max: 0.9281
Epoch 9/10, Train Loss: 0.2292 (value: 0.0012, weighted value: 0.0594, policy: 0.1698, weighted policy: 0.1698), Train Mean Max: 0.9283
Epoch 10/10, Train Loss: 0.2285 (value: 0.0012, weighted value: 0.0587, policy: 0.1698, weighted policy: 0.1698), Train Mean Max: 0.9285
..training done in 63.47 seconds
..evaluation done in 18.77 seconds
Old network+MCTS average reward: 0.6556, min: 0.1389, max: 1.3426, stdev: 0.2253
New network+MCTS average reward: 0.6521, min: 0.0926, max: 1.3426, stdev: 0.2307
Old bare network average reward: 0.5923, min: 0.0278, max: 1.2407, stdev: 0.2402
New bare network average reward: 0.5883, min: -0.1759, max: 1.1759, stdev: 0.2363
External policy "random" average reward: 0.2391, min: -0.2963, max: 0.9444, stdev: 0.2215
External policy "individual greedy" average reward: 0.5285, min: 0.0278, max: 1.2778, stdev: 0.2285
External policy "total greedy" average reward: 0.6498, min: 0.1574, max: 1.3704, stdev: 0.2230
New network won 79 and tied 138 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 147 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.97 seconds
Training examples lengths: [64603, 64759, 64781, 64832, 64857, 64545, 64636, 65016, 64683, 64697]
Total value: 426982.19
Training on 647409 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2953 (value: 0.0021, weighted value: 0.1059, policy: 0.1893, weighted policy: 0.1893), Train Mean Max: 0.9250
Epoch 2/10, Train Loss: 0.2752 (value: 0.0018, weighted value: 0.0915, policy: 0.1837, weighted policy: 0.1837), Train Mean Max: 0.9255
Epoch 3/10, Train Loss: 0.2660 (value: 0.0017, weighted value: 0.0855, policy: 0.1805, weighted policy: 0.1805), Train Mean Max: 0.9260
Epoch 4/10, Train Loss: 0.2598 (value: 0.0016, weighted value: 0.0819, policy: 0.1779, weighted policy: 0.1779), Train Mean Max: 0.9263
Epoch 5/10, Train Loss: 0.2540 (value: 0.0015, weighted value: 0.0772, policy: 0.1768, weighted policy: 0.1768), Train Mean Max: 0.9263
Epoch 6/10, Train Loss: 0.2506 (value: 0.0015, weighted value: 0.0743, policy: 0.1763, weighted policy: 0.1763), Train Mean Max: 0.9267
Epoch 7/10, Train Loss: 0.2442 (value: 0.0014, weighted value: 0.0707, policy: 0.1735, weighted policy: 0.1735), Train Mean Max: 0.9271
Epoch 8/10, Train Loss: 0.2442 (value: 0.0014, weighted value: 0.0707, policy: 0.1734, weighted policy: 0.1734), Train Mean Max: 0.9272
Epoch 9/10, Train Loss: 0.2384 (value: 0.0013, weighted value: 0.0659, policy: 0.1725, weighted policy: 0.1725), Train Mean Max: 0.9274
Epoch 10/10, Train Loss: 0.2355 (value: 0.0013, weighted value: 0.0638, policy: 0.1717, weighted policy: 0.1717), Train Mean Max: 0.9277
..training done in 59.50 seconds
..evaluation done in 18.58 seconds
Old network+MCTS average reward: 0.6440, min: 0.1574, max: 1.5278, stdev: 0.2252
New network+MCTS average reward: 0.6420, min: 0.0833, max: 1.5278, stdev: 0.2238
Old bare network average reward: 0.5842, min: 0.0648, max: 1.5278, stdev: 0.2348
New bare network average reward: 0.5856, min: -0.0185, max: 1.4444, stdev: 0.2353
External policy "random" average reward: 0.2604, min: -0.2407, max: 0.9815, stdev: 0.2281
External policy "individual greedy" average reward: 0.5272, min: -0.0185, max: 1.3796, stdev: 0.2378
External policy "total greedy" average reward: 0.6465, min: 0.0278, max: 1.3796, stdev: 0.2338
New network won 96 and tied 106 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 148 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.60 seconds
Training examples lengths: [64759, 64781, 64832, 64857, 64545, 64636, 65016, 64683, 64697, 64891]
Total value: 427407.29
Training on 647697 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3209 (value: 0.0025, weighted value: 0.1232, policy: 0.1977, weighted policy: 0.1977), Train Mean Max: 0.9239
Epoch 2/10, Train Loss: 0.2953 (value: 0.0021, weighted value: 0.1066, policy: 0.1887, weighted policy: 0.1887), Train Mean Max: 0.9247
Epoch 3/10, Train Loss: 0.2805 (value: 0.0019, weighted value: 0.0965, policy: 0.1841, weighted policy: 0.1841), Train Mean Max: 0.9252
Epoch 4/10, Train Loss: 0.2719 (value: 0.0018, weighted value: 0.0916, policy: 0.1802, weighted policy: 0.1802), Train Mean Max: 0.9254
Epoch 5/10, Train Loss: 0.2642 (value: 0.0017, weighted value: 0.0863, policy: 0.1778, weighted policy: 0.1778), Train Mean Max: 0.9259
Epoch 6/10, Train Loss: 0.2576 (value: 0.0016, weighted value: 0.0811, policy: 0.1764, weighted policy: 0.1764), Train Mean Max: 0.9261
Epoch 7/10, Train Loss: 0.2542 (value: 0.0016, weighted value: 0.0783, policy: 0.1760, weighted policy: 0.1760), Train Mean Max: 0.9262
Epoch 8/10, Train Loss: 0.2492 (value: 0.0015, weighted value: 0.0745, policy: 0.1747, weighted policy: 0.1747), Train Mean Max: 0.9265
Epoch 9/10, Train Loss: 0.2451 (value: 0.0014, weighted value: 0.0722, policy: 0.1729, weighted policy: 0.1729), Train Mean Max: 0.9269
Epoch 10/10, Train Loss: 0.2419 (value: 0.0014, weighted value: 0.0691, policy: 0.1728, weighted policy: 0.1728), Train Mean Max: 0.9272
..training done in 58.76 seconds
..evaluation done in 17.71 seconds
Old network+MCTS average reward: 0.6435, min: 0.0648, max: 1.4167, stdev: 0.2248
New network+MCTS average reward: 0.6489, min: 0.1019, max: 1.3704, stdev: 0.2210
Old bare network average reward: 0.5848, min: 0.0000, max: 1.3426, stdev: 0.2255
New bare network average reward: 0.5876, min: 0.0833, max: 1.3426, stdev: 0.2205
External policy "random" average reward: 0.2703, min: -0.4259, max: 0.9722, stdev: 0.2110
External policy "individual greedy" average reward: 0.5435, min: 0.0556, max: 1.2315, stdev: 0.2280
External policy "total greedy" average reward: 0.6476, min: 0.0741, max: 1.4722, stdev: 0.2220
New network won 91 and tied 122 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 149 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.63 seconds
Training examples lengths: [64781, 64832, 64857, 64545, 64636, 65016, 64683, 64697, 64891, 64806]
Total value: 427705.92
Training on 647744 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2669 (value: 0.0017, weighted value: 0.0866, policy: 0.1803, weighted policy: 0.1803), Train Mean Max: 0.9261
Epoch 2/10, Train Loss: 0.2568 (value: 0.0016, weighted value: 0.0801, policy: 0.1767, weighted policy: 0.1767), Train Mean Max: 0.9267
Epoch 3/10, Train Loss: 0.2512 (value: 0.0015, weighted value: 0.0755, policy: 0.1758, weighted policy: 0.1758), Train Mean Max: 0.9269
Epoch 4/10, Train Loss: 0.2455 (value: 0.0014, weighted value: 0.0717, policy: 0.1737, weighted policy: 0.1737), Train Mean Max: 0.9272
Epoch 5/10, Train Loss: 0.2414 (value: 0.0014, weighted value: 0.0694, policy: 0.1720, weighted policy: 0.1720), Train Mean Max: 0.9275
Epoch 6/10, Train Loss: 0.2389 (value: 0.0013, weighted value: 0.0674, policy: 0.1715, weighted policy: 0.1715), Train Mean Max: 0.9277
Epoch 7/10, Train Loss: 0.2354 (value: 0.0013, weighted value: 0.0652, policy: 0.1702, weighted policy: 0.1702), Train Mean Max: 0.9280
Epoch 8/10, Train Loss: 0.2327 (value: 0.0012, weighted value: 0.0624, policy: 0.1703, weighted policy: 0.1703), Train Mean Max: 0.9281
Epoch 9/10, Train Loss: 0.2315 (value: 0.0012, weighted value: 0.0613, policy: 0.1703, weighted policy: 0.1703), Train Mean Max: 0.9284
Epoch 10/10, Train Loss: 0.2282 (value: 0.0012, weighted value: 0.0597, policy: 0.1685, weighted policy: 0.1685), Train Mean Max: 0.9286
..training done in 62.83 seconds
..evaluation done in 17.26 seconds
Old network+MCTS average reward: 0.6510, min: -0.0278, max: 1.1852, stdev: 0.2250
New network+MCTS average reward: 0.6519, min: -0.1574, max: 1.1944, stdev: 0.2271
Old bare network average reward: 0.5984, min: -0.0833, max: 1.2130, stdev: 0.2344
New bare network average reward: 0.5950, min: -0.1389, max: 1.1481, stdev: 0.2320
External policy "random" average reward: 0.2496, min: -0.5370, max: 0.7870, stdev: 0.2276
External policy "individual greedy" average reward: 0.5371, min: -0.0833, max: 1.2130, stdev: 0.2330
External policy "total greedy" average reward: 0.6601, min: 0.0648, max: 1.3519, stdev: 0.2298
New network won 83 and tied 146 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network

Training iteration 150 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.57 seconds
Training examples lengths: [64832, 64857, 64545, 64636, 65016, 64683, 64697, 64891, 64806, 64591]
Total value: 427780.57
Training on 647554 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2574 (value: 0.0016, weighted value: 0.0787, policy: 0.1787, weighted policy: 0.1787), Train Mean Max: 0.9275
Epoch 2/10, Train Loss: 0.2476 (value: 0.0014, weighted value: 0.0714, policy: 0.1763, weighted policy: 0.1763), Train Mean Max: 0.9278
Epoch 3/10, Train Loss: 0.2405 (value: 0.0014, weighted value: 0.0679, policy: 0.1726, weighted policy: 0.1726), Train Mean Max: 0.9283
Epoch 4/10, Train Loss: 0.2379 (value: 0.0013, weighted value: 0.0667, policy: 0.1712, weighted policy: 0.1712), Train Mean Max: 0.9284
Epoch 5/10, Train Loss: 0.2320 (value: 0.0012, weighted value: 0.0622, policy: 0.1697, weighted policy: 0.1697), Train Mean Max: 0.9286
Epoch 6/10, Train Loss: 0.2308 (value: 0.0012, weighted value: 0.0609, policy: 0.1699, weighted policy: 0.1699), Train Mean Max: 0.9288
Epoch 7/10, Train Loss: 0.2278 (value: 0.0012, weighted value: 0.0584, policy: 0.1694, weighted policy: 0.1694), Train Mean Max: 0.9290
Epoch 8/10, Train Loss: 0.2276 (value: 0.0012, weighted value: 0.0589, policy: 0.1688, weighted policy: 0.1688), Train Mean Max: 0.9292
Epoch 9/10, Train Loss: 0.2240 (value: 0.0011, weighted value: 0.0563, policy: 0.1677, weighted policy: 0.1677), Train Mean Max: 0.9294
Epoch 10/10, Train Loss: 0.2227 (value: 0.0011, weighted value: 0.0553, policy: 0.1674, weighted policy: 0.1674), Train Mean Max: 0.9297
..training done in 59.77 seconds
..evaluation done in 17.91 seconds
Old network+MCTS average reward: 0.6598, min: 0.0833, max: 1.2222, stdev: 0.2181
New network+MCTS average reward: 0.6610, min: 0.0556, max: 1.1667, stdev: 0.2153
Old bare network average reward: 0.5973, min: 0.0278, max: 1.2222, stdev: 0.2231
New bare network average reward: 0.5999, min: 0.0278, max: 1.1574, stdev: 0.2236
External policy "random" average reward: 0.2762, min: -0.2778, max: 0.8519, stdev: 0.2123
External policy "individual greedy" average reward: 0.5439, min: -0.0833, max: 1.1019, stdev: 0.2161
External policy "total greedy" average reward: 0.6593, min: 0.0463, max: 1.3148, stdev: 0.2124
New network won 80 and tied 133 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 151 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.48 seconds
Training examples lengths: [64857, 64545, 64636, 65016, 64683, 64697, 64891, 64806, 64591, 64917]
Total value: 428473.00
Training on 647639 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2864 (value: 0.0020, weighted value: 0.0980, policy: 0.1884, weighted policy: 0.1884), Train Mean Max: 0.9263
Epoch 2/10, Train Loss: 0.2696 (value: 0.0018, weighted value: 0.0894, policy: 0.1802, weighted policy: 0.1802), Train Mean Max: 0.9269
Epoch 3/10, Train Loss: 0.2568 (value: 0.0016, weighted value: 0.0801, policy: 0.1767, weighted policy: 0.1767), Train Mean Max: 0.9271
Epoch 4/10, Train Loss: 0.2502 (value: 0.0015, weighted value: 0.0752, policy: 0.1750, weighted policy: 0.1750), Train Mean Max: 0.9275
Epoch 5/10, Train Loss: 0.2472 (value: 0.0015, weighted value: 0.0730, policy: 0.1742, weighted policy: 0.1742), Train Mean Max: 0.9276
Epoch 6/10, Train Loss: 0.2411 (value: 0.0014, weighted value: 0.0695, policy: 0.1717, weighted policy: 0.1717), Train Mean Max: 0.9280
Epoch 7/10, Train Loss: 0.2386 (value: 0.0013, weighted value: 0.0667, policy: 0.1719, weighted policy: 0.1719), Train Mean Max: 0.9282
Epoch 8/10, Train Loss: 0.2356 (value: 0.0013, weighted value: 0.0646, policy: 0.1710, weighted policy: 0.1710), Train Mean Max: 0.9285
Epoch 9/10, Train Loss: 0.2329 (value: 0.0013, weighted value: 0.0634, policy: 0.1694, weighted policy: 0.1694), Train Mean Max: 0.9287
Epoch 10/10, Train Loss: 0.2320 (value: 0.0012, weighted value: 0.0623, policy: 0.1696, weighted policy: 0.1696), Train Mean Max: 0.9287
..training done in 59.00 seconds
..evaluation done in 18.54 seconds
Old network+MCTS average reward: 0.6644, min: -0.0833, max: 1.2870, stdev: 0.2264
New network+MCTS average reward: 0.6708, min: 0.0093, max: 1.2870, stdev: 0.2255
Old bare network average reward: 0.6023, min: -0.0093, max: 1.2870, stdev: 0.2350
New bare network average reward: 0.6100, min: -0.0556, max: 1.2870, stdev: 0.2345
External policy "random" average reward: 0.2626, min: -0.2963, max: 1.0648, stdev: 0.2217
External policy "individual greedy" average reward: 0.5527, min: -0.0556, max: 1.4352, stdev: 0.2223
External policy "total greedy" average reward: 0.6609, min: 0.0556, max: 1.6667, stdev: 0.2132
New network won 98 and tied 133 out of 300 games (54.83% wins where ties are half wins)
Keeping the new network

Training iteration 152 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.33 seconds
Training examples lengths: [64545, 64636, 65016, 64683, 64697, 64891, 64806, 64591, 64917, 65068]
Total value: 428326.06
Training on 647850 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2580 (value: 0.0016, weighted value: 0.0795, policy: 0.1785, weighted policy: 0.1785), Train Mean Max: 0.9277
Epoch 2/10, Train Loss: 0.2461 (value: 0.0014, weighted value: 0.0723, policy: 0.1738, weighted policy: 0.1738), Train Mean Max: 0.9282
Epoch 3/10, Train Loss: 0.2420 (value: 0.0014, weighted value: 0.0686, policy: 0.1733, weighted policy: 0.1733), Train Mean Max: 0.9284
Epoch 4/10, Train Loss: 0.2380 (value: 0.0013, weighted value: 0.0660, policy: 0.1720, weighted policy: 0.1720), Train Mean Max: 0.9286
Epoch 5/10, Train Loss: 0.2330 (value: 0.0013, weighted value: 0.0637, policy: 0.1693, weighted policy: 0.1693), Train Mean Max: 0.9290
Epoch 6/10, Train Loss: 0.2312 (value: 0.0012, weighted value: 0.0613, policy: 0.1699, weighted policy: 0.1699), Train Mean Max: 0.9290
Epoch 7/10, Train Loss: 0.2285 (value: 0.0012, weighted value: 0.0596, policy: 0.1689, weighted policy: 0.1689), Train Mean Max: 0.9293
Epoch 8/10, Train Loss: 0.2269 (value: 0.0012, weighted value: 0.0595, policy: 0.1675, weighted policy: 0.1675), Train Mean Max: 0.9295
Epoch 9/10, Train Loss: 0.2228 (value: 0.0011, weighted value: 0.0553, policy: 0.1675, weighted policy: 0.1675), Train Mean Max: 0.9297
Epoch 10/10, Train Loss: 0.2238 (value: 0.0011, weighted value: 0.0565, policy: 0.1673, weighted policy: 0.1673), Train Mean Max: 0.9299
..training done in 59.62 seconds
..evaluation done in 19.04 seconds
Old network+MCTS average reward: 0.6427, min: -0.0278, max: 1.3056, stdev: 0.2096
New network+MCTS average reward: 0.6457, min: -0.0741, max: 1.3796, stdev: 0.2152
Old bare network average reward: 0.5888, min: -0.3241, max: 1.2593, stdev: 0.2249
New bare network average reward: 0.5899, min: -0.1481, max: 1.2593, stdev: 0.2207
External policy "random" average reward: 0.2516, min: -0.3241, max: 0.9537, stdev: 0.2138
External policy "individual greedy" average reward: 0.5304, min: -0.1667, max: 1.3611, stdev: 0.2181
External policy "total greedy" average reward: 0.6500, min: -0.0741, max: 1.4259, stdev: 0.2178
New network won 93 and tied 130 out of 300 games (52.67% wins where ties are half wins)
Keeping the new network

Training iteration 153 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.96 seconds
Training examples lengths: [64636, 65016, 64683, 64697, 64891, 64806, 64591, 64917, 65068, 64973]
Total value: 429054.31
Training on 648278 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2530 (value: 0.0015, weighted value: 0.0753, policy: 0.1777, weighted policy: 0.1777), Train Mean Max: 0.9281
Epoch 2/10, Train Loss: 0.2422 (value: 0.0014, weighted value: 0.0690, policy: 0.1732, weighted policy: 0.1732), Train Mean Max: 0.9287
Epoch 3/10, Train Loss: 0.2348 (value: 0.0013, weighted value: 0.0640, policy: 0.1708, weighted policy: 0.1708), Train Mean Max: 0.9292
Epoch 4/10, Train Loss: 0.2329 (value: 0.0013, weighted value: 0.0632, policy: 0.1698, weighted policy: 0.1698), Train Mean Max: 0.9292
Epoch 5/10, Train Loss: 0.2278 (value: 0.0012, weighted value: 0.0595, policy: 0.1683, weighted policy: 0.1683), Train Mean Max: 0.9295
Epoch 6/10, Train Loss: 0.2277 (value: 0.0012, weighted value: 0.0599, policy: 0.1678, weighted policy: 0.1678), Train Mean Max: 0.9297
Epoch 7/10, Train Loss: 0.2233 (value: 0.0011, weighted value: 0.0567, policy: 0.1666, weighted policy: 0.1666), Train Mean Max: 0.9300
Epoch 8/10, Train Loss: 0.2226 (value: 0.0011, weighted value: 0.0557, policy: 0.1669, weighted policy: 0.1669), Train Mean Max: 0.9300
Epoch 9/10, Train Loss: 0.2193 (value: 0.0011, weighted value: 0.0534, policy: 0.1659, weighted policy: 0.1659), Train Mean Max: 0.9302
Epoch 10/10, Train Loss: 0.2192 (value: 0.0011, weighted value: 0.0532, policy: 0.1659, weighted policy: 0.1659), Train Mean Max: 0.9304
..training done in 59.37 seconds
..evaluation done in 17.77 seconds
Old network+MCTS average reward: 0.6448, min: -0.0185, max: 1.4630, stdev: 0.2278
New network+MCTS average reward: 0.6433, min: -0.0556, max: 1.4630, stdev: 0.2326
Old bare network average reward: 0.5838, min: 0.0093, max: 1.4259, stdev: 0.2292
New bare network average reward: 0.5835, min: -0.0463, max: 1.4630, stdev: 0.2305
External policy "random" average reward: 0.2622, min: -0.3056, max: 1.0093, stdev: 0.2215
External policy "individual greedy" average reward: 0.5366, min: 0.0278, max: 1.2315, stdev: 0.2287
External policy "total greedy" average reward: 0.6505, min: 0.0093, max: 1.3889, stdev: 0.2249
New network won 81 and tied 139 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 154 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.40 seconds
Training examples lengths: [65016, 64683, 64697, 64891, 64806, 64591, 64917, 65068, 64973, 64572]
Total value: 428889.43
Training on 648214 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2484 (value: 0.0015, weighted value: 0.0730, policy: 0.1754, weighted policy: 0.1754), Train Mean Max: 0.9291
Epoch 2/10, Train Loss: 0.2388 (value: 0.0013, weighted value: 0.0660, policy: 0.1728, weighted policy: 0.1728), Train Mean Max: 0.9293
Epoch 3/10, Train Loss: 0.2325 (value: 0.0012, weighted value: 0.0622, policy: 0.1703, weighted policy: 0.1703), Train Mean Max: 0.9297
Epoch 4/10, Train Loss: 0.2295 (value: 0.0012, weighted value: 0.0609, policy: 0.1686, weighted policy: 0.1686), Train Mean Max: 0.9300
Epoch 5/10, Train Loss: 0.2252 (value: 0.0012, weighted value: 0.0584, policy: 0.1669, weighted policy: 0.1669), Train Mean Max: 0.9302
Epoch 6/10, Train Loss: 0.2224 (value: 0.0011, weighted value: 0.0564, policy: 0.1660, weighted policy: 0.1660), Train Mean Max: 0.9303
Epoch 7/10, Train Loss: 0.2210 (value: 0.0011, weighted value: 0.0553, policy: 0.1657, weighted policy: 0.1657), Train Mean Max: 0.9305
Epoch 8/10, Train Loss: 0.2193 (value: 0.0011, weighted value: 0.0539, policy: 0.1654, weighted policy: 0.1654), Train Mean Max: 0.9306
Epoch 9/10, Train Loss: 0.2167 (value: 0.0010, weighted value: 0.0517, policy: 0.1651, weighted policy: 0.1651), Train Mean Max: 0.9309
Epoch 10/10, Train Loss: 0.2169 (value: 0.0010, weighted value: 0.0520, policy: 0.1649, weighted policy: 0.1649), Train Mean Max: 0.9309
..training done in 68.28 seconds
..evaluation done in 19.14 seconds
Old network+MCTS average reward: 0.6696, min: 0.0833, max: 1.6667, stdev: 0.2563
New network+MCTS average reward: 0.6694, min: 0.0833, max: 1.6667, stdev: 0.2527
Old bare network average reward: 0.6132, min: 0.0463, max: 1.6667, stdev: 0.2579
New bare network average reward: 0.6102, min: 0.0556, max: 1.6667, stdev: 0.2654
External policy "random" average reward: 0.2689, min: -0.5463, max: 0.9259, stdev: 0.2495
External policy "individual greedy" average reward: 0.5486, min: -0.1111, max: 1.3056, stdev: 0.2677
External policy "total greedy" average reward: 0.6678, min: 0.0370, max: 1.5926, stdev: 0.2627
New network won 72 and tied 147 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 155 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.31 seconds
Training examples lengths: [64683, 64697, 64891, 64806, 64591, 64917, 65068, 64973, 64572, 64727]
Total value: 428667.93
Training on 647925 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2773 (value: 0.0018, weighted value: 0.0918, policy: 0.1855, weighted policy: 0.1855), Train Mean Max: 0.9275
Epoch 2/10, Train Loss: 0.2621 (value: 0.0016, weighted value: 0.0823, policy: 0.1797, weighted policy: 0.1797), Train Mean Max: 0.9283
Epoch 3/10, Train Loss: 0.2502 (value: 0.0015, weighted value: 0.0752, policy: 0.1750, weighted policy: 0.1750), Train Mean Max: 0.9287
Epoch 4/10, Train Loss: 0.2439 (value: 0.0015, weighted value: 0.0727, policy: 0.1713, weighted policy: 0.1713), Train Mean Max: 0.9289
Epoch 5/10, Train Loss: 0.2380 (value: 0.0014, weighted value: 0.0681, policy: 0.1700, weighted policy: 0.1700), Train Mean Max: 0.9290
Epoch 6/10, Train Loss: 0.2349 (value: 0.0013, weighted value: 0.0651, policy: 0.1698, weighted policy: 0.1698), Train Mean Max: 0.9293
Epoch 7/10, Train Loss: 0.2329 (value: 0.0013, weighted value: 0.0641, policy: 0.1688, weighted policy: 0.1688), Train Mean Max: 0.9294
Epoch 8/10, Train Loss: 0.2302 (value: 0.0012, weighted value: 0.0616, policy: 0.1687, weighted policy: 0.1687), Train Mean Max: 0.9295
Epoch 9/10, Train Loss: 0.2256 (value: 0.0012, weighted value: 0.0583, policy: 0.1673, weighted policy: 0.1673), Train Mean Max: 0.9298
Epoch 10/10, Train Loss: 0.2252 (value: 0.0012, weighted value: 0.0584, policy: 0.1668, weighted policy: 0.1668), Train Mean Max: 0.9300
..training done in 67.89 seconds
..evaluation done in 21.52 seconds
Old network+MCTS average reward: 0.6704, min: -0.0556, max: 1.5463, stdev: 0.2340
New network+MCTS average reward: 0.6697, min: -0.1759, max: 1.4722, stdev: 0.2315
Old bare network average reward: 0.6149, min: -0.1759, max: 1.4074, stdev: 0.2462
New bare network average reward: 0.6165, min: -0.1574, max: 1.4074, stdev: 0.2344
External policy "random" average reward: 0.2475, min: -0.3796, max: 0.9167, stdev: 0.2240
External policy "individual greedy" average reward: 0.5434, min: -0.0926, max: 1.2685, stdev: 0.2333
External policy "total greedy" average reward: 0.6594, min: 0.0833, max: 1.4167, stdev: 0.2285
New network won 83 and tied 138 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 156 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.71 seconds
Training examples lengths: [64697, 64891, 64806, 64591, 64917, 65068, 64973, 64572, 64727, 64912]
Total value: 429206.55
Training on 648154 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2517 (value: 0.0015, weighted value: 0.0756, policy: 0.1761, weighted policy: 0.1761), Train Mean Max: 0.9287
Epoch 2/10, Train Loss: 0.2442 (value: 0.0014, weighted value: 0.0716, policy: 0.1726, weighted policy: 0.1726), Train Mean Max: 0.9291
Epoch 3/10, Train Loss: 0.2385 (value: 0.0014, weighted value: 0.0678, policy: 0.1706, weighted policy: 0.1706), Train Mean Max: 0.9294
Epoch 4/10, Train Loss: 0.2323 (value: 0.0013, weighted value: 0.0632, policy: 0.1691, weighted policy: 0.1691), Train Mean Max: 0.9299
Epoch 5/10, Train Loss: 0.2297 (value: 0.0012, weighted value: 0.0618, policy: 0.1679, weighted policy: 0.1679), Train Mean Max: 0.9299
Epoch 6/10, Train Loss: 0.2272 (value: 0.0012, weighted value: 0.0599, policy: 0.1672, weighted policy: 0.1672), Train Mean Max: 0.9300
Epoch 7/10, Train Loss: 0.2246 (value: 0.0012, weighted value: 0.0586, policy: 0.1661, weighted policy: 0.1661), Train Mean Max: 0.9302
Epoch 8/10, Train Loss: 0.2217 (value: 0.0011, weighted value: 0.0562, policy: 0.1655, weighted policy: 0.1655), Train Mean Max: 0.9304
Epoch 9/10, Train Loss: 0.2199 (value: 0.0011, weighted value: 0.0545, policy: 0.1654, weighted policy: 0.1654), Train Mean Max: 0.9306
Epoch 10/10, Train Loss: 0.2181 (value: 0.0011, weighted value: 0.0537, policy: 0.1644, weighted policy: 0.1644), Train Mean Max: 0.9307
..training done in 74.64 seconds
..evaluation done in 17.88 seconds
Old network+MCTS average reward: 0.6683, min: 0.0185, max: 1.4074, stdev: 0.2282
New network+MCTS average reward: 0.6659, min: -0.1574, max: 1.4074, stdev: 0.2288
Old bare network average reward: 0.6097, min: -0.0926, max: 1.4074, stdev: 0.2366
New bare network average reward: 0.6149, min: -0.0926, max: 1.4074, stdev: 0.2391
External policy "random" average reward: 0.2735, min: -0.4444, max: 1.0648, stdev: 0.2382
External policy "individual greedy" average reward: 0.5465, min: -0.1481, max: 1.3148, stdev: 0.2385
External policy "total greedy" average reward: 0.6642, min: 0.1111, max: 1.4907, stdev: 0.2323
New network won 69 and tied 154 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 157 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.17 seconds
Training examples lengths: [64891, 64806, 64591, 64917, 65068, 64973, 64572, 64727, 64912, 64876]
Total value: 430299.90
Training on 648333 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2823 (value: 0.0020, weighted value: 0.0982, policy: 0.1841, weighted policy: 0.1841), Train Mean Max: 0.9274
Epoch 2/10, Train Loss: 0.2620 (value: 0.0017, weighted value: 0.0831, policy: 0.1789, weighted policy: 0.1789), Train Mean Max: 0.9282
Epoch 3/10, Train Loss: 0.2536 (value: 0.0016, weighted value: 0.0794, policy: 0.1742, weighted policy: 0.1742), Train Mean Max: 0.9285
Epoch 4/10, Train Loss: 0.2490 (value: 0.0015, weighted value: 0.0768, policy: 0.1722, weighted policy: 0.1722), Train Mean Max: 0.9286
Epoch 5/10, Train Loss: 0.2423 (value: 0.0014, weighted value: 0.0705, policy: 0.1719, weighted policy: 0.1719), Train Mean Max: 0.9288
Epoch 6/10, Train Loss: 0.2388 (value: 0.0014, weighted value: 0.0684, policy: 0.1704, weighted policy: 0.1704), Train Mean Max: 0.9290
Epoch 7/10, Train Loss: 0.2338 (value: 0.0013, weighted value: 0.0653, policy: 0.1685, weighted policy: 0.1685), Train Mean Max: 0.9292
Epoch 8/10, Train Loss: 0.2320 (value: 0.0013, weighted value: 0.0647, policy: 0.1673, weighted policy: 0.1673), Train Mean Max: 0.9295
Epoch 9/10, Train Loss: 0.2273 (value: 0.0012, weighted value: 0.0600, policy: 0.1672, weighted policy: 0.1672), Train Mean Max: 0.9298
Epoch 10/10, Train Loss: 0.2273 (value: 0.0012, weighted value: 0.0604, policy: 0.1668, weighted policy: 0.1668), Train Mean Max: 0.9298
..training done in 63.08 seconds
..evaluation done in 19.06 seconds
Old network+MCTS average reward: 0.6619, min: 0.0926, max: 1.5278, stdev: 0.2322
New network+MCTS average reward: 0.6606, min: 0.0926, max: 1.5370, stdev: 0.2310
Old bare network average reward: 0.6058, min: 0.0278, max: 1.3333, stdev: 0.2426
New bare network average reward: 0.6073, min: 0.0000, max: 1.4537, stdev: 0.2449
External policy "random" average reward: 0.2516, min: -0.3056, max: 0.9259, stdev: 0.2382
External policy "individual greedy" average reward: 0.5347, min: -0.0093, max: 1.2963, stdev: 0.2381
External policy "total greedy" average reward: 0.6527, min: 0.1389, max: 1.4259, stdev: 0.2296
New network won 84 and tied 139 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 158 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.90 seconds
Training examples lengths: [64806, 64591, 64917, 65068, 64973, 64572, 64727, 64912, 64876, 64933]
Total value: 430843.26
Training on 648375 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2546 (value: 0.0016, weighted value: 0.0782, policy: 0.1763, weighted policy: 0.1763), Train Mean Max: 0.9287
Epoch 2/10, Train Loss: 0.2439 (value: 0.0014, weighted value: 0.0718, policy: 0.1720, weighted policy: 0.1720), Train Mean Max: 0.9291
Epoch 3/10, Train Loss: 0.2389 (value: 0.0014, weighted value: 0.0687, policy: 0.1703, weighted policy: 0.1703), Train Mean Max: 0.9292
Epoch 4/10, Train Loss: 0.2353 (value: 0.0013, weighted value: 0.0660, policy: 0.1693, weighted policy: 0.1693), Train Mean Max: 0.9296
Epoch 5/10, Train Loss: 0.2306 (value: 0.0013, weighted value: 0.0632, policy: 0.1673, weighted policy: 0.1673), Train Mean Max: 0.9298
Epoch 6/10, Train Loss: 0.2281 (value: 0.0012, weighted value: 0.0615, policy: 0.1666, weighted policy: 0.1666), Train Mean Max: 0.9301
Epoch 7/10, Train Loss: 0.2252 (value: 0.0012, weighted value: 0.0581, policy: 0.1672, weighted policy: 0.1672), Train Mean Max: 0.9301
Epoch 8/10, Train Loss: 0.2225 (value: 0.0011, weighted value: 0.0566, policy: 0.1659, weighted policy: 0.1659), Train Mean Max: 0.9303
Epoch 9/10, Train Loss: 0.2213 (value: 0.0011, weighted value: 0.0560, policy: 0.1653, weighted policy: 0.1653), Train Mean Max: 0.9305
Epoch 10/10, Train Loss: 0.2204 (value: 0.0011, weighted value: 0.0551, policy: 0.1653, weighted policy: 0.1653), Train Mean Max: 0.9306
..training done in 63.09 seconds
..evaluation done in 17.66 seconds
Old network+MCTS average reward: 0.6552, min: 0.0185, max: 1.4167, stdev: 0.2409
New network+MCTS average reward: 0.6537, min: 0.0185, max: 1.4167, stdev: 0.2408
Old bare network average reward: 0.5973, min: 0.0093, max: 1.2593, stdev: 0.2455
New bare network average reward: 0.5974, min: -0.1019, max: 1.2593, stdev: 0.2486
External policy "random" average reward: 0.2649, min: -0.4167, max: 1.1204, stdev: 0.2499
External policy "individual greedy" average reward: 0.5439, min: -0.0926, max: 1.3519, stdev: 0.2404
External policy "total greedy" average reward: 0.6476, min: 0.0370, max: 1.2963, stdev: 0.2364
New network won 77 and tied 133 out of 300 games (47.83% wins where ties are half wins)
Reverting to the old network

Training iteration 159 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.70 seconds
Training examples lengths: [64591, 64917, 65068, 64973, 64572, 64727, 64912, 64876, 64933, 64854]
Total value: 431258.53
Training on 648423 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2815 (value: 0.0020, weighted value: 0.0977, policy: 0.1839, weighted policy: 0.1839), Train Mean Max: 0.9273
Epoch 2/10, Train Loss: 0.2642 (value: 0.0017, weighted value: 0.0861, policy: 0.1781, weighted policy: 0.1781), Train Mean Max: 0.9279
Epoch 3/10, Train Loss: 0.2550 (value: 0.0016, weighted value: 0.0804, policy: 0.1745, weighted policy: 0.1745), Train Mean Max: 0.9281
Epoch 4/10, Train Loss: 0.2480 (value: 0.0015, weighted value: 0.0757, policy: 0.1723, weighted policy: 0.1723), Train Mean Max: 0.9285
Epoch 5/10, Train Loss: 0.2438 (value: 0.0015, weighted value: 0.0727, policy: 0.1711, weighted policy: 0.1711), Train Mean Max: 0.9287
Epoch 6/10, Train Loss: 0.2394 (value: 0.0014, weighted value: 0.0695, policy: 0.1699, weighted policy: 0.1699), Train Mean Max: 0.9290
Epoch 7/10, Train Loss: 0.2343 (value: 0.0013, weighted value: 0.0656, policy: 0.1688, weighted policy: 0.1688), Train Mean Max: 0.9292
Epoch 8/10, Train Loss: 0.2328 (value: 0.0013, weighted value: 0.0646, policy: 0.1682, weighted policy: 0.1682), Train Mean Max: 0.9294
Epoch 9/10, Train Loss: 0.2299 (value: 0.0012, weighted value: 0.0621, policy: 0.1678, weighted policy: 0.1678), Train Mean Max: 0.9296
Epoch 10/10, Train Loss: 0.2279 (value: 0.0012, weighted value: 0.0600, policy: 0.1679, weighted policy: 0.1679), Train Mean Max: 0.9297
..training done in 64.73 seconds
..evaluation done in 18.09 seconds
Old network+MCTS average reward: 0.6567, min: 0.0000, max: 1.5093, stdev: 0.2080
New network+MCTS average reward: 0.6554, min: 0.1296, max: 1.5093, stdev: 0.2088
Old bare network average reward: 0.6015, min: -0.0556, max: 1.3333, stdev: 0.2134
New bare network average reward: 0.5943, min: -0.0185, max: 1.5093, stdev: 0.2212
External policy "random" average reward: 0.2498, min: -0.2500, max: 0.8241, stdev: 0.2122
External policy "individual greedy" average reward: 0.5412, min: -0.1019, max: 1.3611, stdev: 0.2155
External policy "total greedy" average reward: 0.6541, min: 0.1389, max: 1.3796, stdev: 0.2129
New network won 69 and tied 145 out of 300 games (47.17% wins where ties are half wins)
Reverting to the old network

Training iteration 160 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.60 seconds
Training examples lengths: [64917, 65068, 64973, 64572, 64727, 64912, 64876, 64933, 64854, 65016]
Total value: 431744.31
Training on 648848 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3079 (value: 0.0023, weighted value: 0.1166, policy: 0.1913, weighted policy: 0.1913), Train Mean Max: 0.9261
Epoch 2/10, Train Loss: 0.2860 (value: 0.0020, weighted value: 0.1013, policy: 0.1847, weighted policy: 0.1847), Train Mean Max: 0.9269
Epoch 3/10, Train Loss: 0.2719 (value: 0.0019, weighted value: 0.0931, policy: 0.1788, weighted policy: 0.1788), Train Mean Max: 0.9273
Epoch 4/10, Train Loss: 0.2611 (value: 0.0017, weighted value: 0.0857, policy: 0.1754, weighted policy: 0.1754), Train Mean Max: 0.9277
Epoch 5/10, Train Loss: 0.2554 (value: 0.0016, weighted value: 0.0810, policy: 0.1744, weighted policy: 0.1744), Train Mean Max: 0.9278
Epoch 6/10, Train Loss: 0.2492 (value: 0.0015, weighted value: 0.0773, policy: 0.1718, weighted policy: 0.1718), Train Mean Max: 0.9282
Epoch 7/10, Train Loss: 0.2450 (value: 0.0015, weighted value: 0.0729, policy: 0.1721, weighted policy: 0.1721), Train Mean Max: 0.9283
Epoch 8/10, Train Loss: 0.2406 (value: 0.0014, weighted value: 0.0708, policy: 0.1698, weighted policy: 0.1698), Train Mean Max: 0.9286
Epoch 9/10, Train Loss: 0.2377 (value: 0.0014, weighted value: 0.0680, policy: 0.1697, weighted policy: 0.1697), Train Mean Max: 0.9287
Epoch 10/10, Train Loss: 0.2339 (value: 0.0013, weighted value: 0.0646, policy: 0.1694, weighted policy: 0.1694), Train Mean Max: 0.9291
..training done in 64.12 seconds
..evaluation done in 17.92 seconds
Old network+MCTS average reward: 0.6416, min: 0.0093, max: 1.2963, stdev: 0.2161
New network+MCTS average reward: 0.6470, min: 0.0926, max: 1.3241, stdev: 0.2118
Old bare network average reward: 0.5808, min: -0.0278, max: 1.3426, stdev: 0.2141
New bare network average reward: 0.5835, min: -0.0278, max: 1.3426, stdev: 0.2202
External policy "random" average reward: 0.2502, min: -0.3056, max: 0.8519, stdev: 0.2054
External policy "individual greedy" average reward: 0.5135, min: -0.0463, max: 1.0833, stdev: 0.2165
External policy "total greedy" average reward: 0.6348, min: 0.1019, max: 1.1759, stdev: 0.2118
New network won 98 and tied 137 out of 300 games (55.50% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_160

Training iteration 161 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 55.59 seconds
Training examples lengths: [65068, 64973, 64572, 64727, 64912, 64876, 64933, 64854, 65016, 64573]
Total value: 431641.31
Training on 648504 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2636 (value: 0.0017, weighted value: 0.0847, policy: 0.1789, weighted policy: 0.1789), Train Mean Max: 0.9278
Epoch 2/10, Train Loss: 0.2510 (value: 0.0015, weighted value: 0.0765, policy: 0.1745, weighted policy: 0.1745), Train Mean Max: 0.9283
Epoch 3/10, Train Loss: 0.2440 (value: 0.0014, weighted value: 0.0715, policy: 0.1725, weighted policy: 0.1725), Train Mean Max: 0.9285
Epoch 4/10, Train Loss: 0.2398 (value: 0.0014, weighted value: 0.0688, policy: 0.1710, weighted policy: 0.1710), Train Mean Max: 0.9286
Epoch 5/10, Train Loss: 0.2377 (value: 0.0014, weighted value: 0.0682, policy: 0.1694, weighted policy: 0.1694), Train Mean Max: 0.9289
Epoch 6/10, Train Loss: 0.2317 (value: 0.0013, weighted value: 0.0630, policy: 0.1688, weighted policy: 0.1688), Train Mean Max: 0.9292
Epoch 7/10, Train Loss: 0.2325 (value: 0.0013, weighted value: 0.0640, policy: 0.1686, weighted policy: 0.1686), Train Mean Max: 0.9294
Epoch 8/10, Train Loss: 0.2274 (value: 0.0012, weighted value: 0.0595, policy: 0.1679, weighted policy: 0.1679), Train Mean Max: 0.9295
Epoch 9/10, Train Loss: 0.2253 (value: 0.0012, weighted value: 0.0589, policy: 0.1664, weighted policy: 0.1664), Train Mean Max: 0.9298
Epoch 10/10, Train Loss: 0.2233 (value: 0.0011, weighted value: 0.0569, policy: 0.1664, weighted policy: 0.1664), Train Mean Max: 0.9299
..training done in 63.18 seconds
..evaluation done in 18.48 seconds
Old network+MCTS average reward: 0.6411, min: -0.0741, max: 1.4167, stdev: 0.2194
New network+MCTS average reward: 0.6439, min: -0.0741, max: 1.4074, stdev: 0.2173
Old bare network average reward: 0.5875, min: -0.0833, max: 1.3981, stdev: 0.2269
New bare network average reward: 0.5908, min: -0.0556, max: 1.4167, stdev: 0.2269
External policy "random" average reward: 0.2497, min: -0.3241, max: 0.9444, stdev: 0.2229
External policy "individual greedy" average reward: 0.5335, min: -0.1667, max: 1.2130, stdev: 0.2131
External policy "total greedy" average reward: 0.6365, min: -0.0093, max: 1.2963, stdev: 0.2108
New network won 84 and tied 151 out of 300 games (53.17% wins where ties are half wins)
Keeping the new network

Training iteration 162 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.04 seconds
Training examples lengths: [64973, 64572, 64727, 64912, 64876, 64933, 64854, 65016, 64573, 64763]
Total value: 431759.56
Training on 648199 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2519 (value: 0.0015, weighted value: 0.0755, policy: 0.1764, weighted policy: 0.1764), Train Mean Max: 0.9286
Epoch 2/10, Train Loss: 0.2411 (value: 0.0014, weighted value: 0.0693, policy: 0.1717, weighted policy: 0.1717), Train Mean Max: 0.9290
Epoch 3/10, Train Loss: 0.2350 (value: 0.0013, weighted value: 0.0658, policy: 0.1692, weighted policy: 0.1692), Train Mean Max: 0.9293
Epoch 4/10, Train Loss: 0.2325 (value: 0.0013, weighted value: 0.0628, policy: 0.1697, weighted policy: 0.1697), Train Mean Max: 0.9295
Epoch 5/10, Train Loss: 0.2297 (value: 0.0012, weighted value: 0.0619, policy: 0.1677, weighted policy: 0.1677), Train Mean Max: 0.9298
Epoch 6/10, Train Loss: 0.2246 (value: 0.0012, weighted value: 0.0584, policy: 0.1662, weighted policy: 0.1662), Train Mean Max: 0.9300
Epoch 7/10, Train Loss: 0.2244 (value: 0.0011, weighted value: 0.0575, policy: 0.1670, weighted policy: 0.1670), Train Mean Max: 0.9299
Epoch 8/10, Train Loss: 0.2224 (value: 0.0011, weighted value: 0.0562, policy: 0.1662, weighted policy: 0.1662), Train Mean Max: 0.9303
Epoch 9/10, Train Loss: 0.2194 (value: 0.0011, weighted value: 0.0542, policy: 0.1652, weighted policy: 0.1652), Train Mean Max: 0.9304
Epoch 10/10, Train Loss: 0.2175 (value: 0.0011, weighted value: 0.0529, policy: 0.1646, weighted policy: 0.1646), Train Mean Max: 0.9308
..training done in 59.03 seconds
..evaluation done in 17.82 seconds
Old network+MCTS average reward: 0.6711, min: 0.0648, max: 1.3426, stdev: 0.2432
New network+MCTS average reward: 0.6697, min: 0.0648, max: 1.3426, stdev: 0.2396
Old bare network average reward: 0.6184, min: 0.0741, max: 1.2778, stdev: 0.2409
New bare network average reward: 0.6179, min: 0.0370, max: 1.3426, stdev: 0.2389
External policy "random" average reward: 0.2590, min: -0.3519, max: 0.8611, stdev: 0.2256
External policy "individual greedy" average reward: 0.5639, min: -0.0741, max: 1.3241, stdev: 0.2363
External policy "total greedy" average reward: 0.6707, min: 0.0278, max: 1.4444, stdev: 0.2281
New network won 71 and tied 150 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 163 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.52 seconds
Training examples lengths: [64572, 64727, 64912, 64876, 64933, 64854, 65016, 64573, 64763, 64958]
Total value: 431276.96
Training on 648184 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2826 (value: 0.0019, weighted value: 0.0974, policy: 0.1852, weighted policy: 0.1852), Train Mean Max: 0.9271
Epoch 2/10, Train Loss: 0.2638 (value: 0.0017, weighted value: 0.0851, policy: 0.1787, weighted policy: 0.1787), Train Mean Max: 0.9280
Epoch 3/10, Train Loss: 0.2537 (value: 0.0016, weighted value: 0.0794, policy: 0.1742, weighted policy: 0.1742), Train Mean Max: 0.9282
Epoch 4/10, Train Loss: 0.2455 (value: 0.0015, weighted value: 0.0734, policy: 0.1722, weighted policy: 0.1722), Train Mean Max: 0.9285
Epoch 5/10, Train Loss: 0.2435 (value: 0.0014, weighted value: 0.0724, policy: 0.1711, weighted policy: 0.1711), Train Mean Max: 0.9287
Epoch 6/10, Train Loss: 0.2382 (value: 0.0014, weighted value: 0.0686, policy: 0.1696, weighted policy: 0.1696), Train Mean Max: 0.9288
Epoch 7/10, Train Loss: 0.2348 (value: 0.0013, weighted value: 0.0660, policy: 0.1688, weighted policy: 0.1688), Train Mean Max: 0.9291
Epoch 8/10, Train Loss: 0.2298 (value: 0.0012, weighted value: 0.0624, policy: 0.1674, weighted policy: 0.1674), Train Mean Max: 0.9294
Epoch 9/10, Train Loss: 0.2302 (value: 0.0013, weighted value: 0.0628, policy: 0.1674, weighted policy: 0.1674), Train Mean Max: 0.9295
Epoch 10/10, Train Loss: 0.2258 (value: 0.0012, weighted value: 0.0582, policy: 0.1675, weighted policy: 0.1675), Train Mean Max: 0.9296
..training done in 64.63 seconds
..evaluation done in 18.31 seconds
Old network+MCTS average reward: 0.6744, min: 0.1019, max: 1.3981, stdev: 0.2230
New network+MCTS average reward: 0.6739, min: 0.0648, max: 1.3981, stdev: 0.2219
Old bare network average reward: 0.6211, min: 0.0370, max: 1.3981, stdev: 0.2317
New bare network average reward: 0.6199, min: 0.0278, max: 1.3333, stdev: 0.2279
External policy "random" average reward: 0.2722, min: -0.3704, max: 1.2222, stdev: 0.2388
External policy "individual greedy" average reward: 0.5537, min: -0.0093, max: 1.4815, stdev: 0.2270
External policy "total greedy" average reward: 0.6710, min: 0.0370, max: 1.5556, stdev: 0.2156
New network won 78 and tied 136 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 164 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.05 seconds
Training examples lengths: [64727, 64912, 64876, 64933, 64854, 65016, 64573, 64763, 64958, 64857]
Total value: 432141.90
Training on 648469 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3093 (value: 0.0023, weighted value: 0.1160, policy: 0.1933, weighted policy: 0.1933), Train Mean Max: 0.9261
Epoch 2/10, Train Loss: 0.2818 (value: 0.0020, weighted value: 0.0983, policy: 0.1834, weighted policy: 0.1834), Train Mean Max: 0.9268
Epoch 3/10, Train Loss: 0.2683 (value: 0.0018, weighted value: 0.0893, policy: 0.1790, weighted policy: 0.1790), Train Mean Max: 0.9273
Epoch 4/10, Train Loss: 0.2594 (value: 0.0017, weighted value: 0.0848, policy: 0.1747, weighted policy: 0.1747), Train Mean Max: 0.9276
Epoch 5/10, Train Loss: 0.2522 (value: 0.0016, weighted value: 0.0789, policy: 0.1733, weighted policy: 0.1733), Train Mean Max: 0.9277
Epoch 6/10, Train Loss: 0.2477 (value: 0.0015, weighted value: 0.0751, policy: 0.1726, weighted policy: 0.1726), Train Mean Max: 0.9280
Epoch 7/10, Train Loss: 0.2440 (value: 0.0015, weighted value: 0.0726, policy: 0.1714, weighted policy: 0.1714), Train Mean Max: 0.9282
Epoch 8/10, Train Loss: 0.2397 (value: 0.0014, weighted value: 0.0686, policy: 0.1711, weighted policy: 0.1711), Train Mean Max: 0.9283
Epoch 9/10, Train Loss: 0.2380 (value: 0.0014, weighted value: 0.0676, policy: 0.1704, weighted policy: 0.1704), Train Mean Max: 0.9286
Epoch 10/10, Train Loss: 0.2344 (value: 0.0013, weighted value: 0.0651, policy: 0.1693, weighted policy: 0.1693), Train Mean Max: 0.9288
..training done in 62.66 seconds
..evaluation done in 18.41 seconds
Old network+MCTS average reward: 0.6535, min: -0.2315, max: 1.3889, stdev: 0.2326
New network+MCTS average reward: 0.6561, min: -0.0093, max: 1.3889, stdev: 0.2294
Old bare network average reward: 0.5917, min: -0.0741, max: 1.3056, stdev: 0.2335
New bare network average reward: 0.6009, min: -0.1204, max: 1.3056, stdev: 0.2361
External policy "random" average reward: 0.2648, min: -0.3704, max: 1.0556, stdev: 0.2240
External policy "individual greedy" average reward: 0.5308, min: -0.0463, max: 1.1944, stdev: 0.2114
External policy "total greedy" average reward: 0.6510, min: 0.0556, max: 1.1759, stdev: 0.2128
New network won 89 and tied 133 out of 300 games (51.83% wins where ties are half wins)
Keeping the new network

Training iteration 165 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.59 seconds
Training examples lengths: [64912, 64876, 64933, 64854, 65016, 64573, 64763, 64958, 64857, 64579]
Total value: 432256.05
Training on 648321 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2613 (value: 0.0017, weighted value: 0.0834, policy: 0.1779, weighted policy: 0.1779), Train Mean Max: 0.9276
Epoch 2/10, Train Loss: 0.2503 (value: 0.0015, weighted value: 0.0743, policy: 0.1759, weighted policy: 0.1759), Train Mean Max: 0.9279
Epoch 3/10, Train Loss: 0.2445 (value: 0.0014, weighted value: 0.0715, policy: 0.1730, weighted policy: 0.1730), Train Mean Max: 0.9284
Epoch 4/10, Train Loss: 0.2389 (value: 0.0014, weighted value: 0.0678, policy: 0.1711, weighted policy: 0.1711), Train Mean Max: 0.9284
Epoch 5/10, Train Loss: 0.2354 (value: 0.0013, weighted value: 0.0657, policy: 0.1697, weighted policy: 0.1697), Train Mean Max: 0.9287
Epoch 6/10, Train Loss: 0.2323 (value: 0.0013, weighted value: 0.0635, policy: 0.1688, weighted policy: 0.1688), Train Mean Max: 0.9288
Epoch 7/10, Train Loss: 0.2316 (value: 0.0013, weighted value: 0.0627, policy: 0.1689, weighted policy: 0.1689), Train Mean Max: 0.9290
Epoch 8/10, Train Loss: 0.2263 (value: 0.0012, weighted value: 0.0585, policy: 0.1678, weighted policy: 0.1678), Train Mean Max: 0.9293
Epoch 9/10, Train Loss: 0.2242 (value: 0.0011, weighted value: 0.0568, policy: 0.1674, weighted policy: 0.1674), Train Mean Max: 0.9295
Epoch 10/10, Train Loss: 0.2250 (value: 0.0012, weighted value: 0.0585, policy: 0.1664, weighted policy: 0.1664), Train Mean Max: 0.9297
..training done in 73.14 seconds
..evaluation done in 19.89 seconds
Old network+MCTS average reward: 0.6681, min: 0.0833, max: 1.3148, stdev: 0.2140
New network+MCTS average reward: 0.6644, min: 0.0833, max: 1.3148, stdev: 0.2119
Old bare network average reward: 0.6101, min: 0.0185, max: 1.2315, stdev: 0.2201
New bare network average reward: 0.6087, min: 0.0833, max: 1.3148, stdev: 0.2220
External policy "random" average reward: 0.2612, min: -0.3981, max: 0.9722, stdev: 0.2333
External policy "individual greedy" average reward: 0.5360, min: -0.1389, max: 1.3981, stdev: 0.2138
External policy "total greedy" average reward: 0.6528, min: -0.0556, max: 1.2870, stdev: 0.2070
New network won 70 and tied 147 out of 300 games (47.83% wins where ties are half wins)
Reverting to the old network

Training iteration 166 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.92 seconds
Training examples lengths: [64876, 64933, 64854, 65016, 64573, 64763, 64958, 64857, 64579, 64831]
Total value: 432189.67
Training on 648240 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2872 (value: 0.0020, weighted value: 0.1009, policy: 0.1863, weighted policy: 0.1863), Train Mean Max: 0.9263
Epoch 2/10, Train Loss: 0.2709 (value: 0.0018, weighted value: 0.0900, policy: 0.1809, weighted policy: 0.1809), Train Mean Max: 0.9269
Epoch 3/10, Train Loss: 0.2602 (value: 0.0017, weighted value: 0.0825, policy: 0.1777, weighted policy: 0.1777), Train Mean Max: 0.9272
Epoch 4/10, Train Loss: 0.2557 (value: 0.0016, weighted value: 0.0799, policy: 0.1758, weighted policy: 0.1758), Train Mean Max: 0.9274
Epoch 5/10, Train Loss: 0.2482 (value: 0.0015, weighted value: 0.0751, policy: 0.1731, weighted policy: 0.1731), Train Mean Max: 0.9277
Epoch 6/10, Train Loss: 0.2428 (value: 0.0014, weighted value: 0.0711, policy: 0.1717, weighted policy: 0.1717), Train Mean Max: 0.9280
Epoch 7/10, Train Loss: 0.2393 (value: 0.0014, weighted value: 0.0685, policy: 0.1708, weighted policy: 0.1708), Train Mean Max: 0.9281
Epoch 8/10, Train Loss: 0.2358 (value: 0.0013, weighted value: 0.0651, policy: 0.1707, weighted policy: 0.1707), Train Mean Max: 0.9283
Epoch 9/10, Train Loss: 0.2346 (value: 0.0013, weighted value: 0.0644, policy: 0.1702, weighted policy: 0.1702), Train Mean Max: 0.9285
Epoch 10/10, Train Loss: 0.2327 (value: 0.0013, weighted value: 0.0628, policy: 0.1699, weighted policy: 0.1699), Train Mean Max: 0.9288
..training done in 69.55 seconds
..evaluation done in 21.14 seconds
Old network+MCTS average reward: 0.6688, min: 0.0833, max: 1.3889, stdev: 0.2258
New network+MCTS average reward: 0.6703, min: -0.0185, max: 1.3889, stdev: 0.2282
Old bare network average reward: 0.6188, min: -0.0648, max: 1.3889, stdev: 0.2388
New bare network average reward: 0.6174, min: -0.0648, max: 1.3889, stdev: 0.2411
External policy "random" average reward: 0.2708, min: -0.3148, max: 0.9815, stdev: 0.2187
External policy "individual greedy" average reward: 0.5452, min: 0.0000, max: 1.2593, stdev: 0.2171
External policy "total greedy" average reward: 0.6626, min: 0.1111, max: 1.2315, stdev: 0.2136
New network won 80 and tied 150 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 167 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.50 seconds
Training examples lengths: [64933, 64854, 65016, 64573, 64763, 64958, 64857, 64579, 64831, 64970]
Total value: 431863.68
Training on 648334 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2568 (value: 0.0016, weighted value: 0.0788, policy: 0.1780, weighted policy: 0.1780), Train Mean Max: 0.9274
Epoch 2/10, Train Loss: 0.2472 (value: 0.0014, weighted value: 0.0724, policy: 0.1748, weighted policy: 0.1748), Train Mean Max: 0.9279
Epoch 3/10, Train Loss: 0.2410 (value: 0.0014, weighted value: 0.0690, policy: 0.1720, weighted policy: 0.1720), Train Mean Max: 0.9283
Epoch 4/10, Train Loss: 0.2379 (value: 0.0013, weighted value: 0.0672, policy: 0.1707, weighted policy: 0.1707), Train Mean Max: 0.9285
Epoch 5/10, Train Loss: 0.2328 (value: 0.0013, weighted value: 0.0632, policy: 0.1696, weighted policy: 0.1696), Train Mean Max: 0.9288
Epoch 6/10, Train Loss: 0.2312 (value: 0.0012, weighted value: 0.0612, policy: 0.1701, weighted policy: 0.1701), Train Mean Max: 0.9288
Epoch 7/10, Train Loss: 0.2276 (value: 0.0012, weighted value: 0.0591, policy: 0.1685, weighted policy: 0.1685), Train Mean Max: 0.9292
Epoch 8/10, Train Loss: 0.2266 (value: 0.0012, weighted value: 0.0587, policy: 0.1678, weighted policy: 0.1678), Train Mean Max: 0.9294
Epoch 9/10, Train Loss: 0.2234 (value: 0.0011, weighted value: 0.0565, policy: 0.1669, weighted policy: 0.1669), Train Mean Max: 0.9295
Epoch 10/10, Train Loss: 0.2245 (value: 0.0011, weighted value: 0.0564, policy: 0.1681, weighted policy: 0.1681), Train Mean Max: 0.9294
..training done in 61.13 seconds
..evaluation done in 19.61 seconds
Old network+MCTS average reward: 0.6611, min: 0.1481, max: 1.3056, stdev: 0.2150
New network+MCTS average reward: 0.6603, min: 0.1111, max: 1.2685, stdev: 0.2191
Old bare network average reward: 0.6079, min: 0.1667, max: 1.3056, stdev: 0.2146
New bare network average reward: 0.6086, min: 0.1019, max: 1.2222, stdev: 0.2169
External policy "random" average reward: 0.2659, min: -0.3056, max: 0.9722, stdev: 0.2058
External policy "individual greedy" average reward: 0.5340, min: -0.0093, max: 1.2222, stdev: 0.2112
External policy "total greedy" average reward: 0.6429, min: 0.1759, max: 1.2407, stdev: 0.2051
New network won 73 and tied 156 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 168 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.50 seconds
Training examples lengths: [64854, 65016, 64573, 64763, 64958, 64857, 64579, 64831, 64970, 64781]
Total value: 431873.00
Training on 648182 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2485 (value: 0.0014, weighted value: 0.0724, policy: 0.1761, weighted policy: 0.1761), Train Mean Max: 0.9284
Epoch 2/10, Train Loss: 0.2409 (value: 0.0014, weighted value: 0.0681, policy: 0.1728, weighted policy: 0.1728), Train Mean Max: 0.9289
Epoch 3/10, Train Loss: 0.2350 (value: 0.0013, weighted value: 0.0642, policy: 0.1707, weighted policy: 0.1707), Train Mean Max: 0.9291
Epoch 4/10, Train Loss: 0.2304 (value: 0.0012, weighted value: 0.0613, policy: 0.1691, weighted policy: 0.1691), Train Mean Max: 0.9294
Epoch 5/10, Train Loss: 0.2284 (value: 0.0012, weighted value: 0.0602, policy: 0.1682, weighted policy: 0.1682), Train Mean Max: 0.9296
Epoch 6/10, Train Loss: 0.2243 (value: 0.0011, weighted value: 0.0564, policy: 0.1679, weighted policy: 0.1679), Train Mean Max: 0.9298
Epoch 7/10, Train Loss: 0.2243 (value: 0.0012, weighted value: 0.0578, policy: 0.1665, weighted policy: 0.1665), Train Mean Max: 0.9300
Epoch 8/10, Train Loss: 0.2199 (value: 0.0011, weighted value: 0.0537, policy: 0.1662, weighted policy: 0.1662), Train Mean Max: 0.9300
Epoch 9/10, Train Loss: 0.2197 (value: 0.0011, weighted value: 0.0531, policy: 0.1666, weighted policy: 0.1666), Train Mean Max: 0.9302
Epoch 10/10, Train Loss: 0.2166 (value: 0.0010, weighted value: 0.0517, policy: 0.1649, weighted policy: 0.1649), Train Mean Max: 0.9304
..training done in 64.64 seconds
..evaluation done in 20.36 seconds
Old network+MCTS average reward: 0.6689, min: 0.1111, max: 1.2963, stdev: 0.2243
New network+MCTS average reward: 0.6705, min: 0.1019, max: 1.2593, stdev: 0.2242
Old bare network average reward: 0.6084, min: 0.0185, max: 1.2593, stdev: 0.2317
New bare network average reward: 0.6095, min: -0.0278, max: 1.2593, stdev: 0.2314
External policy "random" average reward: 0.2771, min: -0.4444, max: 0.9630, stdev: 0.2333
External policy "individual greedy" average reward: 0.5447, min: 0.0000, max: 1.1574, stdev: 0.2234
External policy "total greedy" average reward: 0.6603, min: 0.1296, max: 1.4167, stdev: 0.2134
New network won 75 and tied 145 out of 300 games (49.17% wins where ties are half wins)
Reverting to the old network

Training iteration 169 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.32 seconds
Training examples lengths: [65016, 64573, 64763, 64958, 64857, 64579, 64831, 64970, 64781, 64985]
Total value: 432667.89
Training on 648313 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2760 (value: 0.0018, weighted value: 0.0920, policy: 0.1840, weighted policy: 0.1840), Train Mean Max: 0.9272
Epoch 2/10, Train Loss: 0.2620 (value: 0.0016, weighted value: 0.0822, policy: 0.1797, weighted policy: 0.1797), Train Mean Max: 0.9277
Epoch 3/10, Train Loss: 0.2509 (value: 0.0015, weighted value: 0.0758, policy: 0.1751, weighted policy: 0.1751), Train Mean Max: 0.9283
Epoch 4/10, Train Loss: 0.2444 (value: 0.0014, weighted value: 0.0720, policy: 0.1724, weighted policy: 0.1724), Train Mean Max: 0.9285
Epoch 5/10, Train Loss: 0.2394 (value: 0.0014, weighted value: 0.0684, policy: 0.1711, weighted policy: 0.1711), Train Mean Max: 0.9287
Epoch 6/10, Train Loss: 0.2359 (value: 0.0013, weighted value: 0.0657, policy: 0.1702, weighted policy: 0.1702), Train Mean Max: 0.9289
Epoch 7/10, Train Loss: 0.2322 (value: 0.0013, weighted value: 0.0628, policy: 0.1694, weighted policy: 0.1694), Train Mean Max: 0.9292
Epoch 8/10, Train Loss: 0.2300 (value: 0.0012, weighted value: 0.0614, policy: 0.1686, weighted policy: 0.1686), Train Mean Max: 0.9293
Epoch 9/10, Train Loss: 0.2286 (value: 0.0012, weighted value: 0.0610, policy: 0.1676, weighted policy: 0.1676), Train Mean Max: 0.9295
Epoch 10/10, Train Loss: 0.2234 (value: 0.0011, weighted value: 0.0566, policy: 0.1668, weighted policy: 0.1668), Train Mean Max: 0.9298
..training done in 61.90 seconds
..evaluation done in 18.37 seconds
Old network+MCTS average reward: 0.6417, min: 0.0741, max: 1.1852, stdev: 0.2199
New network+MCTS average reward: 0.6432, min: 0.1019, max: 1.1852, stdev: 0.2184
Old bare network average reward: 0.5794, min: -0.0370, max: 1.1389, stdev: 0.2282
New bare network average reward: 0.5812, min: -0.0370, max: 1.1389, stdev: 0.2235
External policy "random" average reward: 0.2337, min: -0.3704, max: 0.8519, stdev: 0.2283
External policy "individual greedy" average reward: 0.5154, min: -0.0556, max: 1.1852, stdev: 0.2191
External policy "total greedy" average reward: 0.6245, min: 0.0185, max: 1.3148, stdev: 0.2219
New network won 81 and tied 135 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 170 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.07 seconds
Training examples lengths: [64573, 64763, 64958, 64857, 64579, 64831, 64970, 64781, 64985, 64943]
Total value: 433085.72
Training on 648240 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3049 (value: 0.0022, weighted value: 0.1115, policy: 0.1935, weighted policy: 0.1935), Train Mean Max: 0.9262
Epoch 2/10, Train Loss: 0.2788 (value: 0.0019, weighted value: 0.0952, policy: 0.1836, weighted policy: 0.1836), Train Mean Max: 0.9269
Epoch 3/10, Train Loss: 0.2674 (value: 0.0018, weighted value: 0.0886, policy: 0.1788, weighted policy: 0.1788), Train Mean Max: 0.9272
Epoch 4/10, Train Loss: 0.2576 (value: 0.0017, weighted value: 0.0828, policy: 0.1748, weighted policy: 0.1748), Train Mean Max: 0.9276
Epoch 5/10, Train Loss: 0.2512 (value: 0.0015, weighted value: 0.0770, policy: 0.1741, weighted policy: 0.1741), Train Mean Max: 0.9276
Epoch 6/10, Train Loss: 0.2469 (value: 0.0015, weighted value: 0.0745, policy: 0.1724, weighted policy: 0.1724), Train Mean Max: 0.9279
Epoch 7/10, Train Loss: 0.2427 (value: 0.0014, weighted value: 0.0709, policy: 0.1718, weighted policy: 0.1718), Train Mean Max: 0.9281
Epoch 8/10, Train Loss: 0.2388 (value: 0.0014, weighted value: 0.0684, policy: 0.1705, weighted policy: 0.1705), Train Mean Max: 0.9285
Epoch 9/10, Train Loss: 0.2360 (value: 0.0013, weighted value: 0.0658, policy: 0.1702, weighted policy: 0.1702), Train Mean Max: 0.9288
Epoch 10/10, Train Loss: 0.2317 (value: 0.0013, weighted value: 0.0634, policy: 0.1684, weighted policy: 0.1684), Train Mean Max: 0.9289
..training done in 63.69 seconds
..evaluation done in 19.17 seconds
Old network+MCTS average reward: 0.6458, min: 0.0185, max: 1.4444, stdev: 0.2369
New network+MCTS average reward: 0.6531, min: 0.0093, max: 1.4444, stdev: 0.2399
Old bare network average reward: 0.5926, min: 0.0000, max: 1.3889, stdev: 0.2462
New bare network average reward: 0.5948, min: -0.0741, max: 1.3889, stdev: 0.2448
External policy "random" average reward: 0.2552, min: -0.4259, max: 0.7500, stdev: 0.2131
External policy "individual greedy" average reward: 0.5327, min: 0.0185, max: 1.2870, stdev: 0.2326
External policy "total greedy" average reward: 0.6411, min: 0.1204, max: 1.4167, stdev: 0.2218
New network won 93 and tied 141 out of 300 games (54.50% wins where ties are half wins)
Keeping the new network

Training iteration 171 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.83 seconds
Training examples lengths: [64763, 64958, 64857, 64579, 64831, 64970, 64781, 64985, 64943, 64948]
Total value: 433927.15
Training on 648615 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2566 (value: 0.0016, weighted value: 0.0800, policy: 0.1766, weighted policy: 0.1766), Train Mean Max: 0.9278
Epoch 2/10, Train Loss: 0.2480 (value: 0.0015, weighted value: 0.0741, policy: 0.1739, weighted policy: 0.1739), Train Mean Max: 0.9281
Epoch 3/10, Train Loss: 0.2422 (value: 0.0014, weighted value: 0.0701, policy: 0.1720, weighted policy: 0.1720), Train Mean Max: 0.9285
Epoch 4/10, Train Loss: 0.2380 (value: 0.0014, weighted value: 0.0676, policy: 0.1704, weighted policy: 0.1704), Train Mean Max: 0.9286
Epoch 5/10, Train Loss: 0.2351 (value: 0.0013, weighted value: 0.0658, policy: 0.1693, weighted policy: 0.1693), Train Mean Max: 0.9289
Epoch 6/10, Train Loss: 0.2306 (value: 0.0013, weighted value: 0.0625, policy: 0.1681, weighted policy: 0.1681), Train Mean Max: 0.9291
Epoch 7/10, Train Loss: 0.2273 (value: 0.0012, weighted value: 0.0605, policy: 0.1668, weighted policy: 0.1668), Train Mean Max: 0.9293
Epoch 8/10, Train Loss: 0.2246 (value: 0.0012, weighted value: 0.0583, policy: 0.1663, weighted policy: 0.1663), Train Mean Max: 0.9296
Epoch 9/10, Train Loss: 0.2251 (value: 0.0012, weighted value: 0.0580, policy: 0.1670, weighted policy: 0.1670), Train Mean Max: 0.9297
Epoch 10/10, Train Loss: 0.2220 (value: 0.0011, weighted value: 0.0556, policy: 0.1665, weighted policy: 0.1665), Train Mean Max: 0.9299
..training done in 66.00 seconds
..evaluation done in 17.66 seconds
Old network+MCTS average reward: 0.6511, min: 0.1111, max: 1.3889, stdev: 0.2319
New network+MCTS average reward: 0.6502, min: 0.1111, max: 1.2963, stdev: 0.2309
Old bare network average reward: 0.5885, min: 0.0463, max: 1.2963, stdev: 0.2408
New bare network average reward: 0.5940, min: 0.0463, max: 1.2963, stdev: 0.2341
External policy "random" average reward: 0.2693, min: -0.4444, max: 0.9537, stdev: 0.2299
External policy "individual greedy" average reward: 0.5438, min: -0.0556, max: 1.4167, stdev: 0.2408
External policy "total greedy" average reward: 0.6495, min: 0.1296, max: 1.4722, stdev: 0.2371
New network won 69 and tied 160 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 172 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 60.75 seconds
Training examples lengths: [64958, 64857, 64579, 64831, 64970, 64781, 64985, 64943, 64948, 64834]
Total value: 434866.44
Training on 648686 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2842 (value: 0.0020, weighted value: 0.0988, policy: 0.1854, weighted policy: 0.1854), Train Mean Max: 0.9267
Epoch 2/10, Train Loss: 0.2651 (value: 0.0017, weighted value: 0.0868, policy: 0.1783, weighted policy: 0.1783), Train Mean Max: 0.9274
Epoch 3/10, Train Loss: 0.2558 (value: 0.0016, weighted value: 0.0809, policy: 0.1750, weighted policy: 0.1750), Train Mean Max: 0.9277
Epoch 4/10, Train Loss: 0.2490 (value: 0.0015, weighted value: 0.0765, policy: 0.1724, weighted policy: 0.1724), Train Mean Max: 0.9281
Epoch 5/10, Train Loss: 0.2443 (value: 0.0015, weighted value: 0.0729, policy: 0.1714, weighted policy: 0.1714), Train Mean Max: 0.9282
Epoch 6/10, Train Loss: 0.2397 (value: 0.0014, weighted value: 0.0699, policy: 0.1697, weighted policy: 0.1697), Train Mean Max: 0.9286
Epoch 7/10, Train Loss: 0.2368 (value: 0.0014, weighted value: 0.0683, policy: 0.1685, weighted policy: 0.1685), Train Mean Max: 0.9288
Epoch 8/10, Train Loss: 0.2330 (value: 0.0013, weighted value: 0.0644, policy: 0.1686, weighted policy: 0.1686), Train Mean Max: 0.9291
Epoch 9/10, Train Loss: 0.2309 (value: 0.0013, weighted value: 0.0631, policy: 0.1678, weighted policy: 0.1678), Train Mean Max: 0.9294
Epoch 10/10, Train Loss: 0.2268 (value: 0.0012, weighted value: 0.0604, policy: 0.1664, weighted policy: 0.1664), Train Mean Max: 0.9295
..training done in 66.05 seconds
..evaluation done in 17.70 seconds
Old network+MCTS average reward: 0.6453, min: -0.0556, max: 1.5000, stdev: 0.2330
New network+MCTS average reward: 0.6443, min: -0.0741, max: 1.4815, stdev: 0.2351
Old bare network average reward: 0.5910, min: -0.0370, max: 1.5000, stdev: 0.2436
New bare network average reward: 0.5901, min: -0.0741, max: 1.4074, stdev: 0.2451
External policy "random" average reward: 0.2291, min: -0.3426, max: 0.9444, stdev: 0.2310
External policy "individual greedy" average reward: 0.5147, min: -0.1944, max: 1.2593, stdev: 0.2323
External policy "total greedy" average reward: 0.6339, min: 0.1111, max: 1.4815, stdev: 0.2202
New network won 64 and tied 164 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 173 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.84 seconds
Training examples lengths: [64857, 64579, 64831, 64970, 64781, 64985, 64943, 64948, 64834, 64778]
Total value: 435280.39
Training on 648506 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3074 (value: 0.0023, weighted value: 0.1165, policy: 0.1909, weighted policy: 0.1909), Train Mean Max: 0.9261
Epoch 2/10, Train Loss: 0.2837 (value: 0.0020, weighted value: 0.1004, policy: 0.1833, weighted policy: 0.1833), Train Mean Max: 0.9268
Epoch 3/10, Train Loss: 0.2694 (value: 0.0018, weighted value: 0.0920, policy: 0.1775, weighted policy: 0.1775), Train Mean Max: 0.9272
Epoch 4/10, Train Loss: 0.2618 (value: 0.0017, weighted value: 0.0864, policy: 0.1754, weighted policy: 0.1754), Train Mean Max: 0.9275
Epoch 5/10, Train Loss: 0.2525 (value: 0.0016, weighted value: 0.0803, policy: 0.1722, weighted policy: 0.1722), Train Mean Max: 0.9278
Epoch 6/10, Train Loss: 0.2470 (value: 0.0015, weighted value: 0.0765, policy: 0.1705, weighted policy: 0.1705), Train Mean Max: 0.9281
Epoch 7/10, Train Loss: 0.2465 (value: 0.0015, weighted value: 0.0753, policy: 0.1712, weighted policy: 0.1712), Train Mean Max: 0.9284
Epoch 8/10, Train Loss: 0.2390 (value: 0.0014, weighted value: 0.0698, policy: 0.1692, weighted policy: 0.1692), Train Mean Max: 0.9287
Epoch 9/10, Train Loss: 0.2361 (value: 0.0014, weighted value: 0.0678, policy: 0.1683, weighted policy: 0.1683), Train Mean Max: 0.9290
Epoch 10/10, Train Loss: 0.2340 (value: 0.0013, weighted value: 0.0656, policy: 0.1684, weighted policy: 0.1684), Train Mean Max: 0.9290
..training done in 65.43 seconds
..evaluation done in 19.17 seconds
Old network+MCTS average reward: 0.6407, min: 0.0741, max: 1.2870, stdev: 0.2271
New network+MCTS average reward: 0.6421, min: 0.0741, max: 1.2870, stdev: 0.2286
Old bare network average reward: 0.5872, min: -0.0833, max: 1.2870, stdev: 0.2371
New bare network average reward: 0.5851, min: -0.0278, max: 1.2778, stdev: 0.2421
External policy "random" average reward: 0.2406, min: -0.3981, max: 0.8889, stdev: 0.2194
External policy "individual greedy" average reward: 0.5239, min: -0.1019, max: 1.1111, stdev: 0.2160
External policy "total greedy" average reward: 0.6356, min: 0.0000, max: 1.2500, stdev: 0.2183
New network won 71 and tied 157 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 174 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.63 seconds
Training examples lengths: [64579, 64831, 64970, 64781, 64985, 64943, 64948, 64834, 64778, 64642]
Total value: 435310.28
Training on 648291 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3311 (value: 0.0027, weighted value: 0.1334, policy: 0.1977, weighted policy: 0.1977), Train Mean Max: 0.9249
Epoch 2/10, Train Loss: 0.2999 (value: 0.0023, weighted value: 0.1130, policy: 0.1869, weighted policy: 0.1869), Train Mean Max: 0.9259
Epoch 3/10, Train Loss: 0.2845 (value: 0.0021, weighted value: 0.1026, policy: 0.1819, weighted policy: 0.1819), Train Mean Max: 0.9263
Epoch 4/10, Train Loss: 0.2743 (value: 0.0019, weighted value: 0.0962, policy: 0.1781, weighted policy: 0.1781), Train Mean Max: 0.9266
Epoch 5/10, Train Loss: 0.2639 (value: 0.0017, weighted value: 0.0875, policy: 0.1765, weighted policy: 0.1765), Train Mean Max: 0.9271
Epoch 6/10, Train Loss: 0.2586 (value: 0.0017, weighted value: 0.0844, policy: 0.1742, weighted policy: 0.1742), Train Mean Max: 0.9272
Epoch 7/10, Train Loss: 0.2533 (value: 0.0016, weighted value: 0.0813, policy: 0.1720, weighted policy: 0.1720), Train Mean Max: 0.9275
Epoch 8/10, Train Loss: 0.2477 (value: 0.0015, weighted value: 0.0766, policy: 0.1711, weighted policy: 0.1711), Train Mean Max: 0.9278
Epoch 9/10, Train Loss: 0.2437 (value: 0.0015, weighted value: 0.0732, policy: 0.1704, weighted policy: 0.1704), Train Mean Max: 0.9281
Epoch 10/10, Train Loss: 0.2402 (value: 0.0014, weighted value: 0.0701, policy: 0.1701, weighted policy: 0.1701), Train Mean Max: 0.9283
..training done in 66.50 seconds
..evaluation done in 18.42 seconds
Old network+MCTS average reward: 0.6857, min: 0.1204, max: 1.4352, stdev: 0.2268
New network+MCTS average reward: 0.6897, min: 0.0370, max: 1.4352, stdev: 0.2265
Old bare network average reward: 0.6316, min: 0.0278, max: 1.4537, stdev: 0.2337
New bare network average reward: 0.6347, min: 0.0370, max: 1.4352, stdev: 0.2297
External policy "random" average reward: 0.2636, min: -0.3889, max: 1.0648, stdev: 0.2197
External policy "individual greedy" average reward: 0.5551, min: -0.0093, max: 1.1389, stdev: 0.2225
External policy "total greedy" average reward: 0.6635, min: 0.0463, max: 1.3981, stdev: 0.2250
New network won 72 and tied 158 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 175 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.00 seconds
Training examples lengths: [64831, 64970, 64781, 64985, 64943, 64948, 64834, 64778, 64642, 64922]
Total value: 436420.13
Training on 648634 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2617 (value: 0.0017, weighted value: 0.0852, policy: 0.1765, weighted policy: 0.1765), Train Mean Max: 0.9275
Epoch 2/10, Train Loss: 0.2537 (value: 0.0016, weighted value: 0.0797, policy: 0.1739, weighted policy: 0.1739), Train Mean Max: 0.9279
Epoch 3/10, Train Loss: 0.2460 (value: 0.0015, weighted value: 0.0749, policy: 0.1711, weighted policy: 0.1711), Train Mean Max: 0.9283
Epoch 4/10, Train Loss: 0.2407 (value: 0.0014, weighted value: 0.0714, policy: 0.1693, weighted policy: 0.1693), Train Mean Max: 0.9287
Epoch 5/10, Train Loss: 0.2364 (value: 0.0014, weighted value: 0.0676, policy: 0.1688, weighted policy: 0.1688), Train Mean Max: 0.9287
Epoch 6/10, Train Loss: 0.2332 (value: 0.0013, weighted value: 0.0659, policy: 0.1673, weighted policy: 0.1673), Train Mean Max: 0.9292
Epoch 7/10, Train Loss: 0.2321 (value: 0.0013, weighted value: 0.0650, policy: 0.1671, weighted policy: 0.1671), Train Mean Max: 0.9293
Epoch 8/10, Train Loss: 0.2268 (value: 0.0012, weighted value: 0.0610, policy: 0.1658, weighted policy: 0.1658), Train Mean Max: 0.9298
Epoch 9/10, Train Loss: 0.2262 (value: 0.0012, weighted value: 0.0608, policy: 0.1655, weighted policy: 0.1655), Train Mean Max: 0.9299
Epoch 10/10, Train Loss: 0.2228 (value: 0.0012, weighted value: 0.0577, policy: 0.1651, weighted policy: 0.1651), Train Mean Max: 0.9302
..training done in 64.84 seconds
..evaluation done in 18.56 seconds
Old network+MCTS average reward: 0.6651, min: 0.1574, max: 1.2222, stdev: 0.2210
New network+MCTS average reward: 0.6634, min: 0.1389, max: 1.2222, stdev: 0.2189
Old bare network average reward: 0.6082, min: 0.0556, max: 1.1852, stdev: 0.2262
New bare network average reward: 0.6046, min: -0.0093, max: 1.2685, stdev: 0.2285
External policy "random" average reward: 0.2410, min: -0.2500, max: 0.8704, stdev: 0.2152
External policy "individual greedy" average reward: 0.5332, min: -0.0648, max: 1.2407, stdev: 0.2228
External policy "total greedy" average reward: 0.6542, min: 0.2037, max: 1.3056, stdev: 0.2247
New network won 78 and tied 137 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 176 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.48 seconds
Training examples lengths: [64970, 64781, 64985, 64943, 64948, 64834, 64778, 64642, 64922, 64790]
Total value: 436865.56
Training on 648593 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2889 (value: 0.0021, weighted value: 0.1031, policy: 0.1858, weighted policy: 0.1858), Train Mean Max: 0.9263
Epoch 2/10, Train Loss: 0.2689 (value: 0.0018, weighted value: 0.0909, policy: 0.1781, weighted policy: 0.1781), Train Mean Max: 0.9273
Epoch 3/10, Train Loss: 0.2615 (value: 0.0017, weighted value: 0.0868, policy: 0.1747, weighted policy: 0.1747), Train Mean Max: 0.9275
Epoch 4/10, Train Loss: 0.2521 (value: 0.0016, weighted value: 0.0795, policy: 0.1726, weighted policy: 0.1726), Train Mean Max: 0.9279
Epoch 5/10, Train Loss: 0.2471 (value: 0.0015, weighted value: 0.0769, policy: 0.1702, weighted policy: 0.1702), Train Mean Max: 0.9282
Epoch 6/10, Train Loss: 0.2431 (value: 0.0015, weighted value: 0.0737, policy: 0.1694, weighted policy: 0.1694), Train Mean Max: 0.9284
Epoch 7/10, Train Loss: 0.2382 (value: 0.0014, weighted value: 0.0694, policy: 0.1689, weighted policy: 0.1689), Train Mean Max: 0.9287
Epoch 8/10, Train Loss: 0.2372 (value: 0.0014, weighted value: 0.0687, policy: 0.1686, weighted policy: 0.1686), Train Mean Max: 0.9288
Epoch 9/10, Train Loss: 0.2322 (value: 0.0013, weighted value: 0.0656, policy: 0.1666, weighted policy: 0.1666), Train Mean Max: 0.9294
Epoch 10/10, Train Loss: 0.2300 (value: 0.0013, weighted value: 0.0638, policy: 0.1663, weighted policy: 0.1663), Train Mean Max: 0.9295
..training done in 63.64 seconds
..evaluation done in 19.23 seconds
Old network+MCTS average reward: 0.6682, min: -0.2037, max: 1.2315, stdev: 0.2196
New network+MCTS average reward: 0.6670, min: -0.2593, max: 1.2315, stdev: 0.2185
Old bare network average reward: 0.6070, min: -0.1019, max: 1.2315, stdev: 0.2253
New bare network average reward: 0.6095, min: -0.1019, max: 1.2315, stdev: 0.2257
External policy "random" average reward: 0.2565, min: -0.3611, max: 0.7778, stdev: 0.2147
External policy "individual greedy" average reward: 0.5360, min: -0.1759, max: 1.1944, stdev: 0.2210
External policy "total greedy" average reward: 0.6533, min: -0.0741, max: 1.2778, stdev: 0.2138
New network won 85 and tied 137 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 177 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.96 seconds
Training examples lengths: [64781, 64985, 64943, 64948, 64834, 64778, 64642, 64922, 64790, 64900]
Total value: 437280.34
Training on 648523 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2527 (value: 0.0016, weighted value: 0.0791, policy: 0.1736, weighted policy: 0.1736), Train Mean Max: 0.9285
Epoch 2/10, Train Loss: 0.2441 (value: 0.0015, weighted value: 0.0735, policy: 0.1706, weighted policy: 0.1706), Train Mean Max: 0.9289
Epoch 3/10, Train Loss: 0.2394 (value: 0.0014, weighted value: 0.0703, policy: 0.1690, weighted policy: 0.1690), Train Mean Max: 0.9292
Epoch 4/10, Train Loss: 0.2340 (value: 0.0013, weighted value: 0.0667, policy: 0.1674, weighted policy: 0.1674), Train Mean Max: 0.9295
Epoch 5/10, Train Loss: 0.2306 (value: 0.0013, weighted value: 0.0644, policy: 0.1663, weighted policy: 0.1663), Train Mean Max: 0.9299
Epoch 6/10, Train Loss: 0.2279 (value: 0.0013, weighted value: 0.0627, policy: 0.1652, weighted policy: 0.1652), Train Mean Max: 0.9301
Epoch 7/10, Train Loss: 0.2228 (value: 0.0012, weighted value: 0.0590, policy: 0.1639, weighted policy: 0.1639), Train Mean Max: 0.9303
Epoch 8/10, Train Loss: 0.2237 (value: 0.0012, weighted value: 0.0593, policy: 0.1644, weighted policy: 0.1644), Train Mean Max: 0.9306
Epoch 9/10, Train Loss: 0.2215 (value: 0.0012, weighted value: 0.0578, policy: 0.1637, weighted policy: 0.1637), Train Mean Max: 0.9307
Epoch 10/10, Train Loss: 0.2169 (value: 0.0011, weighted value: 0.0542, policy: 0.1628, weighted policy: 0.1628), Train Mean Max: 0.9310
..training done in 64.84 seconds
..evaluation done in 17.83 seconds
Old network+MCTS average reward: 0.6751, min: -0.0185, max: 1.5185, stdev: 0.2206
New network+MCTS average reward: 0.6751, min: 0.0833, max: 1.5185, stdev: 0.2231
Old bare network average reward: 0.6243, min: -0.0370, max: 1.5185, stdev: 0.2363
New bare network average reward: 0.6241, min: 0.0185, max: 1.5185, stdev: 0.2318
External policy "random" average reward: 0.2679, min: -0.4444, max: 0.8889, stdev: 0.2333
External policy "individual greedy" average reward: 0.5502, min: -0.0370, max: 1.3981, stdev: 0.2294
External policy "total greedy" average reward: 0.6665, min: 0.0185, max: 1.4167, stdev: 0.2219
New network won 70 and tied 156 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 178 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.07 seconds
Training examples lengths: [64985, 64943, 64948, 64834, 64778, 64642, 64922, 64790, 64900, 64891]
Total value: 437876.81
Training on 648633 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2791 (value: 0.0019, weighted value: 0.0969, policy: 0.1822, weighted policy: 0.1822), Train Mean Max: 0.9274
Epoch 2/10, Train Loss: 0.2635 (value: 0.0017, weighted value: 0.0870, policy: 0.1764, weighted policy: 0.1764), Train Mean Max: 0.9279
Epoch 3/10, Train Loss: 0.2522 (value: 0.0016, weighted value: 0.0798, policy: 0.1724, weighted policy: 0.1724), Train Mean Max: 0.9284
Epoch 4/10, Train Loss: 0.2464 (value: 0.0015, weighted value: 0.0758, policy: 0.1706, weighted policy: 0.1706), Train Mean Max: 0.9287
Epoch 5/10, Train Loss: 0.2413 (value: 0.0015, weighted value: 0.0727, policy: 0.1686, weighted policy: 0.1686), Train Mean Max: 0.9290
Epoch 6/10, Train Loss: 0.2371 (value: 0.0014, weighted value: 0.0695, policy: 0.1676, weighted policy: 0.1676), Train Mean Max: 0.9293
Epoch 7/10, Train Loss: 0.2335 (value: 0.0013, weighted value: 0.0668, policy: 0.1667, weighted policy: 0.1667), Train Mean Max: 0.9295
Epoch 8/10, Train Loss: 0.2305 (value: 0.0013, weighted value: 0.0650, policy: 0.1655, weighted policy: 0.1655), Train Mean Max: 0.9300
Epoch 9/10, Train Loss: 0.2273 (value: 0.0013, weighted value: 0.0628, policy: 0.1644, weighted policy: 0.1644), Train Mean Max: 0.9301
Epoch 10/10, Train Loss: 0.2249 (value: 0.0012, weighted value: 0.0606, policy: 0.1644, weighted policy: 0.1644), Train Mean Max: 0.9304
..training done in 71.70 seconds
..evaluation done in 21.27 seconds
Old network+MCTS average reward: 0.6554, min: 0.1019, max: 1.4167, stdev: 0.2079
New network+MCTS average reward: 0.6621, min: 0.1019, max: 1.4167, stdev: 0.2151
Old bare network average reward: 0.6032, min: -0.1852, max: 1.3889, stdev: 0.2244
New bare network average reward: 0.6035, min: 0.0926, max: 1.4167, stdev: 0.2231
External policy "random" average reward: 0.2553, min: -0.2130, max: 1.0556, stdev: 0.2241
External policy "individual greedy" average reward: 0.5426, min: 0.0556, max: 1.2500, stdev: 0.2167
External policy "total greedy" average reward: 0.6627, min: 0.1389, max: 1.2315, stdev: 0.2076
New network won 99 and tied 136 out of 300 games (55.67% wins where ties are half wins)
Keeping the new network

Training iteration 179 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.55 seconds
Training examples lengths: [64943, 64948, 64834, 64778, 64642, 64922, 64790, 64900, 64891, 64880]
Total value: 437901.79
Training on 648528 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2510 (value: 0.0016, weighted value: 0.0786, policy: 0.1725, weighted policy: 0.1725), Train Mean Max: 0.9291
Epoch 2/10, Train Loss: 0.2391 (value: 0.0014, weighted value: 0.0701, policy: 0.1689, weighted policy: 0.1689), Train Mean Max: 0.9298
Epoch 3/10, Train Loss: 0.2345 (value: 0.0014, weighted value: 0.0682, policy: 0.1663, weighted policy: 0.1663), Train Mean Max: 0.9301
Epoch 4/10, Train Loss: 0.2298 (value: 0.0013, weighted value: 0.0645, policy: 0.1653, weighted policy: 0.1653), Train Mean Max: 0.9305
Epoch 5/10, Train Loss: 0.2255 (value: 0.0012, weighted value: 0.0611, policy: 0.1644, weighted policy: 0.1644), Train Mean Max: 0.9308
Epoch 6/10, Train Loss: 0.2253 (value: 0.0012, weighted value: 0.0621, policy: 0.1632, weighted policy: 0.1632), Train Mean Max: 0.9309
Epoch 7/10, Train Loss: 0.2206 (value: 0.0012, weighted value: 0.0576, policy: 0.1629, weighted policy: 0.1629), Train Mean Max: 0.9311
Epoch 8/10, Train Loss: 0.2173 (value: 0.0011, weighted value: 0.0556, policy: 0.1618, weighted policy: 0.1618), Train Mean Max: 0.9314
Epoch 9/10, Train Loss: 0.2176 (value: 0.0011, weighted value: 0.0560, policy: 0.1616, weighted policy: 0.1616), Train Mean Max: 0.9316
Epoch 10/10, Train Loss: 0.2155 (value: 0.0011, weighted value: 0.0536, policy: 0.1619, weighted policy: 0.1619), Train Mean Max: 0.9318
..training done in 68.55 seconds
..evaluation done in 18.55 seconds
Old network+MCTS average reward: 0.6485, min: -0.0370, max: 1.4537, stdev: 0.2169
New network+MCTS average reward: 0.6515, min: -0.1296, max: 1.3611, stdev: 0.2119
Old bare network average reward: 0.5928, min: -0.1296, max: 1.3519, stdev: 0.2254
New bare network average reward: 0.5937, min: -0.1296, max: 1.3611, stdev: 0.2224
External policy "random" average reward: 0.2530, min: -0.3519, max: 1.0741, stdev: 0.2208
External policy "individual greedy" average reward: 0.5234, min: -0.0648, max: 1.3148, stdev: 0.2144
External policy "total greedy" average reward: 0.6425, min: 0.0370, max: 1.3519, stdev: 0.2070
New network won 84 and tied 145 out of 300 games (52.17% wins where ties are half wins)
Keeping the new network

Training iteration 180 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.11 seconds
Training examples lengths: [64948, 64834, 64778, 64642, 64922, 64790, 64900, 64891, 64880, 64758]
Total value: 437644.17
Training on 648343 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2416 (value: 0.0014, weighted value: 0.0713, policy: 0.1703, weighted policy: 0.1703), Train Mean Max: 0.9306
Epoch 2/10, Train Loss: 0.2367 (value: 0.0014, weighted value: 0.0681, policy: 0.1685, weighted policy: 0.1685), Train Mean Max: 0.9308
Epoch 3/10, Train Loss: 0.2267 (value: 0.0012, weighted value: 0.0620, policy: 0.1647, weighted policy: 0.1647), Train Mean Max: 0.9314
Epoch 4/10, Train Loss: 0.2227 (value: 0.0012, weighted value: 0.0600, policy: 0.1628, weighted policy: 0.1628), Train Mean Max: 0.9316
Epoch 5/10, Train Loss: 0.2200 (value: 0.0012, weighted value: 0.0576, policy: 0.1624, weighted policy: 0.1624), Train Mean Max: 0.9317
Epoch 6/10, Train Loss: 0.2190 (value: 0.0011, weighted value: 0.0565, policy: 0.1625, weighted policy: 0.1625), Train Mean Max: 0.9318
Epoch 7/10, Train Loss: 0.2156 (value: 0.0011, weighted value: 0.0547, policy: 0.1609, weighted policy: 0.1609), Train Mean Max: 0.9324
Epoch 8/10, Train Loss: 0.2140 (value: 0.0011, weighted value: 0.0534, policy: 0.1606, weighted policy: 0.1606), Train Mean Max: 0.9323
Epoch 9/10, Train Loss: 0.2121 (value: 0.0010, weighted value: 0.0518, policy: 0.1603, weighted policy: 0.1603), Train Mean Max: 0.9326
Epoch 10/10, Train Loss: 0.2115 (value: 0.0010, weighted value: 0.0511, policy: 0.1604, weighted policy: 0.1604), Train Mean Max: 0.9327
..training done in 60.33 seconds
..evaluation done in 18.54 seconds
Old network+MCTS average reward: 0.6746, min: 0.1204, max: 1.4815, stdev: 0.2176
New network+MCTS average reward: 0.6753, min: 0.1204, max: 1.4815, stdev: 0.2220
Old bare network average reward: 0.6145, min: 0.1204, max: 1.4722, stdev: 0.2209
New bare network average reward: 0.6122, min: 0.1204, max: 1.4444, stdev: 0.2224
External policy "random" average reward: 0.2536, min: -0.2778, max: 1.0463, stdev: 0.2167
External policy "individual greedy" average reward: 0.5353, min: 0.0370, max: 1.3611, stdev: 0.2210
External policy "total greedy" average reward: 0.6481, min: 0.0833, max: 1.3981, stdev: 0.2134
New network won 77 and tied 152 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_180

Training iteration 181 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.79 seconds
Training examples lengths: [64834, 64778, 64642, 64922, 64790, 64900, 64891, 64880, 64758, 65069]
Total value: 437651.55
Training on 648464 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2374 (value: 0.0014, weighted value: 0.0690, policy: 0.1683, weighted policy: 0.1683), Train Mean Max: 0.9314
Epoch 2/10, Train Loss: 0.2286 (value: 0.0013, weighted value: 0.0632, policy: 0.1654, weighted policy: 0.1654), Train Mean Max: 0.9318
Epoch 3/10, Train Loss: 0.2256 (value: 0.0012, weighted value: 0.0614, policy: 0.1642, weighted policy: 0.1642), Train Mean Max: 0.9318
Epoch 4/10, Train Loss: 0.2191 (value: 0.0011, weighted value: 0.0566, policy: 0.1626, weighted policy: 0.1626), Train Mean Max: 0.9321
Epoch 5/10, Train Loss: 0.2168 (value: 0.0011, weighted value: 0.0557, policy: 0.1611, weighted policy: 0.1611), Train Mean Max: 0.9326
Epoch 6/10, Train Loss: 0.2151 (value: 0.0011, weighted value: 0.0541, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9327
Epoch 7/10, Train Loss: 0.2117 (value: 0.0010, weighted value: 0.0517, policy: 0.1599, weighted policy: 0.1599), Train Mean Max: 0.9329
Epoch 8/10, Train Loss: 0.2124 (value: 0.0010, weighted value: 0.0523, policy: 0.1602, weighted policy: 0.1602), Train Mean Max: 0.9329
Epoch 9/10, Train Loss: 0.2095 (value: 0.0010, weighted value: 0.0496, policy: 0.1599, weighted policy: 0.1599), Train Mean Max: 0.9332
Epoch 10/10, Train Loss: 0.2078 (value: 0.0010, weighted value: 0.0496, policy: 0.1582, weighted policy: 0.1582), Train Mean Max: 0.9334
..training done in 59.50 seconds
..evaluation done in 18.49 seconds
Old network+MCTS average reward: 0.6719, min: 0.1019, max: 1.5093, stdev: 0.2329
New network+MCTS average reward: 0.6779, min: 0.1667, max: 1.4074, stdev: 0.2302
Old bare network average reward: 0.6188, min: 0.0463, max: 1.3056, stdev: 0.2308
New bare network average reward: 0.6174, min: 0.0463, max: 1.2963, stdev: 0.2280
External policy "random" average reward: 0.2758, min: -0.2500, max: 1.0648, stdev: 0.2244
External policy "individual greedy" average reward: 0.5530, min: 0.0278, max: 1.2593, stdev: 0.2333
External policy "total greedy" average reward: 0.6696, min: 0.1759, max: 1.3148, stdev: 0.2198
New network won 89 and tied 154 out of 300 games (55.33% wins where ties are half wins)
Keeping the new network

Training iteration 182 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.09 seconds
Training examples lengths: [64778, 64642, 64922, 64790, 64900, 64891, 64880, 64758, 65069, 64594]
Total value: 436974.35
Training on 648224 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2374 (value: 0.0014, weighted value: 0.0689, policy: 0.1685, weighted policy: 0.1685), Train Mean Max: 0.9318
Epoch 2/10, Train Loss: 0.2288 (value: 0.0013, weighted value: 0.0634, policy: 0.1654, weighted policy: 0.1654), Train Mean Max: 0.9322
Epoch 3/10, Train Loss: 0.2234 (value: 0.0012, weighted value: 0.0597, policy: 0.1638, weighted policy: 0.1638), Train Mean Max: 0.9324
Epoch 4/10, Train Loss: 0.2189 (value: 0.0011, weighted value: 0.0567, policy: 0.1622, weighted policy: 0.1622), Train Mean Max: 0.9327
Epoch 5/10, Train Loss: 0.2166 (value: 0.0011, weighted value: 0.0563, policy: 0.1604, weighted policy: 0.1604), Train Mean Max: 0.9329
Epoch 6/10, Train Loss: 0.2127 (value: 0.0011, weighted value: 0.0531, policy: 0.1596, weighted policy: 0.1596), Train Mean Max: 0.9331
Epoch 7/10, Train Loss: 0.2133 (value: 0.0011, weighted value: 0.0531, policy: 0.1602, weighted policy: 0.1602), Train Mean Max: 0.9331
Epoch 8/10, Train Loss: 0.2091 (value: 0.0010, weighted value: 0.0504, policy: 0.1587, weighted policy: 0.1587), Train Mean Max: 0.9335
Epoch 9/10, Train Loss: 0.2092 (value: 0.0010, weighted value: 0.0503, policy: 0.1589, weighted policy: 0.1589), Train Mean Max: 0.9334
Epoch 10/10, Train Loss: 0.2071 (value: 0.0010, weighted value: 0.0479, policy: 0.1591, weighted policy: 0.1591), Train Mean Max: 0.9338
..training done in 59.16 seconds
..evaluation done in 17.71 seconds
Old network+MCTS average reward: 0.6572, min: 0.0926, max: 1.5185, stdev: 0.2284
New network+MCTS average reward: 0.6572, min: 0.0833, max: 1.5185, stdev: 0.2337
Old bare network average reward: 0.6003, min: -0.0093, max: 1.4259, stdev: 0.2385
New bare network average reward: 0.5971, min: -0.0093, max: 1.5185, stdev: 0.2419
External policy "random" average reward: 0.2448, min: -0.4074, max: 1.0741, stdev: 0.2314
External policy "individual greedy" average reward: 0.5232, min: 0.0370, max: 1.5463, stdev: 0.2414
External policy "total greedy" average reward: 0.6346, min: 0.1204, max: 1.4630, stdev: 0.2273
New network won 65 and tied 157 out of 300 games (47.83% wins where ties are half wins)
Reverting to the old network

Training iteration 183 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.79 seconds
Training examples lengths: [64642, 64922, 64790, 64900, 64891, 64880, 64758, 65069, 64594, 64777]
Total value: 437845.94
Training on 648223 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2664 (value: 0.0018, weighted value: 0.0888, policy: 0.1776, weighted policy: 0.1776), Train Mean Max: 0.9301
Epoch 2/10, Train Loss: 0.2492 (value: 0.0015, weighted value: 0.0774, policy: 0.1717, weighted policy: 0.1717), Train Mean Max: 0.9307
Epoch 3/10, Train Loss: 0.2410 (value: 0.0015, weighted value: 0.0730, policy: 0.1680, weighted policy: 0.1680), Train Mean Max: 0.9312
Epoch 4/10, Train Loss: 0.2347 (value: 0.0014, weighted value: 0.0687, policy: 0.1661, weighted policy: 0.1661), Train Mean Max: 0.9316
Epoch 5/10, Train Loss: 0.2305 (value: 0.0013, weighted value: 0.0650, policy: 0.1654, weighted policy: 0.1654), Train Mean Max: 0.9317
Epoch 6/10, Train Loss: 0.2246 (value: 0.0013, weighted value: 0.0625, policy: 0.1621, weighted policy: 0.1621), Train Mean Max: 0.9321
Epoch 7/10, Train Loss: 0.2225 (value: 0.0012, weighted value: 0.0601, policy: 0.1624, weighted policy: 0.1624), Train Mean Max: 0.9322
Epoch 8/10, Train Loss: 0.2206 (value: 0.0012, weighted value: 0.0583, policy: 0.1622, weighted policy: 0.1622), Train Mean Max: 0.9322
Epoch 9/10, Train Loss: 0.2178 (value: 0.0012, weighted value: 0.0575, policy: 0.1603, weighted policy: 0.1603), Train Mean Max: 0.9326
Epoch 10/10, Train Loss: 0.2171 (value: 0.0011, weighted value: 0.0557, policy: 0.1614, weighted policy: 0.1614), Train Mean Max: 0.9326
..training done in 59.75 seconds
..evaluation done in 19.05 seconds
Old network+MCTS average reward: 0.6545, min: 0.1111, max: 1.4259, stdev: 0.2390
New network+MCTS average reward: 0.6510, min: 0.1111, max: 1.4074, stdev: 0.2352
Old bare network average reward: 0.6009, min: -0.0370, max: 1.3796, stdev: 0.2513
New bare network average reward: 0.5949, min: -0.0648, max: 1.3796, stdev: 0.2476
External policy "random" average reward: 0.2527, min: -0.3056, max: 0.8796, stdev: 0.2277
External policy "individual greedy" average reward: 0.5175, min: -0.0278, max: 1.2407, stdev: 0.2371
External policy "total greedy" average reward: 0.6323, min: 0.0833, max: 1.3426, stdev: 0.2352
New network won 75 and tied 148 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 184 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.29 seconds
Training examples lengths: [64922, 64790, 64900, 64891, 64880, 64758, 65069, 64594, 64777, 64946]
Total value: 438782.47
Training on 648527 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2945 (value: 0.0022, weighted value: 0.1083, policy: 0.1862, weighted policy: 0.1862), Train Mean Max: 0.9288
Epoch 2/10, Train Loss: 0.2695 (value: 0.0019, weighted value: 0.0928, policy: 0.1767, weighted policy: 0.1767), Train Mean Max: 0.9296
Epoch 3/10, Train Loss: 0.2562 (value: 0.0017, weighted value: 0.0838, policy: 0.1725, weighted policy: 0.1725), Train Mean Max: 0.9301
Epoch 4/10, Train Loss: 0.2490 (value: 0.0016, weighted value: 0.0801, policy: 0.1689, weighted policy: 0.1689), Train Mean Max: 0.9305
Epoch 5/10, Train Loss: 0.2410 (value: 0.0015, weighted value: 0.0743, policy: 0.1668, weighted policy: 0.1668), Train Mean Max: 0.9308
Epoch 6/10, Train Loss: 0.2380 (value: 0.0014, weighted value: 0.0717, policy: 0.1663, weighted policy: 0.1663), Train Mean Max: 0.9310
Epoch 7/10, Train Loss: 0.2340 (value: 0.0014, weighted value: 0.0689, policy: 0.1651, weighted policy: 0.1651), Train Mean Max: 0.9312
Epoch 8/10, Train Loss: 0.2287 (value: 0.0013, weighted value: 0.0645, policy: 0.1642, weighted policy: 0.1642), Train Mean Max: 0.9313
Epoch 9/10, Train Loss: 0.2262 (value: 0.0012, weighted value: 0.0624, policy: 0.1639, weighted policy: 0.1639), Train Mean Max: 0.9313
Epoch 10/10, Train Loss: 0.2262 (value: 0.0013, weighted value: 0.0631, policy: 0.1631, weighted policy: 0.1631), Train Mean Max: 0.9318
..training done in 59.99 seconds
..evaluation done in 18.16 seconds
Old network+MCTS average reward: 0.6573, min: 0.1111, max: 1.4074, stdev: 0.2333
New network+MCTS average reward: 0.6568, min: 0.0833, max: 1.4537, stdev: 0.2338
Old bare network average reward: 0.6048, min: 0.0278, max: 1.4537, stdev: 0.2371
New bare network average reward: 0.6065, min: 0.0463, max: 1.4537, stdev: 0.2391
External policy "random" average reward: 0.2625, min: -0.2315, max: 0.9907, stdev: 0.2207
External policy "individual greedy" average reward: 0.5337, min: 0.0000, max: 1.3611, stdev: 0.2334
External policy "total greedy" average reward: 0.6408, min: 0.1389, max: 1.4074, stdev: 0.2252
New network won 86 and tied 134 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 185 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.20 seconds
Training examples lengths: [64790, 64900, 64891, 64880, 64758, 65069, 64594, 64777, 64946, 64975]
Total value: 438543.96
Training on 648580 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2495 (value: 0.0016, weighted value: 0.0776, policy: 0.1719, weighted policy: 0.1719), Train Mean Max: 0.9303
Epoch 2/10, Train Loss: 0.2422 (value: 0.0014, weighted value: 0.0724, policy: 0.1698, weighted policy: 0.1698), Train Mean Max: 0.9307
Epoch 3/10, Train Loss: 0.2338 (value: 0.0013, weighted value: 0.0673, policy: 0.1666, weighted policy: 0.1666), Train Mean Max: 0.9310
Epoch 4/10, Train Loss: 0.2299 (value: 0.0013, weighted value: 0.0645, policy: 0.1654, weighted policy: 0.1654), Train Mean Max: 0.9313
Epoch 5/10, Train Loss: 0.2275 (value: 0.0013, weighted value: 0.0630, policy: 0.1645, weighted policy: 0.1645), Train Mean Max: 0.9314
Epoch 6/10, Train Loss: 0.2237 (value: 0.0012, weighted value: 0.0601, policy: 0.1636, weighted policy: 0.1636), Train Mean Max: 0.9316
Epoch 7/10, Train Loss: 0.2217 (value: 0.0012, weighted value: 0.0597, policy: 0.1620, weighted policy: 0.1620), Train Mean Max: 0.9320
Epoch 8/10, Train Loss: 0.2194 (value: 0.0011, weighted value: 0.0570, policy: 0.1624, weighted policy: 0.1624), Train Mean Max: 0.9320
Epoch 9/10, Train Loss: 0.2175 (value: 0.0011, weighted value: 0.0556, policy: 0.1619, weighted policy: 0.1619), Train Mean Max: 0.9322
Epoch 10/10, Train Loss: 0.2153 (value: 0.0011, weighted value: 0.0537, policy: 0.1616, weighted policy: 0.1616), Train Mean Max: 0.9323
..training done in 58.24 seconds
..evaluation done in 18.44 seconds
Old network+MCTS average reward: 0.6656, min: 0.0185, max: 1.3889, stdev: 0.2440
New network+MCTS average reward: 0.6666, min: 0.0185, max: 1.3704, stdev: 0.2461
Old bare network average reward: 0.6169, min: 0.0185, max: 1.3889, stdev: 0.2463
New bare network average reward: 0.6157, min: 0.0185, max: 1.3241, stdev: 0.2477
External policy "random" average reward: 0.2391, min: -0.4259, max: 0.8704, stdev: 0.2228
External policy "individual greedy" average reward: 0.5373, min: -0.1204, max: 1.2500, stdev: 0.2300
External policy "total greedy" average reward: 0.6520, min: 0.0648, max: 1.3426, stdev: 0.2280
New network won 82 and tied 136 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 186 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.42 seconds
Training examples lengths: [64900, 64891, 64880, 64758, 65069, 64594, 64777, 64946, 64975, 64980]
Total value: 439000.38
Training on 648770 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2444 (value: 0.0015, weighted value: 0.0731, policy: 0.1713, weighted policy: 0.1713), Train Mean Max: 0.9309
Epoch 2/10, Train Loss: 0.2327 (value: 0.0013, weighted value: 0.0654, policy: 0.1673, weighted policy: 0.1673), Train Mean Max: 0.9312
Epoch 3/10, Train Loss: 0.2290 (value: 0.0013, weighted value: 0.0644, policy: 0.1646, weighted policy: 0.1646), Train Mean Max: 0.9316
Epoch 4/10, Train Loss: 0.2221 (value: 0.0012, weighted value: 0.0586, policy: 0.1635, weighted policy: 0.1635), Train Mean Max: 0.9318
Epoch 5/10, Train Loss: 0.2209 (value: 0.0012, weighted value: 0.0586, policy: 0.1623, weighted policy: 0.1623), Train Mean Max: 0.9320
Epoch 6/10, Train Loss: 0.2179 (value: 0.0011, weighted value: 0.0561, policy: 0.1618, weighted policy: 0.1618), Train Mean Max: 0.9321
Epoch 7/10, Train Loss: 0.2183 (value: 0.0011, weighted value: 0.0556, policy: 0.1627, weighted policy: 0.1627), Train Mean Max: 0.9323
Epoch 8/10, Train Loss: 0.2145 (value: 0.0011, weighted value: 0.0534, policy: 0.1612, weighted policy: 0.1612), Train Mean Max: 0.9326
Epoch 9/10, Train Loss: 0.2121 (value: 0.0010, weighted value: 0.0515, policy: 0.1606, weighted policy: 0.1606), Train Mean Max: 0.9326
Epoch 10/10, Train Loss: 0.2115 (value: 0.0010, weighted value: 0.0507, policy: 0.1608, weighted policy: 0.1608), Train Mean Max: 0.9326
..training done in 59.19 seconds
..evaluation done in 19.31 seconds
Old network+MCTS average reward: 0.6389, min: 0.0741, max: 1.5093, stdev: 0.2166
New network+MCTS average reward: 0.6384, min: 0.0741, max: 1.4815, stdev: 0.2164
Old bare network average reward: 0.5915, min: 0.0463, max: 1.4074, stdev: 0.2189
New bare network average reward: 0.5882, min: -0.0278, max: 1.4074, stdev: 0.2210
External policy "random" average reward: 0.2419, min: -0.3796, max: 0.8519, stdev: 0.2198
External policy "individual greedy" average reward: 0.5196, min: -0.1296, max: 1.3333, stdev: 0.2091
External policy "total greedy" average reward: 0.6260, min: 0.0370, max: 1.3704, stdev: 0.2069
New network won 70 and tied 157 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 187 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 79.68 seconds
Training examples lengths: [64891, 64880, 64758, 65069, 64594, 64777, 64946, 64975, 64980, 64782]
Total value: 439630.93
Training on 648652 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2722 (value: 0.0018, weighted value: 0.0925, policy: 0.1797, weighted policy: 0.1797), Train Mean Max: 0.9293
Epoch 2/10, Train Loss: 0.2545 (value: 0.0016, weighted value: 0.0802, policy: 0.1743, weighted policy: 0.1743), Train Mean Max: 0.9301
Epoch 3/10, Train Loss: 0.2446 (value: 0.0015, weighted value: 0.0743, policy: 0.1702, weighted policy: 0.1702), Train Mean Max: 0.9304
Epoch 4/10, Train Loss: 0.2379 (value: 0.0014, weighted value: 0.0706, policy: 0.1672, weighted policy: 0.1672), Train Mean Max: 0.9306
Epoch 5/10, Train Loss: 0.2328 (value: 0.0013, weighted value: 0.0669, policy: 0.1659, weighted policy: 0.1659), Train Mean Max: 0.9309
Epoch 6/10, Train Loss: 0.2315 (value: 0.0013, weighted value: 0.0654, policy: 0.1661, weighted policy: 0.1661), Train Mean Max: 0.9310
Epoch 7/10, Train Loss: 0.2254 (value: 0.0012, weighted value: 0.0614, policy: 0.1640, weighted policy: 0.1640), Train Mean Max: 0.9313
Epoch 8/10, Train Loss: 0.2240 (value: 0.0012, weighted value: 0.0608, policy: 0.1632, weighted policy: 0.1632), Train Mean Max: 0.9314
Epoch 9/10, Train Loss: 0.2208 (value: 0.0011, weighted value: 0.0574, policy: 0.1633, weighted policy: 0.1633), Train Mean Max: 0.9316
Epoch 10/10, Train Loss: 0.2178 (value: 0.0011, weighted value: 0.0556, policy: 0.1622, weighted policy: 0.1622), Train Mean Max: 0.9319
..training done in 73.83 seconds
..evaluation done in 18.31 seconds
Old network+MCTS average reward: 0.6694, min: 0.0463, max: 1.3796, stdev: 0.2298
New network+MCTS average reward: 0.6680, min: 0.1019, max: 1.3056, stdev: 0.2278
Old bare network average reward: 0.6146, min: 0.0000, max: 1.2963, stdev: 0.2294
New bare network average reward: 0.6146, min: 0.0000, max: 1.2870, stdev: 0.2358
External policy "random" average reward: 0.2660, min: -0.3426, max: 0.9907, stdev: 0.2393
External policy "individual greedy" average reward: 0.5389, min: -0.0556, max: 1.2222, stdev: 0.2253
External policy "total greedy" average reward: 0.6542, min: 0.0278, max: 1.2870, stdev: 0.2294
New network won 79 and tied 141 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 188 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.50 seconds
Training examples lengths: [64880, 64758, 65069, 64594, 64777, 64946, 64975, 64980, 64782, 64729]
Total value: 439640.27
Training on 648490 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2975 (value: 0.0022, weighted value: 0.1082, policy: 0.1893, weighted policy: 0.1893), Train Mean Max: 0.9280
Epoch 2/10, Train Loss: 0.2748 (value: 0.0019, weighted value: 0.0946, policy: 0.1802, weighted policy: 0.1802), Train Mean Max: 0.9288
Epoch 3/10, Train Loss: 0.2608 (value: 0.0017, weighted value: 0.0858, policy: 0.1750, weighted policy: 0.1750), Train Mean Max: 0.9292
Epoch 4/10, Train Loss: 0.2501 (value: 0.0016, weighted value: 0.0793, policy: 0.1708, weighted policy: 0.1708), Train Mean Max: 0.9296
Epoch 5/10, Train Loss: 0.2451 (value: 0.0015, weighted value: 0.0759, policy: 0.1692, weighted policy: 0.1692), Train Mean Max: 0.9298
Epoch 6/10, Train Loss: 0.2392 (value: 0.0014, weighted value: 0.0720, policy: 0.1672, weighted policy: 0.1672), Train Mean Max: 0.9301
Epoch 7/10, Train Loss: 0.2362 (value: 0.0014, weighted value: 0.0696, policy: 0.1666, weighted policy: 0.1666), Train Mean Max: 0.9301
Epoch 8/10, Train Loss: 0.2318 (value: 0.0013, weighted value: 0.0667, policy: 0.1651, weighted policy: 0.1651), Train Mean Max: 0.9304
Epoch 9/10, Train Loss: 0.2293 (value: 0.0013, weighted value: 0.0635, policy: 0.1658, weighted policy: 0.1658), Train Mean Max: 0.9305
Epoch 10/10, Train Loss: 0.2266 (value: 0.0012, weighted value: 0.0619, policy: 0.1647, weighted policy: 0.1647), Train Mean Max: 0.9308
..training done in 59.92 seconds
..evaluation done in 18.34 seconds
Old network+MCTS average reward: 0.6633, min: 0.2222, max: 1.2500, stdev: 0.2074
New network+MCTS average reward: 0.6674, min: 0.2130, max: 1.2500, stdev: 0.2094
Old bare network average reward: 0.6140, min: 0.0926, max: 1.2593, stdev: 0.2172
New bare network average reward: 0.6081, min: 0.0556, max: 1.2130, stdev: 0.2217
External policy "random" average reward: 0.2644, min: -0.3796, max: 0.8796, stdev: 0.2198
External policy "individual greedy" average reward: 0.5391, min: 0.0185, max: 1.1204, stdev: 0.2063
External policy "total greedy" average reward: 0.6511, min: 0.0833, max: 1.2963, stdev: 0.2072
New network won 100 and tied 128 out of 300 games (54.67% wins where ties are half wins)
Keeping the new network

Training iteration 189 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.74 seconds
Training examples lengths: [64758, 65069, 64594, 64777, 64946, 64975, 64980, 64782, 64729, 65052]
Total value: 439918.11
Training on 648662 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2530 (value: 0.0016, weighted value: 0.0793, policy: 0.1736, weighted policy: 0.1736), Train Mean Max: 0.9296
Epoch 2/10, Train Loss: 0.2426 (value: 0.0014, weighted value: 0.0722, policy: 0.1704, weighted policy: 0.1704), Train Mean Max: 0.9300
Epoch 3/10, Train Loss: 0.2372 (value: 0.0014, weighted value: 0.0696, policy: 0.1676, weighted policy: 0.1676), Train Mean Max: 0.9302
Epoch 4/10, Train Loss: 0.2311 (value: 0.0013, weighted value: 0.0653, policy: 0.1659, weighted policy: 0.1659), Train Mean Max: 0.9304
Epoch 5/10, Train Loss: 0.2271 (value: 0.0012, weighted value: 0.0622, policy: 0.1649, weighted policy: 0.1649), Train Mean Max: 0.9307
Epoch 6/10, Train Loss: 0.2260 (value: 0.0012, weighted value: 0.0621, policy: 0.1639, weighted policy: 0.1639), Train Mean Max: 0.9310
Epoch 7/10, Train Loss: 0.2219 (value: 0.0012, weighted value: 0.0584, policy: 0.1635, weighted policy: 0.1635), Train Mean Max: 0.9310
Epoch 8/10, Train Loss: 0.2197 (value: 0.0011, weighted value: 0.0568, policy: 0.1630, weighted policy: 0.1630), Train Mean Max: 0.9312
Epoch 9/10, Train Loss: 0.2198 (value: 0.0011, weighted value: 0.0565, policy: 0.1632, weighted policy: 0.1632), Train Mean Max: 0.9315
Epoch 10/10, Train Loss: 0.2156 (value: 0.0011, weighted value: 0.0536, policy: 0.1619, weighted policy: 0.1619), Train Mean Max: 0.9317
..training done in 60.10 seconds
..evaluation done in 18.45 seconds
Old network+MCTS average reward: 0.6697, min: 0.0926, max: 1.3426, stdev: 0.2303
New network+MCTS average reward: 0.6721, min: 0.0926, max: 1.3611, stdev: 0.2307
Old bare network average reward: 0.6098, min: 0.0833, max: 1.3241, stdev: 0.2365
New bare network average reward: 0.6142, min: 0.1204, max: 1.3241, stdev: 0.2386
External policy "random" average reward: 0.2601, min: -0.2778, max: 0.7500, stdev: 0.2142
External policy "individual greedy" average reward: 0.5357, min: 0.0000, max: 1.4167, stdev: 0.2173
External policy "total greedy" average reward: 0.6476, min: 0.1389, max: 1.5093, stdev: 0.2183
New network won 83 and tied 146 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network

Training iteration 190 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.92 seconds
Training examples lengths: [65069, 64594, 64777, 64946, 64975, 64980, 64782, 64729, 65052, 64676]
Total value: 440058.41
Training on 648580 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2432 (value: 0.0014, weighted value: 0.0723, policy: 0.1709, weighted policy: 0.1709), Train Mean Max: 0.9305
Epoch 2/10, Train Loss: 0.2332 (value: 0.0013, weighted value: 0.0655, policy: 0.1678, weighted policy: 0.1678), Train Mean Max: 0.9308
Epoch 3/10, Train Loss: 0.2285 (value: 0.0013, weighted value: 0.0631, policy: 0.1653, weighted policy: 0.1653), Train Mean Max: 0.9312
Epoch 4/10, Train Loss: 0.2244 (value: 0.0012, weighted value: 0.0603, policy: 0.1641, weighted policy: 0.1641), Train Mean Max: 0.9315
Epoch 5/10, Train Loss: 0.2186 (value: 0.0011, weighted value: 0.0567, policy: 0.1618, weighted policy: 0.1618), Train Mean Max: 0.9316
Epoch 6/10, Train Loss: 0.2176 (value: 0.0011, weighted value: 0.0564, policy: 0.1612, weighted policy: 0.1612), Train Mean Max: 0.9319
Epoch 7/10, Train Loss: 0.2171 (value: 0.0011, weighted value: 0.0555, policy: 0.1616, weighted policy: 0.1616), Train Mean Max: 0.9320
Epoch 8/10, Train Loss: 0.2141 (value: 0.0011, weighted value: 0.0528, policy: 0.1613, weighted policy: 0.1613), Train Mean Max: 0.9320
Epoch 9/10, Train Loss: 0.2117 (value: 0.0010, weighted value: 0.0516, policy: 0.1601, weighted policy: 0.1601), Train Mean Max: 0.9324
Epoch 10/10, Train Loss: 0.2110 (value: 0.0010, weighted value: 0.0506, policy: 0.1604, weighted policy: 0.1604), Train Mean Max: 0.9324
..training done in 64.67 seconds
..evaluation done in 17.92 seconds
Old network+MCTS average reward: 0.6774, min: 0.0833, max: 1.3889, stdev: 0.2240
New network+MCTS average reward: 0.6703, min: 0.0833, max: 1.3889, stdev: 0.2270
Old bare network average reward: 0.6220, min: -0.0833, max: 1.3889, stdev: 0.2371
New bare network average reward: 0.6225, min: -0.0833, max: 1.3148, stdev: 0.2350
External policy "random" average reward: 0.2682, min: -0.2315, max: 0.8704, stdev: 0.2220
External policy "individual greedy" average reward: 0.5410, min: -0.1944, max: 1.1296, stdev: 0.2235
External policy "total greedy" average reward: 0.6535, min: 0.0648, max: 1.2778, stdev: 0.2230
New network won 53 and tied 163 out of 300 games (44.83% wins where ties are half wins)
Reverting to the old network

Training iteration 191 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.66 seconds
Training examples lengths: [64594, 64777, 64946, 64975, 64980, 64782, 64729, 65052, 64676, 64470]
Total value: 440000.44
Training on 647981 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2692 (value: 0.0018, weighted value: 0.0901, policy: 0.1791, weighted policy: 0.1791), Train Mean Max: 0.9291
Epoch 2/10, Train Loss: 0.2535 (value: 0.0016, weighted value: 0.0809, policy: 0.1726, weighted policy: 0.1726), Train Mean Max: 0.9299
Epoch 3/10, Train Loss: 0.2433 (value: 0.0015, weighted value: 0.0750, policy: 0.1683, weighted policy: 0.1683), Train Mean Max: 0.9302
Epoch 4/10, Train Loss: 0.2358 (value: 0.0014, weighted value: 0.0695, policy: 0.1663, weighted policy: 0.1663), Train Mean Max: 0.9304
Epoch 5/10, Train Loss: 0.2328 (value: 0.0014, weighted value: 0.0676, policy: 0.1652, weighted policy: 0.1652), Train Mean Max: 0.9308
Epoch 6/10, Train Loss: 0.2264 (value: 0.0012, weighted value: 0.0622, policy: 0.1641, weighted policy: 0.1641), Train Mean Max: 0.9311
Epoch 7/10, Train Loss: 0.2274 (value: 0.0013, weighted value: 0.0635, policy: 0.1639, weighted policy: 0.1639), Train Mean Max: 0.9311
Epoch 8/10, Train Loss: 0.2212 (value: 0.0012, weighted value: 0.0587, policy: 0.1624, weighted policy: 0.1624), Train Mean Max: 0.9314
Epoch 9/10, Train Loss: 0.2208 (value: 0.0012, weighted value: 0.0585, policy: 0.1623, weighted policy: 0.1623), Train Mean Max: 0.9316
Epoch 10/10, Train Loss: 0.2187 (value: 0.0012, weighted value: 0.0576, policy: 0.1611, weighted policy: 0.1611), Train Mean Max: 0.9318
..training done in 60.09 seconds
..evaluation done in 17.90 seconds
Old network+MCTS average reward: 0.6822, min: 0.1019, max: 1.5000, stdev: 0.2359
New network+MCTS average reward: 0.6829, min: 0.1019, max: 1.5741, stdev: 0.2374
Old bare network average reward: 0.6291, min: 0.0000, max: 1.4444, stdev: 0.2416
New bare network average reward: 0.6308, min: 0.0926, max: 1.5000, stdev: 0.2406
External policy "random" average reward: 0.2704, min: -0.3148, max: 0.9259, stdev: 0.2323
External policy "individual greedy" average reward: 0.5576, min: -0.0463, max: 1.3148, stdev: 0.2287
External policy "total greedy" average reward: 0.6776, min: 0.0185, max: 1.5093, stdev: 0.2196
New network won 81 and tied 145 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 192 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.00 seconds
Training examples lengths: [64777, 64946, 64975, 64980, 64782, 64729, 65052, 64676, 64470, 64884]
Total value: 440957.89
Training on 648271 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2426 (value: 0.0015, weighted value: 0.0726, policy: 0.1700, weighted policy: 0.1700), Train Mean Max: 0.9306
Epoch 2/10, Train Loss: 0.2344 (value: 0.0013, weighted value: 0.0672, policy: 0.1672, weighted policy: 0.1672), Train Mean Max: 0.9310
Epoch 3/10, Train Loss: 0.2291 (value: 0.0013, weighted value: 0.0638, policy: 0.1652, weighted policy: 0.1652), Train Mean Max: 0.9314
Epoch 4/10, Train Loss: 0.2243 (value: 0.0012, weighted value: 0.0613, policy: 0.1630, weighted policy: 0.1630), Train Mean Max: 0.9317
Epoch 5/10, Train Loss: 0.2217 (value: 0.0012, weighted value: 0.0595, policy: 0.1622, weighted policy: 0.1622), Train Mean Max: 0.9318
Epoch 6/10, Train Loss: 0.2195 (value: 0.0011, weighted value: 0.0575, policy: 0.1621, weighted policy: 0.1621), Train Mean Max: 0.9320
Epoch 7/10, Train Loss: 0.2178 (value: 0.0011, weighted value: 0.0565, policy: 0.1613, weighted policy: 0.1613), Train Mean Max: 0.9321
Epoch 8/10, Train Loss: 0.2138 (value: 0.0011, weighted value: 0.0535, policy: 0.1603, weighted policy: 0.1603), Train Mean Max: 0.9325
Epoch 9/10, Train Loss: 0.2127 (value: 0.0011, weighted value: 0.0529, policy: 0.1599, weighted policy: 0.1599), Train Mean Max: 0.9326
Epoch 10/10, Train Loss: 0.2105 (value: 0.0010, weighted value: 0.0513, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9328
..training done in 59.61 seconds
..evaluation done in 18.08 seconds
Old network+MCTS average reward: 0.6673, min: 0.1389, max: 1.3056, stdev: 0.2162
New network+MCTS average reward: 0.6682, min: 0.1574, max: 1.3611, stdev: 0.2187
Old bare network average reward: 0.6099, min: 0.0000, max: 1.2870, stdev: 0.2234
New bare network average reward: 0.6112, min: 0.0093, max: 1.3611, stdev: 0.2269
External policy "random" average reward: 0.2607, min: -0.3426, max: 1.0648, stdev: 0.2149
External policy "individual greedy" average reward: 0.5298, min: 0.0000, max: 1.2407, stdev: 0.2215
External policy "total greedy" average reward: 0.6562, min: 0.2130, max: 1.2222, stdev: 0.2127
New network won 78 and tied 140 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 193 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.84 seconds
Training examples lengths: [64946, 64975, 64980, 64782, 64729, 65052, 64676, 64470, 64884, 64592]
Total value: 440470.84
Training on 648086 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2712 (value: 0.0018, weighted value: 0.0921, policy: 0.1791, weighted policy: 0.1791), Train Mean Max: 0.9292
Epoch 2/10, Train Loss: 0.2521 (value: 0.0016, weighted value: 0.0791, policy: 0.1730, weighted policy: 0.1730), Train Mean Max: 0.9300
Epoch 3/10, Train Loss: 0.2454 (value: 0.0015, weighted value: 0.0761, policy: 0.1693, weighted policy: 0.1693), Train Mean Max: 0.9304
Epoch 4/10, Train Loss: 0.2381 (value: 0.0014, weighted value: 0.0714, policy: 0.1667, weighted policy: 0.1667), Train Mean Max: 0.9307
Epoch 5/10, Train Loss: 0.2332 (value: 0.0014, weighted value: 0.0684, policy: 0.1649, weighted policy: 0.1649), Train Mean Max: 0.9309
Epoch 6/10, Train Loss: 0.2287 (value: 0.0013, weighted value: 0.0644, policy: 0.1643, weighted policy: 0.1643), Train Mean Max: 0.9311
Epoch 7/10, Train Loss: 0.2256 (value: 0.0013, weighted value: 0.0628, policy: 0.1628, weighted policy: 0.1628), Train Mean Max: 0.9313
Epoch 8/10, Train Loss: 0.2247 (value: 0.0012, weighted value: 0.0620, policy: 0.1627, weighted policy: 0.1627), Train Mean Max: 0.9315
Epoch 9/10, Train Loss: 0.2206 (value: 0.0012, weighted value: 0.0581, policy: 0.1625, weighted policy: 0.1625), Train Mean Max: 0.9317
Epoch 10/10, Train Loss: 0.2176 (value: 0.0011, weighted value: 0.0562, policy: 0.1614, weighted policy: 0.1614), Train Mean Max: 0.9320
..training done in 59.98 seconds
..evaluation done in 17.58 seconds
Old network+MCTS average reward: 0.6572, min: 0.0463, max: 1.2500, stdev: 0.2179
New network+MCTS average reward: 0.6563, min: -0.0463, max: 1.2685, stdev: 0.2157
Old bare network average reward: 0.6029, min: -0.1481, max: 1.2500, stdev: 0.2259
New bare network average reward: 0.6073, min: -0.1481, max: 1.2685, stdev: 0.2272
External policy "random" average reward: 0.2431, min: -0.3611, max: 0.8333, stdev: 0.2148
External policy "individual greedy" average reward: 0.5367, min: -0.0278, max: 1.3981, stdev: 0.2153
External policy "total greedy" average reward: 0.6410, min: 0.0648, max: 1.3241, stdev: 0.2111
New network won 76 and tied 149 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 194 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.34 seconds
Training examples lengths: [64975, 64980, 64782, 64729, 65052, 64676, 64470, 64884, 64592, 65093]
Total value: 440823.13
Training on 648233 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2437 (value: 0.0015, weighted value: 0.0737, policy: 0.1700, weighted policy: 0.1700), Train Mean Max: 0.9307
Epoch 2/10, Train Loss: 0.2363 (value: 0.0014, weighted value: 0.0679, policy: 0.1684, weighted policy: 0.1684), Train Mean Max: 0.9309
Epoch 3/10, Train Loss: 0.2296 (value: 0.0013, weighted value: 0.0652, policy: 0.1644, weighted policy: 0.1644), Train Mean Max: 0.9314
Epoch 4/10, Train Loss: 0.2250 (value: 0.0012, weighted value: 0.0623, policy: 0.1628, weighted policy: 0.1628), Train Mean Max: 0.9317
Epoch 5/10, Train Loss: 0.2222 (value: 0.0012, weighted value: 0.0594, policy: 0.1628, weighted policy: 0.1628), Train Mean Max: 0.9317
Epoch 6/10, Train Loss: 0.2194 (value: 0.0012, weighted value: 0.0579, policy: 0.1615, weighted policy: 0.1615), Train Mean Max: 0.9319
Epoch 7/10, Train Loss: 0.2167 (value: 0.0011, weighted value: 0.0561, policy: 0.1606, weighted policy: 0.1606), Train Mean Max: 0.9321
Epoch 8/10, Train Loss: 0.2166 (value: 0.0011, weighted value: 0.0558, policy: 0.1607, weighted policy: 0.1607), Train Mean Max: 0.9321
Epoch 9/10, Train Loss: 0.2125 (value: 0.0011, weighted value: 0.0526, policy: 0.1599, weighted policy: 0.1599), Train Mean Max: 0.9325
Epoch 10/10, Train Loss: 0.2109 (value: 0.0010, weighted value: 0.0513, policy: 0.1596, weighted policy: 0.1596), Train Mean Max: 0.9326
..training done in 59.25 seconds
..evaluation done in 17.82 seconds
Old network+MCTS average reward: 0.6723, min: 0.0926, max: 1.3611, stdev: 0.2234
New network+MCTS average reward: 0.6703, min: 0.1481, max: 1.3611, stdev: 0.2250
Old bare network average reward: 0.6166, min: 0.0463, max: 1.3611, stdev: 0.2249
New bare network average reward: 0.6226, min: 0.0556, max: 1.3426, stdev: 0.2229
External policy "random" average reward: 0.2595, min: -0.3519, max: 0.8241, stdev: 0.2272
External policy "individual greedy" average reward: 0.5388, min: 0.0278, max: 1.1389, stdev: 0.2109
External policy "total greedy" average reward: 0.6421, min: 0.0648, max: 1.1481, stdev: 0.2142
New network won 74 and tied 142 out of 300 games (48.33% wins where ties are half wins)
Reverting to the old network

Training iteration 195 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.75 seconds
Training examples lengths: [64980, 64782, 64729, 65052, 64676, 64470, 64884, 64592, 65093, 64651]
Total value: 440937.56
Training on 647909 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2743 (value: 0.0019, weighted value: 0.0950, policy: 0.1792, weighted policy: 0.1792), Train Mean Max: 0.9293
Epoch 2/10, Train Loss: 0.2562 (value: 0.0017, weighted value: 0.0829, policy: 0.1733, weighted policy: 0.1733), Train Mean Max: 0.9299
Epoch 3/10, Train Loss: 0.2456 (value: 0.0015, weighted value: 0.0767, policy: 0.1689, weighted policy: 0.1689), Train Mean Max: 0.9304
Epoch 4/10, Train Loss: 0.2369 (value: 0.0014, weighted value: 0.0711, policy: 0.1658, weighted policy: 0.1658), Train Mean Max: 0.9307
Epoch 5/10, Train Loss: 0.2355 (value: 0.0014, weighted value: 0.0704, policy: 0.1651, weighted policy: 0.1651), Train Mean Max: 0.9307
Epoch 6/10, Train Loss: 0.2299 (value: 0.0013, weighted value: 0.0659, policy: 0.1640, weighted policy: 0.1640), Train Mean Max: 0.9310
Epoch 7/10, Train Loss: 0.2280 (value: 0.0013, weighted value: 0.0646, policy: 0.1634, weighted policy: 0.1634), Train Mean Max: 0.9313
Epoch 8/10, Train Loss: 0.2219 (value: 0.0012, weighted value: 0.0597, policy: 0.1623, weighted policy: 0.1623), Train Mean Max: 0.9316
Epoch 9/10, Train Loss: 0.2225 (value: 0.0012, weighted value: 0.0599, policy: 0.1626, weighted policy: 0.1626), Train Mean Max: 0.9317
Epoch 10/10, Train Loss: 0.2187 (value: 0.0012, weighted value: 0.0578, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9319
..training done in 59.32 seconds
..evaluation done in 18.15 seconds
Old network+MCTS average reward: 0.6419, min: 0.0741, max: 1.3519, stdev: 0.2294
New network+MCTS average reward: 0.6430, min: 0.1296, max: 1.3056, stdev: 0.2225
Old bare network average reward: 0.5878, min: -0.0185, max: 1.3148, stdev: 0.2344
New bare network average reward: 0.5887, min: 0.0093, max: 1.3056, stdev: 0.2323
External policy "random" average reward: 0.2477, min: -0.3981, max: 0.8889, stdev: 0.2272
External policy "individual greedy" average reward: 0.5215, min: -0.0278, max: 1.2222, stdev: 0.2233
External policy "total greedy" average reward: 0.6401, min: 0.0926, max: 1.3981, stdev: 0.2213
New network won 82 and tied 147 out of 300 games (51.83% wins where ties are half wins)
Keeping the new network

Training iteration 196 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.13 seconds
Training examples lengths: [64782, 64729, 65052, 64676, 64470, 64884, 64592, 65093, 64651, 64735]
Total value: 440879.82
Training on 647664 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2444 (value: 0.0015, weighted value: 0.0748, policy: 0.1697, weighted policy: 0.1697), Train Mean Max: 0.9307
Epoch 2/10, Train Loss: 0.2353 (value: 0.0014, weighted value: 0.0683, policy: 0.1671, weighted policy: 0.1671), Train Mean Max: 0.9311
Epoch 3/10, Train Loss: 0.2281 (value: 0.0013, weighted value: 0.0639, policy: 0.1642, weighted policy: 0.1642), Train Mean Max: 0.9316
Epoch 4/10, Train Loss: 0.2246 (value: 0.0012, weighted value: 0.0623, policy: 0.1623, weighted policy: 0.1623), Train Mean Max: 0.9319
Epoch 5/10, Train Loss: 0.2211 (value: 0.0012, weighted value: 0.0593, policy: 0.1618, weighted policy: 0.1618), Train Mean Max: 0.9320
Epoch 6/10, Train Loss: 0.2190 (value: 0.0012, weighted value: 0.0575, policy: 0.1614, weighted policy: 0.1614), Train Mean Max: 0.9321
Epoch 7/10, Train Loss: 0.2167 (value: 0.0011, weighted value: 0.0557, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9325
Epoch 8/10, Train Loss: 0.2170 (value: 0.0011, weighted value: 0.0561, policy: 0.1609, weighted policy: 0.1609), Train Mean Max: 0.9325
Epoch 9/10, Train Loss: 0.2127 (value: 0.0011, weighted value: 0.0533, policy: 0.1594, weighted policy: 0.1594), Train Mean Max: 0.9328
Epoch 10/10, Train Loss: 0.2112 (value: 0.0010, weighted value: 0.0521, policy: 0.1590, weighted policy: 0.1590), Train Mean Max: 0.9330
..training done in 68.03 seconds
..evaluation done in 18.74 seconds
Old network+MCTS average reward: 0.6736, min: 0.0833, max: 1.2778, stdev: 0.2129
New network+MCTS average reward: 0.6719, min: 0.1204, max: 1.3148, stdev: 0.2132
Old bare network average reward: 0.6193, min: -0.0185, max: 1.2130, stdev: 0.2151
New bare network average reward: 0.6128, min: -0.0185, max: 1.2778, stdev: 0.2129
External policy "random" average reward: 0.2545, min: -0.4444, max: 1.0370, stdev: 0.2159
External policy "individual greedy" average reward: 0.5235, min: -0.0278, max: 1.0833, stdev: 0.2093
External policy "total greedy" average reward: 0.6475, min: 0.0278, max: 1.2778, stdev: 0.2132
New network won 71 and tied 166 out of 300 games (51.33% wins where ties are half wins)
Keeping the new network

Training iteration 197 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 65.47 seconds
Training examples lengths: [64729, 65052, 64676, 64470, 64884, 64592, 65093, 64651, 64735, 64740]
Total value: 440318.28
Training on 647622 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2364 (value: 0.0014, weighted value: 0.0677, policy: 0.1688, weighted policy: 0.1688), Train Mean Max: 0.9316
Epoch 2/10, Train Loss: 0.2302 (value: 0.0013, weighted value: 0.0650, policy: 0.1652, weighted policy: 0.1652), Train Mean Max: 0.9322
Epoch 3/10, Train Loss: 0.2226 (value: 0.0012, weighted value: 0.0596, policy: 0.1631, weighted policy: 0.1631), Train Mean Max: 0.9325
Epoch 4/10, Train Loss: 0.2198 (value: 0.0012, weighted value: 0.0587, policy: 0.1611, weighted policy: 0.1611), Train Mean Max: 0.9327
Epoch 5/10, Train Loss: 0.2153 (value: 0.0011, weighted value: 0.0555, policy: 0.1598, weighted policy: 0.1598), Train Mean Max: 0.9330
Epoch 6/10, Train Loss: 0.2138 (value: 0.0011, weighted value: 0.0538, policy: 0.1599, weighted policy: 0.1599), Train Mean Max: 0.9330
Epoch 7/10, Train Loss: 0.2130 (value: 0.0011, weighted value: 0.0531, policy: 0.1599, weighted policy: 0.1599), Train Mean Max: 0.9333
Epoch 8/10, Train Loss: 0.2101 (value: 0.0010, weighted value: 0.0516, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9335
Epoch 9/10, Train Loss: 0.2090 (value: 0.0010, weighted value: 0.0503, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9334
Epoch 10/10, Train Loss: 0.2069 (value: 0.0010, weighted value: 0.0495, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9337
..training done in 77.78 seconds
..evaluation done in 19.30 seconds
Old network+MCTS average reward: 0.6875, min: 0.0278, max: 1.3796, stdev: 0.2231
New network+MCTS average reward: 0.6838, min: 0.0463, max: 1.3796, stdev: 0.2261
Old bare network average reward: 0.6288, min: -0.0278, max: 1.3796, stdev: 0.2286
New bare network average reward: 0.6332, min: 0.0556, max: 1.3426, stdev: 0.2306
External policy "random" average reward: 0.2583, min: -0.5093, max: 0.9167, stdev: 0.2314
External policy "individual greedy" average reward: 0.5687, min: -0.1481, max: 1.2870, stdev: 0.2396
External policy "total greedy" average reward: 0.6817, min: 0.0185, max: 1.3889, stdev: 0.2216
New network won 70 and tied 148 out of 300 games (48.00% wins where ties are half wins)
Reverting to the old network

Training iteration 198 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.81 seconds
Training examples lengths: [65052, 64676, 64470, 64884, 64592, 65093, 64651, 64735, 64740, 64729]
Total value: 440289.84
Training on 647622 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2654 (value: 0.0018, weighted value: 0.0885, policy: 0.1770, weighted policy: 0.1770), Train Mean Max: 0.9305
Epoch 2/10, Train Loss: 0.2492 (value: 0.0015, weighted value: 0.0770, policy: 0.1722, weighted policy: 0.1722), Train Mean Max: 0.9308
Epoch 3/10, Train Loss: 0.2399 (value: 0.0014, weighted value: 0.0722, policy: 0.1676, weighted policy: 0.1676), Train Mean Max: 0.9314
Epoch 4/10, Train Loss: 0.2338 (value: 0.0014, weighted value: 0.0684, policy: 0.1654, weighted policy: 0.1654), Train Mean Max: 0.9315
Epoch 5/10, Train Loss: 0.2287 (value: 0.0013, weighted value: 0.0659, policy: 0.1628, weighted policy: 0.1628), Train Mean Max: 0.9318
Epoch 6/10, Train Loss: 0.2241 (value: 0.0012, weighted value: 0.0621, policy: 0.1619, weighted policy: 0.1619), Train Mean Max: 0.9321
Epoch 7/10, Train Loss: 0.2223 (value: 0.0012, weighted value: 0.0602, policy: 0.1621, weighted policy: 0.1621), Train Mean Max: 0.9322
Epoch 8/10, Train Loss: 0.2197 (value: 0.0012, weighted value: 0.0595, policy: 0.1602, weighted policy: 0.1602), Train Mean Max: 0.9326
Epoch 9/10, Train Loss: 0.2156 (value: 0.0011, weighted value: 0.0561, policy: 0.1595, weighted policy: 0.1595), Train Mean Max: 0.9326
Epoch 10/10, Train Loss: 0.2168 (value: 0.0011, weighted value: 0.0561, policy: 0.1607, weighted policy: 0.1607), Train Mean Max: 0.9327
..training done in 59.93 seconds
..evaluation done in 17.70 seconds
Old network+MCTS average reward: 0.6659, min: 0.0556, max: 1.3519, stdev: 0.2329
New network+MCTS average reward: 0.6673, min: 0.1019, max: 1.3333, stdev: 0.2352
Old bare network average reward: 0.6126, min: 0.0000, max: 1.2593, stdev: 0.2343
New bare network average reward: 0.6172, min: 0.0370, max: 1.3333, stdev: 0.2405
External policy "random" average reward: 0.2475, min: -0.2870, max: 1.0741, stdev: 0.2243
External policy "individual greedy" average reward: 0.5288, min: -0.0648, max: 1.3241, stdev: 0.2309
External policy "total greedy" average reward: 0.6467, min: 0.0926, max: 1.3426, stdev: 0.2237
New network won 85 and tied 126 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 199 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.26 seconds
Training examples lengths: [64676, 64470, 64884, 64592, 65093, 64651, 64735, 64740, 64729, 64661]
Total value: 440083.69
Training on 647231 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2914 (value: 0.0021, weighted value: 0.1064, policy: 0.1850, weighted policy: 0.1850), Train Mean Max: 0.9289
Epoch 2/10, Train Loss: 0.2740 (value: 0.0018, weighted value: 0.0906, policy: 0.1834, weighted policy: 0.1834), Train Mean Max: 0.9296
Epoch 3/10, Train Loss: 0.2565 (value: 0.0017, weighted value: 0.0841, policy: 0.1724, weighted policy: 0.1724), Train Mean Max: 0.9304
Epoch 4/10, Train Loss: 0.2482 (value: 0.0016, weighted value: 0.0783, policy: 0.1700, weighted policy: 0.1700), Train Mean Max: 0.9305
Epoch 5/10, Train Loss: 0.2420 (value: 0.0015, weighted value: 0.0746, policy: 0.1674, weighted policy: 0.1674), Train Mean Max: 0.9309
Epoch 6/10, Train Loss: 0.2370 (value: 0.0014, weighted value: 0.0718, policy: 0.1652, weighted policy: 0.1652), Train Mean Max: 0.9311
Epoch 7/10, Train Loss: 0.2346 (value: 0.0014, weighted value: 0.0682, policy: 0.1665, weighted policy: 0.1665), Train Mean Max: 0.9309
Epoch 8/10, Train Loss: 0.2301 (value: 0.0013, weighted value: 0.0659, policy: 0.1642, weighted policy: 0.1642), Train Mean Max: 0.9314
Epoch 9/10, Train Loss: 0.2246 (value: 0.0013, weighted value: 0.0629, policy: 0.1617, weighted policy: 0.1617), Train Mean Max: 0.9320
Epoch 10/10, Train Loss: 0.2235 (value: 0.0012, weighted value: 0.0614, policy: 0.1621, weighted policy: 0.1621), Train Mean Max: 0.9318
..training done in 59.27 seconds
..evaluation done in 20.65 seconds
Old network+MCTS average reward: 0.6855, min: 0.0926, max: 1.4444, stdev: 0.2303
New network+MCTS average reward: 0.6875, min: 0.1019, max: 1.4444, stdev: 0.2379
Old bare network average reward: 0.6284, min: 0.0093, max: 1.4444, stdev: 0.2445
New bare network average reward: 0.6340, min: 0.0463, max: 1.4444, stdev: 0.2362
External policy "random" average reward: 0.2666, min: -0.2963, max: 1.1481, stdev: 0.2334
External policy "individual greedy" average reward: 0.5517, min: 0.0278, max: 1.1852, stdev: 0.2272
External policy "total greedy" average reward: 0.6615, min: 0.0833, max: 1.5000, stdev: 0.2352
New network won 85 and tied 141 out of 300 games (51.83% wins where ties are half wins)
Keeping the new network

Training iteration 200 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.10 seconds
Training examples lengths: [64470, 64884, 64592, 65093, 64651, 64735, 64740, 64729, 64661, 64813]
Total value: 440612.05
Training on 647368 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2502 (value: 0.0016, weighted value: 0.0780, policy: 0.1721, weighted policy: 0.1721), Train Mean Max: 0.9306
Epoch 2/10, Train Loss: 0.2389 (value: 0.0014, weighted value: 0.0704, policy: 0.1686, weighted policy: 0.1686), Train Mean Max: 0.9311
Epoch 3/10, Train Loss: 0.2332 (value: 0.0013, weighted value: 0.0662, policy: 0.1670, weighted policy: 0.1670), Train Mean Max: 0.9312
Epoch 4/10, Train Loss: 0.2299 (value: 0.0013, weighted value: 0.0648, policy: 0.1651, weighted policy: 0.1651), Train Mean Max: 0.9315
Epoch 5/10, Train Loss: 0.2254 (value: 0.0012, weighted value: 0.0621, policy: 0.1634, weighted policy: 0.1634), Train Mean Max: 0.9317
Epoch 6/10, Train Loss: 0.2226 (value: 0.0012, weighted value: 0.0602, policy: 0.1624, weighted policy: 0.1624), Train Mean Max: 0.9319
Epoch 7/10, Train Loss: 0.2194 (value: 0.0011, weighted value: 0.0574, policy: 0.1620, weighted policy: 0.1620), Train Mean Max: 0.9321
Epoch 8/10, Train Loss: 0.2184 (value: 0.0011, weighted value: 0.0563, policy: 0.1621, weighted policy: 0.1621), Train Mean Max: 0.9324
Epoch 9/10, Train Loss: 0.2164 (value: 0.0011, weighted value: 0.0552, policy: 0.1613, weighted policy: 0.1613), Train Mean Max: 0.9324
Epoch 10/10, Train Loss: 0.2136 (value: 0.0011, weighted value: 0.0536, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9327
..training done in 70.53 seconds
..evaluation done in 18.63 seconds
Old network+MCTS average reward: 0.6745, min: 0.0926, max: 1.4907, stdev: 0.2150
New network+MCTS average reward: 0.6744, min: 0.1574, max: 1.5463, stdev: 0.2183
Old bare network average reward: 0.6233, min: 0.0926, max: 1.4907, stdev: 0.2209
New bare network average reward: 0.6238, min: -0.0278, max: 1.4537, stdev: 0.2218
External policy "random" average reward: 0.2711, min: -0.3148, max: 1.0185, stdev: 0.2348
External policy "individual greedy" average reward: 0.5493, min: 0.0648, max: 1.7315, stdev: 0.2388
External policy "total greedy" average reward: 0.6572, min: 0.1852, max: 1.8241, stdev: 0.2263
New network won 80 and tied 133 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_200

Training iteration 201 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.71 seconds
Training examples lengths: [64884, 64592, 65093, 64651, 64735, 64740, 64729, 64661, 64813, 64466]
Total value: 440861.14
Training on 647364 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2759 (value: 0.0019, weighted value: 0.0944, policy: 0.1815, weighted policy: 0.1815), Train Mean Max: 0.9290
Epoch 2/10, Train Loss: 0.2582 (value: 0.0017, weighted value: 0.0838, policy: 0.1744, weighted policy: 0.1744), Train Mean Max: 0.9298
Epoch 3/10, Train Loss: 0.2522 (value: 0.0016, weighted value: 0.0798, policy: 0.1724, weighted policy: 0.1724), Train Mean Max: 0.9301
Epoch 4/10, Train Loss: 0.2419 (value: 0.0015, weighted value: 0.0737, policy: 0.1682, weighted policy: 0.1682), Train Mean Max: 0.9305
Epoch 5/10, Train Loss: 0.2379 (value: 0.0014, weighted value: 0.0705, policy: 0.1674, weighted policy: 0.1674), Train Mean Max: 0.9305
Epoch 6/10, Train Loss: 0.2334 (value: 0.0013, weighted value: 0.0672, policy: 0.1662, weighted policy: 0.1662), Train Mean Max: 0.9307
Epoch 7/10, Train Loss: 0.2302 (value: 0.0013, weighted value: 0.0654, policy: 0.1648, weighted policy: 0.1648), Train Mean Max: 0.9309
Epoch 8/10, Train Loss: 0.2268 (value: 0.0013, weighted value: 0.0626, policy: 0.1642, weighted policy: 0.1642), Train Mean Max: 0.9311
Epoch 9/10, Train Loss: 0.2255 (value: 0.0012, weighted value: 0.0612, policy: 0.1643, weighted policy: 0.1643), Train Mean Max: 0.9313
Epoch 10/10, Train Loss: 0.2217 (value: 0.0012, weighted value: 0.0587, policy: 0.1630, weighted policy: 0.1630), Train Mean Max: 0.9315
..training done in 63.36 seconds
..evaluation done in 19.76 seconds
Old network+MCTS average reward: 0.6622, min: 0.0000, max: 1.2963, stdev: 0.2358
New network+MCTS average reward: 0.6644, min: 0.0000, max: 1.3333, stdev: 0.2297
Old bare network average reward: 0.6043, min: -0.0463, max: 1.2407, stdev: 0.2430
New bare network average reward: 0.6063, min: -0.0278, max: 1.2222, stdev: 0.2376
External policy "random" average reward: 0.2587, min: -0.3333, max: 0.8333, stdev: 0.2252
External policy "individual greedy" average reward: 0.5398, min: 0.0093, max: 1.2130, stdev: 0.2329
External policy "total greedy" average reward: 0.6561, min: 0.1574, max: 1.4167, stdev: 0.2285
New network won 87 and tied 132 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 202 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.15 seconds
Training examples lengths: [64592, 65093, 64651, 64735, 64740, 64729, 64661, 64813, 64466, 64855]
Total value: 441529.06
Training on 647335 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2490 (value: 0.0015, weighted value: 0.0760, policy: 0.1730, weighted policy: 0.1730), Train Mean Max: 0.9302
Epoch 2/10, Train Loss: 0.2385 (value: 0.0014, weighted value: 0.0686, policy: 0.1699, weighted policy: 0.1699), Train Mean Max: 0.9307
Epoch 3/10, Train Loss: 0.2338 (value: 0.0013, weighted value: 0.0674, policy: 0.1663, weighted policy: 0.1663), Train Mean Max: 0.9311
Epoch 4/10, Train Loss: 0.2279 (value: 0.0013, weighted value: 0.0632, policy: 0.1646, weighted policy: 0.1646), Train Mean Max: 0.9312
Epoch 5/10, Train Loss: 0.2237 (value: 0.0012, weighted value: 0.0600, policy: 0.1636, weighted policy: 0.1636), Train Mean Max: 0.9315
Epoch 6/10, Train Loss: 0.2214 (value: 0.0012, weighted value: 0.0588, policy: 0.1627, weighted policy: 0.1627), Train Mean Max: 0.9317
Epoch 7/10, Train Loss: 0.2180 (value: 0.0011, weighted value: 0.0558, policy: 0.1621, weighted policy: 0.1621), Train Mean Max: 0.9320
Epoch 8/10, Train Loss: 0.2191 (value: 0.0011, weighted value: 0.0572, policy: 0.1620, weighted policy: 0.1620), Train Mean Max: 0.9321
Epoch 9/10, Train Loss: 0.2150 (value: 0.0011, weighted value: 0.0541, policy: 0.1609, weighted policy: 0.1609), Train Mean Max: 0.9323
Epoch 10/10, Train Loss: 0.2140 (value: 0.0011, weighted value: 0.0530, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9325
..training done in 61.52 seconds
..evaluation done in 18.54 seconds
Old network+MCTS average reward: 0.6631, min: 0.0278, max: 1.2685, stdev: 0.2258
New network+MCTS average reward: 0.6625, min: -0.0370, max: 1.2685, stdev: 0.2318
Old bare network average reward: 0.6089, min: -0.0556, max: 1.2685, stdev: 0.2428
New bare network average reward: 0.6044, min: -0.0370, max: 1.2593, stdev: 0.2354
External policy "random" average reward: 0.2466, min: -0.4259, max: 0.9352, stdev: 0.2239
External policy "individual greedy" average reward: 0.5236, min: -0.1296, max: 1.2037, stdev: 0.2305
External policy "total greedy" average reward: 0.6352, min: -0.1111, max: 1.3333, stdev: 0.2192
New network won 78 and tied 137 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 203 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.13 seconds
Training examples lengths: [65093, 64651, 64735, 64740, 64729, 64661, 64813, 64466, 64855, 65323]
Total value: 443209.19
Training on 648066 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2748 (value: 0.0019, weighted value: 0.0938, policy: 0.1810, weighted policy: 0.1810), Train Mean Max: 0.9291
Epoch 2/10, Train Loss: 0.2548 (value: 0.0016, weighted value: 0.0817, policy: 0.1731, weighted policy: 0.1731), Train Mean Max: 0.9299
Epoch 3/10, Train Loss: 0.2466 (value: 0.0015, weighted value: 0.0763, policy: 0.1702, weighted policy: 0.1702), Train Mean Max: 0.9302
Epoch 4/10, Train Loss: 0.2394 (value: 0.0014, weighted value: 0.0723, policy: 0.1671, weighted policy: 0.1671), Train Mean Max: 0.9306
Epoch 5/10, Train Loss: 0.2347 (value: 0.0014, weighted value: 0.0693, policy: 0.1654, weighted policy: 0.1654), Train Mean Max: 0.9308
Epoch 6/10, Train Loss: 0.2296 (value: 0.0013, weighted value: 0.0654, policy: 0.1642, weighted policy: 0.1642), Train Mean Max: 0.9310
Epoch 7/10, Train Loss: 0.2282 (value: 0.0013, weighted value: 0.0647, policy: 0.1636, weighted policy: 0.1636), Train Mean Max: 0.9312
Epoch 8/10, Train Loss: 0.2245 (value: 0.0012, weighted value: 0.0617, policy: 0.1628, weighted policy: 0.1628), Train Mean Max: 0.9315
Epoch 9/10, Train Loss: 0.2209 (value: 0.0012, weighted value: 0.0582, policy: 0.1628, weighted policy: 0.1628), Train Mean Max: 0.9317
Epoch 10/10, Train Loss: 0.2197 (value: 0.0012, weighted value: 0.0578, policy: 0.1620, weighted policy: 0.1620), Train Mean Max: 0.9319
..training done in 59.64 seconds
..evaluation done in 18.69 seconds
Old network+MCTS average reward: 0.6727, min: 0.0648, max: 1.4630, stdev: 0.2171
New network+MCTS average reward: 0.6752, min: 0.0741, max: 1.4815, stdev: 0.2138
Old bare network average reward: 0.6193, min: -0.0185, max: 1.4259, stdev: 0.2218
New bare network average reward: 0.6264, min: 0.0741, max: 1.4259, stdev: 0.2163
External policy "random" average reward: 0.2550, min: -0.2778, max: 0.8056, stdev: 0.2110
External policy "individual greedy" average reward: 0.5491, min: -0.0370, max: 1.4167, stdev: 0.2213
External policy "total greedy" average reward: 0.6532, min: 0.0185, max: 1.5000, stdev: 0.2209
New network won 83 and tied 144 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 204 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 61.21 seconds
Training examples lengths: [64651, 64735, 64740, 64729, 64661, 64813, 64466, 64855, 65323, 64742]
Total value: 443088.73
Training on 647715 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2441 (value: 0.0015, weighted value: 0.0744, policy: 0.1697, weighted policy: 0.1697), Train Mean Max: 0.9306
Epoch 2/10, Train Loss: 0.2361 (value: 0.0014, weighted value: 0.0689, policy: 0.1672, weighted policy: 0.1672), Train Mean Max: 0.9311
Epoch 3/10, Train Loss: 0.2291 (value: 0.0013, weighted value: 0.0648, policy: 0.1643, weighted policy: 0.1643), Train Mean Max: 0.9315
Epoch 4/10, Train Loss: 0.2243 (value: 0.0012, weighted value: 0.0614, policy: 0.1629, weighted policy: 0.1629), Train Mean Max: 0.9317
Epoch 5/10, Train Loss: 0.2226 (value: 0.0012, weighted value: 0.0604, policy: 0.1622, weighted policy: 0.1622), Train Mean Max: 0.9319
Epoch 6/10, Train Loss: 0.2194 (value: 0.0012, weighted value: 0.0585, policy: 0.1609, weighted policy: 0.1609), Train Mean Max: 0.9321
Epoch 7/10, Train Loss: 0.2142 (value: 0.0011, weighted value: 0.0545, policy: 0.1597, weighted policy: 0.1597), Train Mean Max: 0.9324
Epoch 8/10, Train Loss: 0.2147 (value: 0.0011, weighted value: 0.0546, policy: 0.1601, weighted policy: 0.1601), Train Mean Max: 0.9325
Epoch 9/10, Train Loss: 0.2131 (value: 0.0011, weighted value: 0.0529, policy: 0.1601, weighted policy: 0.1601), Train Mean Max: 0.9328
Epoch 10/10, Train Loss: 0.2098 (value: 0.0010, weighted value: 0.0511, policy: 0.1587, weighted policy: 0.1587), Train Mean Max: 0.9330
..training done in 67.74 seconds
..evaluation done in 19.04 seconds
Old network+MCTS average reward: 0.6900, min: 0.1019, max: 1.4630, stdev: 0.2261
New network+MCTS average reward: 0.6887, min: 0.1019, max: 1.4630, stdev: 0.2278
Old bare network average reward: 0.6374, min: 0.0463, max: 1.4630, stdev: 0.2317
New bare network average reward: 0.6413, min: -0.0648, max: 1.4630, stdev: 0.2381
External policy "random" average reward: 0.2718, min: -0.3148, max: 0.8333, stdev: 0.2325
External policy "individual greedy" average reward: 0.5565, min: -0.0370, max: 1.2130, stdev: 0.2314
External policy "total greedy" average reward: 0.6672, min: -0.0093, max: 1.3056, stdev: 0.2195
New network won 76 and tied 135 out of 300 games (47.83% wins where ties are half wins)
Reverting to the old network

Training iteration 205 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.86 seconds
Training examples lengths: [64735, 64740, 64729, 64661, 64813, 64466, 64855, 65323, 64742, 64864]
Total value: 443500.29
Training on 647928 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2691 (value: 0.0018, weighted value: 0.0916, policy: 0.1775, weighted policy: 0.1775), Train Mean Max: 0.9295
Epoch 2/10, Train Loss: 0.2526 (value: 0.0016, weighted value: 0.0809, policy: 0.1717, weighted policy: 0.1717), Train Mean Max: 0.9301
Epoch 3/10, Train Loss: 0.2443 (value: 0.0015, weighted value: 0.0760, policy: 0.1683, weighted policy: 0.1683), Train Mean Max: 0.9306
Epoch 4/10, Train Loss: 0.2371 (value: 0.0014, weighted value: 0.0710, policy: 0.1661, weighted policy: 0.1661), Train Mean Max: 0.9309
Epoch 5/10, Train Loss: 0.2325 (value: 0.0014, weighted value: 0.0680, policy: 0.1645, weighted policy: 0.1645), Train Mean Max: 0.9310
Epoch 6/10, Train Loss: 0.2282 (value: 0.0013, weighted value: 0.0649, policy: 0.1633, weighted policy: 0.1633), Train Mean Max: 0.9314
Epoch 7/10, Train Loss: 0.2251 (value: 0.0013, weighted value: 0.0630, policy: 0.1621, weighted policy: 0.1621), Train Mean Max: 0.9317
Epoch 8/10, Train Loss: 0.2219 (value: 0.0012, weighted value: 0.0604, policy: 0.1615, weighted policy: 0.1615), Train Mean Max: 0.9318
Epoch 9/10, Train Loss: 0.2202 (value: 0.0012, weighted value: 0.0595, policy: 0.1606, weighted policy: 0.1606), Train Mean Max: 0.9321
Epoch 10/10, Train Loss: 0.2176 (value: 0.0011, weighted value: 0.0571, policy: 0.1605, weighted policy: 0.1605), Train Mean Max: 0.9322
..training done in 59.72 seconds
..evaluation done in 18.53 seconds
Old network+MCTS average reward: 0.6495, min: -0.0185, max: 1.2407, stdev: 0.2156
New network+MCTS average reward: 0.6588, min: -0.0833, max: 1.3241, stdev: 0.2176
Old bare network average reward: 0.5983, min: -0.0833, max: 1.2407, stdev: 0.2240
New bare network average reward: 0.6036, min: -0.0833, max: 1.2407, stdev: 0.2207
External policy "random" average reward: 0.2523, min: -0.3889, max: 0.9722, stdev: 0.2177
External policy "individual greedy" average reward: 0.5198, min: -0.1296, max: 1.2593, stdev: 0.2192
External policy "total greedy" average reward: 0.6225, min: -0.0278, max: 1.2500, stdev: 0.2029
New network won 104 and tied 125 out of 300 games (55.50% wins where ties are half wins)
Keeping the new network

Training iteration 206 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.68 seconds
Training examples lengths: [64740, 64729, 64661, 64813, 64466, 64855, 65323, 64742, 64864, 64807]
Total value: 444222.68
Training on 648000 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2426 (value: 0.0015, weighted value: 0.0730, policy: 0.1696, weighted policy: 0.1696), Train Mean Max: 0.9310
Epoch 2/10, Train Loss: 0.2345 (value: 0.0014, weighted value: 0.0685, policy: 0.1660, weighted policy: 0.1660), Train Mean Max: 0.9316
Epoch 3/10, Train Loss: 0.2279 (value: 0.0013, weighted value: 0.0649, policy: 0.1630, weighted policy: 0.1630), Train Mean Max: 0.9318
Epoch 4/10, Train Loss: 0.2246 (value: 0.0013, weighted value: 0.0625, policy: 0.1621, weighted policy: 0.1621), Train Mean Max: 0.9319
Epoch 5/10, Train Loss: 0.2211 (value: 0.0012, weighted value: 0.0593, policy: 0.1618, weighted policy: 0.1618), Train Mean Max: 0.9322
Epoch 6/10, Train Loss: 0.2192 (value: 0.0012, weighted value: 0.0585, policy: 0.1607, weighted policy: 0.1607), Train Mean Max: 0.9324
Epoch 7/10, Train Loss: 0.2155 (value: 0.0011, weighted value: 0.0559, policy: 0.1595, weighted policy: 0.1595), Train Mean Max: 0.9327
Epoch 8/10, Train Loss: 0.2129 (value: 0.0011, weighted value: 0.0541, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9328
Epoch 9/10, Train Loss: 0.2112 (value: 0.0011, weighted value: 0.0526, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9331
Epoch 10/10, Train Loss: 0.2118 (value: 0.0011, weighted value: 0.0531, policy: 0.1587, weighted policy: 0.1587), Train Mean Max: 0.9331
..training done in 59.98 seconds
..evaluation done in 18.25 seconds
Old network+MCTS average reward: 0.6645, min: 0.0926, max: 1.3889, stdev: 0.2321
New network+MCTS average reward: 0.6628, min: 0.0926, max: 1.3889, stdev: 0.2324
Old bare network average reward: 0.6174, min: 0.0648, max: 1.3889, stdev: 0.2305
New bare network average reward: 0.6174, min: 0.0648, max: 1.3889, stdev: 0.2351
External policy "random" average reward: 0.2543, min: -0.3426, max: 0.8981, stdev: 0.2303
External policy "individual greedy" average reward: 0.5311, min: -0.0741, max: 1.2315, stdev: 0.2397
External policy "total greedy" average reward: 0.6471, min: 0.0926, max: 1.3333, stdev: 0.2293
New network won 80 and tied 140 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 207 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.52 seconds
Training examples lengths: [64729, 64661, 64813, 64466, 64855, 65323, 64742, 64864, 64807, 64893]
Total value: 445621.31
Training on 648153 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2363 (value: 0.0014, weighted value: 0.0687, policy: 0.1676, weighted policy: 0.1676), Train Mean Max: 0.9318
Epoch 2/10, Train Loss: 0.2280 (value: 0.0013, weighted value: 0.0635, policy: 0.1645, weighted policy: 0.1645), Train Mean Max: 0.9323
Epoch 3/10, Train Loss: 0.2249 (value: 0.0012, weighted value: 0.0624, policy: 0.1625, weighted policy: 0.1625), Train Mean Max: 0.9327
Epoch 4/10, Train Loss: 0.2182 (value: 0.0012, weighted value: 0.0580, policy: 0.1601, weighted policy: 0.1601), Train Mean Max: 0.9329
Epoch 5/10, Train Loss: 0.2125 (value: 0.0011, weighted value: 0.0538, policy: 0.1587, weighted policy: 0.1587), Train Mean Max: 0.9331
Epoch 6/10, Train Loss: 0.2141 (value: 0.0011, weighted value: 0.0555, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9333
Epoch 7/10, Train Loss: 0.2103 (value: 0.0011, weighted value: 0.0527, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9337
Epoch 8/10, Train Loss: 0.2088 (value: 0.0010, weighted value: 0.0513, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9335
Epoch 9/10, Train Loss: 0.2072 (value: 0.0010, weighted value: 0.0504, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9338
Epoch 10/10, Train Loss: 0.2063 (value: 0.0010, weighted value: 0.0494, policy: 0.1569, weighted policy: 0.1569), Train Mean Max: 0.9338
..training done in 68.67 seconds
..evaluation done in 18.89 seconds
Old network+MCTS average reward: 0.6519, min: 0.1667, max: 1.3889, stdev: 0.2173
New network+MCTS average reward: 0.6496, min: 0.1574, max: 1.3889, stdev: 0.2179
Old bare network average reward: 0.5992, min: -0.0370, max: 1.3889, stdev: 0.2244
New bare network average reward: 0.6009, min: 0.0741, max: 1.3889, stdev: 0.2243
External policy "random" average reward: 0.2478, min: -0.3981, max: 0.8611, stdev: 0.2284
External policy "individual greedy" average reward: 0.5331, min: 0.0463, max: 1.2500, stdev: 0.2201
External policy "total greedy" average reward: 0.6324, min: 0.0370, max: 1.3889, stdev: 0.2117
New network won 78 and tied 132 out of 300 games (48.00% wins where ties are half wins)
Reverting to the old network

Training iteration 208 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.43 seconds
Training examples lengths: [64661, 64813, 64466, 64855, 65323, 64742, 64864, 64807, 64893, 64674]
Total value: 446313.06
Training on 648098 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2628 (value: 0.0017, weighted value: 0.0861, policy: 0.1767, weighted policy: 0.1767), Train Mean Max: 0.9306
Epoch 2/10, Train Loss: 0.2456 (value: 0.0015, weighted value: 0.0765, policy: 0.1691, weighted policy: 0.1691), Train Mean Max: 0.9313
Epoch 3/10, Train Loss: 0.2375 (value: 0.0014, weighted value: 0.0718, policy: 0.1657, weighted policy: 0.1657), Train Mean Max: 0.9317
Epoch 4/10, Train Loss: 0.2309 (value: 0.0014, weighted value: 0.0684, policy: 0.1625, weighted policy: 0.1625), Train Mean Max: 0.9320
Epoch 5/10, Train Loss: 0.2273 (value: 0.0013, weighted value: 0.0652, policy: 0.1622, weighted policy: 0.1622), Train Mean Max: 0.9321
Epoch 6/10, Train Loss: 0.2229 (value: 0.0012, weighted value: 0.0624, policy: 0.1606, weighted policy: 0.1606), Train Mean Max: 0.9325
Epoch 7/10, Train Loss: 0.2183 (value: 0.0012, weighted value: 0.0590, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9327
Epoch 8/10, Train Loss: 0.2156 (value: 0.0011, weighted value: 0.0564, policy: 0.1592, weighted policy: 0.1592), Train Mean Max: 0.9330
Epoch 9/10, Train Loss: 0.2157 (value: 0.0011, weighted value: 0.0566, policy: 0.1591, weighted policy: 0.1591), Train Mean Max: 0.9331
Epoch 10/10, Train Loss: 0.2130 (value: 0.0011, weighted value: 0.0549, policy: 0.1580, weighted policy: 0.1580), Train Mean Max: 0.9332
..training done in 63.06 seconds
..evaluation done in 18.48 seconds
Old network+MCTS average reward: 0.6767, min: -0.1759, max: 1.3148, stdev: 0.2330
New network+MCTS average reward: 0.6784, min: 0.0556, max: 1.3704, stdev: 0.2293
Old bare network average reward: 0.6260, min: -0.1852, max: 1.3148, stdev: 0.2355
New bare network average reward: 0.6271, min: -0.1852, max: 1.3148, stdev: 0.2353
External policy "random" average reward: 0.2663, min: -0.3148, max: 0.8704, stdev: 0.2281
External policy "individual greedy" average reward: 0.5519, min: -0.0648, max: 1.2407, stdev: 0.2357
External policy "total greedy" average reward: 0.6595, min: 0.1481, max: 1.2963, stdev: 0.2294
New network won 67 and tied 160 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 209 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 63.93 seconds
Training examples lengths: [64813, 64466, 64855, 65323, 64742, 64864, 64807, 64893, 64674, 64635]
Total value: 446423.13
Training on 648072 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2888 (value: 0.0021, weighted value: 0.1055, policy: 0.1834, weighted policy: 0.1834), Train Mean Max: 0.9294
Epoch 2/10, Train Loss: 0.2636 (value: 0.0018, weighted value: 0.0896, policy: 0.1739, weighted policy: 0.1739), Train Mean Max: 0.9302
Epoch 3/10, Train Loss: 0.2511 (value: 0.0016, weighted value: 0.0818, policy: 0.1693, weighted policy: 0.1693), Train Mean Max: 0.9307
Epoch 4/10, Train Loss: 0.2455 (value: 0.0016, weighted value: 0.0783, policy: 0.1672, weighted policy: 0.1672), Train Mean Max: 0.9310
Epoch 5/10, Train Loss: 0.2372 (value: 0.0015, weighted value: 0.0728, policy: 0.1644, weighted policy: 0.1644), Train Mean Max: 0.9314
Epoch 6/10, Train Loss: 0.2322 (value: 0.0014, weighted value: 0.0692, policy: 0.1631, weighted policy: 0.1631), Train Mean Max: 0.9315
Epoch 7/10, Train Loss: 0.2293 (value: 0.0013, weighted value: 0.0668, policy: 0.1624, weighted policy: 0.1624), Train Mean Max: 0.9318
Epoch 8/10, Train Loss: 0.2255 (value: 0.0013, weighted value: 0.0640, policy: 0.1615, weighted policy: 0.1615), Train Mean Max: 0.9321
Epoch 9/10, Train Loss: 0.2218 (value: 0.0012, weighted value: 0.0619, policy: 0.1599, weighted policy: 0.1599), Train Mean Max: 0.9324
Epoch 10/10, Train Loss: 0.2202 (value: 0.0012, weighted value: 0.0599, policy: 0.1603, weighted policy: 0.1603), Train Mean Max: 0.9324
..training done in 72.32 seconds
..evaluation done in 18.12 seconds
Old network+MCTS average reward: 0.6982, min: 0.1389, max: 1.2685, stdev: 0.2221
New network+MCTS average reward: 0.6982, min: 0.1111, max: 1.2685, stdev: 0.2202
Old bare network average reward: 0.6497, min: 0.0833, max: 1.2685, stdev: 0.2310
New bare network average reward: 0.6452, min: 0.0556, max: 1.2407, stdev: 0.2260
External policy "random" average reward: 0.2832, min: -0.2593, max: 0.9907, stdev: 0.2254
External policy "individual greedy" average reward: 0.5673, min: -0.0278, max: 1.1574, stdev: 0.2204
External policy "total greedy" average reward: 0.6715, min: 0.1019, max: 1.3426, stdev: 0.2238
New network won 78 and tied 148 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 210 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.97 seconds
Training examples lengths: [64466, 64855, 65323, 64742, 64864, 64807, 64893, 64674, 64635, 64840]
Total value: 446719.21
Training on 648099 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2438 (value: 0.0015, weighted value: 0.0744, policy: 0.1694, weighted policy: 0.1694), Train Mean Max: 0.9312
Epoch 2/10, Train Loss: 0.2366 (value: 0.0014, weighted value: 0.0707, policy: 0.1658, weighted policy: 0.1658), Train Mean Max: 0.9317
Epoch 3/10, Train Loss: 0.2283 (value: 0.0013, weighted value: 0.0662, policy: 0.1621, weighted policy: 0.1621), Train Mean Max: 0.9322
Epoch 4/10, Train Loss: 0.2246 (value: 0.0013, weighted value: 0.0629, policy: 0.1618, weighted policy: 0.1618), Train Mean Max: 0.9324
Epoch 5/10, Train Loss: 0.2213 (value: 0.0012, weighted value: 0.0600, policy: 0.1613, weighted policy: 0.1613), Train Mean Max: 0.9325
Epoch 6/10, Train Loss: 0.2182 (value: 0.0012, weighted value: 0.0582, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9326
Epoch 7/10, Train Loss: 0.2157 (value: 0.0012, weighted value: 0.0575, policy: 0.1581, weighted policy: 0.1581), Train Mean Max: 0.9330
Epoch 8/10, Train Loss: 0.2129 (value: 0.0011, weighted value: 0.0545, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9331
Epoch 9/10, Train Loss: 0.2136 (value: 0.0011, weighted value: 0.0547, policy: 0.1589, weighted policy: 0.1589), Train Mean Max: 0.9332
Epoch 10/10, Train Loss: 0.2092 (value: 0.0010, weighted value: 0.0518, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9336
..training done in 61.03 seconds
..evaluation done in 28.10 seconds
Old network+MCTS average reward: 0.6843, min: 0.1111, max: 1.5185, stdev: 0.2323
New network+MCTS average reward: 0.6836, min: 0.1204, max: 1.5185, stdev: 0.2307
Old bare network average reward: 0.6353, min: -0.0093, max: 1.4537, stdev: 0.2360
New bare network average reward: 0.6382, min: 0.0556, max: 1.5185, stdev: 0.2371
External policy "random" average reward: 0.2537, min: -0.2870, max: 0.8796, stdev: 0.2248
External policy "individual greedy" average reward: 0.5428, min: -0.0648, max: 1.4074, stdev: 0.2272
External policy "total greedy" average reward: 0.6595, min: 0.1204, max: 1.2870, stdev: 0.2203
New network won 74 and tied 157 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 211 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.36 seconds
Training examples lengths: [64855, 65323, 64742, 64864, 64807, 64893, 64674, 64635, 64840, 64635]
Total value: 447111.92
Training on 648268 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2360 (value: 0.0014, weighted value: 0.0690, policy: 0.1670, weighted policy: 0.1670), Train Mean Max: 0.9323
Epoch 2/10, Train Loss: 0.2265 (value: 0.0013, weighted value: 0.0636, policy: 0.1629, weighted policy: 0.1629), Train Mean Max: 0.9328
Epoch 3/10, Train Loss: 0.2196 (value: 0.0012, weighted value: 0.0589, policy: 0.1606, weighted policy: 0.1606), Train Mean Max: 0.9331
Epoch 4/10, Train Loss: 0.2181 (value: 0.0012, weighted value: 0.0581, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9332
Epoch 5/10, Train Loss: 0.2150 (value: 0.0011, weighted value: 0.0569, policy: 0.1581, weighted policy: 0.1581), Train Mean Max: 0.9335
Epoch 6/10, Train Loss: 0.2109 (value: 0.0011, weighted value: 0.0535, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9337
Epoch 7/10, Train Loss: 0.2105 (value: 0.0011, weighted value: 0.0527, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9338
Epoch 8/10, Train Loss: 0.2077 (value: 0.0010, weighted value: 0.0514, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9342
Epoch 9/10, Train Loss: 0.2064 (value: 0.0010, weighted value: 0.0504, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9342
Epoch 10/10, Train Loss: 0.2051 (value: 0.0010, weighted value: 0.0488, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9343
..training done in 60.19 seconds
..evaluation done in 19.21 seconds
Old network+MCTS average reward: 0.6899, min: 0.0926, max: 1.4074, stdev: 0.2465
New network+MCTS average reward: 0.6904, min: 0.1204, max: 1.4074, stdev: 0.2461
Old bare network average reward: 0.6304, min: 0.0648, max: 1.4259, stdev: 0.2519
New bare network average reward: 0.6340, min: 0.0556, max: 1.5278, stdev: 0.2534
External policy "random" average reward: 0.2588, min: -0.3148, max: 1.0093, stdev: 0.2362
External policy "individual greedy" average reward: 0.5450, min: -0.0093, max: 1.4537, stdev: 0.2368
External policy "total greedy" average reward: 0.6647, min: 0.1019, max: 1.7130, stdev: 0.2416
New network won 65 and tied 166 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 212 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.24 seconds
Training examples lengths: [65323, 64742, 64864, 64807, 64893, 64674, 64635, 64840, 64635, 64841]
Total value: 446364.20
Training on 648254 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2622 (value: 0.0017, weighted value: 0.0870, policy: 0.1752, weighted policy: 0.1752), Train Mean Max: 0.9306
Epoch 2/10, Train Loss: 0.2450 (value: 0.0015, weighted value: 0.0759, policy: 0.1690, weighted policy: 0.1690), Train Mean Max: 0.9315
Epoch 3/10, Train Loss: 0.2357 (value: 0.0014, weighted value: 0.0700, policy: 0.1657, weighted policy: 0.1657), Train Mean Max: 0.9318
Epoch 4/10, Train Loss: 0.2297 (value: 0.0013, weighted value: 0.0670, policy: 0.1628, weighted policy: 0.1628), Train Mean Max: 0.9321
Epoch 5/10, Train Loss: 0.2247 (value: 0.0013, weighted value: 0.0634, policy: 0.1613, weighted policy: 0.1613), Train Mean Max: 0.9324
Epoch 6/10, Train Loss: 0.2226 (value: 0.0012, weighted value: 0.0623, policy: 0.1602, weighted policy: 0.1602), Train Mean Max: 0.9326
Epoch 7/10, Train Loss: 0.2188 (value: 0.0012, weighted value: 0.0589, policy: 0.1599, weighted policy: 0.1599), Train Mean Max: 0.9329
Epoch 8/10, Train Loss: 0.2152 (value: 0.0011, weighted value: 0.0567, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9331
Epoch 9/10, Train Loss: 0.2145 (value: 0.0011, weighted value: 0.0558, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9332
Epoch 10/10, Train Loss: 0.2131 (value: 0.0011, weighted value: 0.0551, policy: 0.1580, weighted policy: 0.1580), Train Mean Max: 0.9334
..training done in 59.93 seconds
..evaluation done in 18.07 seconds
Old network+MCTS average reward: 0.6812, min: 0.1389, max: 1.4074, stdev: 0.2321
New network+MCTS average reward: 0.6830, min: 0.1759, max: 1.4074, stdev: 0.2292
Old bare network average reward: 0.6330, min: -0.0093, max: 1.3704, stdev: 0.2332
New bare network average reward: 0.6353, min: 0.0370, max: 1.3148, stdev: 0.2307
External policy "random" average reward: 0.2602, min: -0.3241, max: 0.9537, stdev: 0.2303
External policy "individual greedy" average reward: 0.5236, min: -0.0741, max: 1.2685, stdev: 0.2326
External policy "total greedy" average reward: 0.6445, min: 0.0741, max: 1.2963, stdev: 0.2171
New network won 78 and tied 161 out of 300 games (52.83% wins where ties are half wins)
Keeping the new network

Training iteration 213 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.92 seconds
Training examples lengths: [64742, 64864, 64807, 64893, 64674, 64635, 64840, 64635, 64841, 65090]
Total value: 445983.94
Training on 648021 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2368 (value: 0.0014, weighted value: 0.0702, policy: 0.1666, weighted policy: 0.1666), Train Mean Max: 0.9318
Epoch 2/10, Train Loss: 0.2300 (value: 0.0013, weighted value: 0.0651, policy: 0.1649, weighted policy: 0.1649), Train Mean Max: 0.9322
Epoch 3/10, Train Loss: 0.2229 (value: 0.0012, weighted value: 0.0604, policy: 0.1625, weighted policy: 0.1625), Train Mean Max: 0.9327
Epoch 4/10, Train Loss: 0.2195 (value: 0.0012, weighted value: 0.0591, policy: 0.1604, weighted policy: 0.1604), Train Mean Max: 0.9329
Epoch 5/10, Train Loss: 0.2163 (value: 0.0011, weighted value: 0.0567, policy: 0.1596, weighted policy: 0.1596), Train Mean Max: 0.9331
Epoch 6/10, Train Loss: 0.2148 (value: 0.0011, weighted value: 0.0557, policy: 0.1591, weighted policy: 0.1591), Train Mean Max: 0.9332
Epoch 7/10, Train Loss: 0.2106 (value: 0.0011, weighted value: 0.0532, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9336
Epoch 8/10, Train Loss: 0.2091 (value: 0.0010, weighted value: 0.0519, policy: 0.1573, weighted policy: 0.1573), Train Mean Max: 0.9337
Epoch 9/10, Train Loss: 0.2086 (value: 0.0010, weighted value: 0.0508, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9338
Epoch 10/10, Train Loss: 0.2061 (value: 0.0010, weighted value: 0.0493, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9341
..training done in 59.43 seconds
..evaluation done in 18.08 seconds
Old network+MCTS average reward: 0.6615, min: 0.0926, max: 1.6389, stdev: 0.2404
New network+MCTS average reward: 0.6644, min: 0.1111, max: 1.6389, stdev: 0.2392
Old bare network average reward: 0.6166, min: -0.0741, max: 1.4259, stdev: 0.2439
New bare network average reward: 0.6106, min: 0.0000, max: 1.4167, stdev: 0.2443
External policy "random" average reward: 0.2637, min: -0.2778, max: 1.1204, stdev: 0.2481
External policy "individual greedy" average reward: 0.5256, min: -0.0093, max: 1.2870, stdev: 0.2336
External policy "total greedy" average reward: 0.6406, min: 0.0741, max: 1.4167, stdev: 0.2305
New network won 89 and tied 141 out of 300 games (53.17% wins where ties are half wins)
Keeping the new network

Training iteration 214 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.10 seconds
Training examples lengths: [64864, 64807, 64893, 64674, 64635, 64840, 64635, 64841, 65090, 64781]
Total value: 446285.39
Training on 648060 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2324 (value: 0.0013, weighted value: 0.0665, policy: 0.1659, weighted policy: 0.1659), Train Mean Max: 0.9328
Epoch 2/10, Train Loss: 0.2221 (value: 0.0012, weighted value: 0.0607, policy: 0.1614, weighted policy: 0.1614), Train Mean Max: 0.9333
Epoch 3/10, Train Loss: 0.2179 (value: 0.0012, weighted value: 0.0577, policy: 0.1602, weighted policy: 0.1602), Train Mean Max: 0.9336
Epoch 4/10, Train Loss: 0.2152 (value: 0.0011, weighted value: 0.0563, policy: 0.1589, weighted policy: 0.1589), Train Mean Max: 0.9337
Epoch 5/10, Train Loss: 0.2124 (value: 0.0011, weighted value: 0.0548, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9340
Epoch 6/10, Train Loss: 0.2077 (value: 0.0010, weighted value: 0.0517, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9341
Epoch 7/10, Train Loss: 0.2065 (value: 0.0010, weighted value: 0.0502, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9343
Epoch 8/10, Train Loss: 0.2054 (value: 0.0010, weighted value: 0.0491, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9344
Epoch 9/10, Train Loss: 0.2056 (value: 0.0010, weighted value: 0.0504, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9347
Epoch 10/10, Train Loss: 0.2007 (value: 0.0009, weighted value: 0.0457, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9349
..training done in 59.26 seconds
..evaluation done in 18.72 seconds
Old network+MCTS average reward: 0.6644, min: 0.0463, max: 1.5648, stdev: 0.2198
New network+MCTS average reward: 0.6688, min: 0.0370, max: 1.5000, stdev: 0.2212
Old bare network average reward: 0.6190, min: 0.0463, max: 1.3981, stdev: 0.2289
New bare network average reward: 0.6201, min: -0.0370, max: 1.4259, stdev: 0.2254
External policy "random" average reward: 0.2509, min: -0.2037, max: 1.1296, stdev: 0.2131
External policy "individual greedy" average reward: 0.5361, min: -0.0926, max: 1.4074, stdev: 0.2175
External policy "total greedy" average reward: 0.6424, min: -0.0463, max: 1.4167, stdev: 0.2118
New network won 80 and tied 154 out of 300 games (52.33% wins where ties are half wins)
Keeping the new network

Training iteration 215 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.61 seconds
Training examples lengths: [64807, 64893, 64674, 64635, 64840, 64635, 64841, 65090, 64781, 64735]
Total value: 446539.19
Training on 647931 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2293 (value: 0.0013, weighted value: 0.0641, policy: 0.1652, weighted policy: 0.1652), Train Mean Max: 0.9331
Epoch 2/10, Train Loss: 0.2218 (value: 0.0012, weighted value: 0.0593, policy: 0.1625, weighted policy: 0.1625), Train Mean Max: 0.9336
Epoch 3/10, Train Loss: 0.2162 (value: 0.0011, weighted value: 0.0572, policy: 0.1590, weighted policy: 0.1590), Train Mean Max: 0.9341
Epoch 4/10, Train Loss: 0.2113 (value: 0.0011, weighted value: 0.0533, policy: 0.1581, weighted policy: 0.1581), Train Mean Max: 0.9343
Epoch 5/10, Train Loss: 0.2086 (value: 0.0010, weighted value: 0.0523, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9345
Epoch 6/10, Train Loss: 0.2083 (value: 0.0011, weighted value: 0.0525, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9347
Epoch 7/10, Train Loss: 0.2043 (value: 0.0010, weighted value: 0.0487, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9348
Epoch 8/10, Train Loss: 0.2040 (value: 0.0010, weighted value: 0.0494, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9350
Epoch 9/10, Train Loss: 0.2014 (value: 0.0009, weighted value: 0.0466, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9349
Epoch 10/10, Train Loss: 0.2004 (value: 0.0009, weighted value: 0.0463, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9352
..training done in 58.59 seconds
..evaluation done in 18.26 seconds
Old network+MCTS average reward: 0.6960, min: 0.1111, max: 1.6574, stdev: 0.2330
New network+MCTS average reward: 0.6949, min: 0.1852, max: 1.6574, stdev: 0.2365
Old bare network average reward: 0.6432, min: 0.0370, max: 1.6111, stdev: 0.2368
New bare network average reward: 0.6473, min: 0.1204, max: 1.6111, stdev: 0.2380
External policy "random" average reward: 0.2777, min: -0.2778, max: 0.9907, stdev: 0.2250
External policy "individual greedy" average reward: 0.5479, min: -0.0463, max: 1.4259, stdev: 0.2383
External policy "total greedy" average reward: 0.6762, min: 0.0741, max: 1.6481, stdev: 0.2341
New network won 77 and tied 139 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 216 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 61.03 seconds
Training examples lengths: [64893, 64674, 64635, 64840, 64635, 64841, 65090, 64781, 64735, 64943]
Total value: 446941.86
Training on 648067 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2568 (value: 0.0017, weighted value: 0.0828, policy: 0.1740, weighted policy: 0.1740), Train Mean Max: 0.9317
Epoch 2/10, Train Loss: 0.2406 (value: 0.0015, weighted value: 0.0730, policy: 0.1675, weighted policy: 0.1675), Train Mean Max: 0.9326
Epoch 3/10, Train Loss: 0.2308 (value: 0.0014, weighted value: 0.0677, policy: 0.1631, weighted policy: 0.1631), Train Mean Max: 0.9330
Epoch 4/10, Train Loss: 0.2238 (value: 0.0013, weighted value: 0.0638, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9334
Epoch 5/10, Train Loss: 0.2205 (value: 0.0012, weighted value: 0.0616, policy: 0.1590, weighted policy: 0.1590), Train Mean Max: 0.9336
Epoch 6/10, Train Loss: 0.2179 (value: 0.0012, weighted value: 0.0596, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9336
Epoch 7/10, Train Loss: 0.2140 (value: 0.0011, weighted value: 0.0559, policy: 0.1581, weighted policy: 0.1581), Train Mean Max: 0.9338
Epoch 8/10, Train Loss: 0.2124 (value: 0.0011, weighted value: 0.0554, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9341
Epoch 9/10, Train Loss: 0.2108 (value: 0.0011, weighted value: 0.0542, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9343
Epoch 10/10, Train Loss: 0.2075 (value: 0.0010, weighted value: 0.0519, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9345
..training done in 70.16 seconds
..evaluation done in 18.96 seconds
Old network+MCTS average reward: 0.6689, min: 0.0648, max: 1.5185, stdev: 0.2372
New network+MCTS average reward: 0.6674, min: 0.0741, max: 1.5278, stdev: 0.2412
Old bare network average reward: 0.6228, min: -0.0648, max: 1.3241, stdev: 0.2397
New bare network average reward: 0.6271, min: -0.1111, max: 1.5185, stdev: 0.2470
External policy "random" average reward: 0.2590, min: -0.3704, max: 0.9907, stdev: 0.2220
External policy "individual greedy" average reward: 0.5350, min: -0.0556, max: 1.3241, stdev: 0.2146
External policy "total greedy" average reward: 0.6497, min: 0.0926, max: 1.5000, stdev: 0.2269
New network won 69 and tied 151 out of 300 games (48.17% wins where ties are half wins)
Reverting to the old network

Training iteration 217 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.32 seconds
Training examples lengths: [64674, 64635, 64840, 64635, 64841, 65090, 64781, 64735, 64943, 64746]
Total value: 446127.91
Training on 647920 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2836 (value: 0.0020, weighted value: 0.1007, policy: 0.1829, weighted policy: 0.1829), Train Mean Max: 0.9305
Epoch 2/10, Train Loss: 0.2590 (value: 0.0017, weighted value: 0.0850, policy: 0.1740, weighted policy: 0.1740), Train Mean Max: 0.9314
Epoch 3/10, Train Loss: 0.2460 (value: 0.0016, weighted value: 0.0796, policy: 0.1664, weighted policy: 0.1664), Train Mean Max: 0.9321
Epoch 4/10, Train Loss: 0.2383 (value: 0.0015, weighted value: 0.0742, policy: 0.1642, weighted policy: 0.1642), Train Mean Max: 0.9322
Epoch 5/10, Train Loss: 0.2333 (value: 0.0014, weighted value: 0.0700, policy: 0.1633, weighted policy: 0.1633), Train Mean Max: 0.9324
Epoch 6/10, Train Loss: 0.2267 (value: 0.0013, weighted value: 0.0661, policy: 0.1606, weighted policy: 0.1606), Train Mean Max: 0.9328
Epoch 7/10, Train Loss: 0.2241 (value: 0.0013, weighted value: 0.0644, policy: 0.1598, weighted policy: 0.1598), Train Mean Max: 0.9329
Epoch 8/10, Train Loss: 0.2202 (value: 0.0012, weighted value: 0.0608, policy: 0.1594, weighted policy: 0.1594), Train Mean Max: 0.9332
Epoch 9/10, Train Loss: 0.2194 (value: 0.0012, weighted value: 0.0597, policy: 0.1597, weighted policy: 0.1597), Train Mean Max: 0.9332
Epoch 10/10, Train Loss: 0.2152 (value: 0.0011, weighted value: 0.0569, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9334
..training done in 60.68 seconds
..evaluation done in 19.02 seconds
Old network+MCTS average reward: 0.6805, min: -0.0185, max: 1.5370, stdev: 0.2413
New network+MCTS average reward: 0.6804, min: 0.0833, max: 1.5370, stdev: 0.2322
Old bare network average reward: 0.6301, min: -0.0185, max: 1.4815, stdev: 0.2418
New bare network average reward: 0.6332, min: 0.0278, max: 1.4815, stdev: 0.2328
External policy "random" average reward: 0.2606, min: -0.4444, max: 1.1852, stdev: 0.2336
External policy "individual greedy" average reward: 0.5316, min: -0.0463, max: 1.3796, stdev: 0.2335
External policy "total greedy" average reward: 0.6469, min: 0.0556, max: 1.5278, stdev: 0.2261
New network won 83 and tied 133 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 218 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 67.82 seconds
Training examples lengths: [64635, 64840, 64635, 64841, 65090, 64781, 64735, 64943, 64746, 64711]
Total value: 445660.60
Training on 647957 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3049 (value: 0.0023, weighted value: 0.1145, policy: 0.1904, weighted policy: 0.1904), Train Mean Max: 0.9292
Epoch 2/10, Train Loss: 0.2751 (value: 0.0020, weighted value: 0.0983, policy: 0.1768, weighted policy: 0.1768), Train Mean Max: 0.9302
Epoch 3/10, Train Loss: 0.2601 (value: 0.0018, weighted value: 0.0887, policy: 0.1714, weighted policy: 0.1714), Train Mean Max: 0.9308
Epoch 4/10, Train Loss: 0.2503 (value: 0.0016, weighted value: 0.0820, policy: 0.1682, weighted policy: 0.1682), Train Mean Max: 0.9312
Epoch 5/10, Train Loss: 0.2425 (value: 0.0015, weighted value: 0.0772, policy: 0.1653, weighted policy: 0.1653), Train Mean Max: 0.9313
Epoch 6/10, Train Loss: 0.2375 (value: 0.0015, weighted value: 0.0734, policy: 0.1641, weighted policy: 0.1641), Train Mean Max: 0.9316
Epoch 7/10, Train Loss: 0.2334 (value: 0.0014, weighted value: 0.0700, policy: 0.1634, weighted policy: 0.1634), Train Mean Max: 0.9318
Epoch 8/10, Train Loss: 0.2283 (value: 0.0013, weighted value: 0.0666, policy: 0.1616, weighted policy: 0.1616), Train Mean Max: 0.9321
Epoch 9/10, Train Loss: 0.2250 (value: 0.0013, weighted value: 0.0634, policy: 0.1616, weighted policy: 0.1616), Train Mean Max: 0.9323
Epoch 10/10, Train Loss: 0.2221 (value: 0.0012, weighted value: 0.0619, policy: 0.1602, weighted policy: 0.1602), Train Mean Max: 0.9326
..training done in 69.70 seconds
..evaluation done in 18.92 seconds
Old network+MCTS average reward: 0.6806, min: 0.1111, max: 1.4444, stdev: 0.2313
New network+MCTS average reward: 0.6814, min: 0.1111, max: 1.4907, stdev: 0.2321
Old bare network average reward: 0.6317, min: -0.0093, max: 1.3611, stdev: 0.2315
New bare network average reward: 0.6265, min: 0.0556, max: 1.3611, stdev: 0.2385
External policy "random" average reward: 0.2692, min: -0.2593, max: 1.0556, stdev: 0.2134
External policy "individual greedy" average reward: 0.5376, min: 0.0000, max: 1.1759, stdev: 0.2122
External policy "total greedy" average reward: 0.6542, min: 0.1111, max: 1.4444, stdev: 0.2162
New network won 83 and tied 136 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 219 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.61 seconds
Training examples lengths: [64840, 64635, 64841, 65090, 64781, 64735, 64943, 64746, 64711, 64732]
Total value: 445861.06
Training on 648054 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2443 (value: 0.0015, weighted value: 0.0762, policy: 0.1682, weighted policy: 0.1682), Train Mean Max: 0.9313
Epoch 2/10, Train Loss: 0.2373 (value: 0.0014, weighted value: 0.0717, policy: 0.1656, weighted policy: 0.1656), Train Mean Max: 0.9315
Epoch 3/10, Train Loss: 0.2304 (value: 0.0013, weighted value: 0.0666, policy: 0.1638, weighted policy: 0.1638), Train Mean Max: 0.9320
Epoch 4/10, Train Loss: 0.2248 (value: 0.0013, weighted value: 0.0631, policy: 0.1617, weighted policy: 0.1617), Train Mean Max: 0.9324
Epoch 5/10, Train Loss: 0.2218 (value: 0.0012, weighted value: 0.0611, policy: 0.1608, weighted policy: 0.1608), Train Mean Max: 0.9325
Epoch 6/10, Train Loss: 0.2193 (value: 0.0012, weighted value: 0.0594, policy: 0.1599, weighted policy: 0.1599), Train Mean Max: 0.9327
Epoch 7/10, Train Loss: 0.2177 (value: 0.0012, weighted value: 0.0580, policy: 0.1597, weighted policy: 0.1597), Train Mean Max: 0.9328
Epoch 8/10, Train Loss: 0.2141 (value: 0.0011, weighted value: 0.0554, policy: 0.1587, weighted policy: 0.1587), Train Mean Max: 0.9332
Epoch 9/10, Train Loss: 0.2113 (value: 0.0011, weighted value: 0.0538, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9333
Epoch 10/10, Train Loss: 0.2110 (value: 0.0011, weighted value: 0.0530, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9335
..training done in 60.71 seconds
..evaluation done in 17.93 seconds
Old network+MCTS average reward: 0.6998, min: 0.1204, max: 1.2685, stdev: 0.2140
New network+MCTS average reward: 0.6989, min: 0.0463, max: 1.2500, stdev: 0.2182
Old bare network average reward: 0.6497, min: 0.0093, max: 1.1944, stdev: 0.2229
New bare network average reward: 0.6515, min: 0.0093, max: 1.2315, stdev: 0.2223
External policy "random" average reward: 0.2650, min: -0.2778, max: 0.8056, stdev: 0.2085
External policy "individual greedy" average reward: 0.5536, min: -0.0463, max: 1.1296, stdev: 0.2217
External policy "total greedy" average reward: 0.6690, min: 0.1019, max: 1.2407, stdev: 0.2128
New network won 75 and tied 153 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 220 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.56 seconds
Training examples lengths: [64635, 64841, 65090, 64781, 64735, 64943, 64746, 64711, 64732, 64855]
Total value: 445932.09
Training on 648069 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2357 (value: 0.0014, weighted value: 0.0694, policy: 0.1662, weighted policy: 0.1662), Train Mean Max: 0.9321
Epoch 2/10, Train Loss: 0.2285 (value: 0.0013, weighted value: 0.0647, policy: 0.1637, weighted policy: 0.1637), Train Mean Max: 0.9326
Epoch 3/10, Train Loss: 0.2205 (value: 0.0012, weighted value: 0.0593, policy: 0.1612, weighted policy: 0.1612), Train Mean Max: 0.9332
Epoch 4/10, Train Loss: 0.2185 (value: 0.0012, weighted value: 0.0589, policy: 0.1595, weighted policy: 0.1595), Train Mean Max: 0.9333
Epoch 5/10, Train Loss: 0.2124 (value: 0.0011, weighted value: 0.0548, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9336
Epoch 6/10, Train Loss: 0.2127 (value: 0.0011, weighted value: 0.0550, policy: 0.1577, weighted policy: 0.1577), Train Mean Max: 0.9337
Epoch 7/10, Train Loss: 0.2094 (value: 0.0010, weighted value: 0.0523, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9338
Epoch 8/10, Train Loss: 0.2062 (value: 0.0010, weighted value: 0.0499, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9340
Epoch 9/10, Train Loss: 0.2076 (value: 0.0010, weighted value: 0.0512, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9341
Epoch 10/10, Train Loss: 0.2034 (value: 0.0010, weighted value: 0.0479, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9345
..training done in 60.13 seconds
..evaluation done in 18.49 seconds
Old network+MCTS average reward: 0.6607, min: 0.1019, max: 1.2593, stdev: 0.2349
New network+MCTS average reward: 0.6684, min: 0.0648, max: 1.2778, stdev: 0.2333
Old bare network average reward: 0.6119, min: -0.0370, max: 1.2500, stdev: 0.2409
New bare network average reward: 0.6137, min: -0.0370, max: 1.2500, stdev: 0.2376
External policy "random" average reward: 0.2601, min: -0.2778, max: 0.9907, stdev: 0.2266
External policy "individual greedy" average reward: 0.5295, min: -0.0185, max: 1.0833, stdev: 0.2276
External policy "total greedy" average reward: 0.6416, min: 0.0278, max: 1.2500, stdev: 0.2207
New network won 71 and tied 162 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_220

Training iteration 221 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 60.98 seconds
Training examples lengths: [64841, 65090, 64781, 64735, 64943, 64746, 64711, 64732, 64855, 65035]
Total value: 446642.43
Training on 648469 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2327 (value: 0.0013, weighted value: 0.0670, policy: 0.1657, weighted policy: 0.1657), Train Mean Max: 0.9329
Epoch 2/10, Train Loss: 0.2210 (value: 0.0012, weighted value: 0.0607, policy: 0.1603, weighted policy: 0.1603), Train Mean Max: 0.9337
Epoch 3/10, Train Loss: 0.2167 (value: 0.0011, weighted value: 0.0575, policy: 0.1592, weighted policy: 0.1592), Train Mean Max: 0.9338
Epoch 4/10, Train Loss: 0.2119 (value: 0.0011, weighted value: 0.0548, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9341
Epoch 5/10, Train Loss: 0.2088 (value: 0.0011, weighted value: 0.0528, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9344
Epoch 6/10, Train Loss: 0.2073 (value: 0.0010, weighted value: 0.0515, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9345
Epoch 7/10, Train Loss: 0.2056 (value: 0.0010, weighted value: 0.0501, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9346
Epoch 8/10, Train Loss: 0.2038 (value: 0.0010, weighted value: 0.0491, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9348
Epoch 9/10, Train Loss: 0.2022 (value: 0.0010, weighted value: 0.0478, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9350
Epoch 10/10, Train Loss: 0.1994 (value: 0.0009, weighted value: 0.0451, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9353
..training done in 71.10 seconds
..evaluation done in 18.20 seconds
Old network+MCTS average reward: 0.6981, min: 0.1204, max: 1.4907, stdev: 0.2374
New network+MCTS average reward: 0.6981, min: 0.1204, max: 1.4907, stdev: 0.2391
Old bare network average reward: 0.6424, min: -0.0185, max: 1.4907, stdev: 0.2451
New bare network average reward: 0.6487, min: 0.0093, max: 1.4907, stdev: 0.2479
External policy "random" average reward: 0.2835, min: -0.2870, max: 1.0463, stdev: 0.2359
External policy "individual greedy" average reward: 0.5527, min: -0.1667, max: 1.3796, stdev: 0.2479
External policy "total greedy" average reward: 0.6737, min: 0.0278, max: 1.4722, stdev: 0.2391
New network won 73 and tied 145 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 222 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.25 seconds
Training examples lengths: [65090, 64781, 64735, 64943, 64746, 64711, 64732, 64855, 65035, 64623]
Total value: 447201.67
Training on 648251 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2575 (value: 0.0017, weighted value: 0.0839, policy: 0.1735, weighted policy: 0.1735), Train Mean Max: 0.9314
Epoch 2/10, Train Loss: 0.2393 (value: 0.0015, weighted value: 0.0728, policy: 0.1665, weighted policy: 0.1665), Train Mean Max: 0.9325
Epoch 3/10, Train Loss: 0.2308 (value: 0.0014, weighted value: 0.0682, policy: 0.1625, weighted policy: 0.1625), Train Mean Max: 0.9327
Epoch 4/10, Train Loss: 0.2249 (value: 0.0013, weighted value: 0.0648, policy: 0.1601, weighted policy: 0.1601), Train Mean Max: 0.9331
Epoch 5/10, Train Loss: 0.2200 (value: 0.0012, weighted value: 0.0613, policy: 0.1587, weighted policy: 0.1587), Train Mean Max: 0.9334
Epoch 6/10, Train Loss: 0.2172 (value: 0.0012, weighted value: 0.0591, policy: 0.1580, weighted policy: 0.1580), Train Mean Max: 0.9335
Epoch 7/10, Train Loss: 0.2142 (value: 0.0011, weighted value: 0.0563, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9338
Epoch 8/10, Train Loss: 0.2119 (value: 0.0011, weighted value: 0.0554, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9340
Epoch 9/10, Train Loss: 0.2097 (value: 0.0011, weighted value: 0.0538, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9341
Epoch 10/10, Train Loss: 0.2083 (value: 0.0011, weighted value: 0.0525, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9344
..training done in 68.07 seconds
..evaluation done in 18.51 seconds
Old network+MCTS average reward: 0.6818, min: 0.0926, max: 1.5833, stdev: 0.2453
New network+MCTS average reward: 0.6824, min: 0.0648, max: 1.5833, stdev: 0.2438
Old bare network average reward: 0.6339, min: 0.0093, max: 1.5370, stdev: 0.2534
New bare network average reward: 0.6333, min: -0.0556, max: 1.5278, stdev: 0.2531
External policy "random" average reward: 0.2604, min: -0.3426, max: 1.1204, stdev: 0.2339
External policy "individual greedy" average reward: 0.5396, min: -0.0278, max: 1.6019, stdev: 0.2393
External policy "total greedy" average reward: 0.6540, min: -0.0556, max: 1.5278, stdev: 0.2395
New network won 70 and tied 163 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 223 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.40 seconds
Training examples lengths: [64781, 64735, 64943, 64746, 64711, 64732, 64855, 65035, 64623, 64660]
Total value: 446746.32
Training on 647821 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2343 (value: 0.0014, weighted value: 0.0688, policy: 0.1655, weighted policy: 0.1655), Train Mean Max: 0.9328
Epoch 2/10, Train Loss: 0.2251 (value: 0.0013, weighted value: 0.0631, policy: 0.1620, weighted policy: 0.1620), Train Mean Max: 0.9333
Epoch 3/10, Train Loss: 0.2164 (value: 0.0012, weighted value: 0.0579, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9339
Epoch 4/10, Train Loss: 0.2152 (value: 0.0012, weighted value: 0.0578, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9340
Epoch 5/10, Train Loss: 0.2119 (value: 0.0011, weighted value: 0.0552, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9341
Epoch 6/10, Train Loss: 0.2091 (value: 0.0011, weighted value: 0.0536, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9345
Epoch 7/10, Train Loss: 0.2057 (value: 0.0010, weighted value: 0.0509, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9346
Epoch 8/10, Train Loss: 0.2062 (value: 0.0010, weighted value: 0.0509, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9346
Epoch 9/10, Train Loss: 0.2029 (value: 0.0010, weighted value: 0.0491, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9350
Epoch 10/10, Train Loss: 0.2021 (value: 0.0009, weighted value: 0.0466, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9348
..training done in 63.55 seconds
..evaluation done in 18.68 seconds
Old network+MCTS average reward: 0.6932, min: 0.1944, max: 1.3333, stdev: 0.2281
New network+MCTS average reward: 0.6932, min: 0.1019, max: 1.3796, stdev: 0.2277
Old bare network average reward: 0.6422, min: 0.0278, max: 1.3333, stdev: 0.2397
New bare network average reward: 0.6404, min: 0.1019, max: 1.3796, stdev: 0.2359
External policy "random" average reward: 0.2669, min: -0.3981, max: 0.7870, stdev: 0.2398
External policy "individual greedy" average reward: 0.5403, min: 0.0000, max: 1.2130, stdev: 0.2322
External policy "total greedy" average reward: 0.6610, min: 0.0370, max: 1.3796, stdev: 0.2264
New network won 78 and tied 152 out of 300 games (51.33% wins where ties are half wins)
Keeping the new network

Training iteration 224 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.75 seconds
Training examples lengths: [64735, 64943, 64746, 64711, 64732, 64855, 65035, 64623, 64660, 64455]
Total value: 446556.16
Training on 647495 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2310 (value: 0.0013, weighted value: 0.0655, policy: 0.1654, weighted policy: 0.1654), Train Mean Max: 0.9334
Epoch 2/10, Train Loss: 0.2199 (value: 0.0012, weighted value: 0.0597, policy: 0.1603, weighted policy: 0.1603), Train Mean Max: 0.9340
Epoch 3/10, Train Loss: 0.2146 (value: 0.0011, weighted value: 0.0557, policy: 0.1589, weighted policy: 0.1589), Train Mean Max: 0.9343
Epoch 4/10, Train Loss: 0.2109 (value: 0.0011, weighted value: 0.0542, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9345
Epoch 5/10, Train Loss: 0.2076 (value: 0.0010, weighted value: 0.0520, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9346
Epoch 6/10, Train Loss: 0.2067 (value: 0.0010, weighted value: 0.0514, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9348
Epoch 7/10, Train Loss: 0.2043 (value: 0.0010, weighted value: 0.0491, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9351
Epoch 8/10, Train Loss: 0.2017 (value: 0.0009, weighted value: 0.0473, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9351
Epoch 9/10, Train Loss: 0.2012 (value: 0.0009, weighted value: 0.0469, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9353
Epoch 10/10, Train Loss: 0.2000 (value: 0.0009, weighted value: 0.0461, policy: 0.1539, weighted policy: 0.1539), Train Mean Max: 0.9355
..training done in 64.78 seconds
..evaluation done in 18.76 seconds
Old network+MCTS average reward: 0.6610, min: 0.0926, max: 1.3704, stdev: 0.2261
New network+MCTS average reward: 0.6591, min: 0.0926, max: 1.3704, stdev: 0.2227
Old bare network average reward: 0.6185, min: 0.0833, max: 1.3704, stdev: 0.2295
New bare network average reward: 0.6147, min: 0.0833, max: 1.3704, stdev: 0.2273
External policy "random" average reward: 0.2524, min: -0.3611, max: 0.9907, stdev: 0.2255
External policy "individual greedy" average reward: 0.5340, min: -0.0556, max: 1.3611, stdev: 0.2248
External policy "total greedy" average reward: 0.6507, min: 0.0278, max: 1.4444, stdev: 0.2157
New network won 73 and tied 139 out of 300 games (47.50% wins where ties are half wins)
Reverting to the old network

Training iteration 225 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 61.95 seconds
Training examples lengths: [64943, 64746, 64711, 64732, 64855, 65035, 64623, 64660, 64455, 64800]
Total value: 446752.05
Training on 647560 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2558 (value: 0.0017, weighted value: 0.0829, policy: 0.1729, weighted policy: 0.1729), Train Mean Max: 0.9321
Epoch 2/10, Train Loss: 0.2399 (value: 0.0014, weighted value: 0.0725, policy: 0.1674, weighted policy: 0.1674), Train Mean Max: 0.9328
Epoch 3/10, Train Loss: 0.2312 (value: 0.0014, weighted value: 0.0677, policy: 0.1635, weighted policy: 0.1635), Train Mean Max: 0.9333
Epoch 4/10, Train Loss: 0.2233 (value: 0.0013, weighted value: 0.0634, policy: 0.1599, weighted policy: 0.1599), Train Mean Max: 0.9335
Epoch 5/10, Train Loss: 0.2205 (value: 0.0012, weighted value: 0.0617, policy: 0.1587, weighted policy: 0.1587), Train Mean Max: 0.9337
Epoch 6/10, Train Loss: 0.2148 (value: 0.0012, weighted value: 0.0576, policy: 0.1573, weighted policy: 0.1573), Train Mean Max: 0.9338
Epoch 7/10, Train Loss: 0.2133 (value: 0.0011, weighted value: 0.0557, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9340
Epoch 8/10, Train Loss: 0.2117 (value: 0.0011, weighted value: 0.0545, policy: 0.1572, weighted policy: 0.1572), Train Mean Max: 0.9342
Epoch 9/10, Train Loss: 0.2090 (value: 0.0011, weighted value: 0.0528, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9345
Epoch 10/10, Train Loss: 0.2064 (value: 0.0010, weighted value: 0.0512, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9346
..training done in 72.22 seconds
..evaluation done in 17.96 seconds
Old network+MCTS average reward: 0.6948, min: 0.1111, max: 1.4815, stdev: 0.2272
New network+MCTS average reward: 0.6901, min: 0.1111, max: 1.4815, stdev: 0.2283
Old bare network average reward: 0.6485, min: 0.0556, max: 1.4815, stdev: 0.2410
New bare network average reward: 0.6427, min: 0.0185, max: 1.4815, stdev: 0.2387
External policy "random" average reward: 0.2578, min: -0.3704, max: 0.8796, stdev: 0.2247
External policy "individual greedy" average reward: 0.5395, min: -0.1019, max: 1.5093, stdev: 0.2346
External policy "total greedy" average reward: 0.6578, min: 0.1204, max: 1.4444, stdev: 0.2317
New network won 71 and tied 151 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 226 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.71 seconds
Training examples lengths: [64746, 64711, 64732, 64855, 65035, 64623, 64660, 64455, 64800, 64978]
Total value: 446892.52
Training on 647595 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2825 (value: 0.0020, weighted value: 0.1004, policy: 0.1821, weighted policy: 0.1821), Train Mean Max: 0.9307
Epoch 2/10, Train Loss: 0.2579 (value: 0.0017, weighted value: 0.0854, policy: 0.1725, weighted policy: 0.1725), Train Mean Max: 0.9317
Epoch 3/10, Train Loss: 0.2443 (value: 0.0016, weighted value: 0.0777, policy: 0.1667, weighted policy: 0.1667), Train Mean Max: 0.9323
Epoch 4/10, Train Loss: 0.2370 (value: 0.0015, weighted value: 0.0731, policy: 0.1639, weighted policy: 0.1639), Train Mean Max: 0.9324
Epoch 5/10, Train Loss: 0.2315 (value: 0.0014, weighted value: 0.0696, policy: 0.1619, weighted policy: 0.1619), Train Mean Max: 0.9327
Epoch 6/10, Train Loss: 0.2261 (value: 0.0013, weighted value: 0.0660, policy: 0.1601, weighted policy: 0.1601), Train Mean Max: 0.9329
Epoch 7/10, Train Loss: 0.2227 (value: 0.0012, weighted value: 0.0620, policy: 0.1607, weighted policy: 0.1607), Train Mean Max: 0.9330
Epoch 8/10, Train Loss: 0.2201 (value: 0.0012, weighted value: 0.0615, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9334
Epoch 9/10, Train Loss: 0.2166 (value: 0.0012, weighted value: 0.0578, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9334
Epoch 10/10, Train Loss: 0.2144 (value: 0.0011, weighted value: 0.0568, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9337
..training done in 66.49 seconds
..evaluation done in 18.63 seconds
Old network+MCTS average reward: 0.6905, min: 0.0370, max: 1.5093, stdev: 0.2306
New network+MCTS average reward: 0.6904, min: 0.0370, max: 1.5093, stdev: 0.2314
Old bare network average reward: 0.6391, min: 0.0185, max: 1.5093, stdev: 0.2357
New bare network average reward: 0.6446, min: 0.0185, max: 1.5093, stdev: 0.2364
External policy "random" average reward: 0.2562, min: -0.3889, max: 1.0278, stdev: 0.2211
External policy "individual greedy" average reward: 0.5414, min: -0.1574, max: 1.3981, stdev: 0.2271
External policy "total greedy" average reward: 0.6571, min: 0.1019, max: 1.4074, stdev: 0.2247
New network won 67 and tied 161 out of 300 games (49.17% wins where ties are half wins)
Reverting to the old network

Training iteration 227 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.91 seconds
Training examples lengths: [64711, 64732, 64855, 65035, 64623, 64660, 64455, 64800, 64978, 64826]
Total value: 447011.60
Training on 647675 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3070 (value: 0.0024, weighted value: 0.1180, policy: 0.1891, weighted policy: 0.1891), Train Mean Max: 0.9297
Epoch 2/10, Train Loss: 0.2754 (value: 0.0020, weighted value: 0.0985, policy: 0.1769, weighted policy: 0.1769), Train Mean Max: 0.9307
Epoch 3/10, Train Loss: 0.2578 (value: 0.0018, weighted value: 0.0882, policy: 0.1695, weighted policy: 0.1695), Train Mean Max: 0.9312
Epoch 4/10, Train Loss: 0.2498 (value: 0.0016, weighted value: 0.0825, policy: 0.1673, weighted policy: 0.1673), Train Mean Max: 0.9315
Epoch 5/10, Train Loss: 0.2423 (value: 0.0015, weighted value: 0.0773, policy: 0.1650, weighted policy: 0.1650), Train Mean Max: 0.9316
Epoch 6/10, Train Loss: 0.2377 (value: 0.0015, weighted value: 0.0734, policy: 0.1642, weighted policy: 0.1642), Train Mean Max: 0.9318
Epoch 7/10, Train Loss: 0.2320 (value: 0.0014, weighted value: 0.0696, policy: 0.1624, weighted policy: 0.1624), Train Mean Max: 0.9322
Epoch 8/10, Train Loss: 0.2296 (value: 0.0014, weighted value: 0.0676, policy: 0.1620, weighted policy: 0.1620), Train Mean Max: 0.9323
Epoch 9/10, Train Loss: 0.2238 (value: 0.0013, weighted value: 0.0636, policy: 0.1602, weighted policy: 0.1602), Train Mean Max: 0.9327
Epoch 10/10, Train Loss: 0.2210 (value: 0.0012, weighted value: 0.0615, policy: 0.1595, weighted policy: 0.1595), Train Mean Max: 0.9328
..training done in 62.38 seconds
..evaluation done in 18.55 seconds
Old network+MCTS average reward: 0.6780, min: 0.1481, max: 1.3148, stdev: 0.2125
New network+MCTS average reward: 0.6814, min: 0.1944, max: 1.3148, stdev: 0.2166
Old bare network average reward: 0.6243, min: 0.0278, max: 1.3056, stdev: 0.2240
New bare network average reward: 0.6299, min: 0.1296, max: 1.3148, stdev: 0.2256
External policy "random" average reward: 0.2500, min: -0.2963, max: 1.0093, stdev: 0.2113
External policy "individual greedy" average reward: 0.5394, min: -0.0185, max: 1.3333, stdev: 0.2210
External policy "total greedy" average reward: 0.6500, min: 0.1667, max: 1.3241, stdev: 0.2094
New network won 86 and tied 148 out of 300 games (53.33% wins where ties are half wins)
Keeping the new network

Training iteration 228 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.71 seconds
Training examples lengths: [64732, 64855, 65035, 64623, 64660, 64455, 64800, 64978, 64826, 64668]
Total value: 447529.17
Training on 647632 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2439 (value: 0.0015, weighted value: 0.0758, policy: 0.1681, weighted policy: 0.1681), Train Mean Max: 0.9316
Epoch 2/10, Train Loss: 0.2344 (value: 0.0014, weighted value: 0.0696, policy: 0.1648, weighted policy: 0.1648), Train Mean Max: 0.9321
Epoch 3/10, Train Loss: 0.2287 (value: 0.0013, weighted value: 0.0662, policy: 0.1626, weighted policy: 0.1626), Train Mean Max: 0.9324
Epoch 4/10, Train Loss: 0.2249 (value: 0.0013, weighted value: 0.0636, policy: 0.1614, weighted policy: 0.1614), Train Mean Max: 0.9327
Epoch 5/10, Train Loss: 0.2214 (value: 0.0012, weighted value: 0.0618, policy: 0.1596, weighted policy: 0.1596), Train Mean Max: 0.9330
Epoch 6/10, Train Loss: 0.2175 (value: 0.0012, weighted value: 0.0583, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9331
Epoch 7/10, Train Loss: 0.2143 (value: 0.0011, weighted value: 0.0558, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9335
Epoch 8/10, Train Loss: 0.2137 (value: 0.0011, weighted value: 0.0556, policy: 0.1580, weighted policy: 0.1580), Train Mean Max: 0.9335
Epoch 9/10, Train Loss: 0.2112 (value: 0.0011, weighted value: 0.0538, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9338
Epoch 10/10, Train Loss: 0.2094 (value: 0.0011, weighted value: 0.0530, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9338
..training done in 65.22 seconds
..evaluation done in 18.46 seconds
Old network+MCTS average reward: 0.6865, min: 0.1389, max: 1.3056, stdev: 0.2184
New network+MCTS average reward: 0.6893, min: 0.1852, max: 1.3333, stdev: 0.2197
Old bare network average reward: 0.6384, min: 0.1204, max: 1.2130, stdev: 0.2201
New bare network average reward: 0.6411, min: 0.1019, max: 1.2593, stdev: 0.2202
External policy "random" average reward: 0.2465, min: -0.4259, max: 0.9537, stdev: 0.2139
External policy "individual greedy" average reward: 0.5312, min: 0.0093, max: 1.2037, stdev: 0.2133
External policy "total greedy" average reward: 0.6477, min: 0.1667, max: 1.3611, stdev: 0.2116
New network won 73 and tied 159 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 229 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.24 seconds
Training examples lengths: [64855, 65035, 64623, 64660, 64455, 64800, 64978, 64826, 64668, 64598]
Total value: 447635.30
Training on 647498 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2350 (value: 0.0014, weighted value: 0.0682, policy: 0.1667, weighted policy: 0.1667), Train Mean Max: 0.9327
Epoch 2/10, Train Loss: 0.2247 (value: 0.0013, weighted value: 0.0625, policy: 0.1622, weighted policy: 0.1622), Train Mean Max: 0.9330
Epoch 3/10, Train Loss: 0.2218 (value: 0.0012, weighted value: 0.0609, policy: 0.1609, weighted policy: 0.1609), Train Mean Max: 0.9334
Epoch 4/10, Train Loss: 0.2163 (value: 0.0011, weighted value: 0.0569, policy: 0.1594, weighted policy: 0.1594), Train Mean Max: 0.9336
Epoch 5/10, Train Loss: 0.2143 (value: 0.0011, weighted value: 0.0558, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9336
Epoch 6/10, Train Loss: 0.2097 (value: 0.0010, weighted value: 0.0525, policy: 0.1572, weighted policy: 0.1572), Train Mean Max: 0.9340
Epoch 7/10, Train Loss: 0.2085 (value: 0.0010, weighted value: 0.0522, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9342
Epoch 8/10, Train Loss: 0.2084 (value: 0.0010, weighted value: 0.0510, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9341
Epoch 9/10, Train Loss: 0.2053 (value: 0.0010, weighted value: 0.0492, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9345
Epoch 10/10, Train Loss: 0.2051 (value: 0.0010, weighted value: 0.0486, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9345
..training done in 59.86 seconds
..evaluation done in 18.37 seconds
Old network+MCTS average reward: 0.6906, min: 0.0833, max: 1.3333, stdev: 0.2301
New network+MCTS average reward: 0.6902, min: 0.1019, max: 1.4259, stdev: 0.2331
Old bare network average reward: 0.6388, min: 0.0741, max: 1.3333, stdev: 0.2382
New bare network average reward: 0.6424, min: 0.0741, max: 1.3333, stdev: 0.2376
External policy "random" average reward: 0.2591, min: -0.3796, max: 0.9352, stdev: 0.2203
External policy "individual greedy" average reward: 0.5516, min: -0.0093, max: 1.2130, stdev: 0.2260
External policy "total greedy" average reward: 0.6535, min: 0.0926, max: 1.3796, stdev: 0.2204
New network won 71 and tied 155 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 230 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.22 seconds
Training examples lengths: [65035, 64623, 64660, 64455, 64800, 64978, 64826, 64668, 64598, 64884]
Total value: 448463.86
Training on 647527 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2628 (value: 0.0017, weighted value: 0.0867, policy: 0.1761, weighted policy: 0.1761), Train Mean Max: 0.9312
Epoch 2/10, Train Loss: 0.2441 (value: 0.0015, weighted value: 0.0753, policy: 0.1688, weighted policy: 0.1688), Train Mean Max: 0.9319
Epoch 3/10, Train Loss: 0.2358 (value: 0.0014, weighted value: 0.0708, policy: 0.1650, weighted policy: 0.1650), Train Mean Max: 0.9322
Epoch 4/10, Train Loss: 0.2292 (value: 0.0013, weighted value: 0.0665, policy: 0.1627, weighted policy: 0.1627), Train Mean Max: 0.9325
Epoch 5/10, Train Loss: 0.2252 (value: 0.0013, weighted value: 0.0640, policy: 0.1612, weighted policy: 0.1612), Train Mean Max: 0.9327
Epoch 6/10, Train Loss: 0.2219 (value: 0.0012, weighted value: 0.0615, policy: 0.1604, weighted policy: 0.1604), Train Mean Max: 0.9330
Epoch 7/10, Train Loss: 0.2193 (value: 0.0012, weighted value: 0.0605, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9331
Epoch 8/10, Train Loss: 0.2153 (value: 0.0011, weighted value: 0.0563, policy: 0.1590, weighted policy: 0.1590), Train Mean Max: 0.9334
Epoch 9/10, Train Loss: 0.2131 (value: 0.0011, weighted value: 0.0544, policy: 0.1587, weighted policy: 0.1587), Train Mean Max: 0.9335
Epoch 10/10, Train Loss: 0.2121 (value: 0.0011, weighted value: 0.0543, policy: 0.1577, weighted policy: 0.1577), Train Mean Max: 0.9337
..training done in 67.35 seconds
..evaluation done in 18.88 seconds
Old network+MCTS average reward: 0.6695, min: 0.0463, max: 1.3796, stdev: 0.2129
New network+MCTS average reward: 0.6721, min: 0.0741, max: 1.3981, stdev: 0.2171
Old bare network average reward: 0.6286, min: -0.0833, max: 1.4074, stdev: 0.2307
New bare network average reward: 0.6235, min: -0.1389, max: 1.3981, stdev: 0.2300
External policy "random" average reward: 0.2540, min: -0.3426, max: 0.9722, stdev: 0.2228
External policy "individual greedy" average reward: 0.5397, min: -0.0833, max: 1.3333, stdev: 0.2155
External policy "total greedy" average reward: 0.6348, min: 0.0463, max: 1.4074, stdev: 0.2075
New network won 93 and tied 135 out of 300 games (53.50% wins where ties are half wins)
Keeping the new network

Training iteration 231 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.72 seconds
Training examples lengths: [64623, 64660, 64455, 64800, 64978, 64826, 64668, 64598, 64884, 64756]
Total value: 448370.41
Training on 647248 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2378 (value: 0.0014, weighted value: 0.0696, policy: 0.1683, weighted policy: 0.1683), Train Mean Max: 0.9321
Epoch 2/10, Train Loss: 0.2298 (value: 0.0013, weighted value: 0.0646, policy: 0.1653, weighted policy: 0.1653), Train Mean Max: 0.9325
Epoch 3/10, Train Loss: 0.2236 (value: 0.0012, weighted value: 0.0611, policy: 0.1624, weighted policy: 0.1624), Train Mean Max: 0.9329
Epoch 4/10, Train Loss: 0.2221 (value: 0.0012, weighted value: 0.0599, policy: 0.1622, weighted policy: 0.1622), Train Mean Max: 0.9329
Epoch 5/10, Train Loss: 0.2170 (value: 0.0011, weighted value: 0.0568, policy: 0.1602, weighted policy: 0.1602), Train Mean Max: 0.9332
Epoch 6/10, Train Loss: 0.2126 (value: 0.0011, weighted value: 0.0534, policy: 0.1592, weighted policy: 0.1592), Train Mean Max: 0.9335
Epoch 7/10, Train Loss: 0.2177 (value: 0.0011, weighted value: 0.0550, policy: 0.1627, weighted policy: 0.1627), Train Mean Max: 0.9333
Epoch 8/10, Train Loss: 0.2106 (value: 0.0010, weighted value: 0.0513, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9337
Epoch 9/10, Train Loss: 0.2078 (value: 0.0010, weighted value: 0.0504, policy: 0.1573, weighted policy: 0.1573), Train Mean Max: 0.9340
Epoch 10/10, Train Loss: 0.2051 (value: 0.0010, weighted value: 0.0480, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9341
..training done in 63.94 seconds
..evaluation done in 18.76 seconds
Old network+MCTS average reward: 0.7210, min: 0.1481, max: 1.3889, stdev: 0.2372
New network+MCTS average reward: 0.7215, min: 0.1481, max: 1.4074, stdev: 0.2400
Old bare network average reward: 0.6678, min: 0.0556, max: 1.3704, stdev: 0.2413
New bare network average reward: 0.6710, min: 0.0556, max: 1.3889, stdev: 0.2432
External policy "random" average reward: 0.2796, min: -0.2685, max: 1.1019, stdev: 0.2298
External policy "individual greedy" average reward: 0.5619, min: -0.0093, max: 1.4630, stdev: 0.2315
External policy "total greedy" average reward: 0.6815, min: 0.1481, max: 1.5000, stdev: 0.2288
New network won 78 and tied 145 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 232 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.70 seconds
Training examples lengths: [64660, 64455, 64800, 64978, 64826, 64668, 64598, 64884, 64756, 64865]
Total value: 448816.76
Training on 647490 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2333 (value: 0.0013, weighted value: 0.0663, policy: 0.1670, weighted policy: 0.1670), Train Mean Max: 0.9325
Epoch 2/10, Train Loss: 0.2230 (value: 0.0012, weighted value: 0.0595, policy: 0.1635, weighted policy: 0.1635), Train Mean Max: 0.9331
Epoch 3/10, Train Loss: 0.2174 (value: 0.0011, weighted value: 0.0571, policy: 0.1603, weighted policy: 0.1603), Train Mean Max: 0.9334
Epoch 4/10, Train Loss: 0.2140 (value: 0.0011, weighted value: 0.0547, policy: 0.1592, weighted policy: 0.1592), Train Mean Max: 0.9334
Epoch 5/10, Train Loss: 0.2123 (value: 0.0011, weighted value: 0.0532, policy: 0.1591, weighted policy: 0.1591), Train Mean Max: 0.9335
Epoch 6/10, Train Loss: 0.2092 (value: 0.0010, weighted value: 0.0510, policy: 0.1581, weighted policy: 0.1581), Train Mean Max: 0.9339
Epoch 7/10, Train Loss: 0.2068 (value: 0.0010, weighted value: 0.0500, policy: 0.1569, weighted policy: 0.1569), Train Mean Max: 0.9340
Epoch 8/10, Train Loss: 0.2047 (value: 0.0010, weighted value: 0.0484, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9342
Epoch 9/10, Train Loss: 0.2044 (value: 0.0010, weighted value: 0.0482, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9343
Epoch 10/10, Train Loss: 0.2030 (value: 0.0009, weighted value: 0.0464, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9345
..training done in 59.67 seconds
..evaluation done in 18.50 seconds
Old network+MCTS average reward: 0.7115, min: -0.0833, max: 1.3611, stdev: 0.2230
New network+MCTS average reward: 0.7099, min: -0.0741, max: 1.3611, stdev: 0.2269
Old bare network average reward: 0.6599, min: -0.0741, max: 1.3611, stdev: 0.2277
New bare network average reward: 0.6593, min: -0.0926, max: 1.3056, stdev: 0.2330
External policy "random" average reward: 0.2725, min: -0.3148, max: 0.9167, stdev: 0.2235
External policy "individual greedy" average reward: 0.5656, min: -0.0741, max: 1.2870, stdev: 0.2265
External policy "total greedy" average reward: 0.6743, min: 0.0093, max: 1.2963, stdev: 0.2209
New network won 82 and tied 142 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 233 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 63.13 seconds
Training examples lengths: [64455, 64800, 64978, 64826, 64668, 64598, 64884, 64756, 64865, 65150]
Total value: 448855.02
Training on 647980 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2312 (value: 0.0013, weighted value: 0.0654, policy: 0.1658, weighted policy: 0.1658), Train Mean Max: 0.9330
Epoch 2/10, Train Loss: 0.2195 (value: 0.0012, weighted value: 0.0581, policy: 0.1614, weighted policy: 0.1614), Train Mean Max: 0.9336
Epoch 3/10, Train Loss: 0.2163 (value: 0.0011, weighted value: 0.0567, policy: 0.1597, weighted policy: 0.1597), Train Mean Max: 0.9337
Epoch 4/10, Train Loss: 0.2122 (value: 0.0011, weighted value: 0.0538, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9339
Epoch 5/10, Train Loss: 0.2086 (value: 0.0010, weighted value: 0.0511, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9341
Epoch 6/10, Train Loss: 0.2066 (value: 0.0010, weighted value: 0.0506, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9344
Epoch 7/10, Train Loss: 0.2041 (value: 0.0010, weighted value: 0.0486, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9345
Epoch 8/10, Train Loss: 0.2034 (value: 0.0010, weighted value: 0.0483, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9346
Epoch 9/10, Train Loss: 0.2023 (value: 0.0009, weighted value: 0.0471, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9348
Epoch 10/10, Train Loss: 0.2018 (value: 0.0009, weighted value: 0.0458, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9349
..training done in 70.01 seconds
..evaluation done in 18.76 seconds
Old network+MCTS average reward: 0.6818, min: -0.0278, max: 1.4815, stdev: 0.2436
New network+MCTS average reward: 0.6811, min: -0.0463, max: 1.4815, stdev: 0.2424
Old bare network average reward: 0.6303, min: -0.0463, max: 1.4815, stdev: 0.2515
New bare network average reward: 0.6341, min: -0.0463, max: 1.4815, stdev: 0.2493
External policy "random" average reward: 0.2665, min: -0.3981, max: 1.0093, stdev: 0.2247
External policy "individual greedy" average reward: 0.5421, min: -0.1667, max: 1.3611, stdev: 0.2445
External policy "total greedy" average reward: 0.6593, min: -0.0648, max: 1.5093, stdev: 0.2250
New network won 76 and tied 142 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 234 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.30 seconds
Training examples lengths: [64800, 64978, 64826, 64668, 64598, 64884, 64756, 64865, 65150, 64868]
Total value: 449112.73
Training on 648393 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2551 (value: 0.0016, weighted value: 0.0814, policy: 0.1736, weighted policy: 0.1736), Train Mean Max: 0.9318
Epoch 2/10, Train Loss: 0.2382 (value: 0.0014, weighted value: 0.0713, policy: 0.1669, weighted policy: 0.1669), Train Mean Max: 0.9325
Epoch 3/10, Train Loss: 0.2303 (value: 0.0013, weighted value: 0.0669, policy: 0.1634, weighted policy: 0.1634), Train Mean Max: 0.9328
Epoch 4/10, Train Loss: 0.2253 (value: 0.0013, weighted value: 0.0641, policy: 0.1612, weighted policy: 0.1612), Train Mean Max: 0.9331
Epoch 5/10, Train Loss: 0.2210 (value: 0.0012, weighted value: 0.0612, policy: 0.1598, weighted policy: 0.1598), Train Mean Max: 0.9334
Epoch 6/10, Train Loss: 0.2177 (value: 0.0012, weighted value: 0.0582, policy: 0.1595, weighted policy: 0.1595), Train Mean Max: 0.9334
Epoch 7/10, Train Loss: 0.2135 (value: 0.0011, weighted value: 0.0552, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9338
Epoch 8/10, Train Loss: 0.2118 (value: 0.0011, weighted value: 0.0541, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9340
Epoch 9/10, Train Loss: 0.2081 (value: 0.0010, weighted value: 0.0519, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9343
Epoch 10/10, Train Loss: 0.2079 (value: 0.0010, weighted value: 0.0514, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9343
..training done in 63.83 seconds
..evaluation done in 19.27 seconds
Old network+MCTS average reward: 0.7062, min: 0.2130, max: 1.5093, stdev: 0.2145
New network+MCTS average reward: 0.7066, min: 0.2130, max: 1.5648, stdev: 0.2141
Old bare network average reward: 0.6604, min: 0.0556, max: 1.4259, stdev: 0.2172
New bare network average reward: 0.6628, min: 0.1389, max: 1.4259, stdev: 0.2173
External policy "random" average reward: 0.2827, min: -0.3426, max: 1.0648, stdev: 0.2279
External policy "individual greedy" average reward: 0.5621, min: 0.0370, max: 1.2315, stdev: 0.2270
External policy "total greedy" average reward: 0.6670, min: 0.1852, max: 1.5648, stdev: 0.2245
New network won 67 and tied 171 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 235 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.70 seconds
Training examples lengths: [64978, 64826, 64668, 64598, 64884, 64756, 64865, 65150, 64868, 64872]
Total value: 449308.42
Training on 648465 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2320 (value: 0.0013, weighted value: 0.0674, policy: 0.1645, weighted policy: 0.1645), Train Mean Max: 0.9329
Epoch 2/10, Train Loss: 0.2231 (value: 0.0012, weighted value: 0.0613, policy: 0.1617, weighted policy: 0.1617), Train Mean Max: 0.9332
Epoch 3/10, Train Loss: 0.2159 (value: 0.0011, weighted value: 0.0571, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9337
Epoch 4/10, Train Loss: 0.2144 (value: 0.0011, weighted value: 0.0562, policy: 0.1582, weighted policy: 0.1582), Train Mean Max: 0.9340
Epoch 5/10, Train Loss: 0.2115 (value: 0.0011, weighted value: 0.0538, policy: 0.1577, weighted policy: 0.1577), Train Mean Max: 0.9340
Epoch 6/10, Train Loss: 0.2093 (value: 0.0011, weighted value: 0.0527, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9343
Epoch 7/10, Train Loss: 0.2062 (value: 0.0010, weighted value: 0.0507, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9345
Epoch 8/10, Train Loss: 0.2053 (value: 0.0010, weighted value: 0.0502, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9347
Epoch 9/10, Train Loss: 0.2047 (value: 0.0010, weighted value: 0.0487, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9346
Epoch 10/10, Train Loss: 0.2012 (value: 0.0009, weighted value: 0.0468, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9350
..training done in 64.15 seconds
..evaluation done in 18.45 seconds
Old network+MCTS average reward: 0.6660, min: 0.1204, max: 1.5093, stdev: 0.2254
New network+MCTS average reward: 0.6674, min: 0.1204, max: 1.4630, stdev: 0.2238
Old bare network average reward: 0.6167, min: 0.0463, max: 1.4630, stdev: 0.2304
New bare network average reward: 0.6183, min: 0.0370, max: 1.4630, stdev: 0.2343
External policy "random" average reward: 0.2544, min: -0.3426, max: 1.0556, stdev: 0.2340
External policy "individual greedy" average reward: 0.5145, min: -0.0278, max: 1.2685, stdev: 0.2404
External policy "total greedy" average reward: 0.6280, min: 0.0648, max: 1.3981, stdev: 0.2283
New network won 77 and tied 160 out of 300 games (52.33% wins where ties are half wins)
Keeping the new network

Training iteration 236 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.10 seconds
Training examples lengths: [64826, 64668, 64598, 64884, 64756, 64865, 65150, 64868, 64872, 64748]
Total value: 449573.34
Training on 648235 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2278 (value: 0.0013, weighted value: 0.0643, policy: 0.1636, weighted policy: 0.1636), Train Mean Max: 0.9336
Epoch 2/10, Train Loss: 0.2191 (value: 0.0012, weighted value: 0.0588, policy: 0.1602, weighted policy: 0.1602), Train Mean Max: 0.9340
Epoch 3/10, Train Loss: 0.2121 (value: 0.0011, weighted value: 0.0544, policy: 0.1577, weighted policy: 0.1577), Train Mean Max: 0.9345
Epoch 4/10, Train Loss: 0.2106 (value: 0.0011, weighted value: 0.0541, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9348
Epoch 5/10, Train Loss: 0.2077 (value: 0.0010, weighted value: 0.0516, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9348
Epoch 6/10, Train Loss: 0.2057 (value: 0.0010, weighted value: 0.0499, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9349
Epoch 7/10, Train Loss: 0.2020 (value: 0.0010, weighted value: 0.0478, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9353
Epoch 8/10, Train Loss: 0.2026 (value: 0.0010, weighted value: 0.0480, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9352
Epoch 9/10, Train Loss: 0.2005 (value: 0.0009, weighted value: 0.0462, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9355
Epoch 10/10, Train Loss: 0.1992 (value: 0.0009, weighted value: 0.0461, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9357
..training done in 68.18 seconds
..evaluation done in 19.55 seconds
Old network+MCTS average reward: 0.6570, min: -0.0093, max: 1.5648, stdev: 0.2290
New network+MCTS average reward: 0.6575, min: 0.0278, max: 1.5648, stdev: 0.2288
Old bare network average reward: 0.6098, min: 0.0463, max: 1.5370, stdev: 0.2328
New bare network average reward: 0.6079, min: -0.0833, max: 1.5000, stdev: 0.2323
External policy "random" average reward: 0.2465, min: -0.3519, max: 1.1481, stdev: 0.2281
External policy "individual greedy" average reward: 0.5215, min: -0.0093, max: 1.3333, stdev: 0.2207
External policy "total greedy" average reward: 0.6365, min: 0.1296, max: 1.4074, stdev: 0.2155
New network won 76 and tied 144 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 237 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.29 seconds
Training examples lengths: [64668, 64598, 64884, 64756, 64865, 65150, 64868, 64872, 64748, 64716]
Total value: 449707.45
Training on 648125 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2522 (value: 0.0016, weighted value: 0.0797, policy: 0.1725, weighted policy: 0.1725), Train Mean Max: 0.9321
Epoch 2/10, Train Loss: 0.2370 (value: 0.0014, weighted value: 0.0703, policy: 0.1667, weighted policy: 0.1667), Train Mean Max: 0.9330
Epoch 3/10, Train Loss: 0.2299 (value: 0.0014, weighted value: 0.0677, policy: 0.1622, weighted policy: 0.1622), Train Mean Max: 0.9334
Epoch 4/10, Train Loss: 0.2227 (value: 0.0013, weighted value: 0.0627, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9335
Epoch 5/10, Train Loss: 0.2168 (value: 0.0012, weighted value: 0.0591, policy: 0.1577, weighted policy: 0.1577), Train Mean Max: 0.9339
Epoch 6/10, Train Loss: 0.2144 (value: 0.0012, weighted value: 0.0581, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9341
Epoch 7/10, Train Loss: 0.2109 (value: 0.0011, weighted value: 0.0546, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9343
Epoch 8/10, Train Loss: 0.2105 (value: 0.0011, weighted value: 0.0541, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9343
Epoch 9/10, Train Loss: 0.2073 (value: 0.0010, weighted value: 0.0524, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9347
Epoch 10/10, Train Loss: 0.2060 (value: 0.0010, weighted value: 0.0509, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9348
..training done in 65.85 seconds
..evaluation done in 20.61 seconds
Old network+MCTS average reward: 0.7107, min: 0.0741, max: 1.5556, stdev: 0.2454
New network+MCTS average reward: 0.7082, min: 0.1111, max: 1.5556, stdev: 0.2473
Old bare network average reward: 0.6577, min: 0.0741, max: 1.5556, stdev: 0.2523
New bare network average reward: 0.6587, min: 0.0278, max: 1.4537, stdev: 0.2526
External policy "random" average reward: 0.2869, min: -0.2500, max: 1.0556, stdev: 0.2177
External policy "individual greedy" average reward: 0.5772, min: 0.0463, max: 1.5093, stdev: 0.2424
External policy "total greedy" average reward: 0.6779, min: 0.0000, max: 1.4444, stdev: 0.2368
New network won 63 and tied 163 out of 300 games (48.17% wins where ties are half wins)
Reverting to the old network

Training iteration 238 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.87 seconds
Training examples lengths: [64598, 64884, 64756, 64865, 65150, 64868, 64872, 64748, 64716, 64934]
Total value: 451036.39
Training on 648391 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2758 (value: 0.0019, weighted value: 0.0956, policy: 0.1802, weighted policy: 0.1802), Train Mean Max: 0.9310
Epoch 2/10, Train Loss: 0.2539 (value: 0.0017, weighted value: 0.0835, policy: 0.1704, weighted policy: 0.1704), Train Mean Max: 0.9319
Epoch 3/10, Train Loss: 0.2418 (value: 0.0015, weighted value: 0.0772, policy: 0.1646, weighted policy: 0.1646), Train Mean Max: 0.9324
Epoch 4/10, Train Loss: 0.2319 (value: 0.0014, weighted value: 0.0705, policy: 0.1614, weighted policy: 0.1614), Train Mean Max: 0.9328
Epoch 5/10, Train Loss: 0.2270 (value: 0.0013, weighted value: 0.0670, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9331
Epoch 6/10, Train Loss: 0.2244 (value: 0.0013, weighted value: 0.0638, policy: 0.1606, weighted policy: 0.1606), Train Mean Max: 0.9332
Epoch 7/10, Train Loss: 0.2207 (value: 0.0012, weighted value: 0.0623, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9335
Epoch 8/10, Train Loss: 0.2163 (value: 0.0012, weighted value: 0.0586, policy: 0.1577, weighted policy: 0.1577), Train Mean Max: 0.9337
Epoch 9/10, Train Loss: 0.2136 (value: 0.0011, weighted value: 0.0568, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9340
Epoch 10/10, Train Loss: 0.2116 (value: 0.0011, weighted value: 0.0554, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9341
..training done in 66.44 seconds
..evaluation done in 18.52 seconds
Old network+MCTS average reward: 0.7030, min: -0.0093, max: 1.4444, stdev: 0.2289
New network+MCTS average reward: 0.7029, min: 0.0093, max: 1.4444, stdev: 0.2286
Old bare network average reward: 0.6530, min: 0.0000, max: 1.4444, stdev: 0.2343
New bare network average reward: 0.6537, min: -0.0093, max: 1.4444, stdev: 0.2377
External policy "random" average reward: 0.2617, min: -0.2778, max: 0.9074, stdev: 0.2341
External policy "individual greedy" average reward: 0.5622, min: 0.0370, max: 1.2407, stdev: 0.2175
External policy "total greedy" average reward: 0.6736, min: 0.0000, max: 1.4259, stdev: 0.2223
New network won 65 and tied 161 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 239 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.20 seconds
Training examples lengths: [64884, 64756, 64865, 65150, 64868, 64872, 64748, 64716, 64934, 64911]
Total value: 452092.03
Training on 648704 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2991 (value: 0.0022, weighted value: 0.1123, policy: 0.1868, weighted policy: 0.1868), Train Mean Max: 0.9298
Epoch 2/10, Train Loss: 0.2694 (value: 0.0019, weighted value: 0.0958, policy: 0.1737, weighted policy: 0.1737), Train Mean Max: 0.9309
Epoch 3/10, Train Loss: 0.2541 (value: 0.0017, weighted value: 0.0854, policy: 0.1686, weighted policy: 0.1686), Train Mean Max: 0.9314
Epoch 4/10, Train Loss: 0.2468 (value: 0.0016, weighted value: 0.0820, policy: 0.1649, weighted policy: 0.1649), Train Mean Max: 0.9318
Epoch 5/10, Train Loss: 0.2363 (value: 0.0015, weighted value: 0.0739, policy: 0.1624, weighted policy: 0.1624), Train Mean Max: 0.9322
Epoch 6/10, Train Loss: 0.2339 (value: 0.0014, weighted value: 0.0723, policy: 0.1616, weighted policy: 0.1616), Train Mean Max: 0.9323
Epoch 7/10, Train Loss: 0.2265 (value: 0.0013, weighted value: 0.0663, policy: 0.1601, weighted policy: 0.1601), Train Mean Max: 0.9325
Epoch 8/10, Train Loss: 0.2249 (value: 0.0013, weighted value: 0.0657, policy: 0.1592, weighted policy: 0.1592), Train Mean Max: 0.9328
Epoch 9/10, Train Loss: 0.2211 (value: 0.0012, weighted value: 0.0620, policy: 0.1592, weighted policy: 0.1592), Train Mean Max: 0.9330
Epoch 10/10, Train Loss: 0.2175 (value: 0.0012, weighted value: 0.0601, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9334
..training done in 67.84 seconds
..evaluation done in 18.51 seconds
Old network+MCTS average reward: 0.6794, min: 0.1296, max: 1.3426, stdev: 0.2256
New network+MCTS average reward: 0.6773, min: 0.1111, max: 1.3426, stdev: 0.2221
Old bare network average reward: 0.6368, min: 0.0648, max: 1.3426, stdev: 0.2342
New bare network average reward: 0.6375, min: 0.0741, max: 1.3426, stdev: 0.2336
External policy "random" average reward: 0.2556, min: -0.3796, max: 0.8333, stdev: 0.2223
External policy "individual greedy" average reward: 0.5398, min: -0.0185, max: 1.1481, stdev: 0.2203
External policy "total greedy" average reward: 0.6509, min: 0.1574, max: 1.2963, stdev: 0.2127
New network won 75 and tied 141 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 240 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.38 seconds
Training examples lengths: [64756, 64865, 65150, 64868, 64872, 64748, 64716, 64934, 64911, 64743]
Total value: 451872.45
Training on 648563 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3218 (value: 0.0026, weighted value: 0.1278, policy: 0.1940, weighted policy: 0.1940), Train Mean Max: 0.9286
Epoch 2/10, Train Loss: 0.2849 (value: 0.0021, weighted value: 0.1062, policy: 0.1787, weighted policy: 0.1787), Train Mean Max: 0.9301
Epoch 3/10, Train Loss: 0.2665 (value: 0.0019, weighted value: 0.0953, policy: 0.1712, weighted policy: 0.1712), Train Mean Max: 0.9305
Epoch 4/10, Train Loss: 0.2564 (value: 0.0018, weighted value: 0.0884, policy: 0.1680, weighted policy: 0.1680), Train Mean Max: 0.9308
Epoch 5/10, Train Loss: 0.2466 (value: 0.0016, weighted value: 0.0817, policy: 0.1649, weighted policy: 0.1649), Train Mean Max: 0.9313
Epoch 6/10, Train Loss: 0.2417 (value: 0.0016, weighted value: 0.0787, policy: 0.1631, weighted policy: 0.1631), Train Mean Max: 0.9315
Epoch 7/10, Train Loss: 0.2364 (value: 0.0015, weighted value: 0.0745, policy: 0.1619, weighted policy: 0.1619), Train Mean Max: 0.9317
Epoch 8/10, Train Loss: 0.2313 (value: 0.0014, weighted value: 0.0698, policy: 0.1615, weighted policy: 0.1615), Train Mean Max: 0.9319
Epoch 9/10, Train Loss: 0.2271 (value: 0.0013, weighted value: 0.0666, policy: 0.1605, weighted policy: 0.1605), Train Mean Max: 0.9322
Epoch 10/10, Train Loss: 0.2246 (value: 0.0013, weighted value: 0.0647, policy: 0.1599, weighted policy: 0.1599), Train Mean Max: 0.9324
..training done in 59.42 seconds
..evaluation done in 17.66 seconds
Old network+MCTS average reward: 0.6793, min: 0.2222, max: 1.2593, stdev: 0.2007
New network+MCTS average reward: 0.6805, min: 0.1944, max: 1.2407, stdev: 0.2019
Old bare network average reward: 0.6320, min: 0.0926, max: 1.2407, stdev: 0.2082
New bare network average reward: 0.6283, min: 0.0463, max: 1.1574, stdev: 0.2069
External policy "random" average reward: 0.2687, min: -0.2500, max: 1.0278, stdev: 0.2087
External policy "individual greedy" average reward: 0.5423, min: 0.0370, max: 1.1667, stdev: 0.2073
External policy "total greedy" average reward: 0.6496, min: 0.0833, max: 1.1667, stdev: 0.1967
New network won 77 and tied 151 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_240

Training iteration 241 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.86 seconds
Training examples lengths: [64865, 65150, 64868, 64872, 64748, 64716, 64934, 64911, 64743, 64329]
Total value: 451124.50
Training on 648136 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2442 (value: 0.0016, weighted value: 0.0782, policy: 0.1659, weighted policy: 0.1659), Train Mean Max: 0.9315
Epoch 2/10, Train Loss: 0.2347 (value: 0.0014, weighted value: 0.0717, policy: 0.1630, weighted policy: 0.1630), Train Mean Max: 0.9321
Epoch 3/10, Train Loss: 0.2281 (value: 0.0013, weighted value: 0.0670, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9324
Epoch 4/10, Train Loss: 0.2243 (value: 0.0013, weighted value: 0.0647, policy: 0.1596, weighted policy: 0.1596), Train Mean Max: 0.9327
Epoch 5/10, Train Loss: 0.2202 (value: 0.0013, weighted value: 0.0626, policy: 0.1577, weighted policy: 0.1577), Train Mean Max: 0.9330
Epoch 6/10, Train Loss: 0.2187 (value: 0.0012, weighted value: 0.0605, policy: 0.1582, weighted policy: 0.1582), Train Mean Max: 0.9332
Epoch 7/10, Train Loss: 0.2136 (value: 0.0011, weighted value: 0.0571, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9335
Epoch 8/10, Train Loss: 0.2138 (value: 0.0012, weighted value: 0.0575, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9335
Epoch 9/10, Train Loss: 0.2105 (value: 0.0011, weighted value: 0.0543, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9339
Epoch 10/10, Train Loss: 0.2098 (value: 0.0011, weighted value: 0.0545, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9341
..training done in 62.32 seconds
..evaluation done in 18.95 seconds
Old network+MCTS average reward: 0.6765, min: 0.1204, max: 1.3519, stdev: 0.2099
New network+MCTS average reward: 0.6773, min: 0.1204, max: 1.3056, stdev: 0.2103
Old bare network average reward: 0.6300, min: -0.0556, max: 1.2315, stdev: 0.2106
New bare network average reward: 0.6265, min: -0.0370, max: 1.2315, stdev: 0.2119
External policy "random" average reward: 0.2610, min: -0.2870, max: 0.9259, stdev: 0.2149
External policy "individual greedy" average reward: 0.5368, min: -0.0556, max: 1.2963, stdev: 0.2093
External policy "total greedy" average reward: 0.6482, min: 0.1204, max: 1.3333, stdev: 0.2032
New network won 78 and tied 147 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 242 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.25 seconds
Training examples lengths: [65150, 64868, 64872, 64748, 64716, 64934, 64911, 64743, 64329, 64550]
Total value: 450928.06
Training on 647821 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2319 (value: 0.0013, weighted value: 0.0666, policy: 0.1654, weighted policy: 0.1654), Train Mean Max: 0.9324
Epoch 2/10, Train Loss: 0.2250 (value: 0.0013, weighted value: 0.0635, policy: 0.1614, weighted policy: 0.1614), Train Mean Max: 0.9332
Epoch 3/10, Train Loss: 0.2192 (value: 0.0012, weighted value: 0.0596, policy: 0.1595, weighted policy: 0.1595), Train Mean Max: 0.9334
Epoch 4/10, Train Loss: 0.2149 (value: 0.0011, weighted value: 0.0572, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9337
Epoch 5/10, Train Loss: 0.2109 (value: 0.0011, weighted value: 0.0547, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9339
Epoch 6/10, Train Loss: 0.2109 (value: 0.0011, weighted value: 0.0544, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9339
Epoch 7/10, Train Loss: 0.2075 (value: 0.0010, weighted value: 0.0515, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9343
Epoch 8/10, Train Loss: 0.2059 (value: 0.0010, weighted value: 0.0506, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9346
Epoch 9/10, Train Loss: 0.2042 (value: 0.0010, weighted value: 0.0500, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9347
Epoch 10/10, Train Loss: 0.2030 (value: 0.0010, weighted value: 0.0485, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9347
..training done in 65.26 seconds
..evaluation done in 18.82 seconds
Old network+MCTS average reward: 0.6745, min: 0.0556, max: 1.4444, stdev: 0.2301
New network+MCTS average reward: 0.6727, min: -0.0370, max: 1.4537, stdev: 0.2333
Old bare network average reward: 0.6255, min: -0.0463, max: 1.4537, stdev: 0.2465
New bare network average reward: 0.6266, min: -0.0463, max: 1.4537, stdev: 0.2431
External policy "random" average reward: 0.2598, min: -0.3148, max: 0.9907, stdev: 0.2331
External policy "individual greedy" average reward: 0.5300, min: -0.0926, max: 1.3148, stdev: 0.2272
External policy "total greedy" average reward: 0.6450, min: -0.1296, max: 1.5185, stdev: 0.2305
New network won 68 and tied 148 out of 300 games (47.33% wins where ties are half wins)
Reverting to the old network

Training iteration 243 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.04 seconds
Training examples lengths: [64868, 64872, 64748, 64716, 64934, 64911, 64743, 64329, 64550, 64887]
Total value: 451580.44
Training on 647558 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2568 (value: 0.0016, weighted value: 0.0823, policy: 0.1744, weighted policy: 0.1744), Train Mean Max: 0.9313
Epoch 2/10, Train Loss: 0.2428 (value: 0.0015, weighted value: 0.0744, policy: 0.1684, weighted policy: 0.1684), Train Mean Max: 0.9319
Epoch 3/10, Train Loss: 0.2322 (value: 0.0014, weighted value: 0.0691, policy: 0.1631, weighted policy: 0.1631), Train Mean Max: 0.9325
Epoch 4/10, Train Loss: 0.2265 (value: 0.0013, weighted value: 0.0655, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9328
Epoch 5/10, Train Loss: 0.2218 (value: 0.0013, weighted value: 0.0628, policy: 0.1590, weighted policy: 0.1590), Train Mean Max: 0.9331
Epoch 6/10, Train Loss: 0.2178 (value: 0.0012, weighted value: 0.0601, policy: 0.1577, weighted policy: 0.1577), Train Mean Max: 0.9334
Epoch 7/10, Train Loss: 0.2143 (value: 0.0011, weighted value: 0.0570, policy: 0.1573, weighted policy: 0.1573), Train Mean Max: 0.9335
Epoch 8/10, Train Loss: 0.2163 (value: 0.0012, weighted value: 0.0591, policy: 0.1572, weighted policy: 0.1572), Train Mean Max: 0.9336
Epoch 9/10, Train Loss: 0.2089 (value: 0.0011, weighted value: 0.0526, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9339
Epoch 10/10, Train Loss: 0.2096 (value: 0.0011, weighted value: 0.0541, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9341
..training done in 66.82 seconds
..evaluation done in 19.27 seconds
Old network+MCTS average reward: 0.7178, min: 0.0556, max: 1.6574, stdev: 0.2364
New network+MCTS average reward: 0.7155, min: 0.0556, max: 1.5556, stdev: 0.2435
Old bare network average reward: 0.6673, min: 0.0556, max: 1.5926, stdev: 0.2405
New bare network average reward: 0.6676, min: 0.0556, max: 1.4167, stdev: 0.2382
External policy "random" average reward: 0.2818, min: -0.2315, max: 1.1296, stdev: 0.2294
External policy "individual greedy" average reward: 0.5626, min: -0.0278, max: 1.3519, stdev: 0.2337
External policy "total greedy" average reward: 0.6846, min: 0.1852, max: 1.5278, stdev: 0.2256
New network won 67 and tied 147 out of 300 games (46.83% wins where ties are half wins)
Reverting to the old network

Training iteration 244 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.08 seconds
Training examples lengths: [64872, 64748, 64716, 64934, 64911, 64743, 64329, 64550, 64887, 64975]
Total value: 451445.30
Training on 647665 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2813 (value: 0.0020, weighted value: 0.0981, policy: 0.1832, weighted policy: 0.1832), Train Mean Max: 0.9299
Epoch 2/10, Train Loss: 0.2580 (value: 0.0017, weighted value: 0.0851, policy: 0.1729, weighted policy: 0.1729), Train Mean Max: 0.9309
Epoch 3/10, Train Loss: 0.2466 (value: 0.0016, weighted value: 0.0792, policy: 0.1675, weighted policy: 0.1675), Train Mean Max: 0.9314
Epoch 4/10, Train Loss: 0.2380 (value: 0.0015, weighted value: 0.0743, policy: 0.1638, weighted policy: 0.1638), Train Mean Max: 0.9317
Epoch 5/10, Train Loss: 0.2321 (value: 0.0014, weighted value: 0.0693, policy: 0.1628, weighted policy: 0.1628), Train Mean Max: 0.9319
Epoch 6/10, Train Loss: 0.2300 (value: 0.0014, weighted value: 0.0691, policy: 0.1609, weighted policy: 0.1609), Train Mean Max: 0.9322
Epoch 7/10, Train Loss: 0.2227 (value: 0.0012, weighted value: 0.0621, policy: 0.1605, weighted policy: 0.1605), Train Mean Max: 0.9324
Epoch 8/10, Train Loss: 0.2200 (value: 0.0012, weighted value: 0.0612, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9326
Epoch 9/10, Train Loss: 0.2168 (value: 0.0012, weighted value: 0.0591, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9330
Epoch 10/10, Train Loss: 0.2151 (value: 0.0011, weighted value: 0.0571, policy: 0.1580, weighted policy: 0.1580), Train Mean Max: 0.9332
..training done in 61.30 seconds
..evaluation done in 19.20 seconds
Old network+MCTS average reward: 0.6862, min: 0.1204, max: 1.3704, stdev: 0.2392
New network+MCTS average reward: 0.6910, min: 0.1204, max: 1.3889, stdev: 0.2428
Old bare network average reward: 0.6389, min: 0.0185, max: 1.3148, stdev: 0.2500
New bare network average reward: 0.6444, min: 0.0185, max: 1.3704, stdev: 0.2484
External policy "random" average reward: 0.2449, min: -0.2963, max: 0.8519, stdev: 0.2267
External policy "individual greedy" average reward: 0.5318, min: -0.1296, max: 1.3889, stdev: 0.2321
External policy "total greedy" average reward: 0.6481, min: 0.0000, max: 1.4444, stdev: 0.2327
New network won 92 and tied 140 out of 300 games (54.00% wins where ties are half wins)
Keeping the new network

Training iteration 245 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.79 seconds
Training examples lengths: [64748, 64716, 64934, 64911, 64743, 64329, 64550, 64887, 64975, 65129]
Total value: 452046.78
Training on 647922 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2387 (value: 0.0014, weighted value: 0.0715, policy: 0.1672, weighted policy: 0.1672), Train Mean Max: 0.9320
Epoch 2/10, Train Loss: 0.2314 (value: 0.0014, weighted value: 0.0685, policy: 0.1629, weighted policy: 0.1629), Train Mean Max: 0.9325
Epoch 3/10, Train Loss: 0.2210 (value: 0.0012, weighted value: 0.0618, policy: 0.1592, weighted policy: 0.1592), Train Mean Max: 0.9329
Epoch 4/10, Train Loss: 0.2180 (value: 0.0012, weighted value: 0.0594, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9332
Epoch 5/10, Train Loss: 0.2162 (value: 0.0011, weighted value: 0.0572, policy: 0.1590, weighted policy: 0.1590), Train Mean Max: 0.9333
Epoch 6/10, Train Loss: 0.2116 (value: 0.0011, weighted value: 0.0554, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9336
Epoch 7/10, Train Loss: 0.2116 (value: 0.0011, weighted value: 0.0552, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9339
Epoch 8/10, Train Loss: 0.2069 (value: 0.0010, weighted value: 0.0511, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9342
Epoch 9/10, Train Loss: 0.2069 (value: 0.0010, weighted value: 0.0519, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9343
Epoch 10/10, Train Loss: 0.2033 (value: 0.0010, weighted value: 0.0492, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9345
..training done in 67.61 seconds
..evaluation done in 19.84 seconds
Old network+MCTS average reward: 0.7012, min: 0.1296, max: 1.3241, stdev: 0.2205
New network+MCTS average reward: 0.7004, min: 0.1389, max: 1.3241, stdev: 0.2214
Old bare network average reward: 0.6493, min: 0.0926, max: 1.3241, stdev: 0.2226
New bare network average reward: 0.6501, min: 0.0926, max: 1.3241, stdev: 0.2280
External policy "random" average reward: 0.2636, min: -0.3333, max: 0.9444, stdev: 0.2202
External policy "individual greedy" average reward: 0.5508, min: 0.0463, max: 1.3333, stdev: 0.2278
External policy "total greedy" average reward: 0.6628, min: 0.0278, max: 1.4074, stdev: 0.2126
New network won 71 and tied 147 out of 300 games (48.17% wins where ties are half wins)
Reverting to the old network

Training iteration 246 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.72 seconds
Training examples lengths: [64716, 64934, 64911, 64743, 64329, 64550, 64887, 64975, 65129, 64698]
Total value: 452196.69
Training on 647872 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2606 (value: 0.0017, weighted value: 0.0859, policy: 0.1748, weighted policy: 0.1748), Train Mean Max: 0.9308
Epoch 2/10, Train Loss: 0.2458 (value: 0.0016, weighted value: 0.0777, policy: 0.1681, weighted policy: 0.1681), Train Mean Max: 0.9317
Epoch 3/10, Train Loss: 0.2365 (value: 0.0015, weighted value: 0.0727, policy: 0.1638, weighted policy: 0.1638), Train Mean Max: 0.9321
Epoch 4/10, Train Loss: 0.2287 (value: 0.0013, weighted value: 0.0672, policy: 0.1615, weighted policy: 0.1615), Train Mean Max: 0.9324
Epoch 5/10, Train Loss: 0.2245 (value: 0.0013, weighted value: 0.0643, policy: 0.1602, weighted policy: 0.1602), Train Mean Max: 0.9326
Epoch 6/10, Train Loss: 0.2204 (value: 0.0012, weighted value: 0.0614, policy: 0.1591, weighted policy: 0.1591), Train Mean Max: 0.9330
Epoch 7/10, Train Loss: 0.2183 (value: 0.0012, weighted value: 0.0606, policy: 0.1577, weighted policy: 0.1577), Train Mean Max: 0.9332
Epoch 8/10, Train Loss: 0.2147 (value: 0.0011, weighted value: 0.0570, policy: 0.1577, weighted policy: 0.1577), Train Mean Max: 0.9334
Epoch 9/10, Train Loss: 0.2113 (value: 0.0011, weighted value: 0.0548, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9337
Epoch 10/10, Train Loss: 0.2100 (value: 0.0011, weighted value: 0.0537, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9338
..training done in 60.70 seconds
..evaluation done in 18.06 seconds
Old network+MCTS average reward: 0.6984, min: 0.2037, max: 1.2593, stdev: 0.2017
New network+MCTS average reward: 0.6936, min: 0.1296, max: 1.2778, stdev: 0.2018
Old bare network average reward: 0.6512, min: 0.1019, max: 1.2593, stdev: 0.2081
New bare network average reward: 0.6506, min: 0.0833, max: 1.2222, stdev: 0.2089
External policy "random" average reward: 0.2740, min: -0.2222, max: 0.8148, stdev: 0.2120
External policy "individual greedy" average reward: 0.5581, min: -0.0370, max: 1.2778, stdev: 0.2168
External policy "total greedy" average reward: 0.6727, min: 0.1574, max: 1.3426, stdev: 0.2055
New network won 64 and tied 148 out of 300 games (46.00% wins where ties are half wins)
Reverting to the old network

Training iteration 247 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.33 seconds
Training examples lengths: [64934, 64911, 64743, 64329, 64550, 64887, 64975, 65129, 64698, 64632]
Total value: 452563.21
Training on 647788 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2858 (value: 0.0021, weighted value: 0.1037, policy: 0.1821, weighted policy: 0.1821), Train Mean Max: 0.9298
Epoch 2/10, Train Loss: 0.2608 (value: 0.0018, weighted value: 0.0886, policy: 0.1723, weighted policy: 0.1723), Train Mean Max: 0.9309
Epoch 3/10, Train Loss: 0.2483 (value: 0.0016, weighted value: 0.0811, policy: 0.1673, weighted policy: 0.1673), Train Mean Max: 0.9313
Epoch 4/10, Train Loss: 0.2404 (value: 0.0015, weighted value: 0.0762, policy: 0.1643, weighted policy: 0.1643), Train Mean Max: 0.9317
Epoch 5/10, Train Loss: 0.2337 (value: 0.0014, weighted value: 0.0714, policy: 0.1624, weighted policy: 0.1624), Train Mean Max: 0.9320
Epoch 6/10, Train Loss: 0.2294 (value: 0.0014, weighted value: 0.0693, policy: 0.1601, weighted policy: 0.1601), Train Mean Max: 0.9324
Epoch 7/10, Train Loss: 0.2258 (value: 0.0013, weighted value: 0.0664, policy: 0.1595, weighted policy: 0.1595), Train Mean Max: 0.9325
Epoch 8/10, Train Loss: 0.2207 (value: 0.0012, weighted value: 0.0620, policy: 0.1587, weighted policy: 0.1587), Train Mean Max: 0.9327
Epoch 9/10, Train Loss: 0.2178 (value: 0.0012, weighted value: 0.0600, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9331
Epoch 10/10, Train Loss: 0.2169 (value: 0.0012, weighted value: 0.0591, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9333
..training done in 59.91 seconds
..evaluation done in 19.34 seconds
Old network+MCTS average reward: 0.6613, min: 0.1296, max: 1.2963, stdev: 0.2297
New network+MCTS average reward: 0.6604, min: 0.0741, max: 1.3426, stdev: 0.2288
Old bare network average reward: 0.6128, min: 0.0000, max: 1.2963, stdev: 0.2333
New bare network average reward: 0.6214, min: 0.0000, max: 1.2963, stdev: 0.2306
External policy "random" average reward: 0.2472, min: -0.2778, max: 1.0370, stdev: 0.2355
External policy "individual greedy" average reward: 0.5279, min: 0.0370, max: 1.2963, stdev: 0.2272
External policy "total greedy" average reward: 0.6320, min: 0.1111, max: 1.3889, stdev: 0.2265
New network won 74 and tied 145 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 248 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.86 seconds
Training examples lengths: [64911, 64743, 64329, 64550, 64887, 64975, 65129, 64698, 64632, 64780]
Total value: 452052.01
Training on 647634 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3048 (value: 0.0023, weighted value: 0.1163, policy: 0.1885, weighted policy: 0.1885), Train Mean Max: 0.9288
Epoch 2/10, Train Loss: 0.2759 (value: 0.0020, weighted value: 0.0990, policy: 0.1769, weighted policy: 0.1769), Train Mean Max: 0.9297
Epoch 3/10, Train Loss: 0.2598 (value: 0.0018, weighted value: 0.0891, policy: 0.1707, weighted policy: 0.1707), Train Mean Max: 0.9305
Epoch 4/10, Train Loss: 0.2510 (value: 0.0017, weighted value: 0.0840, policy: 0.1670, weighted policy: 0.1670), Train Mean Max: 0.9308
Epoch 5/10, Train Loss: 0.2437 (value: 0.0016, weighted value: 0.0782, policy: 0.1655, weighted policy: 0.1655), Train Mean Max: 0.9311
Epoch 6/10, Train Loss: 0.2362 (value: 0.0015, weighted value: 0.0735, policy: 0.1627, weighted policy: 0.1627), Train Mean Max: 0.9315
Epoch 7/10, Train Loss: 0.2335 (value: 0.0014, weighted value: 0.0702, policy: 0.1633, weighted policy: 0.1633), Train Mean Max: 0.9317
Epoch 8/10, Train Loss: 0.2305 (value: 0.0014, weighted value: 0.0696, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9322
Epoch 9/10, Train Loss: 0.2228 (value: 0.0013, weighted value: 0.0634, policy: 0.1595, weighted policy: 0.1595), Train Mean Max: 0.9325
Epoch 10/10, Train Loss: 0.2213 (value: 0.0013, weighted value: 0.0626, policy: 0.1587, weighted policy: 0.1587), Train Mean Max: 0.9327
..training done in 59.85 seconds
..evaluation done in 18.07 seconds
Old network+MCTS average reward: 0.6808, min: 0.0463, max: 1.5185, stdev: 0.2200
New network+MCTS average reward: 0.6871, min: 0.0463, max: 1.5000, stdev: 0.2216
Old bare network average reward: 0.6365, min: 0.0463, max: 1.5185, stdev: 0.2256
New bare network average reward: 0.6361, min: 0.0463, max: 1.5185, stdev: 0.2281
External policy "random" average reward: 0.2512, min: -0.2222, max: 1.0370, stdev: 0.2207
External policy "individual greedy" average reward: 0.5370, min: 0.0463, max: 1.3704, stdev: 0.2157
External policy "total greedy" average reward: 0.6527, min: 0.1759, max: 1.5278, stdev: 0.2156
New network won 95 and tied 137 out of 300 games (54.50% wins where ties are half wins)
Keeping the new network

Training iteration 249 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.43 seconds
Training examples lengths: [64743, 64329, 64550, 64887, 64975, 65129, 64698, 64632, 64780, 64767]
Total value: 451624.49
Training on 647490 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2433 (value: 0.0015, weighted value: 0.0761, policy: 0.1672, weighted policy: 0.1672), Train Mean Max: 0.9314
Epoch 2/10, Train Loss: 0.2333 (value: 0.0014, weighted value: 0.0694, policy: 0.1639, weighted policy: 0.1639), Train Mean Max: 0.9321
Epoch 3/10, Train Loss: 0.2283 (value: 0.0013, weighted value: 0.0655, policy: 0.1627, weighted policy: 0.1627), Train Mean Max: 0.9324
Epoch 4/10, Train Loss: 0.2234 (value: 0.0013, weighted value: 0.0636, policy: 0.1598, weighted policy: 0.1598), Train Mean Max: 0.9328
Epoch 5/10, Train Loss: 0.2196 (value: 0.0012, weighted value: 0.0605, policy: 0.1591, weighted policy: 0.1591), Train Mean Max: 0.9329
Epoch 6/10, Train Loss: 0.2163 (value: 0.0012, weighted value: 0.0585, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9332
Epoch 7/10, Train Loss: 0.2147 (value: 0.0011, weighted value: 0.0574, policy: 0.1573, weighted policy: 0.1573), Train Mean Max: 0.9334
Epoch 8/10, Train Loss: 0.2122 (value: 0.0011, weighted value: 0.0549, policy: 0.1573, weighted policy: 0.1573), Train Mean Max: 0.9338
Epoch 9/10, Train Loss: 0.2106 (value: 0.0011, weighted value: 0.0545, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9340
Epoch 10/10, Train Loss: 0.2079 (value: 0.0011, weighted value: 0.0526, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9341
..training done in 60.59 seconds
..evaluation done in 18.66 seconds
Old network+MCTS average reward: 0.6909, min: -0.0185, max: 1.3704, stdev: 0.2316
New network+MCTS average reward: 0.6956, min: 0.0556, max: 1.3889, stdev: 0.2290
Old bare network average reward: 0.6447, min: -0.0185, max: 1.2963, stdev: 0.2400
New bare network average reward: 0.6501, min: -0.0185, max: 1.2963, stdev: 0.2373
External policy "random" average reward: 0.2637, min: -0.4444, max: 1.0648, stdev: 0.2273
External policy "individual greedy" average reward: 0.5469, min: 0.0278, max: 1.2593, stdev: 0.2224
External policy "total greedy" average reward: 0.6568, min: 0.1667, max: 1.3704, stdev: 0.2146
New network won 85 and tied 144 out of 300 games (52.33% wins where ties are half wins)
Keeping the new network

Training iteration 250 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.27 seconds
Training examples lengths: [64329, 64550, 64887, 64975, 65129, 64698, 64632, 64780, 64767, 64639]
Total value: 451780.69
Training on 647386 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2308 (value: 0.0013, weighted value: 0.0670, policy: 0.1638, weighted policy: 0.1638), Train Mean Max: 0.9328
Epoch 2/10, Train Loss: 0.2234 (value: 0.0012, weighted value: 0.0617, policy: 0.1617, weighted policy: 0.1617), Train Mean Max: 0.9332
Epoch 3/10, Train Loss: 0.2173 (value: 0.0012, weighted value: 0.0581, policy: 0.1592, weighted policy: 0.1592), Train Mean Max: 0.9338
Epoch 4/10, Train Loss: 0.2138 (value: 0.0011, weighted value: 0.0564, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9339
Epoch 5/10, Train Loss: 0.2123 (value: 0.0011, weighted value: 0.0545, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9341
Epoch 6/10, Train Loss: 0.2087 (value: 0.0011, weighted value: 0.0534, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9344
Epoch 7/10, Train Loss: 0.2067 (value: 0.0010, weighted value: 0.0513, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9346
Epoch 8/10, Train Loss: 0.2035 (value: 0.0010, weighted value: 0.0497, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9348
Epoch 9/10, Train Loss: 0.2031 (value: 0.0010, weighted value: 0.0483, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9348
Epoch 10/10, Train Loss: 0.2023 (value: 0.0010, weighted value: 0.0480, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9350
..training done in 60.47 seconds
..evaluation done in 18.71 seconds
Old network+MCTS average reward: 0.7051, min: 0.1111, max: 1.6111, stdev: 0.2259
New network+MCTS average reward: 0.6985, min: 0.0648, max: 1.5556, stdev: 0.2273
Old bare network average reward: 0.6576, min: -0.0463, max: 1.6111, stdev: 0.2351
New bare network average reward: 0.6553, min: 0.0648, max: 1.5463, stdev: 0.2346
External policy "random" average reward: 0.2723, min: -0.2593, max: 1.0463, stdev: 0.2198
External policy "individual greedy" average reward: 0.5488, min: -0.0463, max: 1.3241, stdev: 0.2212
External policy "total greedy" average reward: 0.6644, min: 0.1574, max: 1.3796, stdev: 0.2158
New network won 74 and tied 134 out of 300 games (47.00% wins where ties are half wins)
Reverting to the old network

Training iteration 251 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.54 seconds
Training examples lengths: [64550, 64887, 64975, 65129, 64698, 64632, 64780, 64767, 64639, 64861]
Total value: 452846.54
Training on 647918 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2557 (value: 0.0016, weighted value: 0.0816, policy: 0.1741, weighted policy: 0.1741), Train Mean Max: 0.9312
Epoch 2/10, Train Loss: 0.2404 (value: 0.0015, weighted value: 0.0730, policy: 0.1674, weighted policy: 0.1674), Train Mean Max: 0.9321
Epoch 3/10, Train Loss: 0.2326 (value: 0.0014, weighted value: 0.0697, policy: 0.1629, weighted policy: 0.1629), Train Mean Max: 0.9325
Epoch 4/10, Train Loss: 0.2245 (value: 0.0013, weighted value: 0.0642, policy: 0.1603, weighted policy: 0.1603), Train Mean Max: 0.9328
Epoch 5/10, Train Loss: 0.2208 (value: 0.0012, weighted value: 0.0616, policy: 0.1592, weighted policy: 0.1592), Train Mean Max: 0.9331
Epoch 6/10, Train Loss: 0.2164 (value: 0.0012, weighted value: 0.0586, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9334
Epoch 7/10, Train Loss: 0.2133 (value: 0.0011, weighted value: 0.0570, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9337
Epoch 8/10, Train Loss: 0.2122 (value: 0.0011, weighted value: 0.0562, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9338
Epoch 9/10, Train Loss: 0.2090 (value: 0.0011, weighted value: 0.0533, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9339
Epoch 10/10, Train Loss: 0.2069 (value: 0.0010, weighted value: 0.0519, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9342
..training done in 60.46 seconds
..evaluation done in 18.78 seconds
Old network+MCTS average reward: 0.6990, min: 0.1389, max: 1.4722, stdev: 0.2256
New network+MCTS average reward: 0.7052, min: 0.1389, max: 1.4907, stdev: 0.2275
Old bare network average reward: 0.6513, min: 0.0556, max: 1.4722, stdev: 0.2337
New bare network average reward: 0.6540, min: 0.0185, max: 1.3796, stdev: 0.2304
External policy "random" average reward: 0.2501, min: -0.2778, max: 0.9907, stdev: 0.2326
External policy "individual greedy" average reward: 0.5484, min: -0.0556, max: 1.2130, stdev: 0.2165
External policy "total greedy" average reward: 0.6665, min: -0.0648, max: 1.3519, stdev: 0.2156
New network won 82 and tied 142 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 252 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.29 seconds
Training examples lengths: [64887, 64975, 65129, 64698, 64632, 64780, 64767, 64639, 64861, 64916]
Total value: 453022.94
Training on 648284 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2327 (value: 0.0013, weighted value: 0.0674, policy: 0.1653, weighted policy: 0.1653), Train Mean Max: 0.9328
Epoch 2/10, Train Loss: 0.2228 (value: 0.0012, weighted value: 0.0616, policy: 0.1612, weighted policy: 0.1612), Train Mean Max: 0.9334
Epoch 3/10, Train Loss: 0.2177 (value: 0.0012, weighted value: 0.0586, policy: 0.1591, weighted policy: 0.1591), Train Mean Max: 0.9337
Epoch 4/10, Train Loss: 0.2150 (value: 0.0011, weighted value: 0.0572, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9339
Epoch 5/10, Train Loss: 0.2124 (value: 0.0011, weighted value: 0.0554, policy: 0.1569, weighted policy: 0.1569), Train Mean Max: 0.9341
Epoch 6/10, Train Loss: 0.2063 (value: 0.0010, weighted value: 0.0513, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9345
Epoch 7/10, Train Loss: 0.2055 (value: 0.0010, weighted value: 0.0505, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9346
Epoch 8/10, Train Loss: 0.2040 (value: 0.0010, weighted value: 0.0496, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9347
Epoch 9/10, Train Loss: 0.2027 (value: 0.0010, weighted value: 0.0491, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9350
Epoch 10/10, Train Loss: 0.1992 (value: 0.0009, weighted value: 0.0456, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9351
..training done in 59.38 seconds
..evaluation done in 18.59 seconds
Old network+MCTS average reward: 0.6752, min: 0.1759, max: 1.2870, stdev: 0.2161
New network+MCTS average reward: 0.6806, min: 0.1852, max: 1.2870, stdev: 0.2148
Old bare network average reward: 0.6278, min: 0.1204, max: 1.2870, stdev: 0.2230
New bare network average reward: 0.6280, min: 0.0556, max: 1.2870, stdev: 0.2200
External policy "random" average reward: 0.2498, min: -0.3148, max: 0.8426, stdev: 0.2084
External policy "individual greedy" average reward: 0.5297, min: 0.0556, max: 1.3704, stdev: 0.2142
External policy "total greedy" average reward: 0.6469, min: 0.0833, max: 1.4444, stdev: 0.2113
New network won 73 and tied 161 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 253 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.65 seconds
Training examples lengths: [64975, 65129, 64698, 64632, 64780, 64767, 64639, 64861, 64916, 64681]
Total value: 453107.66
Training on 648078 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2281 (value: 0.0013, weighted value: 0.0648, policy: 0.1633, weighted policy: 0.1633), Train Mean Max: 0.9335
Epoch 2/10, Train Loss: 0.2181 (value: 0.0012, weighted value: 0.0577, policy: 0.1604, weighted policy: 0.1604), Train Mean Max: 0.9340
Epoch 3/10, Train Loss: 0.2129 (value: 0.0011, weighted value: 0.0549, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9345
Epoch 4/10, Train Loss: 0.2080 (value: 0.0010, weighted value: 0.0520, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9347
Epoch 5/10, Train Loss: 0.2054 (value: 0.0010, weighted value: 0.0511, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9350
Epoch 6/10, Train Loss: 0.2045 (value: 0.0010, weighted value: 0.0500, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9351
Epoch 7/10, Train Loss: 0.2003 (value: 0.0009, weighted value: 0.0468, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9354
Epoch 8/10, Train Loss: 0.2013 (value: 0.0009, weighted value: 0.0473, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9354
Epoch 9/10, Train Loss: 0.2009 (value: 0.0009, weighted value: 0.0471, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9355
Epoch 10/10, Train Loss: 0.1968 (value: 0.0009, weighted value: 0.0439, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9359
..training done in 67.28 seconds
..evaluation done in 18.78 seconds
Old network+MCTS average reward: 0.6797, min: 0.1574, max: 1.3704, stdev: 0.2208
New network+MCTS average reward: 0.6823, min: 0.1296, max: 1.3704, stdev: 0.2216
Old bare network average reward: 0.6335, min: 0.0463, max: 1.3704, stdev: 0.2260
New bare network average reward: 0.6335, min: 0.0463, max: 1.3704, stdev: 0.2276
External policy "random" average reward: 0.2636, min: -0.2593, max: 0.8426, stdev: 0.2074
External policy "individual greedy" average reward: 0.5365, min: -0.0093, max: 1.3148, stdev: 0.2109
External policy "total greedy" average reward: 0.6553, min: 0.1667, max: 1.4074, stdev: 0.2107
New network won 67 and tied 165 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 254 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.16 seconds
Training examples lengths: [65129, 64698, 64632, 64780, 64767, 64639, 64861, 64916, 64681, 65164]
Total value: 454005.19
Training on 648267 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2493 (value: 0.0016, weighted value: 0.0785, policy: 0.1709, weighted policy: 0.1709), Train Mean Max: 0.9323
Epoch 2/10, Train Loss: 0.2346 (value: 0.0014, weighted value: 0.0696, policy: 0.1651, weighted policy: 0.1651), Train Mean Max: 0.9331
Epoch 3/10, Train Loss: 0.2256 (value: 0.0013, weighted value: 0.0642, policy: 0.1614, weighted policy: 0.1614), Train Mean Max: 0.9335
Epoch 4/10, Train Loss: 0.2221 (value: 0.0013, weighted value: 0.0628, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9338
Epoch 5/10, Train Loss: 0.2158 (value: 0.0012, weighted value: 0.0583, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9340
Epoch 6/10, Train Loss: 0.2125 (value: 0.0011, weighted value: 0.0566, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9343
Epoch 7/10, Train Loss: 0.2091 (value: 0.0011, weighted value: 0.0534, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9344
Epoch 8/10, Train Loss: 0.2067 (value: 0.0010, weighted value: 0.0517, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9347
Epoch 9/10, Train Loss: 0.2073 (value: 0.0010, weighted value: 0.0521, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9348
Epoch 10/10, Train Loss: 0.2031 (value: 0.0010, weighted value: 0.0488, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9351
..training done in 59.37 seconds
..evaluation done in 18.55 seconds
Old network+MCTS average reward: 0.6955, min: 0.0648, max: 1.5463, stdev: 0.2360
New network+MCTS average reward: 0.6934, min: 0.0463, max: 1.5463, stdev: 0.2389
Old bare network average reward: 0.6505, min: 0.0463, max: 1.5278, stdev: 0.2400
New bare network average reward: 0.6503, min: 0.0463, max: 1.5463, stdev: 0.2423
External policy "random" average reward: 0.2767, min: -0.3519, max: 0.9259, stdev: 0.2208
External policy "individual greedy" average reward: 0.5446, min: -0.0278, max: 1.3796, stdev: 0.2349
External policy "total greedy" average reward: 0.6462, min: 0.1389, max: 1.4722, stdev: 0.2287
New network won 77 and tied 150 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 255 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.37 seconds
Training examples lengths: [64698, 64632, 64780, 64767, 64639, 64861, 64916, 64681, 65164, 64925]
Total value: 453830.39
Training on 648063 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2291 (value: 0.0013, weighted value: 0.0661, policy: 0.1631, weighted policy: 0.1631), Train Mean Max: 0.9336
Epoch 2/10, Train Loss: 0.2194 (value: 0.0012, weighted value: 0.0590, policy: 0.1604, weighted policy: 0.1604), Train Mean Max: 0.9340
Epoch 3/10, Train Loss: 0.2143 (value: 0.0011, weighted value: 0.0573, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9343
Epoch 4/10, Train Loss: 0.2093 (value: 0.0011, weighted value: 0.0530, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9346
Epoch 5/10, Train Loss: 0.2081 (value: 0.0011, weighted value: 0.0529, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9348
Epoch 6/10, Train Loss: 0.2053 (value: 0.0010, weighted value: 0.0508, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9350
Epoch 7/10, Train Loss: 0.2036 (value: 0.0010, weighted value: 0.0501, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9352
Epoch 8/10, Train Loss: 0.2007 (value: 0.0009, weighted value: 0.0473, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9353
Epoch 9/10, Train Loss: 0.2000 (value: 0.0009, weighted value: 0.0466, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9353
Epoch 10/10, Train Loss: 0.1987 (value: 0.0009, weighted value: 0.0449, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9356
..training done in 64.64 seconds
..evaluation done in 18.73 seconds
Old network+MCTS average reward: 0.7020, min: 0.0463, max: 1.5185, stdev: 0.2530
New network+MCTS average reward: 0.7048, min: 0.0000, max: 1.5185, stdev: 0.2488
Old bare network average reward: 0.6586, min: -0.0093, max: 1.5185, stdev: 0.2585
New bare network average reward: 0.6630, min: -0.0093, max: 1.5185, stdev: 0.2533
External policy "random" average reward: 0.2859, min: -0.3981, max: 1.0741, stdev: 0.2393
External policy "individual greedy" average reward: 0.5535, min: -0.2500, max: 1.3426, stdev: 0.2521
External policy "total greedy" average reward: 0.6651, min: -0.0648, max: 1.6481, stdev: 0.2453
New network won 82 and tied 152 out of 300 games (52.67% wins where ties are half wins)
Keeping the new network

Training iteration 256 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.04 seconds
Training examples lengths: [64632, 64780, 64767, 64639, 64861, 64916, 64681, 65164, 64925, 64974]
Total value: 454347.88
Training on 648339 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2255 (value: 0.0012, weighted value: 0.0620, policy: 0.1635, weighted policy: 0.1635), Train Mean Max: 0.9342
Epoch 2/10, Train Loss: 0.2182 (value: 0.0012, weighted value: 0.0588, policy: 0.1594, weighted policy: 0.1594), Train Mean Max: 0.9347
Epoch 3/10, Train Loss: 0.2088 (value: 0.0010, weighted value: 0.0523, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9349
Epoch 4/10, Train Loss: 0.2079 (value: 0.0011, weighted value: 0.0530, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9352
Epoch 5/10, Train Loss: 0.2039 (value: 0.0010, weighted value: 0.0498, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9352
Epoch 6/10, Train Loss: 0.2014 (value: 0.0010, weighted value: 0.0479, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9356
Epoch 7/10, Train Loss: 0.2002 (value: 0.0009, weighted value: 0.0472, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9356
Epoch 8/10, Train Loss: 0.1989 (value: 0.0009, weighted value: 0.0458, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9359
Epoch 9/10, Train Loss: 0.1976 (value: 0.0009, weighted value: 0.0453, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9361
Epoch 10/10, Train Loss: 0.1964 (value: 0.0009, weighted value: 0.0435, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9360
..training done in 60.04 seconds
..evaluation done in 18.22 seconds
Old network+MCTS average reward: 0.6910, min: -0.2870, max: 1.3704, stdev: 0.2306
New network+MCTS average reward: 0.6919, min: -0.2870, max: 1.3704, stdev: 0.2295
Old bare network average reward: 0.6450, min: -0.1204, max: 1.3704, stdev: 0.2333
New bare network average reward: 0.6473, min: -0.1944, max: 1.3704, stdev: 0.2367
External policy "random" average reward: 0.2578, min: -0.4444, max: 0.8148, stdev: 0.2205
External policy "individual greedy" average reward: 0.5337, min: -0.2500, max: 1.2963, stdev: 0.2273
External policy "total greedy" average reward: 0.6469, min: -0.1389, max: 1.5463, stdev: 0.2245
New network won 76 and tied 150 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 257 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.59 seconds
Training examples lengths: [64780, 64767, 64639, 64861, 64916, 64681, 65164, 64925, 64974, 64791]
Total value: 454623.61
Training on 648498 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2251 (value: 0.0012, weighted value: 0.0615, policy: 0.1636, weighted policy: 0.1636), Train Mean Max: 0.9344
Epoch 2/10, Train Loss: 0.2138 (value: 0.0011, weighted value: 0.0544, policy: 0.1594, weighted policy: 0.1594), Train Mean Max: 0.9349
Epoch 3/10, Train Loss: 0.2077 (value: 0.0010, weighted value: 0.0521, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9354
Epoch 4/10, Train Loss: 0.2066 (value: 0.0010, weighted value: 0.0515, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9354
Epoch 5/10, Train Loss: 0.2020 (value: 0.0010, weighted value: 0.0478, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9356
Epoch 6/10, Train Loss: 0.2014 (value: 0.0010, weighted value: 0.0477, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9358
Epoch 7/10, Train Loss: 0.1978 (value: 0.0009, weighted value: 0.0456, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9359
Epoch 8/10, Train Loss: 0.1979 (value: 0.0009, weighted value: 0.0447, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9360
Epoch 9/10, Train Loss: 0.1963 (value: 0.0009, weighted value: 0.0434, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9361
Epoch 10/10, Train Loss: 0.1949 (value: 0.0009, weighted value: 0.0431, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9363
..training done in 59.72 seconds
..evaluation done in 18.17 seconds
Old network+MCTS average reward: 0.6944, min: -0.0093, max: 1.2870, stdev: 0.2281
New network+MCTS average reward: 0.6945, min: -0.0093, max: 1.2870, stdev: 0.2313
Old bare network average reward: 0.6453, min: -0.0093, max: 1.3148, stdev: 0.2368
New bare network average reward: 0.6489, min: 0.0000, max: 1.2685, stdev: 0.2351
External policy "random" average reward: 0.2574, min: -0.3889, max: 0.9907, stdev: 0.2343
External policy "individual greedy" average reward: 0.5445, min: -0.0741, max: 1.2315, stdev: 0.2314
External policy "total greedy" average reward: 0.6604, min: -0.0741, max: 1.2685, stdev: 0.2245
New network won 73 and tied 148 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 258 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.48 seconds
Training examples lengths: [64767, 64639, 64861, 64916, 64681, 65164, 64925, 64974, 64791, 64827]
Total value: 454711.88
Training on 648545 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2516 (value: 0.0016, weighted value: 0.0780, policy: 0.1736, weighted policy: 0.1736), Train Mean Max: 0.9327
Epoch 2/10, Train Loss: 0.2325 (value: 0.0013, weighted value: 0.0675, policy: 0.1650, weighted policy: 0.1650), Train Mean Max: 0.9336
Epoch 3/10, Train Loss: 0.2253 (value: 0.0013, weighted value: 0.0636, policy: 0.1617, weighted policy: 0.1617), Train Mean Max: 0.9338
Epoch 4/10, Train Loss: 0.2186 (value: 0.0012, weighted value: 0.0593, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9342
Epoch 5/10, Train Loss: 0.2143 (value: 0.0011, weighted value: 0.0570, policy: 0.1573, weighted policy: 0.1573), Train Mean Max: 0.9344
Epoch 6/10, Train Loss: 0.2118 (value: 0.0011, weighted value: 0.0551, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9345
Epoch 7/10, Train Loss: 0.2076 (value: 0.0010, weighted value: 0.0516, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9349
Epoch 8/10, Train Loss: 0.2073 (value: 0.0010, weighted value: 0.0515, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9347
Epoch 9/10, Train Loss: 0.2038 (value: 0.0010, weighted value: 0.0491, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9350
Epoch 10/10, Train Loss: 0.2037 (value: 0.0010, weighted value: 0.0490, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9351
..training done in 67.76 seconds
..evaluation done in 20.00 seconds
Old network+MCTS average reward: 0.6779, min: 0.1389, max: 1.2870, stdev: 0.2316
New network+MCTS average reward: 0.6772, min: 0.0833, max: 1.2778, stdev: 0.2320
Old bare network average reward: 0.6319, min: 0.0833, max: 1.2870, stdev: 0.2349
New bare network average reward: 0.6331, min: 0.0833, max: 1.2500, stdev: 0.2345
External policy "random" average reward: 0.2562, min: -0.3333, max: 0.9352, stdev: 0.2343
External policy "individual greedy" average reward: 0.5260, min: -0.0833, max: 1.2315, stdev: 0.2322
External policy "total greedy" average reward: 0.6381, min: 0.0185, max: 1.3426, stdev: 0.2232
New network won 82 and tied 130 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 259 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.38 seconds
Training examples lengths: [64639, 64861, 64916, 64681, 65164, 64925, 64974, 64791, 64827, 64489]
Total value: 455112.86
Training on 648267 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2744 (value: 0.0019, weighted value: 0.0926, policy: 0.1818, weighted policy: 0.1818), Train Mean Max: 0.9314
Epoch 2/10, Train Loss: 0.2527 (value: 0.0016, weighted value: 0.0804, policy: 0.1723, weighted policy: 0.1723), Train Mean Max: 0.9322
Epoch 3/10, Train Loss: 0.2394 (value: 0.0015, weighted value: 0.0734, policy: 0.1659, weighted policy: 0.1659), Train Mean Max: 0.9327
Epoch 4/10, Train Loss: 0.2294 (value: 0.0014, weighted value: 0.0677, policy: 0.1617, weighted policy: 0.1617), Train Mean Max: 0.9332
Epoch 5/10, Train Loss: 0.2239 (value: 0.0013, weighted value: 0.0639, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9334
Epoch 6/10, Train Loss: 0.2212 (value: 0.0012, weighted value: 0.0614, policy: 0.1598, weighted policy: 0.1598), Train Mean Max: 0.9333
Epoch 7/10, Train Loss: 0.2181 (value: 0.0012, weighted value: 0.0595, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9336
Epoch 8/10, Train Loss: 0.2141 (value: 0.0011, weighted value: 0.0571, policy: 0.1569, weighted policy: 0.1569), Train Mean Max: 0.9339
Epoch 9/10, Train Loss: 0.2119 (value: 0.0011, weighted value: 0.0548, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9341
Epoch 10/10, Train Loss: 0.2077 (value: 0.0010, weighted value: 0.0517, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9343
..training done in 66.52 seconds
..evaluation done in 18.51 seconds
Old network+MCTS average reward: 0.7047, min: 0.1111, max: 1.4722, stdev: 0.2326
New network+MCTS average reward: 0.7079, min: 0.1111, max: 1.4722, stdev: 0.2299
Old bare network average reward: 0.6627, min: -0.0741, max: 1.3889, stdev: 0.2398
New bare network average reward: 0.6623, min: 0.0093, max: 1.4074, stdev: 0.2361
External policy "random" average reward: 0.2731, min: -0.4259, max: 1.0741, stdev: 0.2377
External policy "individual greedy" average reward: 0.5502, min: -0.0741, max: 1.3611, stdev: 0.2359
External policy "total greedy" average reward: 0.6664, min: 0.0000, max: 1.2778, stdev: 0.2251
New network won 73 and tied 163 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 260 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.86 seconds
Training examples lengths: [64861, 64916, 64681, 65164, 64925, 64974, 64791, 64827, 64489, 65189]
Total value: 455377.96
Training on 648817 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2333 (value: 0.0014, weighted value: 0.0679, policy: 0.1654, weighted policy: 0.1654), Train Mean Max: 0.9328
Epoch 2/10, Train Loss: 0.2235 (value: 0.0012, weighted value: 0.0619, policy: 0.1616, weighted policy: 0.1616), Train Mean Max: 0.9334
Epoch 3/10, Train Loss: 0.2170 (value: 0.0012, weighted value: 0.0589, policy: 0.1581, weighted policy: 0.1581), Train Mean Max: 0.9338
Epoch 4/10, Train Loss: 0.2142 (value: 0.0011, weighted value: 0.0567, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9340
Epoch 5/10, Train Loss: 0.2106 (value: 0.0011, weighted value: 0.0537, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9343
Epoch 6/10, Train Loss: 0.2088 (value: 0.0010, weighted value: 0.0524, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9344
Epoch 7/10, Train Loss: 0.2063 (value: 0.0010, weighted value: 0.0513, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9347
Epoch 8/10, Train Loss: 0.2040 (value: 0.0010, weighted value: 0.0492, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9348
Epoch 9/10, Train Loss: 0.2037 (value: 0.0010, weighted value: 0.0490, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9348
Epoch 10/10, Train Loss: 0.2005 (value: 0.0009, weighted value: 0.0465, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9351
..training done in 62.03 seconds
..evaluation done in 18.44 seconds
Old network+MCTS average reward: 0.7021, min: 0.1389, max: 1.6296, stdev: 0.2415
New network+MCTS average reward: 0.7044, min: 0.1574, max: 1.6389, stdev: 0.2391
Old bare network average reward: 0.6553, min: 0.0000, max: 1.6296, stdev: 0.2517
New bare network average reward: 0.6584, min: 0.0370, max: 1.6389, stdev: 0.2504
External policy "random" average reward: 0.2634, min: -0.4074, max: 1.2130, stdev: 0.2501
External policy "individual greedy" average reward: 0.5518, min: -0.1574, max: 1.4537, stdev: 0.2513
External policy "total greedy" average reward: 0.6610, min: 0.1296, max: 1.5278, stdev: 0.2370
New network won 75 and tied 164 out of 300 games (52.33% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_260

Training iteration 261 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.11 seconds
Training examples lengths: [64916, 64681, 65164, 64925, 64974, 64791, 64827, 64489, 65189, 64795]
Total value: 455292.81
Training on 648751 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2244 (value: 0.0013, weighted value: 0.0625, policy: 0.1619, weighted policy: 0.1619), Train Mean Max: 0.9337
Epoch 2/10, Train Loss: 0.2186 (value: 0.0012, weighted value: 0.0582, policy: 0.1603, weighted policy: 0.1603), Train Mean Max: 0.9341
Epoch 3/10, Train Loss: 0.2134 (value: 0.0011, weighted value: 0.0553, policy: 0.1581, weighted policy: 0.1581), Train Mean Max: 0.9344
Epoch 4/10, Train Loss: 0.2083 (value: 0.0010, weighted value: 0.0524, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9347
Epoch 5/10, Train Loss: 0.2049 (value: 0.0010, weighted value: 0.0501, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9348
Epoch 6/10, Train Loss: 0.2038 (value: 0.0010, weighted value: 0.0496, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9351
Epoch 7/10, Train Loss: 0.2012 (value: 0.0009, weighted value: 0.0471, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9352
Epoch 8/10, Train Loss: 0.2008 (value: 0.0010, weighted value: 0.0476, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9355
Epoch 9/10, Train Loss: 0.1989 (value: 0.0009, weighted value: 0.0456, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9355
Epoch 10/10, Train Loss: 0.1967 (value: 0.0009, weighted value: 0.0437, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9357
..training done in 67.44 seconds
..evaluation done in 21.58 seconds
Old network+MCTS average reward: 0.6655, min: -0.0093, max: 1.2778, stdev: 0.2251
New network+MCTS average reward: 0.6665, min: 0.0093, max: 1.2870, stdev: 0.2220
Old bare network average reward: 0.6198, min: 0.0000, max: 1.2778, stdev: 0.2301
New bare network average reward: 0.6198, min: -0.0370, max: 1.2593, stdev: 0.2333
External policy "random" average reward: 0.2384, min: -0.3148, max: 0.9259, stdev: 0.2294
External policy "individual greedy" average reward: 0.5077, min: -0.0185, max: 1.1852, stdev: 0.2296
External policy "total greedy" average reward: 0.6336, min: -0.0185, max: 1.2037, stdev: 0.2140
New network won 70 and tied 154 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 262 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 61.70 seconds
Training examples lengths: [64681, 65164, 64925, 64974, 64791, 64827, 64489, 65189, 64795, 64713]
Total value: 455449.60
Training on 648548 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2492 (value: 0.0016, weighted value: 0.0787, policy: 0.1705, weighted policy: 0.1705), Train Mean Max: 0.9324
Epoch 2/10, Train Loss: 0.2338 (value: 0.0014, weighted value: 0.0695, policy: 0.1643, weighted policy: 0.1643), Train Mean Max: 0.9332
Epoch 3/10, Train Loss: 0.2257 (value: 0.0013, weighted value: 0.0658, policy: 0.1598, weighted policy: 0.1598), Train Mean Max: 0.9336
Epoch 4/10, Train Loss: 0.2195 (value: 0.0012, weighted value: 0.0609, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9340
Epoch 5/10, Train Loss: 0.2162 (value: 0.0012, weighted value: 0.0591, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9341
Epoch 6/10, Train Loss: 0.2123 (value: 0.0011, weighted value: 0.0558, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9343
Epoch 7/10, Train Loss: 0.2106 (value: 0.0011, weighted value: 0.0548, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9344
Epoch 8/10, Train Loss: 0.2060 (value: 0.0010, weighted value: 0.0516, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9348
Epoch 9/10, Train Loss: 0.2060 (value: 0.0010, weighted value: 0.0513, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9349
Epoch 10/10, Train Loss: 0.2034 (value: 0.0010, weighted value: 0.0493, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9351
..training done in 70.92 seconds
..evaluation done in 18.78 seconds
Old network+MCTS average reward: 0.6970, min: 0.1481, max: 1.3889, stdev: 0.2322
New network+MCTS average reward: 0.6952, min: 0.1481, max: 1.3889, stdev: 0.2305
Old bare network average reward: 0.6428, min: 0.1019, max: 1.4630, stdev: 0.2365
New bare network average reward: 0.6470, min: -0.0370, max: 1.3889, stdev: 0.2395
External policy "random" average reward: 0.2651, min: -0.2593, max: 0.8241, stdev: 0.2155
External policy "individual greedy" average reward: 0.5414, min: -0.1204, max: 1.1574, stdev: 0.2269
External policy "total greedy" average reward: 0.6598, min: 0.1296, max: 1.3148, stdev: 0.2208
New network won 63 and tied 165 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 263 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.13 seconds
Training examples lengths: [65164, 64925, 64974, 64791, 64827, 64489, 65189, 64795, 64713, 64800]
Total value: 455718.61
Training on 648667 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2735 (value: 0.0019, weighted value: 0.0946, policy: 0.1789, weighted policy: 0.1789), Train Mean Max: 0.9310
Epoch 2/10, Train Loss: 0.2503 (value: 0.0016, weighted value: 0.0814, policy: 0.1689, weighted policy: 0.1689), Train Mean Max: 0.9322
Epoch 3/10, Train Loss: 0.2380 (value: 0.0015, weighted value: 0.0735, policy: 0.1645, weighted policy: 0.1645), Train Mean Max: 0.9325
Epoch 4/10, Train Loss: 0.2311 (value: 0.0014, weighted value: 0.0694, policy: 0.1617, weighted policy: 0.1617), Train Mean Max: 0.9329
Epoch 5/10, Train Loss: 0.2272 (value: 0.0013, weighted value: 0.0674, policy: 0.1598, weighted policy: 0.1598), Train Mean Max: 0.9332
Epoch 6/10, Train Loss: 0.2205 (value: 0.0013, weighted value: 0.0625, policy: 0.1580, weighted policy: 0.1580), Train Mean Max: 0.9334
Epoch 7/10, Train Loss: 0.2173 (value: 0.0012, weighted value: 0.0603, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9337
Epoch 8/10, Train Loss: 0.2157 (value: 0.0012, weighted value: 0.0588, policy: 0.1569, weighted policy: 0.1569), Train Mean Max: 0.9338
Epoch 9/10, Train Loss: 0.2102 (value: 0.0011, weighted value: 0.0549, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9341
Epoch 10/10, Train Loss: 0.2091 (value: 0.0011, weighted value: 0.0531, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9342
..training done in 65.43 seconds
..evaluation done in 19.38 seconds
Old network+MCTS average reward: 0.6620, min: 0.1111, max: 1.3611, stdev: 0.2379
New network+MCTS average reward: 0.6614, min: 0.0648, max: 1.3611, stdev: 0.2374
Old bare network average reward: 0.6173, min: 0.0648, max: 1.2870, stdev: 0.2423
New bare network average reward: 0.6183, min: 0.0648, max: 1.2870, stdev: 0.2377
External policy "random" average reward: 0.2398, min: -0.2963, max: 0.9259, stdev: 0.2325
External policy "individual greedy" average reward: 0.5138, min: -0.0741, max: 1.2407, stdev: 0.2224
External policy "total greedy" average reward: 0.6277, min: 0.0833, max: 1.3056, stdev: 0.2138
New network won 64 and tied 158 out of 300 games (47.67% wins where ties are half wins)
Reverting to the old network

Training iteration 264 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.47 seconds
Training examples lengths: [64925, 64974, 64791, 64827, 64489, 65189, 64795, 64713, 64800, 64652]
Total value: 455500.33
Training on 648155 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2934 (value: 0.0022, weighted value: 0.1092, policy: 0.1842, weighted policy: 0.1842), Train Mean Max: 0.9301
Epoch 2/10, Train Loss: 0.2662 (value: 0.0018, weighted value: 0.0924, policy: 0.1738, weighted policy: 0.1738), Train Mean Max: 0.9310
Epoch 3/10, Train Loss: 0.2505 (value: 0.0017, weighted value: 0.0833, policy: 0.1672, weighted policy: 0.1672), Train Mean Max: 0.9315
Epoch 4/10, Train Loss: 0.2438 (value: 0.0016, weighted value: 0.0799, policy: 0.1640, weighted policy: 0.1640), Train Mean Max: 0.9320
Epoch 5/10, Train Loss: 0.2345 (value: 0.0015, weighted value: 0.0727, policy: 0.1618, weighted policy: 0.1618), Train Mean Max: 0.9322
Epoch 6/10, Train Loss: 0.2303 (value: 0.0014, weighted value: 0.0689, policy: 0.1615, weighted policy: 0.1615), Train Mean Max: 0.9324
Epoch 7/10, Train Loss: 0.2259 (value: 0.0013, weighted value: 0.0659, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9328
Epoch 8/10, Train Loss: 0.2209 (value: 0.0013, weighted value: 0.0627, policy: 0.1581, weighted policy: 0.1581), Train Mean Max: 0.9330
Epoch 9/10, Train Loss: 0.2196 (value: 0.0012, weighted value: 0.0612, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9331
Epoch 10/10, Train Loss: 0.2150 (value: 0.0012, weighted value: 0.0585, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9335
..training done in 62.35 seconds
..evaluation done in 19.61 seconds
Old network+MCTS average reward: 0.6919, min: 0.1389, max: 1.4537, stdev: 0.2394
New network+MCTS average reward: 0.6955, min: 0.2037, max: 1.4537, stdev: 0.2404
Old bare network average reward: 0.6441, min: 0.0648, max: 1.3704, stdev: 0.2456
New bare network average reward: 0.6471, min: 0.0648, max: 1.4352, stdev: 0.2441
External policy "random" average reward: 0.2519, min: -0.4722, max: 1.0185, stdev: 0.2333
External policy "individual greedy" average reward: 0.5438, min: -0.1944, max: 1.2963, stdev: 0.2321
External policy "total greedy" average reward: 0.6583, min: 0.0093, max: 1.3611, stdev: 0.2246
New network won 81 and tied 156 out of 300 games (53.00% wins where ties are half wins)
Keeping the new network

Training iteration 265 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.97 seconds
Training examples lengths: [64974, 64791, 64827, 64489, 65189, 64795, 64713, 64800, 64652, 64620]
Total value: 455969.69
Training on 647850 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2378 (value: 0.0014, weighted value: 0.0716, policy: 0.1661, weighted policy: 0.1661), Train Mean Max: 0.9321
Epoch 2/10, Train Loss: 0.2302 (value: 0.0013, weighted value: 0.0668, policy: 0.1634, weighted policy: 0.1634), Train Mean Max: 0.9326
Epoch 3/10, Train Loss: 0.2225 (value: 0.0012, weighted value: 0.0622, policy: 0.1603, weighted policy: 0.1603), Train Mean Max: 0.9331
Epoch 4/10, Train Loss: 0.2194 (value: 0.0012, weighted value: 0.0607, policy: 0.1587, weighted policy: 0.1587), Train Mean Max: 0.9333
Epoch 5/10, Train Loss: 0.2144 (value: 0.0011, weighted value: 0.0571, policy: 0.1573, weighted policy: 0.1573), Train Mean Max: 0.9336
Epoch 6/10, Train Loss: 0.2114 (value: 0.0011, weighted value: 0.0548, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9337
Epoch 7/10, Train Loss: 0.2098 (value: 0.0011, weighted value: 0.0533, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9339
Epoch 8/10, Train Loss: 0.2082 (value: 0.0010, weighted value: 0.0522, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9341
Epoch 9/10, Train Loss: 0.2072 (value: 0.0010, weighted value: 0.0522, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9344
Epoch 10/10, Train Loss: 0.2045 (value: 0.0010, weighted value: 0.0491, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9346
..training done in 60.41 seconds
..evaluation done in 19.01 seconds
Old network+MCTS average reward: 0.7135, min: 0.1204, max: 1.4537, stdev: 0.2280
New network+MCTS average reward: 0.7120, min: 0.0463, max: 1.4167, stdev: 0.2282
Old bare network average reward: 0.6708, min: 0.0741, max: 1.3611, stdev: 0.2347
New bare network average reward: 0.6632, min: 0.0463, max: 1.4167, stdev: 0.2348
External policy "random" average reward: 0.2683, min: -0.5185, max: 1.0741, stdev: 0.2332
External policy "individual greedy" average reward: 0.5469, min: -0.1389, max: 1.3241, stdev: 0.2253
External policy "total greedy" average reward: 0.6641, min: 0.0463, max: 1.3333, stdev: 0.2205
New network won 69 and tied 154 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 266 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.11 seconds
Training examples lengths: [64791, 64827, 64489, 65189, 64795, 64713, 64800, 64652, 64620, 64665]
Total value: 454944.93
Training on 647541 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2600 (value: 0.0017, weighted value: 0.0862, policy: 0.1738, weighted policy: 0.1738), Train Mean Max: 0.9309
Epoch 2/10, Train Loss: 0.2472 (value: 0.0016, weighted value: 0.0777, policy: 0.1695, weighted policy: 0.1695), Train Mean Max: 0.9315
Epoch 3/10, Train Loss: 0.2358 (value: 0.0014, weighted value: 0.0715, policy: 0.1643, weighted policy: 0.1643), Train Mean Max: 0.9320
Epoch 4/10, Train Loss: 0.2304 (value: 0.0014, weighted value: 0.0684, policy: 0.1620, weighted policy: 0.1620), Train Mean Max: 0.9323
Epoch 5/10, Train Loss: 0.2237 (value: 0.0013, weighted value: 0.0634, policy: 0.1603, weighted policy: 0.1603), Train Mean Max: 0.9325
Epoch 6/10, Train Loss: 0.2221 (value: 0.0013, weighted value: 0.0627, policy: 0.1594, weighted policy: 0.1594), Train Mean Max: 0.9327
Epoch 7/10, Train Loss: 0.2182 (value: 0.0012, weighted value: 0.0592, policy: 0.1591, weighted policy: 0.1591), Train Mean Max: 0.9329
Epoch 8/10, Train Loss: 0.2145 (value: 0.0011, weighted value: 0.0569, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9332
Epoch 9/10, Train Loss: 0.2140 (value: 0.0011, weighted value: 0.0566, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9333
Epoch 10/10, Train Loss: 0.2112 (value: 0.0011, weighted value: 0.0537, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9337
..training done in 68.35 seconds
..evaluation done in 18.83 seconds
Old network+MCTS average reward: 0.6749, min: 0.1296, max: 1.3611, stdev: 0.2295
New network+MCTS average reward: 0.6736, min: 0.0741, max: 1.3611, stdev: 0.2302
Old bare network average reward: 0.6240, min: -0.0185, max: 1.3611, stdev: 0.2399
New bare network average reward: 0.6275, min: 0.0741, max: 1.3611, stdev: 0.2373
External policy "random" average reward: 0.2546, min: -0.3519, max: 0.8704, stdev: 0.2276
External policy "individual greedy" average reward: 0.5207, min: -0.1204, max: 1.2870, stdev: 0.2241
External policy "total greedy" average reward: 0.6315, min: 0.0833, max: 1.3519, stdev: 0.2196
New network won 75 and tied 141 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 267 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.84 seconds
Training examples lengths: [64827, 64489, 65189, 64795, 64713, 64800, 64652, 64620, 64665, 65130]
Total value: 455070.23
Training on 647880 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2813 (value: 0.0020, weighted value: 0.0994, policy: 0.1819, weighted policy: 0.1819), Train Mean Max: 0.9297
Epoch 2/10, Train Loss: 0.2612 (value: 0.0018, weighted value: 0.0882, policy: 0.1731, weighted policy: 0.1731), Train Mean Max: 0.9307
Epoch 3/10, Train Loss: 0.2493 (value: 0.0016, weighted value: 0.0813, policy: 0.1680, weighted policy: 0.1680), Train Mean Max: 0.9309
Epoch 4/10, Train Loss: 0.2394 (value: 0.0015, weighted value: 0.0747, policy: 0.1647, weighted policy: 0.1647), Train Mean Max: 0.9313
Epoch 5/10, Train Loss: 0.2339 (value: 0.0014, weighted value: 0.0706, policy: 0.1633, weighted policy: 0.1633), Train Mean Max: 0.9317
Epoch 6/10, Train Loss: 0.2294 (value: 0.0013, weighted value: 0.0674, policy: 0.1619, weighted policy: 0.1619), Train Mean Max: 0.9319
Epoch 7/10, Train Loss: 0.2267 (value: 0.0013, weighted value: 0.0656, policy: 0.1611, weighted policy: 0.1611), Train Mean Max: 0.9321
Epoch 8/10, Train Loss: 0.2229 (value: 0.0012, weighted value: 0.0619, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9323
Epoch 9/10, Train Loss: 0.2182 (value: 0.0012, weighted value: 0.0589, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9327
Epoch 10/10, Train Loss: 0.2175 (value: 0.0012, weighted value: 0.0586, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9327
..training done in 60.13 seconds
..evaluation done in 18.53 seconds
Old network+MCTS average reward: 0.6874, min: 0.0833, max: 1.3056, stdev: 0.2251
New network+MCTS average reward: 0.6945, min: 0.1944, max: 1.3611, stdev: 0.2221
Old bare network average reward: 0.6467, min: 0.0833, max: 1.3056, stdev: 0.2343
New bare network average reward: 0.6460, min: 0.1389, max: 1.3056, stdev: 0.2307
External policy "random" average reward: 0.2597, min: -0.2593, max: 0.9074, stdev: 0.2144
External policy "individual greedy" average reward: 0.5179, min: 0.0370, max: 1.1574, stdev: 0.2205
External policy "total greedy" average reward: 0.6347, min: 0.1389, max: 1.2685, stdev: 0.2151
New network won 86 and tied 149 out of 300 games (53.50% wins where ties are half wins)
Keeping the new network

Training iteration 268 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.93 seconds
Training examples lengths: [64489, 65189, 64795, 64713, 64800, 64652, 64620, 64665, 65130, 64749]
Total value: 454946.54
Training on 647802 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2389 (value: 0.0014, weighted value: 0.0714, policy: 0.1675, weighted policy: 0.1675), Train Mean Max: 0.9318
Epoch 2/10, Train Loss: 0.2289 (value: 0.0013, weighted value: 0.0659, policy: 0.1630, weighted policy: 0.1630), Train Mean Max: 0.9323
Epoch 3/10, Train Loss: 0.2239 (value: 0.0013, weighted value: 0.0627, policy: 0.1612, weighted policy: 0.1612), Train Mean Max: 0.9325
Epoch 4/10, Train Loss: 0.2184 (value: 0.0012, weighted value: 0.0595, policy: 0.1589, weighted policy: 0.1589), Train Mean Max: 0.9330
Epoch 5/10, Train Loss: 0.2169 (value: 0.0012, weighted value: 0.0580, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9330
Epoch 6/10, Train Loss: 0.2137 (value: 0.0011, weighted value: 0.0559, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9332
Epoch 7/10, Train Loss: 0.2128 (value: 0.0011, weighted value: 0.0548, policy: 0.1580, weighted policy: 0.1580), Train Mean Max: 0.9333
Epoch 8/10, Train Loss: 0.2078 (value: 0.0010, weighted value: 0.0512, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9338
Epoch 9/10, Train Loss: 0.2074 (value: 0.0010, weighted value: 0.0509, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9339
Epoch 10/10, Train Loss: 0.2054 (value: 0.0010, weighted value: 0.0499, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9340
..training done in 60.15 seconds
..evaluation done in 18.55 seconds
Old network+MCTS average reward: 0.7012, min: 0.1944, max: 1.7685, stdev: 0.2287
New network+MCTS average reward: 0.6985, min: 0.1667, max: 1.5556, stdev: 0.2307
Old bare network average reward: 0.6530, min: 0.1111, max: 1.5185, stdev: 0.2366
New bare network average reward: 0.6541, min: 0.1111, max: 1.4444, stdev: 0.2359
External policy "random" average reward: 0.2621, min: -0.3889, max: 1.3056, stdev: 0.2374
External policy "individual greedy" average reward: 0.5575, min: 0.0185, max: 1.4074, stdev: 0.2270
External policy "total greedy" average reward: 0.6515, min: 0.1389, max: 1.5926, stdev: 0.2250
New network won 70 and tied 151 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 269 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.30 seconds
Training examples lengths: [65189, 64795, 64713, 64800, 64652, 64620, 64665, 65130, 64749, 64803]
Total value: 454868.68
Training on 648116 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2631 (value: 0.0018, weighted value: 0.0876, policy: 0.1755, weighted policy: 0.1755), Train Mean Max: 0.9303
Epoch 2/10, Train Loss: 0.2459 (value: 0.0015, weighted value: 0.0766, policy: 0.1693, weighted policy: 0.1693), Train Mean Max: 0.9311
Epoch 3/10, Train Loss: 0.2377 (value: 0.0015, weighted value: 0.0729, policy: 0.1648, weighted policy: 0.1648), Train Mean Max: 0.9315
Epoch 4/10, Train Loss: 0.2313 (value: 0.0014, weighted value: 0.0678, policy: 0.1635, weighted policy: 0.1635), Train Mean Max: 0.9317
Epoch 5/10, Train Loss: 0.2251 (value: 0.0013, weighted value: 0.0638, policy: 0.1612, weighted policy: 0.1612), Train Mean Max: 0.9321
Epoch 6/10, Train Loss: 0.2228 (value: 0.0012, weighted value: 0.0620, policy: 0.1608, weighted policy: 0.1608), Train Mean Max: 0.9323
Epoch 7/10, Train Loss: 0.2198 (value: 0.0012, weighted value: 0.0598, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9325
Epoch 8/10, Train Loss: 0.2147 (value: 0.0011, weighted value: 0.0565, policy: 0.1582, weighted policy: 0.1582), Train Mean Max: 0.9328
Epoch 9/10, Train Loss: 0.2156 (value: 0.0011, weighted value: 0.0570, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9329
Epoch 10/10, Train Loss: 0.2114 (value: 0.0011, weighted value: 0.0536, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9332
..training done in 69.01 seconds
..evaluation done in 19.70 seconds
Old network+MCTS average reward: 0.6796, min: 0.0648, max: 1.2870, stdev: 0.2245
New network+MCTS average reward: 0.6781, min: 0.0833, max: 1.2870, stdev: 0.2280
Old bare network average reward: 0.6268, min: 0.0093, max: 1.2407, stdev: 0.2350
New bare network average reward: 0.6285, min: 0.0648, max: 1.2407, stdev: 0.2331
External policy "random" average reward: 0.2634, min: -0.4630, max: 0.8519, stdev: 0.2277
External policy "individual greedy" average reward: 0.5234, min: -0.1111, max: 1.1667, stdev: 0.2279
External policy "total greedy" average reward: 0.6371, min: 0.0833, max: 1.2500, stdev: 0.2198
New network won 68 and tied 160 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 270 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.33 seconds
Training examples lengths: [64795, 64713, 64800, 64652, 64620, 64665, 65130, 64749, 64803, 64788]
Total value: 455417.73
Training on 647715 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2842 (value: 0.0020, weighted value: 0.1011, policy: 0.1831, weighted policy: 0.1831), Train Mean Max: 0.9293
Epoch 2/10, Train Loss: 0.2619 (value: 0.0018, weighted value: 0.0886, policy: 0.1733, weighted policy: 0.1733), Train Mean Max: 0.9301
Epoch 3/10, Train Loss: 0.2487 (value: 0.0016, weighted value: 0.0798, policy: 0.1688, weighted policy: 0.1688), Train Mean Max: 0.9305
Epoch 4/10, Train Loss: 0.2413 (value: 0.0015, weighted value: 0.0757, policy: 0.1656, weighted policy: 0.1656), Train Mean Max: 0.9310
Epoch 5/10, Train Loss: 0.2363 (value: 0.0014, weighted value: 0.0715, policy: 0.1647, weighted policy: 0.1647), Train Mean Max: 0.9311
Epoch 6/10, Train Loss: 0.2302 (value: 0.0013, weighted value: 0.0674, policy: 0.1629, weighted policy: 0.1629), Train Mean Max: 0.9316
Epoch 7/10, Train Loss: 0.2269 (value: 0.0013, weighted value: 0.0652, policy: 0.1617, weighted policy: 0.1617), Train Mean Max: 0.9317
Epoch 8/10, Train Loss: 0.2228 (value: 0.0012, weighted value: 0.0618, policy: 0.1609, weighted policy: 0.1609), Train Mean Max: 0.9320
Epoch 9/10, Train Loss: 0.2189 (value: 0.0012, weighted value: 0.0596, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9323
Epoch 10/10, Train Loss: 0.2174 (value: 0.0012, weighted value: 0.0580, policy: 0.1594, weighted policy: 0.1594), Train Mean Max: 0.9325
..training done in 70.13 seconds
..evaluation done in 18.71 seconds
Old network+MCTS average reward: 0.7113, min: 0.0741, max: 1.3889, stdev: 0.2387
New network+MCTS average reward: 0.7145, min: 0.0648, max: 1.2778, stdev: 0.2389
Old bare network average reward: 0.6660, min: 0.0463, max: 1.3148, stdev: 0.2502
New bare network average reward: 0.6660, min: 0.0463, max: 1.2778, stdev: 0.2438
External policy "random" average reward: 0.2845, min: -0.2963, max: 0.9074, stdev: 0.2504
External policy "individual greedy" average reward: 0.5504, min: 0.0093, max: 1.0926, stdev: 0.2342
External policy "total greedy" average reward: 0.6677, min: 0.0741, max: 1.2870, stdev: 0.2305
New network won 80 and tied 140 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 271 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.75 seconds
Training examples lengths: [64713, 64800, 64652, 64620, 64665, 65130, 64749, 64803, 64788, 64537]
Total value: 455567.55
Training on 647457 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2386 (value: 0.0014, weighted value: 0.0710, policy: 0.1676, weighted policy: 0.1676), Train Mean Max: 0.9315
Epoch 2/10, Train Loss: 0.2292 (value: 0.0013, weighted value: 0.0654, policy: 0.1638, weighted policy: 0.1638), Train Mean Max: 0.9321
Epoch 3/10, Train Loss: 0.2255 (value: 0.0013, weighted value: 0.0630, policy: 0.1625, weighted policy: 0.1625), Train Mean Max: 0.9323
Epoch 4/10, Train Loss: 0.2205 (value: 0.0012, weighted value: 0.0600, policy: 0.1605, weighted policy: 0.1605), Train Mean Max: 0.9328
Epoch 5/10, Train Loss: 0.2170 (value: 0.0012, weighted value: 0.0577, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9328
Epoch 6/10, Train Loss: 0.2130 (value: 0.0011, weighted value: 0.0545, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9331
Epoch 7/10, Train Loss: 0.2110 (value: 0.0011, weighted value: 0.0538, policy: 0.1572, weighted policy: 0.1572), Train Mean Max: 0.9334
Epoch 8/10, Train Loss: 0.2089 (value: 0.0010, weighted value: 0.0515, policy: 0.1573, weighted policy: 0.1573), Train Mean Max: 0.9335
Epoch 9/10, Train Loss: 0.2088 (value: 0.0010, weighted value: 0.0515, policy: 0.1573, weighted policy: 0.1573), Train Mean Max: 0.9338
Epoch 10/10, Train Loss: 0.2056 (value: 0.0010, weighted value: 0.0486, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9339
..training done in 70.74 seconds
..evaluation done in 18.41 seconds
Old network+MCTS average reward: 0.6706, min: 0.1389, max: 1.3704, stdev: 0.2111
New network+MCTS average reward: 0.6703, min: 0.1759, max: 1.3519, stdev: 0.2119
Old bare network average reward: 0.6287, min: 0.0926, max: 1.3333, stdev: 0.2168
New bare network average reward: 0.6267, min: 0.0185, max: 1.3519, stdev: 0.2166
External policy "random" average reward: 0.2348, min: -0.3426, max: 0.9722, stdev: 0.2151
External policy "individual greedy" average reward: 0.5048, min: 0.0093, max: 1.3241, stdev: 0.2145
External policy "total greedy" average reward: 0.6332, min: 0.1389, max: 1.4167, stdev: 0.2138
New network won 76 and tied 144 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 272 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.59 seconds
Training examples lengths: [64800, 64652, 64620, 64665, 65130, 64749, 64803, 64788, 64537, 64759]
Total value: 456240.25
Training on 647503 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2609 (value: 0.0017, weighted value: 0.0850, policy: 0.1759, weighted policy: 0.1759), Train Mean Max: 0.9305
Epoch 2/10, Train Loss: 0.2451 (value: 0.0015, weighted value: 0.0752, policy: 0.1699, weighted policy: 0.1699), Train Mean Max: 0.9312
Epoch 3/10, Train Loss: 0.2372 (value: 0.0014, weighted value: 0.0718, policy: 0.1653, weighted policy: 0.1653), Train Mean Max: 0.9316
Epoch 4/10, Train Loss: 0.2285 (value: 0.0013, weighted value: 0.0659, policy: 0.1627, weighted policy: 0.1627), Train Mean Max: 0.9320
Epoch 5/10, Train Loss: 0.2262 (value: 0.0013, weighted value: 0.0646, policy: 0.1616, weighted policy: 0.1616), Train Mean Max: 0.9320
Epoch 6/10, Train Loss: 0.2221 (value: 0.0012, weighted value: 0.0606, policy: 0.1615, weighted policy: 0.1615), Train Mean Max: 0.9321
Epoch 7/10, Train Loss: 0.2179 (value: 0.0012, weighted value: 0.0583, policy: 0.1596, weighted policy: 0.1596), Train Mean Max: 0.9327
Epoch 8/10, Train Loss: 0.2153 (value: 0.0011, weighted value: 0.0568, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9329
Epoch 9/10, Train Loss: 0.2131 (value: 0.0011, weighted value: 0.0543, policy: 0.1587, weighted policy: 0.1587), Train Mean Max: 0.9331
Epoch 10/10, Train Loss: 0.2121 (value: 0.0011, weighted value: 0.0543, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9333
..training done in 61.23 seconds
..evaluation done in 18.96 seconds
Old network+MCTS average reward: 0.7071, min: 0.1019, max: 1.3426, stdev: 0.2194
New network+MCTS average reward: 0.7001, min: 0.1481, max: 1.3426, stdev: 0.2190
Old bare network average reward: 0.6523, min: 0.1111, max: 1.3241, stdev: 0.2265
New bare network average reward: 0.6525, min: 0.1019, max: 1.3426, stdev: 0.2251
External policy "random" average reward: 0.2709, min: -0.2963, max: 0.9630, stdev: 0.2276
External policy "individual greedy" average reward: 0.5319, min: 0.0093, max: 1.1389, stdev: 0.2190
External policy "total greedy" average reward: 0.6395, min: 0.1481, max: 1.3056, stdev: 0.2139
New network won 65 and tied 142 out of 300 games (45.33% wins where ties are half wins)
Reverting to the old network

Training iteration 273 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.30 seconds
Training examples lengths: [64652, 64620, 64665, 65130, 64749, 64803, 64788, 64537, 64759, 65086]
Total value: 456510.25
Training on 647789 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2831 (value: 0.0020, weighted value: 0.1001, policy: 0.1829, weighted policy: 0.1829), Train Mean Max: 0.9294
Epoch 2/10, Train Loss: 0.2613 (value: 0.0018, weighted value: 0.0882, policy: 0.1730, weighted policy: 0.1730), Train Mean Max: 0.9304
Epoch 3/10, Train Loss: 0.2486 (value: 0.0016, weighted value: 0.0802, policy: 0.1684, weighted policy: 0.1684), Train Mean Max: 0.9308
Epoch 4/10, Train Loss: 0.2406 (value: 0.0015, weighted value: 0.0751, policy: 0.1655, weighted policy: 0.1655), Train Mean Max: 0.9310
Epoch 5/10, Train Loss: 0.2344 (value: 0.0014, weighted value: 0.0704, policy: 0.1640, weighted policy: 0.1640), Train Mean Max: 0.9313
Epoch 6/10, Train Loss: 0.2298 (value: 0.0013, weighted value: 0.0670, policy: 0.1627, weighted policy: 0.1627), Train Mean Max: 0.9315
Epoch 7/10, Train Loss: 0.2256 (value: 0.0013, weighted value: 0.0643, policy: 0.1613, weighted policy: 0.1613), Train Mean Max: 0.9321
Epoch 8/10, Train Loss: 0.2226 (value: 0.0012, weighted value: 0.0625, policy: 0.1601, weighted policy: 0.1601), Train Mean Max: 0.9321
Epoch 9/10, Train Loss: 0.2196 (value: 0.0012, weighted value: 0.0597, policy: 0.1599, weighted policy: 0.1599), Train Mean Max: 0.9323
Epoch 10/10, Train Loss: 0.2162 (value: 0.0011, weighted value: 0.0568, policy: 0.1594, weighted policy: 0.1594), Train Mean Max: 0.9326
..training done in 60.83 seconds
..evaluation done in 19.08 seconds
Old network+MCTS average reward: 0.6983, min: 0.1574, max: 1.3426, stdev: 0.2112
New network+MCTS average reward: 0.6965, min: 0.1667, max: 1.3981, stdev: 0.2077
Old bare network average reward: 0.6514, min: 0.1389, max: 1.3519, stdev: 0.2102
New bare network average reward: 0.6465, min: 0.0463, max: 1.2778, stdev: 0.2091
External policy "random" average reward: 0.2628, min: -0.3426, max: 0.9444, stdev: 0.2077
External policy "individual greedy" average reward: 0.5303, min: -0.0556, max: 1.3889, stdev: 0.2119
External policy "total greedy" average reward: 0.6519, min: 0.1852, max: 1.2963, stdev: 0.1934
New network won 76 and tied 139 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 274 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.02 seconds
Training examples lengths: [64620, 64665, 65130, 64749, 64803, 64788, 64537, 64759, 65086, 64835]
Total value: 456887.86
Training on 647972 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3027 (value: 0.0023, weighted value: 0.1140, policy: 0.1887, weighted policy: 0.1887), Train Mean Max: 0.9283
Epoch 2/10, Train Loss: 0.2762 (value: 0.0020, weighted value: 0.0978, policy: 0.1784, weighted policy: 0.1784), Train Mean Max: 0.9295
Epoch 3/10, Train Loss: 0.2588 (value: 0.0017, weighted value: 0.0873, policy: 0.1715, weighted policy: 0.1715), Train Mean Max: 0.9298
Epoch 4/10, Train Loss: 0.2515 (value: 0.0017, weighted value: 0.0836, policy: 0.1679, weighted policy: 0.1679), Train Mean Max: 0.9302
Epoch 5/10, Train Loss: 0.2416 (value: 0.0015, weighted value: 0.0763, policy: 0.1653, weighted policy: 0.1653), Train Mean Max: 0.9307
Epoch 6/10, Train Loss: 0.2369 (value: 0.0014, weighted value: 0.0721, policy: 0.1648, weighted policy: 0.1648), Train Mean Max: 0.9309
Epoch 7/10, Train Loss: 0.2326 (value: 0.0014, weighted value: 0.0694, policy: 0.1632, weighted policy: 0.1632), Train Mean Max: 0.9311
Epoch 8/10, Train Loss: 0.2289 (value: 0.0013, weighted value: 0.0663, policy: 0.1626, weighted policy: 0.1626), Train Mean Max: 0.9314
Epoch 9/10, Train Loss: 0.2262 (value: 0.0013, weighted value: 0.0644, policy: 0.1618, weighted policy: 0.1618), Train Mean Max: 0.9315
Epoch 10/10, Train Loss: 0.2218 (value: 0.0012, weighted value: 0.0611, policy: 0.1607, weighted policy: 0.1607), Train Mean Max: 0.9320
..training done in 66.96 seconds
..evaluation done in 25.31 seconds
Old network+MCTS average reward: 0.6895, min: 0.1296, max: 1.3148, stdev: 0.2249
New network+MCTS average reward: 0.6900, min: 0.1296, max: 1.4259, stdev: 0.2274
Old bare network average reward: 0.6423, min: 0.1296, max: 1.2870, stdev: 0.2267
New bare network average reward: 0.6398, min: 0.0833, max: 1.2870, stdev: 0.2289
External policy "random" average reward: 0.2571, min: -0.2870, max: 1.0185, stdev: 0.2383
External policy "individual greedy" average reward: 0.5331, min: -0.1019, max: 1.4352, stdev: 0.2281
External policy "total greedy" average reward: 0.6471, min: 0.1111, max: 1.5370, stdev: 0.2217
New network won 74 and tied 151 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 275 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.97 seconds
Training examples lengths: [64665, 65130, 64749, 64803, 64788, 64537, 64759, 65086, 64835, 64725]
Total value: 456509.09
Training on 648077 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3229 (value: 0.0026, weighted value: 0.1297, policy: 0.1932, weighted policy: 0.1932), Train Mean Max: 0.9277
Epoch 2/10, Train Loss: 0.2894 (value: 0.0021, weighted value: 0.1070, policy: 0.1824, weighted policy: 0.1824), Train Mean Max: 0.9287
Epoch 3/10, Train Loss: 0.2713 (value: 0.0019, weighted value: 0.0969, policy: 0.1744, weighted policy: 0.1744), Train Mean Max: 0.9292
Epoch 4/10, Train Loss: 0.2593 (value: 0.0018, weighted value: 0.0891, policy: 0.1702, weighted policy: 0.1702), Train Mean Max: 0.9296
Epoch 5/10, Train Loss: 0.2503 (value: 0.0017, weighted value: 0.0832, policy: 0.1671, weighted policy: 0.1671), Train Mean Max: 0.9300
Epoch 6/10, Train Loss: 0.2446 (value: 0.0016, weighted value: 0.0790, policy: 0.1656, weighted policy: 0.1656), Train Mean Max: 0.9301
Epoch 7/10, Train Loss: 0.2389 (value: 0.0015, weighted value: 0.0740, policy: 0.1648, weighted policy: 0.1648), Train Mean Max: 0.9304
Epoch 8/10, Train Loss: 0.2334 (value: 0.0014, weighted value: 0.0697, policy: 0.1637, weighted policy: 0.1637), Train Mean Max: 0.9308
Epoch 9/10, Train Loss: 0.2314 (value: 0.0014, weighted value: 0.0684, policy: 0.1630, weighted policy: 0.1630), Train Mean Max: 0.9311
Epoch 10/10, Train Loss: 0.2264 (value: 0.0013, weighted value: 0.0649, policy: 0.1615, weighted policy: 0.1615), Train Mean Max: 0.9315
..training done in 70.07 seconds
..evaluation done in 21.79 seconds
Old network+MCTS average reward: 0.6955, min: 0.1852, max: 1.3981, stdev: 0.2255
New network+MCTS average reward: 0.6995, min: 0.0833, max: 1.3981, stdev: 0.2283
Old bare network average reward: 0.6518, min: 0.0833, max: 1.3981, stdev: 0.2254
New bare network average reward: 0.6524, min: 0.0463, max: 1.3981, stdev: 0.2231
External policy "random" average reward: 0.2632, min: -0.3426, max: 0.8148, stdev: 0.2260
External policy "individual greedy" average reward: 0.5261, min: 0.0185, max: 1.1759, stdev: 0.2236
External policy "total greedy" average reward: 0.6452, min: 0.0000, max: 1.3704, stdev: 0.2214
New network won 83 and tied 146 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network

Training iteration 276 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.93 seconds
Training examples lengths: [65130, 64749, 64803, 64788, 64537, 64759, 65086, 64835, 64725, 64972]
Total value: 457828.25
Training on 648384 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2445 (value: 0.0015, weighted value: 0.0774, policy: 0.1672, weighted policy: 0.1672), Train Mean Max: 0.9307
Epoch 2/10, Train Loss: 0.2365 (value: 0.0014, weighted value: 0.0713, policy: 0.1652, weighted policy: 0.1652), Train Mean Max: 0.9312
Epoch 3/10, Train Loss: 0.2296 (value: 0.0013, weighted value: 0.0672, policy: 0.1624, weighted policy: 0.1624), Train Mean Max: 0.9315
Epoch 4/10, Train Loss: 0.2252 (value: 0.0013, weighted value: 0.0642, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9319
Epoch 5/10, Train Loss: 0.2212 (value: 0.0012, weighted value: 0.0607, policy: 0.1605, weighted policy: 0.1605), Train Mean Max: 0.9321
Epoch 6/10, Train Loss: 0.2188 (value: 0.0012, weighted value: 0.0591, policy: 0.1596, weighted policy: 0.1596), Train Mean Max: 0.9325
Epoch 7/10, Train Loss: 0.2156 (value: 0.0011, weighted value: 0.0569, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9327
Epoch 8/10, Train Loss: 0.2139 (value: 0.0011, weighted value: 0.0553, policy: 0.1587, weighted policy: 0.1587), Train Mean Max: 0.9328
Epoch 9/10, Train Loss: 0.2122 (value: 0.0011, weighted value: 0.0547, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9332
Epoch 10/10, Train Loss: 0.2080 (value: 0.0010, weighted value: 0.0516, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9334
..training done in 65.55 seconds
..evaluation done in 19.49 seconds
Old network+MCTS average reward: 0.6872, min: 0.0463, max: 1.5278, stdev: 0.2359
New network+MCTS average reward: 0.6893, min: 0.1019, max: 1.4722, stdev: 0.2323
Old bare network average reward: 0.6413, min: -0.0185, max: 1.4630, stdev: 0.2427
New bare network average reward: 0.6458, min: 0.0000, max: 1.4722, stdev: 0.2415
External policy "random" average reward: 0.2620, min: -0.2593, max: 1.1944, stdev: 0.2196
External policy "individual greedy" average reward: 0.5441, min: 0.0463, max: 1.5185, stdev: 0.2252
External policy "total greedy" average reward: 0.6591, min: 0.1389, max: 1.5648, stdev: 0.2205
New network won 79 and tied 157 out of 300 games (52.50% wins where ties are half wins)
Keeping the new network

Training iteration 277 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.93 seconds
Training examples lengths: [64749, 64803, 64788, 64537, 64759, 65086, 64835, 64725, 64972, 65019]
Total value: 458707.78
Training on 648273 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2317 (value: 0.0013, weighted value: 0.0667, policy: 0.1650, weighted policy: 0.1650), Train Mean Max: 0.9321
Epoch 2/10, Train Loss: 0.2240 (value: 0.0013, weighted value: 0.0626, policy: 0.1614, weighted policy: 0.1614), Train Mean Max: 0.9326
Epoch 3/10, Train Loss: 0.2171 (value: 0.0012, weighted value: 0.0585, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9332
Epoch 4/10, Train Loss: 0.2147 (value: 0.0011, weighted value: 0.0554, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9332
Epoch 5/10, Train Loss: 0.2114 (value: 0.0011, weighted value: 0.0540, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9336
Epoch 6/10, Train Loss: 0.2084 (value: 0.0010, weighted value: 0.0521, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9338
Epoch 7/10, Train Loss: 0.2066 (value: 0.0010, weighted value: 0.0507, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9339
Epoch 8/10, Train Loss: 0.2043 (value: 0.0010, weighted value: 0.0496, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9343
Epoch 9/10, Train Loss: 0.2036 (value: 0.0010, weighted value: 0.0487, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9342
Epoch 10/10, Train Loss: 0.2009 (value: 0.0009, weighted value: 0.0466, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9345
..training done in 66.68 seconds
..evaluation done in 19.04 seconds
Old network+MCTS average reward: 0.7108, min: 0.1111, max: 1.5278, stdev: 0.2458
New network+MCTS average reward: 0.7098, min: 0.1111, max: 1.5278, stdev: 0.2460
Old bare network average reward: 0.6619, min: -0.1574, max: 1.4722, stdev: 0.2507
New bare network average reward: 0.6642, min: -0.1574, max: 1.4722, stdev: 0.2512
External policy "random" average reward: 0.2592, min: -0.4074, max: 1.0926, stdev: 0.2188
External policy "individual greedy" average reward: 0.5379, min: -0.0741, max: 1.2500, stdev: 0.2369
External policy "total greedy" average reward: 0.6620, min: 0.1019, max: 1.3333, stdev: 0.2355
New network won 66 and tied 158 out of 300 games (48.33% wins where ties are half wins)
Reverting to the old network

Training iteration 278 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.55 seconds
Training examples lengths: [64803, 64788, 64537, 64759, 65086, 64835, 64725, 64972, 65019, 64934]
Total value: 459457.98
Training on 648458 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2550 (value: 0.0016, weighted value: 0.0805, policy: 0.1745, weighted policy: 0.1745), Train Mean Max: 0.9309
Epoch 2/10, Train Loss: 0.2400 (value: 0.0014, weighted value: 0.0723, policy: 0.1677, weighted policy: 0.1677), Train Mean Max: 0.9318
Epoch 3/10, Train Loss: 0.2300 (value: 0.0014, weighted value: 0.0679, policy: 0.1621, weighted policy: 0.1621), Train Mean Max: 0.9322
Epoch 4/10, Train Loss: 0.2232 (value: 0.0013, weighted value: 0.0629, policy: 0.1603, weighted policy: 0.1603), Train Mean Max: 0.9325
Epoch 5/10, Train Loss: 0.2198 (value: 0.0012, weighted value: 0.0605, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9327
Epoch 6/10, Train Loss: 0.2179 (value: 0.0012, weighted value: 0.0596, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9329
Epoch 7/10, Train Loss: 0.2130 (value: 0.0011, weighted value: 0.0555, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9332
Epoch 8/10, Train Loss: 0.2114 (value: 0.0011, weighted value: 0.0543, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9333
Epoch 9/10, Train Loss: 0.2094 (value: 0.0010, weighted value: 0.0525, policy: 0.1569, weighted policy: 0.1569), Train Mean Max: 0.9336
Epoch 10/10, Train Loss: 0.2076 (value: 0.0010, weighted value: 0.0515, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9340
..training done in 66.46 seconds
..evaluation done in 19.31 seconds
Old network+MCTS average reward: 0.7072, min: -0.1759, max: 1.2500, stdev: 0.2311
New network+MCTS average reward: 0.7088, min: -0.1759, max: 1.2500, stdev: 0.2288
Old bare network average reward: 0.6591, min: -0.0648, max: 1.2407, stdev: 0.2369
New bare network average reward: 0.6627, min: -0.1111, max: 1.2407, stdev: 0.2313
External policy "random" average reward: 0.2500, min: -0.4444, max: 0.8611, stdev: 0.2200
External policy "individual greedy" average reward: 0.5460, min: 0.0000, max: 1.1852, stdev: 0.2332
External policy "total greedy" average reward: 0.6501, min: 0.0093, max: 1.1759, stdev: 0.2137
New network won 80 and tied 145 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 279 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.68 seconds
Training examples lengths: [64788, 64537, 64759, 65086, 64835, 64725, 64972, 65019, 64934, 64522]
Total value: 459469.93
Training on 648177 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2313 (value: 0.0013, weighted value: 0.0665, policy: 0.1648, weighted policy: 0.1648), Train Mean Max: 0.9326
Epoch 2/10, Train Loss: 0.2222 (value: 0.0012, weighted value: 0.0608, policy: 0.1613, weighted policy: 0.1613), Train Mean Max: 0.9331
Epoch 3/10, Train Loss: 0.2162 (value: 0.0011, weighted value: 0.0572, policy: 0.1590, weighted policy: 0.1590), Train Mean Max: 0.9335
Epoch 4/10, Train Loss: 0.2113 (value: 0.0011, weighted value: 0.0545, policy: 0.1569, weighted policy: 0.1569), Train Mean Max: 0.9339
Epoch 5/10, Train Loss: 0.2092 (value: 0.0011, weighted value: 0.0536, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9340
Epoch 6/10, Train Loss: 0.2055 (value: 0.0010, weighted value: 0.0504, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9343
Epoch 7/10, Train Loss: 0.2062 (value: 0.0010, weighted value: 0.0512, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9344
Epoch 8/10, Train Loss: 0.2029 (value: 0.0010, weighted value: 0.0480, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9345
Epoch 9/10, Train Loss: 0.2032 (value: 0.0010, weighted value: 0.0483, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9347
Epoch 10/10, Train Loss: 0.2002 (value: 0.0009, weighted value: 0.0453, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9348
..training done in 65.80 seconds
..evaluation done in 18.84 seconds
Old network+MCTS average reward: 0.7132, min: 0.1574, max: 1.5556, stdev: 0.2351
New network+MCTS average reward: 0.7061, min: 0.1389, max: 1.5556, stdev: 0.2366
Old bare network average reward: 0.6650, min: 0.0741, max: 1.5556, stdev: 0.2433
New bare network average reward: 0.6620, min: -0.0741, max: 1.5556, stdev: 0.2442
External policy "random" average reward: 0.2650, min: -0.3148, max: 0.9722, stdev: 0.2333
External policy "individual greedy" average reward: 0.5387, min: -0.0093, max: 1.3796, stdev: 0.2251
External policy "total greedy" average reward: 0.6503, min: 0.0741, max: 1.2315, stdev: 0.2188
New network won 61 and tied 152 out of 300 games (45.67% wins where ties are half wins)
Reverting to the old network

Training iteration 280 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.64 seconds
Training examples lengths: [64537, 64759, 65086, 64835, 64725, 64972, 65019, 64934, 64522, 65106]
Total value: 459791.15
Training on 648495 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2539 (value: 0.0016, weighted value: 0.0819, policy: 0.1721, weighted policy: 0.1721), Train Mean Max: 0.9313
Epoch 2/10, Train Loss: 0.2376 (value: 0.0014, weighted value: 0.0712, policy: 0.1664, weighted policy: 0.1664), Train Mean Max: 0.9322
Epoch 3/10, Train Loss: 0.2287 (value: 0.0013, weighted value: 0.0661, policy: 0.1626, weighted policy: 0.1626), Train Mean Max: 0.9325
Epoch 4/10, Train Loss: 0.2235 (value: 0.0013, weighted value: 0.0637, policy: 0.1598, weighted policy: 0.1598), Train Mean Max: 0.9328
Epoch 5/10, Train Loss: 0.2184 (value: 0.0012, weighted value: 0.0601, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9331
Epoch 6/10, Train Loss: 0.2165 (value: 0.0012, weighted value: 0.0576, policy: 0.1589, weighted policy: 0.1589), Train Mean Max: 0.9332
Epoch 7/10, Train Loss: 0.2125 (value: 0.0011, weighted value: 0.0554, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9336
Epoch 8/10, Train Loss: 0.2099 (value: 0.0011, weighted value: 0.0538, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9338
Epoch 9/10, Train Loss: 0.2098 (value: 0.0011, weighted value: 0.0528, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9338
Epoch 10/10, Train Loss: 0.2058 (value: 0.0010, weighted value: 0.0502, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9342
..training done in 59.69 seconds
..evaluation done in 18.60 seconds
Old network+MCTS average reward: 0.6950, min: 0.1296, max: 1.4167, stdev: 0.2184
New network+MCTS average reward: 0.7001, min: 0.1296, max: 1.4444, stdev: 0.2201
Old bare network average reward: 0.6590, min: 0.0370, max: 1.3889, stdev: 0.2239
New bare network average reward: 0.6577, min: 0.1296, max: 1.3889, stdev: 0.2233
External policy "random" average reward: 0.2773, min: -0.2778, max: 0.9167, stdev: 0.2086
External policy "individual greedy" average reward: 0.5389, min: 0.0370, max: 1.1204, stdev: 0.2120
External policy "total greedy" average reward: 0.6547, min: 0.1481, max: 1.3611, stdev: 0.2067
New network won 83 and tied 158 out of 300 games (54.00% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_280

Training iteration 281 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.47 seconds
Training examples lengths: [64759, 65086, 64835, 64725, 64972, 65019, 64934, 64522, 65106, 65121]
Total value: 460113.93
Training on 649079 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2294 (value: 0.0013, weighted value: 0.0644, policy: 0.1650, weighted policy: 0.1650), Train Mean Max: 0.9328
Epoch 2/10, Train Loss: 0.2219 (value: 0.0012, weighted value: 0.0607, policy: 0.1612, weighted policy: 0.1612), Train Mean Max: 0.9334
Epoch 3/10, Train Loss: 0.2145 (value: 0.0011, weighted value: 0.0560, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9337
Epoch 4/10, Train Loss: 0.2129 (value: 0.0011, weighted value: 0.0564, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9340
Epoch 5/10, Train Loss: 0.2075 (value: 0.0010, weighted value: 0.0511, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9343
Epoch 6/10, Train Loss: 0.2055 (value: 0.0010, weighted value: 0.0499, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9343
Epoch 7/10, Train Loss: 0.2053 (value: 0.0010, weighted value: 0.0500, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9346
Epoch 8/10, Train Loss: 0.2024 (value: 0.0010, weighted value: 0.0482, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9348
Epoch 9/10, Train Loss: 0.2009 (value: 0.0009, weighted value: 0.0465, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9349
Epoch 10/10, Train Loss: 0.2003 (value: 0.0009, weighted value: 0.0466, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9351
..training done in 59.78 seconds
..evaluation done in 18.74 seconds
Old network+MCTS average reward: 0.6901, min: 0.1481, max: 1.4722, stdev: 0.2235
New network+MCTS average reward: 0.6907, min: 0.1389, max: 1.4352, stdev: 0.2203
Old bare network average reward: 0.6447, min: 0.0556, max: 1.4722, stdev: 0.2283
New bare network average reward: 0.6427, min: 0.0741, max: 1.4352, stdev: 0.2282
External policy "random" average reward: 0.2577, min: -0.4259, max: 0.9167, stdev: 0.2285
External policy "individual greedy" average reward: 0.5270, min: -0.0833, max: 1.3333, stdev: 0.2336
External policy "total greedy" average reward: 0.6431, min: 0.1111, max: 1.3981, stdev: 0.2146
New network won 71 and tied 167 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 282 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.02 seconds
Training examples lengths: [65086, 64835, 64725, 64972, 65019, 64934, 64522, 65106, 65121, 64679]
Total value: 459574.01
Training on 648999 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2246 (value: 0.0012, weighted value: 0.0608, policy: 0.1639, weighted policy: 0.1639), Train Mean Max: 0.9335
Epoch 2/10, Train Loss: 0.2136 (value: 0.0011, weighted value: 0.0544, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9343
Epoch 3/10, Train Loss: 0.2095 (value: 0.0011, weighted value: 0.0526, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9345
Epoch 4/10, Train Loss: 0.2073 (value: 0.0010, weighted value: 0.0516, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9347
Epoch 5/10, Train Loss: 0.2046 (value: 0.0010, weighted value: 0.0488, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9348
Epoch 6/10, Train Loss: 0.2019 (value: 0.0010, weighted value: 0.0477, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9350
Epoch 7/10, Train Loss: 0.2003 (value: 0.0009, weighted value: 0.0465, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9352
Epoch 8/10, Train Loss: 0.1995 (value: 0.0009, weighted value: 0.0457, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9353
Epoch 9/10, Train Loss: 0.1980 (value: 0.0009, weighted value: 0.0443, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9354
Epoch 10/10, Train Loss: 0.1961 (value: 0.0009, weighted value: 0.0431, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9357
..training done in 59.50 seconds
..evaluation done in 19.58 seconds
Old network+MCTS average reward: 0.6687, min: 0.0000, max: 1.6852, stdev: 0.2334
New network+MCTS average reward: 0.6710, min: 0.0463, max: 1.6852, stdev: 0.2317
Old bare network average reward: 0.6260, min: 0.0000, max: 1.6389, stdev: 0.2390
New bare network average reward: 0.6281, min: 0.0000, max: 1.5556, stdev: 0.2389
External policy "random" average reward: 0.2344, min: -0.3241, max: 1.0833, stdev: 0.2206
External policy "individual greedy" average reward: 0.5056, min: -0.3241, max: 1.5463, stdev: 0.2285
External policy "total greedy" average reward: 0.6229, min: 0.0093, max: 1.5556, stdev: 0.2195
New network won 80 and tied 144 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 283 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.80 seconds
Training examples lengths: [64835, 64725, 64972, 65019, 64934, 64522, 65106, 65121, 64679, 64527]
Total value: 459236.70
Training on 648440 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2204 (value: 0.0012, weighted value: 0.0576, policy: 0.1628, weighted policy: 0.1628), Train Mean Max: 0.9342
Epoch 2/10, Train Loss: 0.2134 (value: 0.0011, weighted value: 0.0548, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9347
Epoch 3/10, Train Loss: 0.2072 (value: 0.0010, weighted value: 0.0508, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9350
Epoch 4/10, Train Loss: 0.2049 (value: 0.0010, weighted value: 0.0496, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9351
Epoch 5/10, Train Loss: 0.2014 (value: 0.0009, weighted value: 0.0472, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9354
Epoch 6/10, Train Loss: 0.1989 (value: 0.0009, weighted value: 0.0455, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9356
Epoch 7/10, Train Loss: 0.1977 (value: 0.0009, weighted value: 0.0441, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9356
Epoch 8/10, Train Loss: 0.1969 (value: 0.0009, weighted value: 0.0433, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9357
Epoch 9/10, Train Loss: 0.1982 (value: 0.0009, weighted value: 0.0450, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9359
Epoch 10/10, Train Loss: 0.1929 (value: 0.0008, weighted value: 0.0408, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9361
..training done in 68.09 seconds
..evaluation done in 19.38 seconds
Old network+MCTS average reward: 0.7101, min: 0.1019, max: 1.5370, stdev: 0.2223
New network+MCTS average reward: 0.7118, min: 0.0926, max: 1.5741, stdev: 0.2222
Old bare network average reward: 0.6602, min: 0.0741, max: 1.5370, stdev: 0.2260
New bare network average reward: 0.6651, min: 0.1204, max: 1.6204, stdev: 0.2292
External policy "random" average reward: 0.2574, min: -0.3426, max: 0.9537, stdev: 0.2301
External policy "individual greedy" average reward: 0.5387, min: 0.0648, max: 1.2870, stdev: 0.2232
External policy "total greedy" average reward: 0.6561, min: 0.0093, max: 1.3611, stdev: 0.2190
New network won 76 and tied 160 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network

Training iteration 284 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.87 seconds
Training examples lengths: [64725, 64972, 65019, 64934, 64522, 65106, 65121, 64679, 64527, 64783]
Total value: 459699.80
Training on 648388 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2203 (value: 0.0012, weighted value: 0.0585, policy: 0.1618, weighted policy: 0.1618), Train Mean Max: 0.9346
Epoch 2/10, Train Loss: 0.2114 (value: 0.0011, weighted value: 0.0531, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9351
Epoch 3/10, Train Loss: 0.2056 (value: 0.0010, weighted value: 0.0504, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9356
Epoch 4/10, Train Loss: 0.2032 (value: 0.0010, weighted value: 0.0495, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9356
Epoch 5/10, Train Loss: 0.1999 (value: 0.0009, weighted value: 0.0468, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9357
Epoch 6/10, Train Loss: 0.1971 (value: 0.0009, weighted value: 0.0445, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9360
Epoch 7/10, Train Loss: 0.1976 (value: 0.0009, weighted value: 0.0452, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9359
Epoch 8/10, Train Loss: 0.1953 (value: 0.0009, weighted value: 0.0428, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9360
Epoch 9/10, Train Loss: 0.1940 (value: 0.0008, weighted value: 0.0420, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9364
Epoch 10/10, Train Loss: 0.1928 (value: 0.0008, weighted value: 0.0417, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9366
..training done in 70.00 seconds
..evaluation done in 21.96 seconds
Old network+MCTS average reward: 0.7148, min: -0.0370, max: 1.5093, stdev: 0.2387
New network+MCTS average reward: 0.7183, min: -0.0278, max: 1.5093, stdev: 0.2363
Old bare network average reward: 0.6691, min: -0.0370, max: 1.5093, stdev: 0.2460
New bare network average reward: 0.6713, min: -0.1389, max: 1.5093, stdev: 0.2488
External policy "random" average reward: 0.2590, min: -0.3796, max: 1.2037, stdev: 0.2349
External policy "individual greedy" average reward: 0.5443, min: -0.1667, max: 1.5556, stdev: 0.2385
External policy "total greedy" average reward: 0.6709, min: -0.0185, max: 1.5648, stdev: 0.2272
New network won 70 and tied 169 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 285 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.54 seconds
Training examples lengths: [64972, 65019, 64934, 64522, 65106, 65121, 64679, 64527, 64783, 64892]
Total value: 460408.05
Training on 648555 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2191 (value: 0.0012, weighted value: 0.0579, policy: 0.1613, weighted policy: 0.1613), Train Mean Max: 0.9349
Epoch 2/10, Train Loss: 0.2106 (value: 0.0011, weighted value: 0.0537, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9354
Epoch 3/10, Train Loss: 0.2046 (value: 0.0010, weighted value: 0.0492, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9356
Epoch 4/10, Train Loss: 0.2016 (value: 0.0010, weighted value: 0.0489, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9361
Epoch 5/10, Train Loss: 0.1989 (value: 0.0009, weighted value: 0.0461, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9360
Epoch 6/10, Train Loss: 0.1984 (value: 0.0009, weighted value: 0.0462, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9361
Epoch 7/10, Train Loss: 0.1965 (value: 0.0009, weighted value: 0.0442, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9363
Epoch 8/10, Train Loss: 0.1949 (value: 0.0009, weighted value: 0.0432, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9364
Epoch 9/10, Train Loss: 0.1915 (value: 0.0008, weighted value: 0.0404, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9366
Epoch 10/10, Train Loss: 0.1920 (value: 0.0008, weighted value: 0.0409, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9366
..training done in 71.58 seconds
..evaluation done in 19.83 seconds
Old network+MCTS average reward: 0.7077, min: 0.1296, max: 1.6111, stdev: 0.2251
New network+MCTS average reward: 0.7107, min: 0.1944, max: 1.5556, stdev: 0.2214
Old bare network average reward: 0.6656, min: 0.1574, max: 1.5093, stdev: 0.2226
New bare network average reward: 0.6681, min: 0.1389, max: 1.5093, stdev: 0.2233
External policy "random" average reward: 0.2650, min: -0.3241, max: 0.8426, stdev: 0.2196
External policy "individual greedy" average reward: 0.5489, min: 0.0000, max: 1.3519, stdev: 0.2238
External policy "total greedy" average reward: 0.6577, min: 0.1111, max: 1.3426, stdev: 0.2157
New network won 73 and tied 156 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 286 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 60.10 seconds
Training examples lengths: [65019, 64934, 64522, 65106, 65121, 64679, 64527, 64783, 64892, 64786]
Total value: 459992.81
Training on 648369 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2176 (value: 0.0012, weighted value: 0.0575, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9351
Epoch 2/10, Train Loss: 0.2078 (value: 0.0010, weighted value: 0.0509, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9356
Epoch 3/10, Train Loss: 0.2036 (value: 0.0010, weighted value: 0.0489, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9359
Epoch 4/10, Train Loss: 0.2012 (value: 0.0009, weighted value: 0.0473, policy: 0.1539, weighted policy: 0.1539), Train Mean Max: 0.9360
Epoch 5/10, Train Loss: 0.1985 (value: 0.0009, weighted value: 0.0463, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9364
Epoch 6/10, Train Loss: 0.1952 (value: 0.0009, weighted value: 0.0443, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9365
Epoch 7/10, Train Loss: 0.1938 (value: 0.0009, weighted value: 0.0428, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9365
Epoch 8/10, Train Loss: 0.1933 (value: 0.0008, weighted value: 0.0424, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9366
Epoch 9/10, Train Loss: 0.1911 (value: 0.0008, weighted value: 0.0405, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9368
Epoch 10/10, Train Loss: 0.1906 (value: 0.0008, weighted value: 0.0399, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9369
..training done in 66.55 seconds
..evaluation done in 19.14 seconds
Old network+MCTS average reward: 0.6963, min: 0.1296, max: 1.3889, stdev: 0.2146
New network+MCTS average reward: 0.6948, min: 0.1389, max: 1.2593, stdev: 0.2119
Old bare network average reward: 0.6502, min: 0.1019, max: 1.3889, stdev: 0.2217
New bare network average reward: 0.6495, min: 0.1019, max: 1.2593, stdev: 0.2231
External policy "random" average reward: 0.2485, min: -0.2593, max: 0.8889, stdev: 0.2143
External policy "individual greedy" average reward: 0.5327, min: -0.0278, max: 1.3333, stdev: 0.2180
External policy "total greedy" average reward: 0.6479, min: 0.0648, max: 1.3241, stdev: 0.2193
New network won 68 and tied 169 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 287 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.39 seconds
Training examples lengths: [64934, 64522, 65106, 65121, 64679, 64527, 64783, 64892, 64786, 64517]
Total value: 459373.69
Training on 647867 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2181 (value: 0.0011, weighted value: 0.0569, policy: 0.1612, weighted policy: 0.1612), Train Mean Max: 0.9353
Epoch 2/10, Train Loss: 0.2103 (value: 0.0011, weighted value: 0.0529, policy: 0.1573, weighted policy: 0.1573), Train Mean Max: 0.9358
Epoch 3/10, Train Loss: 0.2032 (value: 0.0010, weighted value: 0.0492, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9363
Epoch 4/10, Train Loss: 0.1990 (value: 0.0009, weighted value: 0.0465, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9364
Epoch 5/10, Train Loss: 0.1975 (value: 0.0009, weighted value: 0.0454, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9364
Epoch 6/10, Train Loss: 0.1951 (value: 0.0009, weighted value: 0.0437, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9366
Epoch 7/10, Train Loss: 0.1941 (value: 0.0009, weighted value: 0.0427, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9369
Epoch 8/10, Train Loss: 0.1928 (value: 0.0008, weighted value: 0.0421, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9369
Epoch 9/10, Train Loss: 0.1913 (value: 0.0008, weighted value: 0.0410, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9370
Epoch 10/10, Train Loss: 0.1905 (value: 0.0008, weighted value: 0.0402, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9370
..training done in 59.74 seconds
..evaluation done in 18.40 seconds
Old network+MCTS average reward: 0.7059, min: 0.0278, max: 1.4074, stdev: 0.2510
New network+MCTS average reward: 0.7011, min: 0.0278, max: 1.3704, stdev: 0.2479
Old bare network average reward: 0.6618, min: 0.0278, max: 1.3519, stdev: 0.2513
New bare network average reward: 0.6610, min: 0.0278, max: 1.3704, stdev: 0.2502
External policy "random" average reward: 0.2597, min: -0.3426, max: 0.8889, stdev: 0.2388
External policy "individual greedy" average reward: 0.5465, min: 0.0185, max: 1.4444, stdev: 0.2371
External policy "total greedy" average reward: 0.6580, min: 0.0926, max: 1.3981, stdev: 0.2297
New network won 64 and tied 164 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 288 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.10 seconds
Training examples lengths: [64522, 65106, 65121, 64679, 64527, 64783, 64892, 64786, 64517, 64820]
Total value: 459125.52
Training on 647753 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2467 (value: 0.0015, weighted value: 0.0755, policy: 0.1711, weighted policy: 0.1711), Train Mean Max: 0.9334
Epoch 2/10, Train Loss: 0.2267 (value: 0.0013, weighted value: 0.0633, policy: 0.1634, weighted policy: 0.1634), Train Mean Max: 0.9346
Epoch 3/10, Train Loss: 0.2186 (value: 0.0012, weighted value: 0.0595, policy: 0.1591, weighted policy: 0.1591), Train Mean Max: 0.9350
Epoch 4/10, Train Loss: 0.2138 (value: 0.0011, weighted value: 0.0568, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9352
Epoch 5/10, Train Loss: 0.2085 (value: 0.0011, weighted value: 0.0535, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9353
Epoch 6/10, Train Loss: 0.2049 (value: 0.0010, weighted value: 0.0510, policy: 0.1539, weighted policy: 0.1539), Train Mean Max: 0.9355
Epoch 7/10, Train Loss: 0.2039 (value: 0.0010, weighted value: 0.0500, policy: 0.1539, weighted policy: 0.1539), Train Mean Max: 0.9356
Epoch 8/10, Train Loss: 0.2011 (value: 0.0010, weighted value: 0.0479, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9359
Epoch 9/10, Train Loss: 0.1991 (value: 0.0009, weighted value: 0.0465, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9360
Epoch 10/10, Train Loss: 0.1974 (value: 0.0009, weighted value: 0.0454, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9362
..training done in 60.91 seconds
..evaluation done in 19.77 seconds
Old network+MCTS average reward: 0.7175, min: 0.0648, max: 1.6759, stdev: 0.2442
New network+MCTS average reward: 0.7137, min: 0.0648, max: 1.6759, stdev: 0.2424
Old bare network average reward: 0.6708, min: 0.0833, max: 1.5926, stdev: 0.2481
New bare network average reward: 0.6729, min: 0.0463, max: 1.6204, stdev: 0.2470
External policy "random" average reward: 0.2822, min: -0.3056, max: 1.2037, stdev: 0.2344
External policy "individual greedy" average reward: 0.5555, min: 0.0000, max: 1.4630, stdev: 0.2335
External policy "total greedy" average reward: 0.6531, min: -0.0370, max: 1.5926, stdev: 0.2421
New network won 58 and tied 166 out of 300 games (47.00% wins where ties are half wins)
Reverting to the old network

Training iteration 289 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.47 seconds
Training examples lengths: [65106, 65121, 64679, 64527, 64783, 64892, 64786, 64517, 64820, 64603]
Total value: 459107.34
Training on 647834 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2682 (value: 0.0018, weighted value: 0.0883, policy: 0.1799, weighted policy: 0.1799), Train Mean Max: 0.9322
Epoch 2/10, Train Loss: 0.2454 (value: 0.0015, weighted value: 0.0770, policy: 0.1685, weighted policy: 0.1685), Train Mean Max: 0.9334
Epoch 3/10, Train Loss: 0.2316 (value: 0.0014, weighted value: 0.0686, policy: 0.1630, weighted policy: 0.1630), Train Mean Max: 0.9339
Epoch 4/10, Train Loss: 0.2243 (value: 0.0013, weighted value: 0.0644, policy: 0.1599, weighted policy: 0.1599), Train Mean Max: 0.9340
Epoch 5/10, Train Loss: 0.2196 (value: 0.0012, weighted value: 0.0613, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9342
Epoch 6/10, Train Loss: 0.2152 (value: 0.0012, weighted value: 0.0579, policy: 0.1573, weighted policy: 0.1573), Train Mean Max: 0.9342
Epoch 7/10, Train Loss: 0.2116 (value: 0.0011, weighted value: 0.0562, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9347
Epoch 8/10, Train Loss: 0.2096 (value: 0.0011, weighted value: 0.0543, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9348
Epoch 9/10, Train Loss: 0.2069 (value: 0.0010, weighted value: 0.0521, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9350
Epoch 10/10, Train Loss: 0.2032 (value: 0.0010, weighted value: 0.0490, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9352
..training done in 64.95 seconds
..evaluation done in 18.76 seconds
Old network+MCTS average reward: 0.7061, min: 0.1019, max: 1.3796, stdev: 0.2229
New network+MCTS average reward: 0.7078, min: 0.1019, max: 1.3796, stdev: 0.2229
Old bare network average reward: 0.6602, min: 0.0370, max: 1.2963, stdev: 0.2239
New bare network average reward: 0.6598, min: 0.0648, max: 1.2963, stdev: 0.2236
External policy "random" average reward: 0.2717, min: -0.3704, max: 0.8611, stdev: 0.2223
External policy "individual greedy" average reward: 0.5458, min: -0.0648, max: 1.1759, stdev: 0.2225
External policy "total greedy" average reward: 0.6527, min: 0.0556, max: 1.3148, stdev: 0.2076
New network won 72 and tied 166 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 290 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.38 seconds
Training examples lengths: [65121, 64679, 64527, 64783, 64892, 64786, 64517, 64820, 64603, 64803]
Total value: 458786.39
Training on 647531 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2294 (value: 0.0013, weighted value: 0.0664, policy: 0.1630, weighted policy: 0.1630), Train Mean Max: 0.9336
Epoch 2/10, Train Loss: 0.2186 (value: 0.0012, weighted value: 0.0582, policy: 0.1603, weighted policy: 0.1603), Train Mean Max: 0.9342
Epoch 3/10, Train Loss: 0.2141 (value: 0.0011, weighted value: 0.0574, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9346
Epoch 4/10, Train Loss: 0.2097 (value: 0.0011, weighted value: 0.0542, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9349
Epoch 5/10, Train Loss: 0.2069 (value: 0.0010, weighted value: 0.0516, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9350
Epoch 6/10, Train Loss: 0.2058 (value: 0.0010, weighted value: 0.0508, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9350
Epoch 7/10, Train Loss: 0.2032 (value: 0.0010, weighted value: 0.0494, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9354
Epoch 8/10, Train Loss: 0.1999 (value: 0.0009, weighted value: 0.0467, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9355
Epoch 9/10, Train Loss: 0.1992 (value: 0.0009, weighted value: 0.0468, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9357
Epoch 10/10, Train Loss: 0.1972 (value: 0.0009, weighted value: 0.0450, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9359
..training done in 62.69 seconds
..evaluation done in 18.85 seconds
Old network+MCTS average reward: 0.7036, min: 0.1944, max: 1.2407, stdev: 0.2048
New network+MCTS average reward: 0.6976, min: 0.2130, max: 1.2500, stdev: 0.2069
Old bare network average reward: 0.6565, min: 0.1204, max: 1.2778, stdev: 0.2129
New bare network average reward: 0.6546, min: 0.1204, max: 1.2500, stdev: 0.2148
External policy "random" average reward: 0.2535, min: -0.2315, max: 0.8426, stdev: 0.2175
External policy "individual greedy" average reward: 0.5297, min: 0.0000, max: 1.1852, stdev: 0.2076
External policy "total greedy" average reward: 0.6426, min: 0.1296, max: 1.1574, stdev: 0.2042
New network won 62 and tied 157 out of 300 games (46.83% wins where ties are half wins)
Reverting to the old network

Training iteration 291 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 60.75 seconds
Training examples lengths: [64679, 64527, 64783, 64892, 64786, 64517, 64820, 64603, 64803, 65049]
Total value: 459386.12
Training on 647459 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2522 (value: 0.0016, weighted value: 0.0814, policy: 0.1708, weighted policy: 0.1708), Train Mean Max: 0.9324
Epoch 2/10, Train Loss: 0.2361 (value: 0.0014, weighted value: 0.0718, policy: 0.1643, weighted policy: 0.1643), Train Mean Max: 0.9332
Epoch 3/10, Train Loss: 0.2265 (value: 0.0013, weighted value: 0.0659, policy: 0.1606, weighted policy: 0.1606), Train Mean Max: 0.9337
Epoch 4/10, Train Loss: 0.2212 (value: 0.0013, weighted value: 0.0633, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9339
Epoch 5/10, Train Loss: 0.2166 (value: 0.0012, weighted value: 0.0591, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9341
Epoch 6/10, Train Loss: 0.2132 (value: 0.0011, weighted value: 0.0571, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9344
Epoch 7/10, Train Loss: 0.2104 (value: 0.0011, weighted value: 0.0557, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9346
Epoch 8/10, Train Loss: 0.2088 (value: 0.0011, weighted value: 0.0537, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9348
Epoch 9/10, Train Loss: 0.2049 (value: 0.0010, weighted value: 0.0510, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9351
Epoch 10/10, Train Loss: 0.2030 (value: 0.0010, weighted value: 0.0496, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9353
..training done in 65.62 seconds
..evaluation done in 19.00 seconds
Old network+MCTS average reward: 0.7122, min: 0.0833, max: 1.5000, stdev: 0.2444
New network+MCTS average reward: 0.7094, min: 0.0833, max: 1.5000, stdev: 0.2483
Old bare network average reward: 0.6704, min: 0.0185, max: 1.4630, stdev: 0.2481
New bare network average reward: 0.6722, min: -0.0278, max: 1.4630, stdev: 0.2537
External policy "random" average reward: 0.2569, min: -0.2593, max: 1.1111, stdev: 0.2210
External policy "individual greedy" average reward: 0.5414, min: -0.1111, max: 1.2037, stdev: 0.2379
External policy "total greedy" average reward: 0.6635, min: 0.1296, max: 1.3704, stdev: 0.2333
New network won 58 and tied 155 out of 300 games (45.17% wins where ties are half wins)
Reverting to the old network

Training iteration 292 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.39 seconds
Training examples lengths: [64527, 64783, 64892, 64786, 64517, 64820, 64603, 64803, 65049, 64817]
Total value: 460063.99
Training on 647597 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2731 (value: 0.0019, weighted value: 0.0954, policy: 0.1777, weighted policy: 0.1777), Train Mean Max: 0.9313
Epoch 2/10, Train Loss: 0.2503 (value: 0.0016, weighted value: 0.0818, policy: 0.1685, weighted policy: 0.1685), Train Mean Max: 0.9323
Epoch 3/10, Train Loss: 0.2390 (value: 0.0015, weighted value: 0.0751, policy: 0.1639, weighted policy: 0.1639), Train Mean Max: 0.9328
Epoch 4/10, Train Loss: 0.2318 (value: 0.0014, weighted value: 0.0713, policy: 0.1605, weighted policy: 0.1605), Train Mean Max: 0.9331
Epoch 5/10, Train Loss: 0.2246 (value: 0.0013, weighted value: 0.0662, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9335
Epoch 6/10, Train Loss: 0.2210 (value: 0.0013, weighted value: 0.0632, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9337
Epoch 7/10, Train Loss: 0.2184 (value: 0.0012, weighted value: 0.0614, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9340
Epoch 8/10, Train Loss: 0.2134 (value: 0.0012, weighted value: 0.0576, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9342
Epoch 9/10, Train Loss: 0.2102 (value: 0.0011, weighted value: 0.0557, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9344
Epoch 10/10, Train Loss: 0.2078 (value: 0.0011, weighted value: 0.0540, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9347
..training done in 62.93 seconds
..evaluation done in 18.55 seconds
Old network+MCTS average reward: 0.6861, min: -0.0278, max: 1.5926, stdev: 0.2462
New network+MCTS average reward: 0.6860, min: 0.1204, max: 1.5093, stdev: 0.2488
Old bare network average reward: 0.6394, min: -0.0648, max: 1.5278, stdev: 0.2585
New bare network average reward: 0.6458, min: -0.0370, max: 1.5278, stdev: 0.2580
External policy "random" average reward: 0.2499, min: -0.3796, max: 0.8796, stdev: 0.2308
External policy "individual greedy" average reward: 0.5266, min: 0.0000, max: 1.3611, stdev: 0.2347
External policy "total greedy" average reward: 0.6425, min: 0.0741, max: 1.4907, stdev: 0.2285
New network won 61 and tied 162 out of 300 games (47.33% wins where ties are half wins)
Reverting to the old network

Training iteration 293 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.58 seconds
Training examples lengths: [64783, 64892, 64786, 64517, 64820, 64603, 64803, 65049, 64817, 64942]
Total value: 460951.29
Training on 648012 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2935 (value: 0.0022, weighted value: 0.1098, policy: 0.1837, weighted policy: 0.1837), Train Mean Max: 0.9301
Epoch 2/10, Train Loss: 0.2650 (value: 0.0018, weighted value: 0.0916, policy: 0.1734, weighted policy: 0.1734), Train Mean Max: 0.9313
Epoch 3/10, Train Loss: 0.2492 (value: 0.0017, weighted value: 0.0833, policy: 0.1659, weighted policy: 0.1659), Train Mean Max: 0.9320
Epoch 4/10, Train Loss: 0.2412 (value: 0.0016, weighted value: 0.0788, policy: 0.1624, weighted policy: 0.1624), Train Mean Max: 0.9323
Epoch 5/10, Train Loss: 0.2341 (value: 0.0015, weighted value: 0.0732, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9326
Epoch 6/10, Train Loss: 0.2268 (value: 0.0014, weighted value: 0.0678, policy: 0.1590, weighted policy: 0.1590), Train Mean Max: 0.9329
Epoch 7/10, Train Loss: 0.2241 (value: 0.0013, weighted value: 0.0656, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9331
Epoch 8/10, Train Loss: 0.2223 (value: 0.0013, weighted value: 0.0640, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9332
Epoch 9/10, Train Loss: 0.2161 (value: 0.0012, weighted value: 0.0599, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9337
Epoch 10/10, Train Loss: 0.2158 (value: 0.0012, weighted value: 0.0596, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9338
..training done in 60.55 seconds
..evaluation done in 18.78 seconds
Old network+MCTS average reward: 0.6802, min: 0.1296, max: 1.4722, stdev: 0.2206
New network+MCTS average reward: 0.6790, min: 0.1296, max: 1.4722, stdev: 0.2219
Old bare network average reward: 0.6420, min: 0.1296, max: 1.4444, stdev: 0.2236
New bare network average reward: 0.6419, min: 0.1296, max: 1.4444, stdev: 0.2205
External policy "random" average reward: 0.2364, min: -0.2222, max: 0.8519, stdev: 0.2111
External policy "individual greedy" average reward: 0.5309, min: 0.0463, max: 1.3426, stdev: 0.2190
External policy "total greedy" average reward: 0.6426, min: 0.1296, max: 1.2500, stdev: 0.2142
New network won 66 and tied 159 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 294 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 60.78 seconds
Training examples lengths: [64892, 64786, 64517, 64820, 64603, 64803, 65049, 64817, 64942, 64920]
Total value: 461080.05
Training on 648149 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3119 (value: 0.0024, weighted value: 0.1212, policy: 0.1906, weighted policy: 0.1906), Train Mean Max: 0.9291
Epoch 2/10, Train Loss: 0.2791 (value: 0.0020, weighted value: 0.1014, policy: 0.1778, weighted policy: 0.1778), Train Mean Max: 0.9301
Epoch 3/10, Train Loss: 0.2620 (value: 0.0019, weighted value: 0.0928, policy: 0.1693, weighted policy: 0.1693), Train Mean Max: 0.9308
Epoch 4/10, Train Loss: 0.2512 (value: 0.0017, weighted value: 0.0847, policy: 0.1664, weighted policy: 0.1664), Train Mean Max: 0.9311
Epoch 5/10, Train Loss: 0.2435 (value: 0.0016, weighted value: 0.0800, policy: 0.1635, weighted policy: 0.1635), Train Mean Max: 0.9316
Epoch 6/10, Train Loss: 0.2368 (value: 0.0015, weighted value: 0.0743, policy: 0.1625, weighted policy: 0.1625), Train Mean Max: 0.9318
Epoch 7/10, Train Loss: 0.2320 (value: 0.0014, weighted value: 0.0713, policy: 0.1607, weighted policy: 0.1607), Train Mean Max: 0.9322
Epoch 8/10, Train Loss: 0.2266 (value: 0.0013, weighted value: 0.0663, policy: 0.1603, weighted policy: 0.1603), Train Mean Max: 0.9323
Epoch 9/10, Train Loss: 0.2236 (value: 0.0013, weighted value: 0.0651, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9327
Epoch 10/10, Train Loss: 0.2195 (value: 0.0013, weighted value: 0.0628, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9330
..training done in 67.63 seconds
..evaluation done in 19.48 seconds
Old network+MCTS average reward: 0.6885, min: 0.0741, max: 1.3889, stdev: 0.2469
New network+MCTS average reward: 0.6902, min: 0.0741, max: 1.3889, stdev: 0.2460
Old bare network average reward: 0.6504, min: -0.0370, max: 1.3889, stdev: 0.2502
New bare network average reward: 0.6480, min: -0.0185, max: 1.3889, stdev: 0.2450
External policy "random" average reward: 0.2630, min: -0.2963, max: 0.8426, stdev: 0.2258
External policy "individual greedy" average reward: 0.5359, min: 0.0000, max: 1.1852, stdev: 0.2377
External policy "total greedy" average reward: 0.6610, min: 0.1296, max: 1.2685, stdev: 0.2319
New network won 84 and tied 150 out of 300 games (53.00% wins where ties are half wins)
Keeping the new network

Training iteration 295 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.37 seconds
Training examples lengths: [64786, 64517, 64820, 64603, 64803, 65049, 64817, 64942, 64920, 65115]
Total value: 461235.84
Training on 648372 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2374 (value: 0.0015, weighted value: 0.0725, policy: 0.1649, weighted policy: 0.1649), Train Mean Max: 0.9317
Epoch 2/10, Train Loss: 0.2306 (value: 0.0013, weighted value: 0.0674, policy: 0.1632, weighted policy: 0.1632), Train Mean Max: 0.9322
Epoch 3/10, Train Loss: 0.2235 (value: 0.0013, weighted value: 0.0645, policy: 0.1590, weighted policy: 0.1590), Train Mean Max: 0.9328
Epoch 4/10, Train Loss: 0.2193 (value: 0.0012, weighted value: 0.0616, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9332
Epoch 5/10, Train Loss: 0.2155 (value: 0.0012, weighted value: 0.0587, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9333
Epoch 6/10, Train Loss: 0.2128 (value: 0.0011, weighted value: 0.0563, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9336
Epoch 7/10, Train Loss: 0.2115 (value: 0.0011, weighted value: 0.0559, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9339
Epoch 8/10, Train Loss: 0.2063 (value: 0.0010, weighted value: 0.0517, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9342
Epoch 9/10, Train Loss: 0.2068 (value: 0.0010, weighted value: 0.0521, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9343
Epoch 10/10, Train Loss: 0.2027 (value: 0.0010, weighted value: 0.0488, policy: 0.1539, weighted policy: 0.1539), Train Mean Max: 0.9347
..training done in 59.69 seconds
..evaluation done in 17.99 seconds
Old network+MCTS average reward: 0.7119, min: 0.1389, max: 1.4537, stdev: 0.2153
New network+MCTS average reward: 0.7110, min: 0.1667, max: 1.4167, stdev: 0.2100
Old bare network average reward: 0.6667, min: 0.0833, max: 1.4722, stdev: 0.2192
New bare network average reward: 0.6672, min: 0.0833, max: 1.3889, stdev: 0.2183
External policy "random" average reward: 0.2734, min: -0.3241, max: 0.8426, stdev: 0.2111
External policy "individual greedy" average reward: 0.5450, min: -0.0463, max: 1.3796, stdev: 0.2234
External policy "total greedy" average reward: 0.6558, min: 0.0278, max: 1.3981, stdev: 0.2135
New network won 70 and tied 147 out of 300 games (47.83% wins where ties are half wins)
Reverting to the old network

Training iteration 296 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.21 seconds
Training examples lengths: [64517, 64820, 64603, 64803, 65049, 64817, 64942, 64920, 65115, 64496]
Total value: 460897.53
Training on 648082 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2590 (value: 0.0017, weighted value: 0.0859, policy: 0.1730, weighted policy: 0.1730), Train Mean Max: 0.9305
Epoch 2/10, Train Loss: 0.2447 (value: 0.0015, weighted value: 0.0769, policy: 0.1678, weighted policy: 0.1678), Train Mean Max: 0.9312
Epoch 3/10, Train Loss: 0.2350 (value: 0.0014, weighted value: 0.0724, policy: 0.1626, weighted policy: 0.1626), Train Mean Max: 0.9319
Epoch 4/10, Train Loss: 0.2282 (value: 0.0013, weighted value: 0.0672, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9320
Epoch 5/10, Train Loss: 0.2249 (value: 0.0013, weighted value: 0.0653, policy: 0.1596, weighted policy: 0.1596), Train Mean Max: 0.9325
Epoch 6/10, Train Loss: 0.2203 (value: 0.0012, weighted value: 0.0624, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9328
Epoch 7/10, Train Loss: 0.2186 (value: 0.0012, weighted value: 0.0612, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9329
Epoch 8/10, Train Loss: 0.2128 (value: 0.0011, weighted value: 0.0564, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9333
Epoch 9/10, Train Loss: 0.2106 (value: 0.0011, weighted value: 0.0549, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9336
Epoch 10/10, Train Loss: 0.2108 (value: 0.0011, weighted value: 0.0544, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9337
..training done in 68.99 seconds
..evaluation done in 19.19 seconds
Old network+MCTS average reward: 0.7078, min: 0.0185, max: 1.2500, stdev: 0.2208
New network+MCTS average reward: 0.7027, min: 0.0185, max: 1.2130, stdev: 0.2198
Old bare network average reward: 0.6613, min: -0.0093, max: 1.2130, stdev: 0.2258
New bare network average reward: 0.6618, min: 0.0185, max: 1.2130, stdev: 0.2275
External policy "random" average reward: 0.2538, min: -0.3889, max: 0.8889, stdev: 0.2120
External policy "individual greedy" average reward: 0.5445, min: -0.0093, max: 1.1481, stdev: 0.2238
External policy "total greedy" average reward: 0.6620, min: 0.0463, max: 1.2500, stdev: 0.2130
New network won 64 and tied 160 out of 300 games (48.00% wins where ties are half wins)
Reverting to the old network

Training iteration 297 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.33 seconds
Training examples lengths: [64820, 64603, 64803, 65049, 64817, 64942, 64920, 65115, 64496, 64595]
Total value: 461078.76
Training on 648160 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2759 (value: 0.0019, weighted value: 0.0968, policy: 0.1791, weighted policy: 0.1791), Train Mean Max: 0.9297
Epoch 2/10, Train Loss: 0.2601 (value: 0.0018, weighted value: 0.0891, policy: 0.1711, weighted policy: 0.1711), Train Mean Max: 0.9306
Epoch 3/10, Train Loss: 0.2436 (value: 0.0016, weighted value: 0.0779, policy: 0.1657, weighted policy: 0.1657), Train Mean Max: 0.9310
Epoch 4/10, Train Loss: 0.2371 (value: 0.0015, weighted value: 0.0736, policy: 0.1635, weighted policy: 0.1635), Train Mean Max: 0.9315
Epoch 5/10, Train Loss: 0.2310 (value: 0.0014, weighted value: 0.0703, policy: 0.1606, weighted policy: 0.1606), Train Mean Max: 0.9319
Epoch 6/10, Train Loss: 0.2270 (value: 0.0013, weighted value: 0.0675, policy: 0.1595, weighted policy: 0.1595), Train Mean Max: 0.9322
Epoch 7/10, Train Loss: 0.2223 (value: 0.0013, weighted value: 0.0640, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9325
Epoch 8/10, Train Loss: 0.2183 (value: 0.0012, weighted value: 0.0605, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9328
Epoch 9/10, Train Loss: 0.2167 (value: 0.0012, weighted value: 0.0588, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9330
Epoch 10/10, Train Loss: 0.2141 (value: 0.0012, weighted value: 0.0579, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9334
..training done in 60.06 seconds
..evaluation done in 18.69 seconds
Old network+MCTS average reward: 0.6980, min: -0.0648, max: 1.4444, stdev: 0.2409
New network+MCTS average reward: 0.6995, min: 0.0370, max: 1.4444, stdev: 0.2429
Old bare network average reward: 0.6527, min: -0.0741, max: 1.4444, stdev: 0.2522
New bare network average reward: 0.6535, min: -0.0741, max: 1.4444, stdev: 0.2530
External policy "random" average reward: 0.2516, min: -0.3056, max: 0.9259, stdev: 0.2311
External policy "individual greedy" average reward: 0.5366, min: -0.1296, max: 1.2963, stdev: 0.2279
External policy "total greedy" average reward: 0.6522, min: 0.0926, max: 1.2500, stdev: 0.2193
New network won 79 and tied 147 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 298 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.35 seconds
Training examples lengths: [64603, 64803, 65049, 64817, 64942, 64920, 65115, 64496, 64595, 64852]
Total value: 461740.62
Training on 648192 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2333 (value: 0.0014, weighted value: 0.0699, policy: 0.1634, weighted policy: 0.1634), Train Mean Max: 0.9326
Epoch 2/10, Train Loss: 0.2245 (value: 0.0013, weighted value: 0.0641, policy: 0.1604, weighted policy: 0.1604), Train Mean Max: 0.9331
Epoch 3/10, Train Loss: 0.2203 (value: 0.0012, weighted value: 0.0614, policy: 0.1590, weighted policy: 0.1590), Train Mean Max: 0.9334
Epoch 4/10, Train Loss: 0.2134 (value: 0.0011, weighted value: 0.0570, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9338
Epoch 5/10, Train Loss: 0.2125 (value: 0.0011, weighted value: 0.0567, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9339
Epoch 6/10, Train Loss: 0.2095 (value: 0.0011, weighted value: 0.0546, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9341
Epoch 7/10, Train Loss: 0.2058 (value: 0.0010, weighted value: 0.0517, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9344
Epoch 8/10, Train Loss: 0.2056 (value: 0.0010, weighted value: 0.0522, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9346
Epoch 9/10, Train Loss: 0.2026 (value: 0.0010, weighted value: 0.0492, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9348
Epoch 10/10, Train Loss: 0.2010 (value: 0.0010, weighted value: 0.0478, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9350
..training done in 59.63 seconds
..evaluation done in 20.24 seconds
Old network+MCTS average reward: 0.7194, min: 0.1204, max: 1.3426, stdev: 0.2213
New network+MCTS average reward: 0.7229, min: 0.1204, max: 1.3519, stdev: 0.2216
Old bare network average reward: 0.6785, min: 0.0833, max: 1.2778, stdev: 0.2233
New bare network average reward: 0.6790, min: 0.0833, max: 1.3333, stdev: 0.2239
External policy "random" average reward: 0.2781, min: -0.2963, max: 1.0000, stdev: 0.2188
External policy "individual greedy" average reward: 0.5507, min: -0.0093, max: 1.2963, stdev: 0.2277
External policy "total greedy" average reward: 0.6694, min: 0.1667, max: 1.4074, stdev: 0.2162
New network won 76 and tied 152 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 299 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.99 seconds
Training examples lengths: [64803, 65049, 64817, 64942, 64920, 65115, 64496, 64595, 64852, 64900]
Total value: 463148.69
Training on 648489 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2223 (value: 0.0012, weighted value: 0.0619, policy: 0.1605, weighted policy: 0.1605), Train Mean Max: 0.9341
Epoch 2/10, Train Loss: 0.2151 (value: 0.0011, weighted value: 0.0572, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9344
Epoch 3/10, Train Loss: 0.2082 (value: 0.0011, weighted value: 0.0535, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9351
Epoch 4/10, Train Loss: 0.2064 (value: 0.0011, weighted value: 0.0525, policy: 0.1539, weighted policy: 0.1539), Train Mean Max: 0.9352
Epoch 5/10, Train Loss: 0.2032 (value: 0.0010, weighted value: 0.0506, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9354
Epoch 6/10, Train Loss: 0.1997 (value: 0.0010, weighted value: 0.0481, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9357
Epoch 7/10, Train Loss: 0.1995 (value: 0.0010, weighted value: 0.0483, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9357
Epoch 8/10, Train Loss: 0.1978 (value: 0.0009, weighted value: 0.0468, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9360
Epoch 9/10, Train Loss: 0.1947 (value: 0.0009, weighted value: 0.0441, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9362
Epoch 10/10, Train Loss: 0.1946 (value: 0.0009, weighted value: 0.0444, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9363
..training done in 61.86 seconds
..evaluation done in 19.00 seconds
Old network+MCTS average reward: 0.7330, min: 0.1389, max: 1.3426, stdev: 0.2320
New network+MCTS average reward: 0.7323, min: 0.1204, max: 1.3426, stdev: 0.2297
Old bare network average reward: 0.6898, min: 0.0185, max: 1.3056, stdev: 0.2388
New bare network average reward: 0.6935, min: 0.0185, max: 1.3056, stdev: 0.2425
External policy "random" average reward: 0.2778, min: -0.3333, max: 0.9907, stdev: 0.2390
External policy "individual greedy" average reward: 0.5628, min: -0.0926, max: 1.2593, stdev: 0.2310
External policy "total greedy" average reward: 0.6774, min: 0.0926, max: 1.2870, stdev: 0.2286
New network won 64 and tied 169 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 300 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.68 seconds
Training examples lengths: [65049, 64817, 64942, 64920, 65115, 64496, 64595, 64852, 64900, 64511]
Total value: 462633.20
Training on 648197 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2458 (value: 0.0015, weighted value: 0.0768, policy: 0.1691, weighted policy: 0.1691), Train Mean Max: 0.9328
Epoch 2/10, Train Loss: 0.2299 (value: 0.0014, weighted value: 0.0675, policy: 0.1623, weighted policy: 0.1623), Train Mean Max: 0.9337
Epoch 3/10, Train Loss: 0.2228 (value: 0.0013, weighted value: 0.0639, policy: 0.1589, weighted policy: 0.1589), Train Mean Max: 0.9341
Epoch 4/10, Train Loss: 0.2156 (value: 0.0012, weighted value: 0.0592, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9346
Epoch 5/10, Train Loss: 0.2115 (value: 0.0012, weighted value: 0.0575, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9349
Epoch 6/10, Train Loss: 0.2087 (value: 0.0011, weighted value: 0.0553, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9350
Epoch 7/10, Train Loss: 0.2059 (value: 0.0011, weighted value: 0.0525, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9351
Epoch 8/10, Train Loss: 0.2034 (value: 0.0010, weighted value: 0.0509, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9354
Epoch 9/10, Train Loss: 0.2017 (value: 0.0010, weighted value: 0.0494, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9356
Epoch 10/10, Train Loss: 0.1998 (value: 0.0010, weighted value: 0.0483, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9359
..training done in 63.47 seconds
..evaluation done in 19.15 seconds
Old network+MCTS average reward: 0.7059, min: 0.0556, max: 1.5000, stdev: 0.2479
New network+MCTS average reward: 0.7018, min: 0.0370, max: 1.5000, stdev: 0.2506
Old bare network average reward: 0.6658, min: 0.0185, max: 1.3889, stdev: 0.2534
New bare network average reward: 0.6641, min: 0.0185, max: 1.3981, stdev: 0.2566
External policy "random" average reward: 0.2713, min: -0.3148, max: 1.0278, stdev: 0.2164
External policy "individual greedy" average reward: 0.5455, min: 0.0093, max: 1.1944, stdev: 0.2280
External policy "total greedy" average reward: 0.6576, min: 0.1111, max: 1.3333, stdev: 0.2284
New network won 62 and tied 167 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_300

Training iteration 301 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.46 seconds
Training examples lengths: [64817, 64942, 64920, 65115, 64496, 64595, 64852, 64900, 64511, 64903]
Total value: 462748.19
Training on 648051 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2678 (value: 0.0018, weighted value: 0.0922, policy: 0.1756, weighted policy: 0.1756), Train Mean Max: 0.9318
Epoch 2/10, Train Loss: 0.2444 (value: 0.0016, weighted value: 0.0782, policy: 0.1662, weighted policy: 0.1662), Train Mean Max: 0.9328
Epoch 3/10, Train Loss: 0.2329 (value: 0.0014, weighted value: 0.0711, policy: 0.1618, weighted policy: 0.1618), Train Mean Max: 0.9334
Epoch 4/10, Train Loss: 0.2276 (value: 0.0014, weighted value: 0.0689, policy: 0.1587, weighted policy: 0.1587), Train Mean Max: 0.9337
Epoch 5/10, Train Loss: 0.2199 (value: 0.0013, weighted value: 0.0634, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9342
Epoch 6/10, Train Loss: 0.2164 (value: 0.0012, weighted value: 0.0605, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9343
Epoch 7/10, Train Loss: 0.2145 (value: 0.0012, weighted value: 0.0598, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9346
Epoch 8/10, Train Loss: 0.2103 (value: 0.0011, weighted value: 0.0564, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9348
Epoch 9/10, Train Loss: 0.2057 (value: 0.0011, weighted value: 0.0525, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9351
Epoch 10/10, Train Loss: 0.2051 (value: 0.0010, weighted value: 0.0521, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9352
..training done in 60.23 seconds
..evaluation done in 17.91 seconds
Old network+MCTS average reward: 0.6878, min: 0.1667, max: 1.3148, stdev: 0.2102
New network+MCTS average reward: 0.6890, min: 0.1667, max: 1.3519, stdev: 0.2100
Old bare network average reward: 0.6402, min: 0.0741, max: 1.3148, stdev: 0.2148
New bare network average reward: 0.6480, min: 0.0648, max: 1.3426, stdev: 0.2172
External policy "random" average reward: 0.2485, min: -0.3333, max: 0.8241, stdev: 0.2157
External policy "individual greedy" average reward: 0.5305, min: -0.0185, max: 1.2685, stdev: 0.2144
External policy "total greedy" average reward: 0.6411, min: 0.1019, max: 1.2778, stdev: 0.2112
New network won 73 and tied 157 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 302 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.84 seconds
Training examples lengths: [64942, 64920, 65115, 64496, 64595, 64852, 64900, 64511, 64903, 64806]
Total value: 462789.97
Training on 648040 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2272 (value: 0.0013, weighted value: 0.0648, policy: 0.1624, weighted policy: 0.1624), Train Mean Max: 0.9339
Epoch 2/10, Train Loss: 0.2178 (value: 0.0012, weighted value: 0.0603, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9346
Epoch 3/10, Train Loss: 0.2128 (value: 0.0011, weighted value: 0.0573, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9349
Epoch 4/10, Train Loss: 0.2084 (value: 0.0011, weighted value: 0.0540, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9352
Epoch 5/10, Train Loss: 0.2061 (value: 0.0011, weighted value: 0.0532, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9353
Epoch 6/10, Train Loss: 0.2026 (value: 0.0010, weighted value: 0.0506, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9356
Epoch 7/10, Train Loss: 0.2020 (value: 0.0010, weighted value: 0.0499, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9357
Epoch 8/10, Train Loss: 0.1985 (value: 0.0010, weighted value: 0.0477, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9361
Epoch 9/10, Train Loss: 0.1969 (value: 0.0009, weighted value: 0.0465, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9362
Epoch 10/10, Train Loss: 0.1965 (value: 0.0009, weighted value: 0.0458, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9364
..training done in 68.57 seconds
..evaluation done in 19.44 seconds
Old network+MCTS average reward: 0.7095, min: 0.1111, max: 1.6852, stdev: 0.2272
New network+MCTS average reward: 0.7102, min: 0.1667, max: 1.6852, stdev: 0.2287
Old bare network average reward: 0.6661, min: 0.0000, max: 1.6296, stdev: 0.2261
New bare network average reward: 0.6696, min: -0.0926, max: 1.6852, stdev: 0.2354
External policy "random" average reward: 0.2704, min: -0.3426, max: 0.8333, stdev: 0.2064
External policy "individual greedy" average reward: 0.5635, min: 0.0278, max: 1.2130, stdev: 0.2225
External policy "total greedy" average reward: 0.6672, min: 0.1204, max: 1.3796, stdev: 0.2141
New network won 68 and tied 167 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 303 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.07 seconds
Training examples lengths: [64920, 65115, 64496, 64595, 64852, 64900, 64511, 64903, 64806, 64796]
Total value: 462422.84
Training on 647894 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2205 (value: 0.0012, weighted value: 0.0605, policy: 0.1601, weighted policy: 0.1601), Train Mean Max: 0.9349
Epoch 2/10, Train Loss: 0.2116 (value: 0.0011, weighted value: 0.0552, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9354
Epoch 3/10, Train Loss: 0.2073 (value: 0.0011, weighted value: 0.0531, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9359
Epoch 4/10, Train Loss: 0.2027 (value: 0.0010, weighted value: 0.0502, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9361
Epoch 5/10, Train Loss: 0.1999 (value: 0.0010, weighted value: 0.0489, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9363
Epoch 6/10, Train Loss: 0.1984 (value: 0.0009, weighted value: 0.0468, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9363
Epoch 7/10, Train Loss: 0.1965 (value: 0.0009, weighted value: 0.0457, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9366
Epoch 8/10, Train Loss: 0.1951 (value: 0.0009, weighted value: 0.0453, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9367
Epoch 9/10, Train Loss: 0.1938 (value: 0.0009, weighted value: 0.0437, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9368
Epoch 10/10, Train Loss: 0.1929 (value: 0.0009, weighted value: 0.0431, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9370
..training done in 68.65 seconds
..evaluation done in 18.95 seconds
Old network+MCTS average reward: 0.7172, min: 0.1759, max: 1.3889, stdev: 0.2237
New network+MCTS average reward: 0.7173, min: 0.1759, max: 1.3519, stdev: 0.2236
Old bare network average reward: 0.6742, min: 0.1111, max: 1.3889, stdev: 0.2235
New bare network average reward: 0.6753, min: 0.1111, max: 1.3889, stdev: 0.2201
External policy "random" average reward: 0.2636, min: -0.3056, max: 0.9537, stdev: 0.2098
External policy "individual greedy" average reward: 0.5412, min: -0.0370, max: 1.1667, stdev: 0.2145
External policy "total greedy" average reward: 0.6660, min: 0.1019, max: 1.3148, stdev: 0.2008
New network won 77 and tied 148 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 304 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.61 seconds
Training examples lengths: [65115, 64496, 64595, 64852, 64900, 64511, 64903, 64806, 64796, 64787]
Total value: 462515.33
Training on 647761 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2173 (value: 0.0012, weighted value: 0.0587, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9356
Epoch 2/10, Train Loss: 0.2102 (value: 0.0011, weighted value: 0.0542, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9361
Epoch 3/10, Train Loss: 0.2032 (value: 0.0010, weighted value: 0.0504, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9364
Epoch 4/10, Train Loss: 0.1982 (value: 0.0009, weighted value: 0.0475, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9368
Epoch 5/10, Train Loss: 0.1977 (value: 0.0009, weighted value: 0.0471, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9368
Epoch 6/10, Train Loss: 0.1956 (value: 0.0009, weighted value: 0.0457, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1934 (value: 0.0009, weighted value: 0.0436, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9371
Epoch 8/10, Train Loss: 0.1930 (value: 0.0009, weighted value: 0.0435, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9372
Epoch 9/10, Train Loss: 0.1906 (value: 0.0008, weighted value: 0.0417, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9375
Epoch 10/10, Train Loss: 0.1898 (value: 0.0008, weighted value: 0.0415, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9377
..training done in 58.59 seconds
..evaluation done in 18.76 seconds
Old network+MCTS average reward: 0.7157, min: 0.1759, max: 1.3519, stdev: 0.2271
New network+MCTS average reward: 0.7119, min: 0.1389, max: 1.3519, stdev: 0.2218
Old bare network average reward: 0.6656, min: 0.1204, max: 1.3241, stdev: 0.2330
New bare network average reward: 0.6673, min: 0.1481, max: 1.2963, stdev: 0.2287
External policy "random" average reward: 0.2648, min: -0.3704, max: 0.9907, stdev: 0.2076
External policy "individual greedy" average reward: 0.5472, min: 0.0741, max: 1.2778, stdev: 0.2099
External policy "total greedy" average reward: 0.6589, min: 0.1204, max: 1.3241, stdev: 0.2134
New network won 69 and tied 141 out of 300 games (46.50% wins where ties are half wins)
Reverting to the old network

Training iteration 305 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.30 seconds
Training examples lengths: [64496, 64595, 64852, 64900, 64511, 64903, 64806, 64796, 64787, 64939]
Total value: 462592.31
Training on 647585 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2406 (value: 0.0014, weighted value: 0.0725, policy: 0.1681, weighted policy: 0.1681), Train Mean Max: 0.9341
Epoch 2/10, Train Loss: 0.2253 (value: 0.0013, weighted value: 0.0645, policy: 0.1608, weighted policy: 0.1608), Train Mean Max: 0.9351
Epoch 3/10, Train Loss: 0.2168 (value: 0.0012, weighted value: 0.0601, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9355
Epoch 4/10, Train Loss: 0.2121 (value: 0.0011, weighted value: 0.0574, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9358
Epoch 5/10, Train Loss: 0.2077 (value: 0.0011, weighted value: 0.0547, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9360
Epoch 6/10, Train Loss: 0.2031 (value: 0.0010, weighted value: 0.0515, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9363
Epoch 7/10, Train Loss: 0.2010 (value: 0.0010, weighted value: 0.0504, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9366
Epoch 8/10, Train Loss: 0.2002 (value: 0.0010, weighted value: 0.0490, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9366
Epoch 9/10, Train Loss: 0.1970 (value: 0.0009, weighted value: 0.0468, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9368
Epoch 10/10, Train Loss: 0.1963 (value: 0.0009, weighted value: 0.0466, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9371
..training done in 59.10 seconds
..evaluation done in 18.88 seconds
Old network+MCTS average reward: 0.7196, min: 0.1296, max: 1.3148, stdev: 0.2174
New network+MCTS average reward: 0.7188, min: 0.0926, max: 1.3241, stdev: 0.2174
Old bare network average reward: 0.6766, min: 0.0926, max: 1.2870, stdev: 0.2217
New bare network average reward: 0.6788, min: 0.0926, max: 1.2870, stdev: 0.2275
External policy "random" average reward: 0.2706, min: -0.2407, max: 0.8333, stdev: 0.2049
External policy "individual greedy" average reward: 0.5469, min: 0.0185, max: 1.1296, stdev: 0.2153
External policy "total greedy" average reward: 0.6607, min: 0.0370, max: 1.3981, stdev: 0.2090
New network won 77 and tied 146 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 306 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.21 seconds
Training examples lengths: [64595, 64852, 64900, 64511, 64903, 64806, 64796, 64787, 64939, 64779]
Total value: 463291.85
Training on 647868 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2214 (value: 0.0012, weighted value: 0.0608, policy: 0.1607, weighted policy: 0.1607), Train Mean Max: 0.9354
Epoch 2/10, Train Loss: 0.2112 (value: 0.0011, weighted value: 0.0557, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9361
Epoch 3/10, Train Loss: 0.2059 (value: 0.0011, weighted value: 0.0527, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9364
Epoch 4/10, Train Loss: 0.2018 (value: 0.0010, weighted value: 0.0507, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9365
Epoch 5/10, Train Loss: 0.1995 (value: 0.0010, weighted value: 0.0481, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9367
Epoch 6/10, Train Loss: 0.1980 (value: 0.0010, weighted value: 0.0481, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1962 (value: 0.0009, weighted value: 0.0462, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9372
Epoch 8/10, Train Loss: 0.1938 (value: 0.0009, weighted value: 0.0446, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9372
Epoch 9/10, Train Loss: 0.1928 (value: 0.0009, weighted value: 0.0441, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9375
Epoch 10/10, Train Loss: 0.1908 (value: 0.0008, weighted value: 0.0424, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9377
..training done in 59.92 seconds
..evaluation done in 18.64 seconds
Old network+MCTS average reward: 0.6976, min: 0.0833, max: 1.3981, stdev: 0.2430
New network+MCTS average reward: 0.6974, min: 0.1296, max: 1.4815, stdev: 0.2433
Old bare network average reward: 0.6527, min: 0.0556, max: 1.3981, stdev: 0.2483
New bare network average reward: 0.6507, min: 0.1019, max: 1.4259, stdev: 0.2541
External policy "random" average reward: 0.2664, min: -0.2778, max: 0.8981, stdev: 0.2318
External policy "individual greedy" average reward: 0.5373, min: -0.1204, max: 1.2593, stdev: 0.2340
External policy "total greedy" average reward: 0.6452, min: 0.0185, max: 1.3519, stdev: 0.2355
New network won 77 and tied 155 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 307 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.76 seconds
Training examples lengths: [64852, 64900, 64511, 64903, 64806, 64796, 64787, 64939, 64779, 64645]
Total value: 463524.36
Training on 647918 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2162 (value: 0.0011, weighted value: 0.0574, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9359
Epoch 2/10, Train Loss: 0.2072 (value: 0.0011, weighted value: 0.0526, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9367
Epoch 3/10, Train Loss: 0.2019 (value: 0.0010, weighted value: 0.0505, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9369
Epoch 4/10, Train Loss: 0.1982 (value: 0.0010, weighted value: 0.0478, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9371
Epoch 5/10, Train Loss: 0.1976 (value: 0.0010, weighted value: 0.0477, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9372
Epoch 6/10, Train Loss: 0.1947 (value: 0.0009, weighted value: 0.0454, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9373
Epoch 7/10, Train Loss: 0.1931 (value: 0.0009, weighted value: 0.0436, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9375
Epoch 8/10, Train Loss: 0.1908 (value: 0.0009, weighted value: 0.0425, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9379
Epoch 9/10, Train Loss: 0.1905 (value: 0.0008, weighted value: 0.0422, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1890 (value: 0.0008, weighted value: 0.0414, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9380
..training done in 59.54 seconds
..evaluation done in 18.52 seconds
Old network+MCTS average reward: 0.6876, min: 0.1389, max: 1.4537, stdev: 0.2290
New network+MCTS average reward: 0.6865, min: 0.1481, max: 1.5093, stdev: 0.2304
Old bare network average reward: 0.6430, min: 0.0556, max: 1.4537, stdev: 0.2417
New bare network average reward: 0.6418, min: 0.1019, max: 1.4537, stdev: 0.2410
External policy "random" average reward: 0.2648, min: -0.2963, max: 0.9259, stdev: 0.2240
External policy "individual greedy" average reward: 0.5269, min: 0.0185, max: 1.2407, stdev: 0.2227
External policy "total greedy" average reward: 0.6391, min: -0.0185, max: 1.3333, stdev: 0.2242
New network won 73 and tied 160 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 308 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.59 seconds
Training examples lengths: [64900, 64511, 64903, 64806, 64796, 64787, 64939, 64779, 64645, 64796]
Total value: 463404.19
Training on 647862 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2156 (value: 0.0011, weighted value: 0.0566, policy: 0.1590, weighted policy: 0.1590), Train Mean Max: 0.9362
Epoch 2/10, Train Loss: 0.2071 (value: 0.0010, weighted value: 0.0524, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9369
Epoch 3/10, Train Loss: 0.2000 (value: 0.0010, weighted value: 0.0491, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9374
Epoch 4/10, Train Loss: 0.1978 (value: 0.0009, weighted value: 0.0474, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9373
Epoch 5/10, Train Loss: 0.1940 (value: 0.0009, weighted value: 0.0453, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1926 (value: 0.0009, weighted value: 0.0442, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9377
Epoch 7/10, Train Loss: 0.1914 (value: 0.0009, weighted value: 0.0427, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9378
Epoch 8/10, Train Loss: 0.1897 (value: 0.0008, weighted value: 0.0419, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9380
Epoch 9/10, Train Loss: 0.1901 (value: 0.0008, weighted value: 0.0413, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9380
Epoch 10/10, Train Loss: 0.1885 (value: 0.0008, weighted value: 0.0407, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9382
..training done in 59.87 seconds
..evaluation done in 18.85 seconds
Old network+MCTS average reward: 0.7070, min: 0.0648, max: 1.3796, stdev: 0.2275
New network+MCTS average reward: 0.7080, min: 0.2407, max: 1.4259, stdev: 0.2250
Old bare network average reward: 0.6673, min: 0.1574, max: 1.4074, stdev: 0.2335
New bare network average reward: 0.6666, min: 0.0741, max: 1.3796, stdev: 0.2327
External policy "random" average reward: 0.2431, min: -0.3148, max: 0.9167, stdev: 0.2084
External policy "individual greedy" average reward: 0.5327, min: -0.0463, max: 1.2685, stdev: 0.2277
External policy "total greedy" average reward: 0.6471, min: 0.1204, max: 1.4259, stdev: 0.2204
New network won 62 and tied 165 out of 300 games (48.17% wins where ties are half wins)
Reverting to the old network

Training iteration 309 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.60 seconds
Training examples lengths: [64511, 64903, 64806, 64796, 64787, 64939, 64779, 64645, 64796, 64808]
Total value: 463206.42
Training on 647770 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2404 (value: 0.0015, weighted value: 0.0739, policy: 0.1666, weighted policy: 0.1666), Train Mean Max: 0.9347
Epoch 2/10, Train Loss: 0.2240 (value: 0.0013, weighted value: 0.0638, policy: 0.1601, weighted policy: 0.1601), Train Mean Max: 0.9354
Epoch 3/10, Train Loss: 0.2150 (value: 0.0012, weighted value: 0.0587, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9360
Epoch 4/10, Train Loss: 0.2099 (value: 0.0011, weighted value: 0.0563, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9363
Epoch 5/10, Train Loss: 0.2061 (value: 0.0011, weighted value: 0.0542, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9366
Epoch 6/10, Train Loss: 0.2022 (value: 0.0010, weighted value: 0.0514, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9367
Epoch 7/10, Train Loss: 0.1998 (value: 0.0010, weighted value: 0.0493, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9368
Epoch 8/10, Train Loss: 0.1966 (value: 0.0009, weighted value: 0.0471, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9370
Epoch 9/10, Train Loss: 0.1968 (value: 0.0009, weighted value: 0.0471, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9371
Epoch 10/10, Train Loss: 0.1955 (value: 0.0009, weighted value: 0.0461, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9374
..training done in 67.46 seconds
..evaluation done in 19.13 seconds
Old network+MCTS average reward: 0.7291, min: 0.0926, max: 1.4259, stdev: 0.2238
New network+MCTS average reward: 0.7319, min: 0.0926, max: 1.4630, stdev: 0.2228
Old bare network average reward: 0.6927, min: 0.0926, max: 1.4630, stdev: 0.2310
New bare network average reward: 0.6905, min: 0.0926, max: 1.4630, stdev: 0.2298
External policy "random" average reward: 0.2669, min: -0.5463, max: 0.9259, stdev: 0.2238
External policy "individual greedy" average reward: 0.5610, min: -0.1574, max: 1.2315, stdev: 0.2277
External policy "total greedy" average reward: 0.6912, min: 0.0278, max: 1.3611, stdev: 0.2187
New network won 85 and tied 144 out of 300 games (52.33% wins where ties are half wins)
Keeping the new network

Training iteration 310 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.75 seconds
Training examples lengths: [64903, 64806, 64796, 64787, 64939, 64779, 64645, 64796, 64808, 64669]
Total value: 463728.82
Training on 647928 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2200 (value: 0.0012, weighted value: 0.0601, policy: 0.1599, weighted policy: 0.1599), Train Mean Max: 0.9356
Epoch 2/10, Train Loss: 0.2113 (value: 0.0011, weighted value: 0.0556, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9363
Epoch 3/10, Train Loss: 0.2050 (value: 0.0011, weighted value: 0.0527, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9366
Epoch 4/10, Train Loss: 0.2015 (value: 0.0010, weighted value: 0.0505, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9369
Epoch 5/10, Train Loss: 0.1987 (value: 0.0010, weighted value: 0.0479, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1976 (value: 0.0009, weighted value: 0.0471, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1963 (value: 0.0009, weighted value: 0.0471, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1924 (value: 0.0009, weighted value: 0.0434, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9376
Epoch 9/10, Train Loss: 0.1918 (value: 0.0009, weighted value: 0.0426, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9376
Epoch 10/10, Train Loss: 0.1906 (value: 0.0008, weighted value: 0.0423, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9379
..training done in 60.33 seconds
..evaluation done in 19.10 seconds
Old network+MCTS average reward: 0.7204, min: 0.0833, max: 1.3148, stdev: 0.2319
New network+MCTS average reward: 0.7221, min: 0.0833, max: 1.2685, stdev: 0.2345
Old bare network average reward: 0.6753, min: 0.0833, max: 1.2685, stdev: 0.2380
New bare network average reward: 0.6733, min: 0.0833, max: 1.2685, stdev: 0.2398
External policy "random" average reward: 0.2733, min: -0.3796, max: 0.9537, stdev: 0.2210
External policy "individual greedy" average reward: 0.5557, min: -0.1019, max: 1.2593, stdev: 0.2400
External policy "total greedy" average reward: 0.6626, min: -0.0370, max: 1.2870, stdev: 0.2359
New network won 81 and tied 155 out of 300 games (52.83% wins where ties are half wins)
Keeping the new network

Training iteration 311 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 64.25 seconds
Training examples lengths: [64806, 64796, 64787, 64939, 64779, 64645, 64796, 64808, 64669, 64864]
Total value: 463786.55
Training on 647889 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2146 (value: 0.0011, weighted value: 0.0563, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9361
Epoch 2/10, Train Loss: 0.2088 (value: 0.0011, weighted value: 0.0530, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9365
Epoch 3/10, Train Loss: 0.2016 (value: 0.0010, weighted value: 0.0493, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9370
Epoch 4/10, Train Loss: 0.1985 (value: 0.0009, weighted value: 0.0475, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9371
Epoch 5/10, Train Loss: 0.1963 (value: 0.0009, weighted value: 0.0470, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9373
Epoch 6/10, Train Loss: 0.1931 (value: 0.0009, weighted value: 0.0440, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9374
Epoch 7/10, Train Loss: 0.1923 (value: 0.0009, weighted value: 0.0436, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9376
Epoch 8/10, Train Loss: 0.1915 (value: 0.0009, weighted value: 0.0429, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9376
Epoch 9/10, Train Loss: 0.1913 (value: 0.0009, weighted value: 0.0427, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9378
Epoch 10/10, Train Loss: 0.1876 (value: 0.0008, weighted value: 0.0390, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9378
..training done in 70.19 seconds
..evaluation done in 19.03 seconds
Old network+MCTS average reward: 0.7027, min: 0.1111, max: 1.4259, stdev: 0.2407
New network+MCTS average reward: 0.7015, min: 0.0278, max: 1.3981, stdev: 0.2413
Old bare network average reward: 0.6592, min: 0.0278, max: 1.3519, stdev: 0.2468
New bare network average reward: 0.6570, min: 0.0278, max: 1.3519, stdev: 0.2433
External policy "random" average reward: 0.2515, min: -0.3704, max: 0.9352, stdev: 0.2235
External policy "individual greedy" average reward: 0.5367, min: -0.0926, max: 1.2037, stdev: 0.2383
External policy "total greedy" average reward: 0.6519, min: 0.0278, max: 1.3981, stdev: 0.2377
New network won 65 and tied 169 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 312 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.40 seconds
Training examples lengths: [64796, 64787, 64939, 64779, 64645, 64796, 64808, 64669, 64864, 64754]
Total value: 463166.69
Training on 647837 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2416 (value: 0.0015, weighted value: 0.0729, policy: 0.1687, weighted policy: 0.1687), Train Mean Max: 0.9342
Epoch 2/10, Train Loss: 0.2237 (value: 0.0013, weighted value: 0.0636, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9353
Epoch 3/10, Train Loss: 0.2150 (value: 0.0012, weighted value: 0.0589, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9358
Epoch 4/10, Train Loss: 0.2106 (value: 0.0011, weighted value: 0.0564, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9359
Epoch 5/10, Train Loss: 0.2084 (value: 0.0011, weighted value: 0.0550, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9359
Epoch 6/10, Train Loss: 0.2025 (value: 0.0010, weighted value: 0.0507, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9364
Epoch 7/10, Train Loss: 0.2012 (value: 0.0010, weighted value: 0.0500, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9364
Epoch 8/10, Train Loss: 0.1989 (value: 0.0010, weighted value: 0.0480, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9366
Epoch 9/10, Train Loss: 0.1988 (value: 0.0009, weighted value: 0.0472, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9366
Epoch 10/10, Train Loss: 0.1951 (value: 0.0009, weighted value: 0.0448, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9370
..training done in 68.91 seconds
..evaluation done in 20.83 seconds
Old network+MCTS average reward: 0.7232, min: 0.1204, max: 1.4630, stdev: 0.2527
New network+MCTS average reward: 0.7179, min: 0.1019, max: 1.4630, stdev: 0.2486
Old bare network average reward: 0.6818, min: -0.0741, max: 1.4630, stdev: 0.2571
New bare network average reward: 0.6787, min: 0.0926, max: 1.4630, stdev: 0.2540
External policy "random" average reward: 0.2773, min: -0.2870, max: 1.0926, stdev: 0.2284
External policy "individual greedy" average reward: 0.5523, min: -0.0278, max: 1.3056, stdev: 0.2505
External policy "total greedy" average reward: 0.6603, min: 0.0278, max: 1.4352, stdev: 0.2363
New network won 61 and tied 151 out of 300 games (45.50% wins where ties are half wins)
Reverting to the old network

Training iteration 313 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.82 seconds
Training examples lengths: [64787, 64939, 64779, 64645, 64796, 64808, 64669, 64864, 64754, 64934]
Total value: 463749.82
Training on 647975 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2627 (value: 0.0017, weighted value: 0.0873, policy: 0.1754, weighted policy: 0.1754), Train Mean Max: 0.9329
Epoch 2/10, Train Loss: 0.2399 (value: 0.0015, weighted value: 0.0739, policy: 0.1660, weighted policy: 0.1660), Train Mean Max: 0.9339
Epoch 3/10, Train Loss: 0.2292 (value: 0.0014, weighted value: 0.0689, policy: 0.1603, weighted policy: 0.1603), Train Mean Max: 0.9344
Epoch 4/10, Train Loss: 0.2209 (value: 0.0013, weighted value: 0.0636, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9349
Epoch 5/10, Train Loss: 0.2161 (value: 0.0012, weighted value: 0.0610, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9351
Epoch 6/10, Train Loss: 0.2125 (value: 0.0012, weighted value: 0.0582, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9353
Epoch 7/10, Train Loss: 0.2085 (value: 0.0011, weighted value: 0.0552, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9355
Epoch 8/10, Train Loss: 0.2057 (value: 0.0011, weighted value: 0.0533, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9357
Epoch 9/10, Train Loss: 0.2042 (value: 0.0010, weighted value: 0.0515, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9358
Epoch 10/10, Train Loss: 0.2028 (value: 0.0010, weighted value: 0.0504, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9359
..training done in 59.99 seconds
..evaluation done in 18.66 seconds
Old network+MCTS average reward: 0.7057, min: 0.1204, max: 1.3519, stdev: 0.2227
New network+MCTS average reward: 0.7060, min: 0.1574, max: 1.3241, stdev: 0.2221
Old bare network average reward: 0.6614, min: 0.1204, max: 1.2963, stdev: 0.2224
New bare network average reward: 0.6602, min: 0.1019, max: 1.2963, stdev: 0.2237
External policy "random" average reward: 0.2543, min: -0.2870, max: 0.8148, stdev: 0.2225
External policy "individual greedy" average reward: 0.5429, min: 0.0000, max: 1.1481, stdev: 0.2117
External policy "total greedy" average reward: 0.6628, min: 0.1389, max: 1.2593, stdev: 0.2122
New network won 78 and tied 151 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 314 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.43 seconds
Training examples lengths: [64939, 64779, 64645, 64796, 64808, 64669, 64864, 64754, 64934, 64653]
Total value: 463949.70
Training on 647841 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2248 (value: 0.0013, weighted value: 0.0631, policy: 0.1617, weighted policy: 0.1617), Train Mean Max: 0.9344
Epoch 2/10, Train Loss: 0.2169 (value: 0.0012, weighted value: 0.0585, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9350
Epoch 3/10, Train Loss: 0.2114 (value: 0.0011, weighted value: 0.0557, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9352
Epoch 4/10, Train Loss: 0.2069 (value: 0.0011, weighted value: 0.0531, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9355
Epoch 5/10, Train Loss: 0.2039 (value: 0.0010, weighted value: 0.0502, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9356
Epoch 6/10, Train Loss: 0.2018 (value: 0.0010, weighted value: 0.0495, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9359
Epoch 7/10, Train Loss: 0.1988 (value: 0.0009, weighted value: 0.0473, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9361
Epoch 8/10, Train Loss: 0.1985 (value: 0.0009, weighted value: 0.0461, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9362
Epoch 9/10, Train Loss: 0.1967 (value: 0.0009, weighted value: 0.0456, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9365
Epoch 10/10, Train Loss: 0.1949 (value: 0.0009, weighted value: 0.0439, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9365
..training done in 59.40 seconds
..evaluation done in 18.87 seconds
Old network+MCTS average reward: 0.7134, min: 0.0000, max: 1.5093, stdev: 0.2378
New network+MCTS average reward: 0.7182, min: 0.0000, max: 1.4815, stdev: 0.2354
Old bare network average reward: 0.6756, min: 0.0000, max: 1.5093, stdev: 0.2400
New bare network average reward: 0.6741, min: -0.1667, max: 1.5093, stdev: 0.2414
External policy "random" average reward: 0.2739, min: -0.3241, max: 1.0370, stdev: 0.2338
External policy "individual greedy" average reward: 0.5542, min: -0.1111, max: 1.2130, stdev: 0.2320
External policy "total greedy" average reward: 0.6585, min: 0.0648, max: 1.3148, stdev: 0.2276
New network won 83 and tied 146 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network

Training iteration 315 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.83 seconds
Training examples lengths: [64779, 64645, 64796, 64808, 64669, 64864, 64754, 64934, 64653, 64742]
Total value: 463919.99
Training on 647644 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2197 (value: 0.0012, weighted value: 0.0592, policy: 0.1604, weighted policy: 0.1604), Train Mean Max: 0.9348
Epoch 2/10, Train Loss: 0.2127 (value: 0.0011, weighted value: 0.0547, policy: 0.1580, weighted policy: 0.1580), Train Mean Max: 0.9353
Epoch 3/10, Train Loss: 0.2084 (value: 0.0011, weighted value: 0.0530, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9356
Epoch 4/10, Train Loss: 0.2030 (value: 0.0010, weighted value: 0.0501, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9359
Epoch 5/10, Train Loss: 0.1997 (value: 0.0010, weighted value: 0.0476, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9361
Epoch 6/10, Train Loss: 0.1974 (value: 0.0009, weighted value: 0.0462, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9362
Epoch 7/10, Train Loss: 0.1947 (value: 0.0009, weighted value: 0.0444, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9365
Epoch 8/10, Train Loss: 0.1958 (value: 0.0009, weighted value: 0.0449, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9365
Epoch 9/10, Train Loss: 0.1946 (value: 0.0009, weighted value: 0.0440, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9367
Epoch 10/10, Train Loss: 0.1920 (value: 0.0008, weighted value: 0.0414, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9367
..training done in 58.64 seconds
..evaluation done in 19.48 seconds
Old network+MCTS average reward: 0.6888, min: 0.1574, max: 1.5926, stdev: 0.2348
New network+MCTS average reward: 0.6906, min: 0.1574, max: 1.5926, stdev: 0.2326
Old bare network average reward: 0.6495, min: 0.0093, max: 1.5648, stdev: 0.2391
New bare network average reward: 0.6534, min: 0.0093, max: 1.5926, stdev: 0.2363
External policy "random" average reward: 0.2639, min: -0.5093, max: 0.9815, stdev: 0.2311
External policy "individual greedy" average reward: 0.5244, min: -0.1019, max: 1.3333, stdev: 0.2202
External policy "total greedy" average reward: 0.6433, min: -0.0648, max: 1.3796, stdev: 0.2213
New network won 84 and tied 152 out of 300 games (53.33% wins where ties are half wins)
Keeping the new network

Training iteration 316 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.55 seconds
Training examples lengths: [64645, 64796, 64808, 64669, 64864, 64754, 64934, 64653, 64742, 65040]
Total value: 464488.41
Training on 647905 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2177 (value: 0.0011, weighted value: 0.0568, policy: 0.1608, weighted policy: 0.1608), Train Mean Max: 0.9351
Epoch 2/10, Train Loss: 0.2084 (value: 0.0010, weighted value: 0.0519, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9360
Epoch 3/10, Train Loss: 0.2028 (value: 0.0010, weighted value: 0.0486, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9362
Epoch 4/10, Train Loss: 0.1990 (value: 0.0009, weighted value: 0.0470, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9364
Epoch 5/10, Train Loss: 0.1986 (value: 0.0009, weighted value: 0.0473, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9366
Epoch 6/10, Train Loss: 0.1951 (value: 0.0009, weighted value: 0.0447, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9367
Epoch 7/10, Train Loss: 0.1931 (value: 0.0009, weighted value: 0.0428, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9369
Epoch 8/10, Train Loss: 0.1917 (value: 0.0008, weighted value: 0.0420, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9370
Epoch 9/10, Train Loss: 0.1925 (value: 0.0008, weighted value: 0.0418, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9370
Epoch 10/10, Train Loss: 0.1899 (value: 0.0008, weighted value: 0.0395, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9371
..training done in 71.21 seconds
..evaluation done in 19.09 seconds
Old network+MCTS average reward: 0.7248, min: 0.2130, max: 1.5000, stdev: 0.2291
New network+MCTS average reward: 0.7285, min: 0.2130, max: 1.6296, stdev: 0.2346
Old bare network average reward: 0.6850, min: 0.1944, max: 1.6296, stdev: 0.2364
New bare network average reward: 0.6860, min: 0.1296, max: 1.6296, stdev: 0.2402
External policy "random" average reward: 0.2715, min: -0.5556, max: 0.9352, stdev: 0.2323
External policy "individual greedy" average reward: 0.5625, min: 0.0278, max: 1.4537, stdev: 0.2378
External policy "total greedy" average reward: 0.6631, min: 0.1574, max: 1.4907, stdev: 0.2373
New network won 91 and tied 143 out of 300 games (54.17% wins where ties are half wins)
Keeping the new network

Training iteration 317 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.61 seconds
Training examples lengths: [64796, 64808, 64669, 64864, 64754, 64934, 64653, 64742, 65040, 64802]
Total value: 464527.24
Training on 648062 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2158 (value: 0.0011, weighted value: 0.0561, policy: 0.1598, weighted policy: 0.1598), Train Mean Max: 0.9356
Epoch 2/10, Train Loss: 0.2058 (value: 0.0010, weighted value: 0.0506, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9364
Epoch 3/10, Train Loss: 0.2007 (value: 0.0010, weighted value: 0.0482, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9366
Epoch 4/10, Train Loss: 0.1967 (value: 0.0009, weighted value: 0.0452, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9367
Epoch 5/10, Train Loss: 0.1960 (value: 0.0009, weighted value: 0.0453, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1936 (value: 0.0009, weighted value: 0.0435, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9369
Epoch 7/10, Train Loss: 0.1919 (value: 0.0008, weighted value: 0.0420, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9372
Epoch 8/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0409, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9373
Epoch 9/10, Train Loss: 0.1893 (value: 0.0008, weighted value: 0.0400, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9374
Epoch 10/10, Train Loss: 0.1882 (value: 0.0008, weighted value: 0.0389, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9375
..training done in 61.40 seconds
..evaluation done in 20.09 seconds
Old network+MCTS average reward: 0.6903, min: -0.0370, max: 1.4352, stdev: 0.2444
New network+MCTS average reward: 0.6915, min: 0.0926, max: 1.4352, stdev: 0.2417
Old bare network average reward: 0.6497, min: 0.0370, max: 1.3889, stdev: 0.2506
New bare network average reward: 0.6474, min: 0.0370, max: 1.3889, stdev: 0.2431
External policy "random" average reward: 0.2501, min: -0.4167, max: 1.2778, stdev: 0.2455
External policy "individual greedy" average reward: 0.5199, min: -0.0185, max: 1.2963, stdev: 0.2442
External policy "total greedy" average reward: 0.6366, min: 0.0833, max: 1.3704, stdev: 0.2325
New network won 71 and tied 165 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 318 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.16 seconds
Training examples lengths: [64808, 64669, 64864, 64754, 64934, 64653, 64742, 65040, 64802, 64708]
Total value: 464551.36
Training on 647974 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2143 (value: 0.0011, weighted value: 0.0544, policy: 0.1599, weighted policy: 0.1599), Train Mean Max: 0.9359
Epoch 2/10, Train Loss: 0.2050 (value: 0.0010, weighted value: 0.0504, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9367
Epoch 3/10, Train Loss: 0.1992 (value: 0.0009, weighted value: 0.0471, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9367
Epoch 4/10, Train Loss: 0.1960 (value: 0.0009, weighted value: 0.0452, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9370
Epoch 5/10, Train Loss: 0.1946 (value: 0.0009, weighted value: 0.0446, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9371
Epoch 6/10, Train Loss: 0.1921 (value: 0.0008, weighted value: 0.0424, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9373
Epoch 7/10, Train Loss: 0.1921 (value: 0.0008, weighted value: 0.0421, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9374
Epoch 8/10, Train Loss: 0.1902 (value: 0.0008, weighted value: 0.0409, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1882 (value: 0.0008, weighted value: 0.0390, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1875 (value: 0.0008, weighted value: 0.0388, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9378
..training done in 59.96 seconds
..evaluation done in 18.63 seconds
Old network+MCTS average reward: 0.6883, min: 0.1111, max: 1.4074, stdev: 0.2416
New network+MCTS average reward: 0.6904, min: 0.1389, max: 1.4074, stdev: 0.2411
Old bare network average reward: 0.6443, min: 0.0833, max: 1.4074, stdev: 0.2483
New bare network average reward: 0.6450, min: 0.0185, max: 1.3519, stdev: 0.2389
External policy "random" average reward: 0.2359, min: -0.3056, max: 1.0556, stdev: 0.2315
External policy "individual greedy" average reward: 0.5142, min: -0.0556, max: 1.2222, stdev: 0.2275
External policy "total greedy" average reward: 0.6371, min: 0.0741, max: 1.3056, stdev: 0.2222
New network won 72 and tied 175 out of 300 games (53.17% wins where ties are half wins)
Keeping the new network

Training iteration 319 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 67.76 seconds
Training examples lengths: [64669, 64864, 64754, 64934, 64653, 64742, 65040, 64802, 64708, 64835]
Total value: 464608.41
Training on 648001 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2115 (value: 0.0011, weighted value: 0.0535, policy: 0.1580, weighted policy: 0.1580), Train Mean Max: 0.9363
Epoch 2/10, Train Loss: 0.2038 (value: 0.0010, weighted value: 0.0494, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9367
Epoch 3/10, Train Loss: 0.1980 (value: 0.0009, weighted value: 0.0462, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9370
Epoch 4/10, Train Loss: 0.1943 (value: 0.0009, weighted value: 0.0435, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9372
Epoch 5/10, Train Loss: 0.1938 (value: 0.0009, weighted value: 0.0436, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9373
Epoch 6/10, Train Loss: 0.1914 (value: 0.0008, weighted value: 0.0419, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9374
Epoch 7/10, Train Loss: 0.1899 (value: 0.0008, weighted value: 0.0410, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9376
Epoch 8/10, Train Loss: 0.1879 (value: 0.0008, weighted value: 0.0386, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1872 (value: 0.0008, weighted value: 0.0389, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1857 (value: 0.0007, weighted value: 0.0374, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9380
..training done in 73.16 seconds
..evaluation done in 18.97 seconds
Old network+MCTS average reward: 0.7134, min: 0.0370, max: 1.7315, stdev: 0.2382
New network+MCTS average reward: 0.7168, min: 0.0741, max: 1.7315, stdev: 0.2362
Old bare network average reward: 0.6701, min: 0.0463, max: 1.7315, stdev: 0.2469
New bare network average reward: 0.6686, min: 0.0000, max: 1.7315, stdev: 0.2444
External policy "random" average reward: 0.2729, min: -0.2685, max: 0.9167, stdev: 0.2209
External policy "individual greedy" average reward: 0.5439, min: 0.0741, max: 1.3056, stdev: 0.2176
External policy "total greedy" average reward: 0.6630, min: 0.0926, max: 1.5370, stdev: 0.2214
New network won 81 and tied 155 out of 300 games (52.83% wins where ties are half wins)
Keeping the new network

Training iteration 320 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.06 seconds
Training examples lengths: [64864, 64754, 64934, 64653, 64742, 65040, 64802, 64708, 64835, 64998]
Total value: 465300.70
Training on 648330 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2137 (value: 0.0011, weighted value: 0.0553, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9361
Epoch 2/10, Train Loss: 0.2029 (value: 0.0010, weighted value: 0.0491, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9368
Epoch 3/10, Train Loss: 0.1986 (value: 0.0009, weighted value: 0.0466, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9371
Epoch 4/10, Train Loss: 0.1959 (value: 0.0009, weighted value: 0.0448, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9373
Epoch 5/10, Train Loss: 0.1937 (value: 0.0009, weighted value: 0.0432, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9374
Epoch 6/10, Train Loss: 0.1899 (value: 0.0008, weighted value: 0.0411, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9377
Epoch 7/10, Train Loss: 0.1912 (value: 0.0008, weighted value: 0.0421, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9376
Epoch 8/10, Train Loss: 0.1878 (value: 0.0008, weighted value: 0.0402, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9379
Epoch 9/10, Train Loss: 0.1883 (value: 0.0008, weighted value: 0.0393, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9378
Epoch 10/10, Train Loss: 0.1860 (value: 0.0008, weighted value: 0.0377, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9381
..training done in 65.77 seconds
..evaluation done in 19.23 seconds
Old network+MCTS average reward: 0.7064, min: 0.0370, max: 1.4167, stdev: 0.2476
New network+MCTS average reward: 0.7067, min: 0.0556, max: 1.3611, stdev: 0.2429
Old bare network average reward: 0.6648, min: -0.0741, max: 1.3611, stdev: 0.2506
New bare network average reward: 0.6643, min: 0.0185, max: 1.4167, stdev: 0.2509
External policy "random" average reward: 0.2655, min: -0.3704, max: 1.1019, stdev: 0.2455
External policy "individual greedy" average reward: 0.5430, min: -0.1944, max: 1.2407, stdev: 0.2385
External policy "total greedy" average reward: 0.6621, min: 0.0741, max: 1.3148, stdev: 0.2333
New network won 76 and tied 156 out of 300 games (51.33% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_320

Training iteration 321 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.07 seconds
Training examples lengths: [64754, 64934, 64653, 64742, 65040, 64802, 64708, 64835, 64998, 64646]
Total value: 465212.20
Training on 648112 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2113 (value: 0.0011, weighted value: 0.0531, policy: 0.1582, weighted policy: 0.1582), Train Mean Max: 0.9364
Epoch 2/10, Train Loss: 0.2032 (value: 0.0010, weighted value: 0.0487, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9369
Epoch 3/10, Train Loss: 0.1991 (value: 0.0010, weighted value: 0.0477, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9371
Epoch 4/10, Train Loss: 0.1940 (value: 0.0009, weighted value: 0.0438, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9373
Epoch 5/10, Train Loss: 0.1921 (value: 0.0009, weighted value: 0.0428, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9377
Epoch 6/10, Train Loss: 0.1908 (value: 0.0008, weighted value: 0.0419, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9376
Epoch 7/10, Train Loss: 0.1901 (value: 0.0008, weighted value: 0.0410, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9377
Epoch 8/10, Train Loss: 0.1873 (value: 0.0008, weighted value: 0.0388, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1879 (value: 0.0008, weighted value: 0.0388, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1872 (value: 0.0008, weighted value: 0.0390, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9381
..training done in 62.20 seconds
..evaluation done in 18.28 seconds
Old network+MCTS average reward: 0.6879, min: 0.0741, max: 1.6481, stdev: 0.2326
New network+MCTS average reward: 0.6913, min: 0.0741, max: 1.6204, stdev: 0.2313
Old bare network average reward: 0.6471, min: 0.0648, max: 1.6204, stdev: 0.2336
New bare network average reward: 0.6473, min: 0.0278, max: 1.5648, stdev: 0.2318
External policy "random" average reward: 0.2487, min: -0.2685, max: 0.8704, stdev: 0.2070
External policy "individual greedy" average reward: 0.5264, min: 0.0093, max: 1.2222, stdev: 0.2271
External policy "total greedy" average reward: 0.6373, min: 0.0833, max: 1.2778, stdev: 0.2150
New network won 86 and tied 151 out of 300 games (53.83% wins where ties are half wins)
Keeping the new network

Training iteration 322 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.04 seconds
Training examples lengths: [64934, 64653, 64742, 65040, 64802, 64708, 64835, 64998, 64646, 64620]
Total value: 465702.42
Training on 647978 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2100 (value: 0.0011, weighted value: 0.0528, policy: 0.1572, weighted policy: 0.1572), Train Mean Max: 0.9367
Epoch 2/10, Train Loss: 0.2020 (value: 0.0010, weighted value: 0.0479, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9371
Epoch 3/10, Train Loss: 0.1962 (value: 0.0009, weighted value: 0.0448, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9375
Epoch 4/10, Train Loss: 0.1943 (value: 0.0009, weighted value: 0.0442, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9377
Epoch 5/10, Train Loss: 0.1910 (value: 0.0009, weighted value: 0.0425, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9378
Epoch 6/10, Train Loss: 0.1890 (value: 0.0008, weighted value: 0.0404, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9378
Epoch 7/10, Train Loss: 0.1882 (value: 0.0008, weighted value: 0.0397, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1873 (value: 0.0008, weighted value: 0.0395, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9382
Epoch 9/10, Train Loss: 0.1850 (value: 0.0008, weighted value: 0.0377, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9382
Epoch 10/10, Train Loss: 0.1838 (value: 0.0007, weighted value: 0.0369, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9383
..training done in 60.45 seconds
..evaluation done in 19.22 seconds
Old network+MCTS average reward: 0.7032, min: 0.1296, max: 1.3611, stdev: 0.2355
New network+MCTS average reward: 0.7040, min: 0.1296, max: 1.3981, stdev: 0.2355
Old bare network average reward: 0.6583, min: 0.1019, max: 1.3056, stdev: 0.2383
New bare network average reward: 0.6620, min: 0.0741, max: 1.3241, stdev: 0.2440
External policy "random" average reward: 0.2499, min: -0.3611, max: 0.9167, stdev: 0.2237
External policy "individual greedy" average reward: 0.5401, min: -0.0185, max: 1.2407, stdev: 0.2318
External policy "total greedy" average reward: 0.6457, min: -0.0278, max: 1.4815, stdev: 0.2272
New network won 69 and tied 171 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 323 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.27 seconds
Training examples lengths: [64653, 64742, 65040, 64802, 64708, 64835, 64998, 64646, 64620, 64558]
Total value: 465446.43
Training on 647602 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2125 (value: 0.0011, weighted value: 0.0532, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9365
Epoch 2/10, Train Loss: 0.2034 (value: 0.0010, weighted value: 0.0487, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9372
Epoch 3/10, Train Loss: 0.1971 (value: 0.0009, weighted value: 0.0451, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9376
Epoch 4/10, Train Loss: 0.1937 (value: 0.0009, weighted value: 0.0431, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9377
Epoch 5/10, Train Loss: 0.1903 (value: 0.0008, weighted value: 0.0423, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9380
Epoch 6/10, Train Loss: 0.1886 (value: 0.0008, weighted value: 0.0404, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9379
Epoch 7/10, Train Loss: 0.1877 (value: 0.0008, weighted value: 0.0397, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9381
Epoch 8/10, Train Loss: 0.1871 (value: 0.0008, weighted value: 0.0387, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9382
Epoch 9/10, Train Loss: 0.1854 (value: 0.0008, weighted value: 0.0376, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1853 (value: 0.0008, weighted value: 0.0382, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9383
..training done in 60.66 seconds
..evaluation done in 19.05 seconds
Old network+MCTS average reward: 0.7202, min: 0.0833, max: 1.3611, stdev: 0.2232
New network+MCTS average reward: 0.7182, min: 0.0926, max: 1.3611, stdev: 0.2261
Old bare network average reward: 0.6807, min: 0.0833, max: 1.3611, stdev: 0.2239
New bare network average reward: 0.6823, min: 0.0278, max: 1.3611, stdev: 0.2276
External policy "random" average reward: 0.2714, min: -0.2870, max: 0.8333, stdev: 0.2084
External policy "individual greedy" average reward: 0.5518, min: -0.1111, max: 1.3148, stdev: 0.2224
External policy "total greedy" average reward: 0.6631, min: 0.0926, max: 1.3611, stdev: 0.2123
New network won 63 and tied 171 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 324 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.37 seconds
Training examples lengths: [64742, 65040, 64802, 64708, 64835, 64998, 64646, 64620, 64558, 64882]
Total value: 465525.27
Training on 647831 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2371 (value: 0.0014, weighted value: 0.0700, policy: 0.1672, weighted policy: 0.1672), Train Mean Max: 0.9351
Epoch 2/10, Train Loss: 0.2181 (value: 0.0012, weighted value: 0.0597, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9361
Epoch 3/10, Train Loss: 0.2110 (value: 0.0011, weighted value: 0.0553, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9365
Epoch 4/10, Train Loss: 0.2062 (value: 0.0011, weighted value: 0.0527, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9366
Epoch 5/10, Train Loss: 0.2012 (value: 0.0010, weighted value: 0.0503, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9370
Epoch 6/10, Train Loss: 0.1975 (value: 0.0009, weighted value: 0.0474, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1956 (value: 0.0009, weighted value: 0.0459, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1939 (value: 0.0009, weighted value: 0.0444, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9373
Epoch 9/10, Train Loss: 0.1938 (value: 0.0009, weighted value: 0.0442, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9375
Epoch 10/10, Train Loss: 0.1913 (value: 0.0008, weighted value: 0.0423, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9376
..training done in 65.63 seconds
..evaluation done in 18.99 seconds
Old network+MCTS average reward: 0.7356, min: 0.0463, max: 1.4259, stdev: 0.2311
New network+MCTS average reward: 0.7322, min: 0.1481, max: 1.4259, stdev: 0.2266
Old bare network average reward: 0.6869, min: -0.0463, max: 1.4259, stdev: 0.2398
New bare network average reward: 0.6869, min: 0.0000, max: 1.4259, stdev: 0.2362
External policy "random" average reward: 0.2772, min: -0.3333, max: 0.8333, stdev: 0.2229
External policy "individual greedy" average reward: 0.5558, min: -0.0833, max: 1.2593, stdev: 0.2326
External policy "total greedy" average reward: 0.6707, min: 0.1389, max: 1.3611, stdev: 0.2268
New network won 64 and tied 152 out of 300 games (46.67% wins where ties are half wins)
Reverting to the old network

Training iteration 325 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.82 seconds
Training examples lengths: [65040, 64802, 64708, 64835, 64998, 64646, 64620, 64558, 64882, 64696]
Total value: 465739.96
Training on 647785 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2604 (value: 0.0017, weighted value: 0.0844, policy: 0.1760, weighted policy: 0.1760), Train Mean Max: 0.9336
Epoch 2/10, Train Loss: 0.2371 (value: 0.0014, weighted value: 0.0721, policy: 0.1650, weighted policy: 0.1650), Train Mean Max: 0.9348
Epoch 3/10, Train Loss: 0.2233 (value: 0.0013, weighted value: 0.0653, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9355
Epoch 4/10, Train Loss: 0.2152 (value: 0.0012, weighted value: 0.0593, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9356
Epoch 5/10, Train Loss: 0.2122 (value: 0.0012, weighted value: 0.0580, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9358
Epoch 6/10, Train Loss: 0.2075 (value: 0.0011, weighted value: 0.0542, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9361
Epoch 7/10, Train Loss: 0.2045 (value: 0.0010, weighted value: 0.0519, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9362
Epoch 8/10, Train Loss: 0.2013 (value: 0.0010, weighted value: 0.0501, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9365
Epoch 9/10, Train Loss: 0.1992 (value: 0.0010, weighted value: 0.0487, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9366
Epoch 10/10, Train Loss: 0.1977 (value: 0.0009, weighted value: 0.0464, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9366
..training done in 64.75 seconds
..evaluation done in 18.46 seconds
Old network+MCTS average reward: 0.7133, min: 0.1019, max: 1.6852, stdev: 0.2289
New network+MCTS average reward: 0.7168, min: 0.1759, max: 1.6852, stdev: 0.2300
Old bare network average reward: 0.6769, min: 0.1111, max: 1.6852, stdev: 0.2349
New bare network average reward: 0.6762, min: 0.1111, max: 1.6111, stdev: 0.2301
External policy "random" average reward: 0.2769, min: -0.3519, max: 0.9722, stdev: 0.2142
External policy "individual greedy" average reward: 0.5516, min: -0.0556, max: 1.5463, stdev: 0.2296
External policy "total greedy" average reward: 0.6662, min: 0.1667, max: 1.6667, stdev: 0.2243
New network won 75 and tied 156 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 326 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.77 seconds
Training examples lengths: [64802, 64708, 64835, 64998, 64646, 64620, 64558, 64882, 64696, 64881]
Total value: 465475.76
Training on 647626 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2220 (value: 0.0012, weighted value: 0.0609, policy: 0.1611, weighted policy: 0.1611), Train Mean Max: 0.9351
Epoch 2/10, Train Loss: 0.2124 (value: 0.0011, weighted value: 0.0557, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9357
Epoch 3/10, Train Loss: 0.2064 (value: 0.0010, weighted value: 0.0522, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9360
Epoch 4/10, Train Loss: 0.2045 (value: 0.0010, weighted value: 0.0516, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9361
Epoch 5/10, Train Loss: 0.2000 (value: 0.0010, weighted value: 0.0480, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9364
Epoch 6/10, Train Loss: 0.1971 (value: 0.0009, weighted value: 0.0462, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9365
Epoch 7/10, Train Loss: 0.1956 (value: 0.0009, weighted value: 0.0451, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9368
Epoch 8/10, Train Loss: 0.1943 (value: 0.0009, weighted value: 0.0446, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9369
Epoch 9/10, Train Loss: 0.1930 (value: 0.0009, weighted value: 0.0427, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9369
Epoch 10/10, Train Loss: 0.1907 (value: 0.0008, weighted value: 0.0415, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9373
..training done in 63.61 seconds
..evaluation done in 18.11 seconds
Old network+MCTS average reward: 0.7097, min: 0.1759, max: 1.3611, stdev: 0.2211
New network+MCTS average reward: 0.7105, min: 0.0556, max: 1.3611, stdev: 0.2249
Old bare network average reward: 0.6658, min: 0.1389, max: 1.3333, stdev: 0.2275
New bare network average reward: 0.6680, min: 0.1389, max: 1.3148, stdev: 0.2243
External policy "random" average reward: 0.2471, min: -0.4907, max: 0.9537, stdev: 0.2307
External policy "individual greedy" average reward: 0.5303, min: -0.0185, max: 1.2130, stdev: 0.2275
External policy "total greedy" average reward: 0.6482, min: 0.0648, max: 1.2870, stdev: 0.2119
New network won 83 and tied 140 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 327 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.24 seconds
Training examples lengths: [64708, 64835, 64998, 64646, 64620, 64558, 64882, 64696, 64881, 64757]
Total value: 465798.54
Training on 647581 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2170 (value: 0.0011, weighted value: 0.0570, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9355
Epoch 2/10, Train Loss: 0.2085 (value: 0.0011, weighted value: 0.0530, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9361
Epoch 3/10, Train Loss: 0.2023 (value: 0.0010, weighted value: 0.0486, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9363
Epoch 4/10, Train Loss: 0.1997 (value: 0.0009, weighted value: 0.0475, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9366
Epoch 5/10, Train Loss: 0.1963 (value: 0.0009, weighted value: 0.0456, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1950 (value: 0.0009, weighted value: 0.0443, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9370
Epoch 7/10, Train Loss: 0.1937 (value: 0.0009, weighted value: 0.0426, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9371
Epoch 8/10, Train Loss: 0.1919 (value: 0.0008, weighted value: 0.0418, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9373
Epoch 9/10, Train Loss: 0.1902 (value: 0.0008, weighted value: 0.0413, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9375
Epoch 10/10, Train Loss: 0.1878 (value: 0.0008, weighted value: 0.0393, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9376
..training done in 61.12 seconds
..evaluation done in 18.82 seconds
Old network+MCTS average reward: 0.7119, min: 0.1852, max: 1.4352, stdev: 0.2123
New network+MCTS average reward: 0.7103, min: 0.2315, max: 1.4537, stdev: 0.2165
Old bare network average reward: 0.6649, min: 0.0926, max: 1.4352, stdev: 0.2196
New bare network average reward: 0.6676, min: 0.0833, max: 1.3889, stdev: 0.2206
External policy "random" average reward: 0.2611, min: -0.2593, max: 0.7222, stdev: 0.2057
External policy "individual greedy" average reward: 0.5462, min: -0.1111, max: 1.0926, stdev: 0.2177
External policy "total greedy" average reward: 0.6478, min: 0.1759, max: 1.2315, stdev: 0.2027
New network won 69 and tied 158 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 328 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.74 seconds
Training examples lengths: [64835, 64998, 64646, 64620, 64558, 64882, 64696, 64881, 64757, 64933]
Total value: 465871.27
Training on 647806 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2416 (value: 0.0015, weighted value: 0.0731, policy: 0.1686, weighted policy: 0.1686), Train Mean Max: 0.9339
Epoch 2/10, Train Loss: 0.2252 (value: 0.0013, weighted value: 0.0632, policy: 0.1620, weighted policy: 0.1620), Train Mean Max: 0.9347
Epoch 3/10, Train Loss: 0.2158 (value: 0.0012, weighted value: 0.0580, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9352
Epoch 4/10, Train Loss: 0.2094 (value: 0.0011, weighted value: 0.0547, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9355
Epoch 5/10, Train Loss: 0.2080 (value: 0.0011, weighted value: 0.0542, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9357
Epoch 6/10, Train Loss: 0.2027 (value: 0.0010, weighted value: 0.0503, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9359
Epoch 7/10, Train Loss: 0.2009 (value: 0.0010, weighted value: 0.0488, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9360
Epoch 8/10, Train Loss: 0.1988 (value: 0.0009, weighted value: 0.0465, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9361
Epoch 9/10, Train Loss: 0.1982 (value: 0.0009, weighted value: 0.0469, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9363
Epoch 10/10, Train Loss: 0.1944 (value: 0.0009, weighted value: 0.0437, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9366
..training done in 60.35 seconds
..evaluation done in 18.27 seconds
Old network+MCTS average reward: 0.7245, min: -0.0093, max: 1.4907, stdev: 0.2523
New network+MCTS average reward: 0.7252, min: -0.0093, max: 1.5370, stdev: 0.2543
Old bare network average reward: 0.6840, min: -0.0463, max: 1.4167, stdev: 0.2525
New bare network average reward: 0.6844, min: -0.0463, max: 1.4537, stdev: 0.2557
External policy "random" average reward: 0.2693, min: -0.3704, max: 0.9167, stdev: 0.2419
External policy "individual greedy" average reward: 0.5560, min: -0.1111, max: 1.2130, stdev: 0.2322
External policy "total greedy" average reward: 0.6677, min: 0.0185, max: 1.3426, stdev: 0.2329
New network won 76 and tied 146 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 329 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.23 seconds
Training examples lengths: [64998, 64646, 64620, 64558, 64882, 64696, 64881, 64757, 64933, 64736]
Total value: 465860.60
Training on 647707 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2634 (value: 0.0017, weighted value: 0.0864, policy: 0.1771, weighted policy: 0.1771), Train Mean Max: 0.9324
Epoch 2/10, Train Loss: 0.2415 (value: 0.0015, weighted value: 0.0743, policy: 0.1672, weighted policy: 0.1672), Train Mean Max: 0.9335
Epoch 3/10, Train Loss: 0.2287 (value: 0.0013, weighted value: 0.0669, policy: 0.1618, weighted policy: 0.1618), Train Mean Max: 0.9341
Epoch 4/10, Train Loss: 0.2222 (value: 0.0013, weighted value: 0.0638, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9343
Epoch 5/10, Train Loss: 0.2166 (value: 0.0012, weighted value: 0.0598, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9345
Epoch 6/10, Train Loss: 0.2122 (value: 0.0011, weighted value: 0.0562, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9348
Epoch 7/10, Train Loss: 0.2100 (value: 0.0011, weighted value: 0.0550, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9349
Epoch 8/10, Train Loss: 0.2054 (value: 0.0010, weighted value: 0.0511, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9353
Epoch 9/10, Train Loss: 0.2051 (value: 0.0010, weighted value: 0.0511, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9354
Epoch 10/10, Train Loss: 0.2019 (value: 0.0010, weighted value: 0.0486, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9356
..training done in 63.76 seconds
..evaluation done in 20.16 seconds
Old network+MCTS average reward: 0.7134, min: 0.0741, max: 1.3704, stdev: 0.2378
New network+MCTS average reward: 0.7204, min: 0.0741, max: 1.3981, stdev: 0.2385
Old bare network average reward: 0.6731, min: -0.0463, max: 1.3704, stdev: 0.2398
New bare network average reward: 0.6712, min: 0.0370, max: 1.3704, stdev: 0.2424
External policy "random" average reward: 0.2816, min: -0.3056, max: 0.9630, stdev: 0.2286
External policy "individual greedy" average reward: 0.5492, min: -0.0741, max: 1.3056, stdev: 0.2368
External policy "total greedy" average reward: 0.6590, min: 0.1019, max: 1.3981, stdev: 0.2293
New network won 90 and tied 142 out of 300 games (53.67% wins where ties are half wins)
Keeping the new network

Training iteration 330 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.01 seconds
Training examples lengths: [64646, 64620, 64558, 64882, 64696, 64881, 64757, 64933, 64736, 64816]
Total value: 465453.64
Training on 647525 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2230 (value: 0.0012, weighted value: 0.0613, policy: 0.1617, weighted policy: 0.1617), Train Mean Max: 0.9342
Epoch 2/10, Train Loss: 0.2149 (value: 0.0011, weighted value: 0.0571, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9346
Epoch 3/10, Train Loss: 0.2096 (value: 0.0011, weighted value: 0.0536, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9349
Epoch 4/10, Train Loss: 0.2067 (value: 0.0010, weighted value: 0.0522, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9353
Epoch 5/10, Train Loss: 0.2028 (value: 0.0010, weighted value: 0.0492, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9355
Epoch 6/10, Train Loss: 0.2014 (value: 0.0010, weighted value: 0.0476, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9357
Epoch 7/10, Train Loss: 0.1999 (value: 0.0010, weighted value: 0.0476, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9358
Epoch 8/10, Train Loss: 0.1980 (value: 0.0009, weighted value: 0.0454, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9359
Epoch 9/10, Train Loss: 0.1957 (value: 0.0009, weighted value: 0.0437, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9361
Epoch 10/10, Train Loss: 0.1937 (value: 0.0009, weighted value: 0.0425, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9363
..training done in 65.70 seconds
..evaluation done in 18.10 seconds
Old network+MCTS average reward: 0.7110, min: 0.1296, max: 1.3796, stdev: 0.2316
New network+MCTS average reward: 0.7122, min: 0.0926, max: 1.4259, stdev: 0.2330
Old bare network average reward: 0.6683, min: 0.0463, max: 1.3796, stdev: 0.2329
New bare network average reward: 0.6669, min: 0.0278, max: 1.2963, stdev: 0.2319
External policy "random" average reward: 0.2610, min: -0.2778, max: 0.9167, stdev: 0.2261
External policy "individual greedy" average reward: 0.5499, min: 0.0278, max: 1.2037, stdev: 0.2145
External policy "total greedy" average reward: 0.6501, min: 0.0926, max: 1.3611, stdev: 0.2156
New network won 76 and tied 149 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 331 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.45 seconds
Training examples lengths: [64620, 64558, 64882, 64696, 64881, 64757, 64933, 64736, 64816, 64853]
Total value: 466044.07
Training on 647732 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2184 (value: 0.0012, weighted value: 0.0577, policy: 0.1608, weighted policy: 0.1608), Train Mean Max: 0.9348
Epoch 2/10, Train Loss: 0.2080 (value: 0.0010, weighted value: 0.0514, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9357
Epoch 3/10, Train Loss: 0.2049 (value: 0.0010, weighted value: 0.0503, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9358
Epoch 4/10, Train Loss: 0.1996 (value: 0.0009, weighted value: 0.0472, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9361
Epoch 5/10, Train Loss: 0.1984 (value: 0.0009, weighted value: 0.0469, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9363
Epoch 6/10, Train Loss: 0.1955 (value: 0.0009, weighted value: 0.0440, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9363
Epoch 7/10, Train Loss: 0.1964 (value: 0.0009, weighted value: 0.0451, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9365
Epoch 8/10, Train Loss: 0.1921 (value: 0.0008, weighted value: 0.0418, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9367
Epoch 9/10, Train Loss: 0.1907 (value: 0.0008, weighted value: 0.0409, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9369
Epoch 10/10, Train Loss: 0.1892 (value: 0.0008, weighted value: 0.0395, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9370
..training done in 68.24 seconds
..evaluation done in 19.41 seconds
Old network+MCTS average reward: 0.7197, min: 0.1019, max: 1.4630, stdev: 0.2255
New network+MCTS average reward: 0.7223, min: 0.0648, max: 1.5185, stdev: 0.2310
Old bare network average reward: 0.6818, min: 0.0648, max: 1.4074, stdev: 0.2325
New bare network average reward: 0.6818, min: 0.0648, max: 1.5093, stdev: 0.2318
External policy "random" average reward: 0.2571, min: -0.3241, max: 0.9074, stdev: 0.2327
External policy "individual greedy" average reward: 0.5387, min: 0.0000, max: 1.3519, stdev: 0.2377
External policy "total greedy" average reward: 0.6551, min: 0.0833, max: 1.2963, stdev: 0.2312
New network won 83 and tied 149 out of 300 games (52.50% wins where ties are half wins)
Keeping the new network

Training iteration 332 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.24 seconds
Training examples lengths: [64558, 64882, 64696, 64881, 64757, 64933, 64736, 64816, 64853, 64860]
Total value: 466682.94
Training on 647972 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2158 (value: 0.0011, weighted value: 0.0550, policy: 0.1608, weighted policy: 0.1608), Train Mean Max: 0.9356
Epoch 2/10, Train Loss: 0.2070 (value: 0.0010, weighted value: 0.0515, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9360
Epoch 3/10, Train Loss: 0.2006 (value: 0.0009, weighted value: 0.0473, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9363
Epoch 4/10, Train Loss: 0.1971 (value: 0.0009, weighted value: 0.0453, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9367
Epoch 5/10, Train Loss: 0.1959 (value: 0.0009, weighted value: 0.0440, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9367
Epoch 6/10, Train Loss: 0.1945 (value: 0.0009, weighted value: 0.0433, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9367
Epoch 7/10, Train Loss: 0.1920 (value: 0.0008, weighted value: 0.0419, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9370
Epoch 8/10, Train Loss: 0.1911 (value: 0.0008, weighted value: 0.0407, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9370
Epoch 9/10, Train Loss: 0.1883 (value: 0.0008, weighted value: 0.0388, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9373
Epoch 10/10, Train Loss: 0.1880 (value: 0.0008, weighted value: 0.0387, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9374
..training done in 62.97 seconds
..evaluation done in 18.62 seconds
Old network+MCTS average reward: 0.6849, min: 0.1204, max: 1.5556, stdev: 0.2397
New network+MCTS average reward: 0.6849, min: 0.1296, max: 1.5556, stdev: 0.2370
Old bare network average reward: 0.6444, min: 0.0000, max: 1.5556, stdev: 0.2457
New bare network average reward: 0.6448, min: 0.0000, max: 1.5556, stdev: 0.2410
External policy "random" average reward: 0.2316, min: -0.3519, max: 0.9444, stdev: 0.2290
External policy "individual greedy" average reward: 0.5119, min: -0.1204, max: 1.3981, stdev: 0.2260
External policy "total greedy" average reward: 0.6267, min: 0.1667, max: 1.4074, stdev: 0.2197
New network won 73 and tied 159 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 333 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.99 seconds
Training examples lengths: [64882, 64696, 64881, 64757, 64933, 64736, 64816, 64853, 64860, 64707]
Total value: 467255.59
Training on 648121 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2142 (value: 0.0011, weighted value: 0.0549, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9357
Epoch 2/10, Train Loss: 0.2062 (value: 0.0010, weighted value: 0.0509, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9363
Epoch 3/10, Train Loss: 0.1990 (value: 0.0009, weighted value: 0.0462, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9367
Epoch 4/10, Train Loss: 0.1962 (value: 0.0009, weighted value: 0.0447, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9369
Epoch 5/10, Train Loss: 0.1937 (value: 0.0009, weighted value: 0.0430, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1920 (value: 0.0008, weighted value: 0.0423, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1913 (value: 0.0008, weighted value: 0.0416, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1903 (value: 0.0008, weighted value: 0.0403, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9372
Epoch 9/10, Train Loss: 0.1869 (value: 0.0008, weighted value: 0.0377, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9375
Epoch 10/10, Train Loss: 0.1875 (value: 0.0008, weighted value: 0.0386, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9376
..training done in 59.76 seconds
..evaluation done in 18.58 seconds
Old network+MCTS average reward: 0.7138, min: 0.2130, max: 1.6204, stdev: 0.2320
New network+MCTS average reward: 0.7167, min: 0.1296, max: 1.6204, stdev: 0.2331
Old bare network average reward: 0.6717, min: 0.1296, max: 1.6204, stdev: 0.2294
New bare network average reward: 0.6719, min: 0.1296, max: 1.6204, stdev: 0.2354
External policy "random" average reward: 0.2689, min: -0.2407, max: 1.1204, stdev: 0.2416
External policy "individual greedy" average reward: 0.5481, min: 0.0463, max: 1.4167, stdev: 0.2292
External policy "total greedy" average reward: 0.6669, min: 0.1852, max: 1.6111, stdev: 0.2183
New network won 80 and tied 161 out of 300 games (53.50% wins where ties are half wins)
Keeping the new network

Training iteration 334 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.58 seconds
Training examples lengths: [64696, 64881, 64757, 64933, 64736, 64816, 64853, 64860, 64707, 65015]
Total value: 468070.44
Training on 648254 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2119 (value: 0.0011, weighted value: 0.0537, policy: 0.1582, weighted policy: 0.1582), Train Mean Max: 0.9361
Epoch 2/10, Train Loss: 0.2023 (value: 0.0010, weighted value: 0.0482, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9368
Epoch 3/10, Train Loss: 0.1969 (value: 0.0009, weighted value: 0.0455, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9371
Epoch 4/10, Train Loss: 0.1943 (value: 0.0009, weighted value: 0.0433, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9371
Epoch 5/10, Train Loss: 0.1942 (value: 0.0009, weighted value: 0.0435, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9372
Epoch 6/10, Train Loss: 0.1902 (value: 0.0008, weighted value: 0.0409, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9374
Epoch 7/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0403, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9377
Epoch 8/10, Train Loss: 0.1863 (value: 0.0008, weighted value: 0.0383, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9379
Epoch 9/10, Train Loss: 0.1872 (value: 0.0008, weighted value: 0.0389, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1859 (value: 0.0008, weighted value: 0.0377, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9378
..training done in 59.60 seconds
..evaluation done in 18.56 seconds
Old network+MCTS average reward: 0.7369, min: 0.0741, max: 1.4722, stdev: 0.2463
New network+MCTS average reward: 0.7383, min: 0.0648, max: 1.3241, stdev: 0.2444
Old bare network average reward: 0.6977, min: 0.0370, max: 1.4722, stdev: 0.2542
New bare network average reward: 0.6942, min: 0.0648, max: 1.3241, stdev: 0.2516
External policy "random" average reward: 0.2958, min: -0.4167, max: 1.0093, stdev: 0.2266
External policy "individual greedy" average reward: 0.5674, min: 0.0185, max: 1.2870, stdev: 0.2292
External policy "total greedy" average reward: 0.6735, min: 0.1667, max: 1.3056, stdev: 0.2219
New network won 77 and tied 151 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 335 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.07 seconds
Training examples lengths: [64881, 64757, 64933, 64736, 64816, 64853, 64860, 64707, 65015, 64743]
Total value: 468474.03
Training on 648301 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2132 (value: 0.0011, weighted value: 0.0548, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9363
Epoch 2/10, Train Loss: 0.2014 (value: 0.0009, weighted value: 0.0470, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9369
Epoch 3/10, Train Loss: 0.1961 (value: 0.0009, weighted value: 0.0453, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9374
Epoch 4/10, Train Loss: 0.1925 (value: 0.0009, weighted value: 0.0428, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9375
Epoch 5/10, Train Loss: 0.1918 (value: 0.0009, weighted value: 0.0426, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0408, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9377
Epoch 7/10, Train Loss: 0.1886 (value: 0.0008, weighted value: 0.0400, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9378
Epoch 8/10, Train Loss: 0.1863 (value: 0.0008, weighted value: 0.0380, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1865 (value: 0.0008, weighted value: 0.0387, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9382
Epoch 10/10, Train Loss: 0.1846 (value: 0.0007, weighted value: 0.0371, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9382
..training done in 60.09 seconds
..evaluation done in 18.30 seconds
Old network+MCTS average reward: 0.7130, min: -0.1204, max: 1.3519, stdev: 0.2321
New network+MCTS average reward: 0.7150, min: 0.0185, max: 1.3611, stdev: 0.2320
Old bare network average reward: 0.6703, min: -0.1204, max: 1.3056, stdev: 0.2332
New bare network average reward: 0.6730, min: -0.0556, max: 1.3056, stdev: 0.2299
External policy "random" average reward: 0.2543, min: -0.5000, max: 0.9352, stdev: 0.2200
External policy "individual greedy" average reward: 0.5512, min: 0.0370, max: 1.1944, stdev: 0.2279
External policy "total greedy" average reward: 0.6556, min: 0.0463, max: 1.4630, stdev: 0.2199
New network won 74 and tied 167 out of 300 games (52.50% wins where ties are half wins)
Keeping the new network

Training iteration 336 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.40 seconds
Training examples lengths: [64757, 64933, 64736, 64816, 64853, 64860, 64707, 65015, 64743, 65007]
Total value: 469314.84
Training on 648427 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2107 (value: 0.0010, weighted value: 0.0523, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9364
Epoch 2/10, Train Loss: 0.1999 (value: 0.0009, weighted value: 0.0469, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9373
Epoch 3/10, Train Loss: 0.1956 (value: 0.0009, weighted value: 0.0448, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9375
Epoch 4/10, Train Loss: 0.1934 (value: 0.0009, weighted value: 0.0430, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1912 (value: 0.0008, weighted value: 0.0418, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9377
Epoch 6/10, Train Loss: 0.1881 (value: 0.0008, weighted value: 0.0397, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9380
Epoch 7/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0400, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1848 (value: 0.0007, weighted value: 0.0368, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9382
Epoch 9/10, Train Loss: 0.1845 (value: 0.0007, weighted value: 0.0373, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9383
Epoch 10/10, Train Loss: 0.1837 (value: 0.0007, weighted value: 0.0365, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9384
..training done in 60.31 seconds
..evaluation done in 18.10 seconds
Old network+MCTS average reward: 0.6962, min: 0.1296, max: 1.4537, stdev: 0.2293
New network+MCTS average reward: 0.6968, min: 0.1759, max: 1.4630, stdev: 0.2280
Old bare network average reward: 0.6589, min: 0.0556, max: 1.4444, stdev: 0.2354
New bare network average reward: 0.6538, min: 0.1111, max: 1.4630, stdev: 0.2310
External policy "random" average reward: 0.2592, min: -0.2685, max: 0.9907, stdev: 0.2162
External policy "individual greedy" average reward: 0.5209, min: -0.0833, max: 1.1574, stdev: 0.2249
External policy "total greedy" average reward: 0.6322, min: -0.0093, max: 1.3056, stdev: 0.2211
New network won 71 and tied 161 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 337 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.22 seconds
Training examples lengths: [64933, 64736, 64816, 64853, 64860, 64707, 65015, 64743, 65007, 64975]
Total value: 469690.60
Training on 648645 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2098 (value: 0.0010, weighted value: 0.0514, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9365
Epoch 2/10, Train Loss: 0.2014 (value: 0.0010, weighted value: 0.0480, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9374
Epoch 3/10, Train Loss: 0.1951 (value: 0.0009, weighted value: 0.0437, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9375
Epoch 4/10, Train Loss: 0.1924 (value: 0.0009, weighted value: 0.0431, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9378
Epoch 5/10, Train Loss: 0.1903 (value: 0.0008, weighted value: 0.0419, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9379
Epoch 6/10, Train Loss: 0.1865 (value: 0.0008, weighted value: 0.0387, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9381
Epoch 7/10, Train Loss: 0.1870 (value: 0.0008, weighted value: 0.0392, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9381
Epoch 8/10, Train Loss: 0.1844 (value: 0.0007, weighted value: 0.0366, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9383
Epoch 9/10, Train Loss: 0.1845 (value: 0.0007, weighted value: 0.0373, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1827 (value: 0.0007, weighted value: 0.0358, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9386
..training done in 59.62 seconds
..evaluation done in 18.80 seconds
Old network+MCTS average reward: 0.7194, min: 0.1296, max: 1.5370, stdev: 0.2309
New network+MCTS average reward: 0.7192, min: 0.0833, max: 1.5463, stdev: 0.2312
Old bare network average reward: 0.6752, min: 0.0833, max: 1.5370, stdev: 0.2371
New bare network average reward: 0.6748, min: 0.0741, max: 1.3519, stdev: 0.2358
External policy "random" average reward: 0.2596, min: -0.3796, max: 0.9444, stdev: 0.2139
External policy "individual greedy" average reward: 0.5447, min: -0.0463, max: 1.2963, stdev: 0.2250
External policy "total greedy" average reward: 0.6570, min: 0.0278, max: 1.4444, stdev: 0.2255
New network won 65 and tied 173 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 338 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.51 seconds
Training examples lengths: [64736, 64816, 64853, 64860, 64707, 65015, 64743, 65007, 64975, 64641]
Total value: 470092.87
Training on 648353 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2107 (value: 0.0011, weighted value: 0.0528, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9368
Epoch 2/10, Train Loss: 0.1989 (value: 0.0009, weighted value: 0.0454, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9374
Epoch 3/10, Train Loss: 0.1961 (value: 0.0009, weighted value: 0.0456, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9376
Epoch 4/10, Train Loss: 0.1908 (value: 0.0008, weighted value: 0.0417, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9378
Epoch 5/10, Train Loss: 0.1887 (value: 0.0008, weighted value: 0.0402, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9381
Epoch 6/10, Train Loss: 0.1885 (value: 0.0008, weighted value: 0.0398, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9380
Epoch 7/10, Train Loss: 0.1880 (value: 0.0008, weighted value: 0.0398, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9383
Epoch 8/10, Train Loss: 0.1833 (value: 0.0007, weighted value: 0.0363, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9385
Epoch 9/10, Train Loss: 0.1845 (value: 0.0008, weighted value: 0.0376, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9387
Epoch 10/10, Train Loss: 0.1821 (value: 0.0007, weighted value: 0.0356, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9386
..training done in 59.44 seconds
..evaluation done in 18.40 seconds
Old network+MCTS average reward: 0.7173, min: 0.1574, max: 1.5093, stdev: 0.2376
New network+MCTS average reward: 0.7155, min: 0.1574, max: 1.5463, stdev: 0.2397
Old bare network average reward: 0.6730, min: 0.1574, max: 1.5093, stdev: 0.2406
New bare network average reward: 0.6747, min: 0.1574, max: 1.5185, stdev: 0.2386
External policy "random" average reward: 0.2592, min: -0.3981, max: 1.1667, stdev: 0.2402
External policy "individual greedy" average reward: 0.5418, min: -0.0648, max: 1.3981, stdev: 0.2382
External policy "total greedy" average reward: 0.6561, min: 0.0556, max: 1.5185, stdev: 0.2265
New network won 61 and tied 175 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 339 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.64 seconds
Training examples lengths: [64816, 64853, 64860, 64707, 65015, 64743, 65007, 64975, 64641, 64821]
Total value: 470423.56
Training on 648438 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2346 (value: 0.0013, weighted value: 0.0671, policy: 0.1675, weighted policy: 0.1675), Train Mean Max: 0.9350
Epoch 2/10, Train Loss: 0.2176 (value: 0.0012, weighted value: 0.0581, policy: 0.1595, weighted policy: 0.1595), Train Mean Max: 0.9361
Epoch 3/10, Train Loss: 0.2074 (value: 0.0011, weighted value: 0.0530, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9368
Epoch 4/10, Train Loss: 0.2028 (value: 0.0010, weighted value: 0.0502, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9368
Epoch 5/10, Train Loss: 0.1980 (value: 0.0009, weighted value: 0.0474, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9371
Epoch 6/10, Train Loss: 0.1962 (value: 0.0009, weighted value: 0.0462, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1941 (value: 0.0009, weighted value: 0.0449, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9374
Epoch 8/10, Train Loss: 0.1914 (value: 0.0009, weighted value: 0.0430, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1917 (value: 0.0008, weighted value: 0.0420, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9375
Epoch 10/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0412, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9379
..training done in 59.87 seconds
..evaluation done in 17.77 seconds
Old network+MCTS average reward: 0.7280, min: 0.0556, max: 1.5648, stdev: 0.2483
New network+MCTS average reward: 0.7259, min: 0.0556, max: 1.5648, stdev: 0.2518
Old bare network average reward: 0.6838, min: 0.0556, max: 1.6111, stdev: 0.2537
New bare network average reward: 0.6832, min: 0.0556, max: 1.4722, stdev: 0.2510
External policy "random" average reward: 0.2715, min: -0.4074, max: 0.9444, stdev: 0.2344
External policy "individual greedy" average reward: 0.5562, min: 0.0093, max: 1.3148, stdev: 0.2390
External policy "total greedy" average reward: 0.6673, min: 0.1019, max: 1.3981, stdev: 0.2370
New network won 68 and tied 161 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 340 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.64 seconds
Training examples lengths: [64853, 64860, 64707, 65015, 64743, 65007, 64975, 64641, 64821, 65052]
Total value: 471297.77
Training on 648674 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2560 (value: 0.0016, weighted value: 0.0813, policy: 0.1747, weighted policy: 0.1747), Train Mean Max: 0.9338
Epoch 2/10, Train Loss: 0.2331 (value: 0.0014, weighted value: 0.0693, policy: 0.1638, weighted policy: 0.1638), Train Mean Max: 0.9351
Epoch 3/10, Train Loss: 0.2218 (value: 0.0013, weighted value: 0.0641, policy: 0.1577, weighted policy: 0.1577), Train Mean Max: 0.9355
Epoch 4/10, Train Loss: 0.2139 (value: 0.0012, weighted value: 0.0585, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9357
Epoch 5/10, Train Loss: 0.2087 (value: 0.0011, weighted value: 0.0553, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9361
Epoch 6/10, Train Loss: 0.2047 (value: 0.0010, weighted value: 0.0525, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9362
Epoch 7/10, Train Loss: 0.2022 (value: 0.0010, weighted value: 0.0512, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9365
Epoch 8/10, Train Loss: 0.1976 (value: 0.0010, weighted value: 0.0475, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9368
Epoch 9/10, Train Loss: 0.1964 (value: 0.0009, weighted value: 0.0468, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9368
Epoch 10/10, Train Loss: 0.1940 (value: 0.0009, weighted value: 0.0445, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9369
..training done in 64.60 seconds
..evaluation done in 18.59 seconds
Old network+MCTS average reward: 0.7130, min: -0.1204, max: 1.7222, stdev: 0.2324
New network+MCTS average reward: 0.7130, min: -0.1204, max: 1.7222, stdev: 0.2319
Old bare network average reward: 0.6705, min: -0.1019, max: 1.7222, stdev: 0.2335
New bare network average reward: 0.6717, min: -0.1019, max: 1.7222, stdev: 0.2312
External policy "random" average reward: 0.2610, min: -0.4074, max: 1.1019, stdev: 0.2226
External policy "individual greedy" average reward: 0.5268, min: -0.1667, max: 1.3704, stdev: 0.2336
External policy "total greedy" average reward: 0.6491, min: -0.0370, max: 1.4630, stdev: 0.2203
New network won 85 and tied 143 out of 300 games (52.17% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_340

Training iteration 341 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.05 seconds
Training examples lengths: [64860, 64707, 65015, 64743, 65007, 64975, 64641, 64821, 65052, 64809]
Total value: 471019.66
Training on 648630 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2172 (value: 0.0012, weighted value: 0.0591, policy: 0.1581, weighted policy: 0.1581), Train Mean Max: 0.9356
Epoch 2/10, Train Loss: 0.2102 (value: 0.0011, weighted value: 0.0554, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9361
Epoch 3/10, Train Loss: 0.2019 (value: 0.0010, weighted value: 0.0494, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9365
Epoch 4/10, Train Loss: 0.2010 (value: 0.0010, weighted value: 0.0501, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9367
Epoch 5/10, Train Loss: 0.1962 (value: 0.0009, weighted value: 0.0461, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1948 (value: 0.0009, weighted value: 0.0445, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1931 (value: 0.0009, weighted value: 0.0445, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1913 (value: 0.0009, weighted value: 0.0426, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1897 (value: 0.0008, weighted value: 0.0410, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9375
Epoch 10/10, Train Loss: 0.1871 (value: 0.0008, weighted value: 0.0395, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9378
..training done in 63.41 seconds
..evaluation done in 19.11 seconds
Old network+MCTS average reward: 0.7187, min: 0.0185, max: 1.3796, stdev: 0.2323
New network+MCTS average reward: 0.7210, min: 0.0000, max: 1.3333, stdev: 0.2331
Old bare network average reward: 0.6796, min: -0.0556, max: 1.3426, stdev: 0.2405
New bare network average reward: 0.6845, min: -0.0556, max: 1.3333, stdev: 0.2387
External policy "random" average reward: 0.2730, min: -0.3704, max: 0.9537, stdev: 0.2237
External policy "individual greedy" average reward: 0.5415, min: -0.1944, max: 1.1759, stdev: 0.2267
External policy "total greedy" average reward: 0.6553, min: -0.0093, max: 1.2407, stdev: 0.2221
New network won 72 and tied 157 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 342 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.94 seconds
Training examples lengths: [64707, 65015, 64743, 65007, 64975, 64641, 64821, 65052, 64809, 65124]
Total value: 471898.54
Training on 648894 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2137 (value: 0.0011, weighted value: 0.0561, policy: 0.1577, weighted policy: 0.1577), Train Mean Max: 0.9360
Epoch 2/10, Train Loss: 0.2039 (value: 0.0010, weighted value: 0.0496, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9367
Epoch 3/10, Train Loss: 0.2012 (value: 0.0010, weighted value: 0.0492, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9371
Epoch 4/10, Train Loss: 0.1938 (value: 0.0009, weighted value: 0.0445, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9372
Epoch 5/10, Train Loss: 0.1924 (value: 0.0009, weighted value: 0.0432, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9374
Epoch 6/10, Train Loss: 0.1912 (value: 0.0009, weighted value: 0.0428, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9375
Epoch 7/10, Train Loss: 0.1904 (value: 0.0008, weighted value: 0.0420, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9375
Epoch 8/10, Train Loss: 0.1876 (value: 0.0008, weighted value: 0.0396, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1887 (value: 0.0008, weighted value: 0.0404, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9378
Epoch 10/10, Train Loss: 0.1859 (value: 0.0008, weighted value: 0.0380, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9381
..training done in 69.67 seconds
..evaluation done in 19.55 seconds
Old network+MCTS average reward: 0.6992, min: 0.0648, max: 1.3796, stdev: 0.2530
New network+MCTS average reward: 0.6949, min: -0.0648, max: 1.3519, stdev: 0.2497
Old bare network average reward: 0.6601, min: -0.0463, max: 1.4167, stdev: 0.2521
New bare network average reward: 0.6504, min: -0.0278, max: 1.3519, stdev: 0.2521
External policy "random" average reward: 0.2492, min: -0.2870, max: 0.8796, stdev: 0.2350
External policy "individual greedy" average reward: 0.5206, min: -0.0926, max: 1.1944, stdev: 0.2504
External policy "total greedy" average reward: 0.6328, min: 0.0093, max: 1.2963, stdev: 0.2448
New network won 62 and tied 151 out of 300 games (45.83% wins where ties are half wins)
Reverting to the old network

Training iteration 343 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.46 seconds
Training examples lengths: [65015, 64743, 65007, 64975, 64641, 64821, 65052, 64809, 65124, 64738]
Total value: 471762.51
Training on 648925 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2400 (value: 0.0015, weighted value: 0.0727, policy: 0.1673, weighted policy: 0.1673), Train Mean Max: 0.9343
Epoch 2/10, Train Loss: 0.2186 (value: 0.0012, weighted value: 0.0595, policy: 0.1591, weighted policy: 0.1591), Train Mean Max: 0.9354
Epoch 3/10, Train Loss: 0.2118 (value: 0.0011, weighted value: 0.0568, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9358
Epoch 4/10, Train Loss: 0.2063 (value: 0.0011, weighted value: 0.0534, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9361
Epoch 5/10, Train Loss: 0.2017 (value: 0.0010, weighted value: 0.0502, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9364
Epoch 6/10, Train Loss: 0.2004 (value: 0.0010, weighted value: 0.0493, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9365
Epoch 7/10, Train Loss: 0.1971 (value: 0.0009, weighted value: 0.0468, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9367
Epoch 8/10, Train Loss: 0.1935 (value: 0.0009, weighted value: 0.0442, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9371
Epoch 9/10, Train Loss: 0.1943 (value: 0.0009, weighted value: 0.0445, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9370
Epoch 10/10, Train Loss: 0.1908 (value: 0.0008, weighted value: 0.0424, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9373
..training done in 64.44 seconds
..evaluation done in 18.57 seconds
Old network+MCTS average reward: 0.7342, min: 0.1019, max: 1.4722, stdev: 0.2282
New network+MCTS average reward: 0.7342, min: 0.1019, max: 1.5093, stdev: 0.2293
Old bare network average reward: 0.6908, min: 0.0278, max: 1.4722, stdev: 0.2272
New bare network average reward: 0.6919, min: 0.0833, max: 1.4722, stdev: 0.2316
External policy "random" average reward: 0.2715, min: -0.4259, max: 0.9259, stdev: 0.2277
External policy "individual greedy" average reward: 0.5461, min: 0.0185, max: 1.3519, stdev: 0.2201
External policy "total greedy" average reward: 0.6709, min: 0.0926, max: 1.4259, stdev: 0.2184
New network won 66 and tied 161 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 344 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.04 seconds
Training examples lengths: [64743, 65007, 64975, 64641, 64821, 65052, 64809, 65124, 64738, 64606]
Total value: 471090.56
Training on 648516 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2608 (value: 0.0017, weighted value: 0.0856, policy: 0.1752, weighted policy: 0.1752), Train Mean Max: 0.9330
Epoch 2/10, Train Loss: 0.2348 (value: 0.0014, weighted value: 0.0716, policy: 0.1631, weighted policy: 0.1631), Train Mean Max: 0.9343
Epoch 3/10, Train Loss: 0.2235 (value: 0.0013, weighted value: 0.0659, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9347
Epoch 4/10, Train Loss: 0.2168 (value: 0.0012, weighted value: 0.0607, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9351
Epoch 5/10, Train Loss: 0.2114 (value: 0.0012, weighted value: 0.0577, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9354
Epoch 6/10, Train Loss: 0.2090 (value: 0.0011, weighted value: 0.0552, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9355
Epoch 7/10, Train Loss: 0.2023 (value: 0.0010, weighted value: 0.0514, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9360
Epoch 8/10, Train Loss: 0.2014 (value: 0.0010, weighted value: 0.0500, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9361
Epoch 9/10, Train Loss: 0.1997 (value: 0.0010, weighted value: 0.0490, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9362
Epoch 10/10, Train Loss: 0.1972 (value: 0.0009, weighted value: 0.0468, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9365
..training done in 64.62 seconds
..evaluation done in 18.23 seconds
Old network+MCTS average reward: 0.7154, min: 0.1389, max: 1.6111, stdev: 0.2251
New network+MCTS average reward: 0.7144, min: 0.1667, max: 1.6944, stdev: 0.2269
Old bare network average reward: 0.6701, min: 0.1296, max: 1.6111, stdev: 0.2329
New bare network average reward: 0.6762, min: 0.0648, max: 1.6944, stdev: 0.2333
External policy "random" average reward: 0.2500, min: -0.3148, max: 0.9074, stdev: 0.2180
External policy "individual greedy" average reward: 0.5437, min: 0.0278, max: 1.5370, stdev: 0.2210
External policy "total greedy" average reward: 0.6583, min: 0.1852, max: 1.4630, stdev: 0.2198
New network won 71 and tied 143 out of 300 games (47.50% wins where ties are half wins)
Reverting to the old network

Training iteration 345 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.16 seconds
Training examples lengths: [65007, 64975, 64641, 64821, 65052, 64809, 65124, 64738, 64606, 64749]
Total value: 471458.01
Training on 648522 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2795 (value: 0.0020, weighted value: 0.0988, policy: 0.1807, weighted policy: 0.1807), Train Mean Max: 0.9316
Epoch 2/10, Train Loss: 0.2502 (value: 0.0016, weighted value: 0.0817, policy: 0.1684, weighted policy: 0.1684), Train Mean Max: 0.9331
Epoch 3/10, Train Loss: 0.2364 (value: 0.0015, weighted value: 0.0744, policy: 0.1621, weighted policy: 0.1621), Train Mean Max: 0.9337
Epoch 4/10, Train Loss: 0.2263 (value: 0.0013, weighted value: 0.0670, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9341
Epoch 5/10, Train Loss: 0.2192 (value: 0.0013, weighted value: 0.0633, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9344
Epoch 6/10, Train Loss: 0.2157 (value: 0.0012, weighted value: 0.0607, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9345
Epoch 7/10, Train Loss: 0.2117 (value: 0.0011, weighted value: 0.0573, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9349
Epoch 8/10, Train Loss: 0.2077 (value: 0.0011, weighted value: 0.0540, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9350
Epoch 9/10, Train Loss: 0.2055 (value: 0.0011, weighted value: 0.0530, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9353
Epoch 10/10, Train Loss: 0.2027 (value: 0.0010, weighted value: 0.0509, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9355
..training done in 64.76 seconds
..evaluation done in 18.43 seconds
Old network+MCTS average reward: 0.7201, min: 0.2037, max: 1.5278, stdev: 0.2267
New network+MCTS average reward: 0.7209, min: 0.2222, max: 1.4722, stdev: 0.2257
Old bare network average reward: 0.6844, min: 0.1296, max: 1.4537, stdev: 0.2314
New bare network average reward: 0.6847, min: 0.1296, max: 1.4722, stdev: 0.2292
External policy "random" average reward: 0.2650, min: -0.3519, max: 1.0093, stdev: 0.2222
External policy "individual greedy" average reward: 0.5354, min: -0.0370, max: 1.2037, stdev: 0.2152
External policy "total greedy" average reward: 0.6545, min: 0.1667, max: 1.3519, stdev: 0.2144
New network won 69 and tied 153 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 346 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.53 seconds
Training examples lengths: [64975, 64641, 64821, 65052, 64809, 65124, 64738, 64606, 64749, 64630]
Total value: 470646.78
Training on 648145 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2985 (value: 0.0022, weighted value: 0.1107, policy: 0.1879, weighted policy: 0.1879), Train Mean Max: 0.9304
Epoch 2/10, Train Loss: 0.2632 (value: 0.0018, weighted value: 0.0907, policy: 0.1725, weighted policy: 0.1725), Train Mean Max: 0.9320
Epoch 3/10, Train Loss: 0.2476 (value: 0.0016, weighted value: 0.0822, policy: 0.1654, weighted policy: 0.1654), Train Mean Max: 0.9328
Epoch 4/10, Train Loss: 0.2357 (value: 0.0015, weighted value: 0.0737, policy: 0.1620, weighted policy: 0.1620), Train Mean Max: 0.9331
Epoch 5/10, Train Loss: 0.2279 (value: 0.0014, weighted value: 0.0691, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9335
Epoch 6/10, Train Loss: 0.2234 (value: 0.0013, weighted value: 0.0657, policy: 0.1577, weighted policy: 0.1577), Train Mean Max: 0.9337
Epoch 7/10, Train Loss: 0.2176 (value: 0.0012, weighted value: 0.0617, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9340
Epoch 8/10, Train Loss: 0.2146 (value: 0.0012, weighted value: 0.0583, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9342
Epoch 9/10, Train Loss: 0.2106 (value: 0.0011, weighted value: 0.0557, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9347
Epoch 10/10, Train Loss: 0.2074 (value: 0.0011, weighted value: 0.0541, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9349
..training done in 64.32 seconds
..evaluation done in 18.77 seconds
Old network+MCTS average reward: 0.7475, min: 0.2130, max: 1.2685, stdev: 0.2331
New network+MCTS average reward: 0.7423, min: 0.1019, max: 1.2963, stdev: 0.2361
Old bare network average reward: 0.7071, min: 0.0648, max: 1.2407, stdev: 0.2348
New bare network average reward: 0.7006, min: 0.0926, max: 1.2963, stdev: 0.2416
External policy "random" average reward: 0.2902, min: -0.4537, max: 1.0278, stdev: 0.2390
External policy "individual greedy" average reward: 0.5641, min: -0.0370, max: 1.2685, stdev: 0.2396
External policy "total greedy" average reward: 0.6802, min: 0.1389, max: 1.2778, stdev: 0.2227
New network won 61 and tied 158 out of 300 games (46.67% wins where ties are half wins)
Reverting to the old network

Training iteration 347 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.78 seconds
Training examples lengths: [64641, 64821, 65052, 64809, 65124, 64738, 64606, 64749, 64630, 64904]
Total value: 471522.81
Training on 648074 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3200 (value: 0.0025, weighted value: 0.1256, policy: 0.1944, weighted policy: 0.1944), Train Mean Max: 0.9295
Epoch 2/10, Train Loss: 0.2761 (value: 0.0020, weighted value: 0.0995, policy: 0.1766, weighted policy: 0.1766), Train Mean Max: 0.9312
Epoch 3/10, Train Loss: 0.2575 (value: 0.0018, weighted value: 0.0887, policy: 0.1689, weighted policy: 0.1689), Train Mean Max: 0.9319
Epoch 4/10, Train Loss: 0.2462 (value: 0.0016, weighted value: 0.0822, policy: 0.1640, weighted policy: 0.1640), Train Mean Max: 0.9322
Epoch 5/10, Train Loss: 0.2356 (value: 0.0015, weighted value: 0.0747, policy: 0.1609, weighted policy: 0.1609), Train Mean Max: 0.9325
Epoch 6/10, Train Loss: 0.2298 (value: 0.0014, weighted value: 0.0705, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9329
Epoch 7/10, Train Loss: 0.2250 (value: 0.0013, weighted value: 0.0668, policy: 0.1582, weighted policy: 0.1582), Train Mean Max: 0.9331
Epoch 8/10, Train Loss: 0.2210 (value: 0.0013, weighted value: 0.0625, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9333
Epoch 9/10, Train Loss: 0.2174 (value: 0.0012, weighted value: 0.0608, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9337
Epoch 10/10, Train Loss: 0.2117 (value: 0.0011, weighted value: 0.0563, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9341
..training done in 59.44 seconds
..evaluation done in 18.53 seconds
Old network+MCTS average reward: 0.7035, min: 0.1111, max: 1.5833, stdev: 0.2328
New network+MCTS average reward: 0.7038, min: 0.1296, max: 1.6204, stdev: 0.2332
Old bare network average reward: 0.6624, min: 0.1019, max: 1.5833, stdev: 0.2416
New bare network average reward: 0.6612, min: 0.1019, max: 1.5833, stdev: 0.2389
External policy "random" average reward: 0.2507, min: -0.3056, max: 1.0741, stdev: 0.2320
External policy "individual greedy" average reward: 0.5340, min: -0.0648, max: 1.3519, stdev: 0.2266
External policy "total greedy" average reward: 0.6485, min: 0.0463, max: 1.5370, stdev: 0.2161
New network won 66 and tied 167 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 348 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.72 seconds
Training examples lengths: [64821, 65052, 64809, 65124, 64738, 64606, 64749, 64630, 64904, 64773]
Total value: 471409.77
Training on 648206 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3359 (value: 0.0027, weighted value: 0.1349, policy: 0.2010, weighted policy: 0.2010), Train Mean Max: 0.9288
Epoch 2/10, Train Loss: 0.2879 (value: 0.0022, weighted value: 0.1080, policy: 0.1799, weighted policy: 0.1799), Train Mean Max: 0.9303
Epoch 3/10, Train Loss: 0.2659 (value: 0.0019, weighted value: 0.0946, policy: 0.1714, weighted policy: 0.1714), Train Mean Max: 0.9309
Epoch 4/10, Train Loss: 0.2563 (value: 0.0018, weighted value: 0.0898, policy: 0.1665, weighted policy: 0.1665), Train Mean Max: 0.9312
Epoch 5/10, Train Loss: 0.2444 (value: 0.0016, weighted value: 0.0804, policy: 0.1640, weighted policy: 0.1640), Train Mean Max: 0.9316
Epoch 6/10, Train Loss: 0.2361 (value: 0.0015, weighted value: 0.0744, policy: 0.1617, weighted policy: 0.1617), Train Mean Max: 0.9320
Epoch 7/10, Train Loss: 0.2297 (value: 0.0014, weighted value: 0.0696, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9323
Epoch 8/10, Train Loss: 0.2252 (value: 0.0013, weighted value: 0.0663, policy: 0.1589, weighted policy: 0.1589), Train Mean Max: 0.9327
Epoch 9/10, Train Loss: 0.2217 (value: 0.0013, weighted value: 0.0636, policy: 0.1581, weighted policy: 0.1581), Train Mean Max: 0.9329
Epoch 10/10, Train Loss: 0.2171 (value: 0.0012, weighted value: 0.0599, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9333
..training done in 58.75 seconds
..evaluation done in 18.62 seconds
Old network+MCTS average reward: 0.6995, min: 0.1481, max: 1.6759, stdev: 0.2493
New network+MCTS average reward: 0.7028, min: 0.0185, max: 1.6111, stdev: 0.2461
Old bare network average reward: 0.6606, min: 0.1019, max: 1.6389, stdev: 0.2490
New bare network average reward: 0.6548, min: 0.0185, max: 1.5926, stdev: 0.2553
External policy "random" average reward: 0.2653, min: -0.3981, max: 1.0463, stdev: 0.2411
External policy "individual greedy" average reward: 0.5350, min: 0.0370, max: 1.2407, stdev: 0.2308
External policy "total greedy" average reward: 0.6417, min: 0.1111, max: 1.4259, stdev: 0.2260
New network won 86 and tied 143 out of 300 games (52.50% wins where ties are half wins)
Keeping the new network

Training iteration 349 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.99 seconds
Training examples lengths: [65052, 64809, 65124, 64738, 64606, 64749, 64630, 64904, 64773, 64968]
Total value: 471454.19
Training on 648353 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2365 (value: 0.0014, weighted value: 0.0717, policy: 0.1647, weighted policy: 0.1647), Train Mean Max: 0.9322
Epoch 2/10, Train Loss: 0.2263 (value: 0.0013, weighted value: 0.0646, policy: 0.1617, weighted policy: 0.1617), Train Mean Max: 0.9326
Epoch 3/10, Train Loss: 0.2196 (value: 0.0012, weighted value: 0.0607, policy: 0.1589, weighted policy: 0.1589), Train Mean Max: 0.9332
Epoch 4/10, Train Loss: 0.2144 (value: 0.0011, weighted value: 0.0575, policy: 0.1569, weighted policy: 0.1569), Train Mean Max: 0.9336
Epoch 5/10, Train Loss: 0.2125 (value: 0.0011, weighted value: 0.0569, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9337
Epoch 6/10, Train Loss: 0.2082 (value: 0.0011, weighted value: 0.0527, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9339
Epoch 7/10, Train Loss: 0.2068 (value: 0.0010, weighted value: 0.0524, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9341
Epoch 8/10, Train Loss: 0.2050 (value: 0.0010, weighted value: 0.0502, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9344
Epoch 9/10, Train Loss: 0.2019 (value: 0.0010, weighted value: 0.0488, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9347
Epoch 10/10, Train Loss: 0.1988 (value: 0.0009, weighted value: 0.0456, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9348
..training done in 59.17 seconds
..evaluation done in 18.72 seconds
Old network+MCTS average reward: 0.7258, min: 0.1296, max: 1.4722, stdev: 0.2386
New network+MCTS average reward: 0.7245, min: 0.1389, max: 1.4722, stdev: 0.2367
Old bare network average reward: 0.6852, min: 0.0278, max: 1.5000, stdev: 0.2417
New bare network average reward: 0.6871, min: 0.0278, max: 1.4630, stdev: 0.2416
External policy "random" average reward: 0.2714, min: -0.3611, max: 1.0556, stdev: 0.2307
External policy "individual greedy" average reward: 0.5573, min: -0.0185, max: 1.2870, stdev: 0.2352
External policy "total greedy" average reward: 0.6614, min: 0.0463, max: 1.5093, stdev: 0.2296
New network won 62 and tied 176 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 350 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.47 seconds
Training examples lengths: [64809, 65124, 64738, 64606, 64749, 64630, 64904, 64773, 64968, 64855]
Total value: 471042.58
Training on 648156 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2230 (value: 0.0012, weighted value: 0.0605, policy: 0.1625, weighted policy: 0.1625), Train Mean Max: 0.9336
Epoch 2/10, Train Loss: 0.2133 (value: 0.0011, weighted value: 0.0549, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9340
Epoch 3/10, Train Loss: 0.2086 (value: 0.0011, weighted value: 0.0527, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9344
Epoch 4/10, Train Loss: 0.2056 (value: 0.0010, weighted value: 0.0508, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9347
Epoch 5/10, Train Loss: 0.2014 (value: 0.0010, weighted value: 0.0478, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9350
Epoch 6/10, Train Loss: 0.1994 (value: 0.0009, weighted value: 0.0463, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9352
Epoch 7/10, Train Loss: 0.1990 (value: 0.0009, weighted value: 0.0469, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9354
Epoch 8/10, Train Loss: 0.1959 (value: 0.0009, weighted value: 0.0438, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9355
Epoch 9/10, Train Loss: 0.1938 (value: 0.0009, weighted value: 0.0429, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9358
Epoch 10/10, Train Loss: 0.1935 (value: 0.0008, weighted value: 0.0423, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9358
..training done in 60.31 seconds
..evaluation done in 18.40 seconds
Old network+MCTS average reward: 0.7245, min: 0.0741, max: 1.3981, stdev: 0.2425
New network+MCTS average reward: 0.7266, min: 0.0556, max: 1.4537, stdev: 0.2419
Old bare network average reward: 0.6821, min: 0.0093, max: 1.3704, stdev: 0.2455
New bare network average reward: 0.6834, min: -0.0093, max: 1.3704, stdev: 0.2470
External policy "random" average reward: 0.2729, min: -0.3333, max: 0.8981, stdev: 0.2298
External policy "individual greedy" average reward: 0.5465, min: -0.0185, max: 1.2870, stdev: 0.2384
External policy "total greedy" average reward: 0.6605, min: 0.0093, max: 1.2778, stdev: 0.2356
New network won 70 and tied 154 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 351 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.06 seconds
Training examples lengths: [65124, 64738, 64606, 64749, 64630, 64904, 64773, 64968, 64855, 64605]
Total value: 470969.83
Training on 647952 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2449 (value: 0.0015, weighted value: 0.0744, policy: 0.1705, weighted policy: 0.1705), Train Mean Max: 0.9322
Epoch 2/10, Train Loss: 0.2301 (value: 0.0013, weighted value: 0.0665, policy: 0.1637, weighted policy: 0.1637), Train Mean Max: 0.9331
Epoch 3/10, Train Loss: 0.2207 (value: 0.0012, weighted value: 0.0605, policy: 0.1602, weighted policy: 0.1602), Train Mean Max: 0.9335
Epoch 4/10, Train Loss: 0.2141 (value: 0.0011, weighted value: 0.0565, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9338
Epoch 5/10, Train Loss: 0.2112 (value: 0.0011, weighted value: 0.0552, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9340
Epoch 6/10, Train Loss: 0.2084 (value: 0.0011, weighted value: 0.0530, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9343
Epoch 7/10, Train Loss: 0.2048 (value: 0.0010, weighted value: 0.0501, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9344
Epoch 8/10, Train Loss: 0.2037 (value: 0.0010, weighted value: 0.0504, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9348
Epoch 9/10, Train Loss: 0.1985 (value: 0.0009, weighted value: 0.0460, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9350
Epoch 10/10, Train Loss: 0.1994 (value: 0.0009, weighted value: 0.0459, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9351
..training done in 59.85 seconds
..evaluation done in 18.14 seconds
Old network+MCTS average reward: 0.7400, min: 0.0833, max: 1.6296, stdev: 0.2320
New network+MCTS average reward: 0.7387, min: 0.1389, max: 1.6296, stdev: 0.2321
Old bare network average reward: 0.6980, min: -0.0093, max: 1.6111, stdev: 0.2411
New bare network average reward: 0.6956, min: -0.0093, max: 1.6111, stdev: 0.2381
External policy "random" average reward: 0.2827, min: -0.2778, max: 1.0000, stdev: 0.2335
External policy "individual greedy" average reward: 0.5618, min: -0.0185, max: 1.2963, stdev: 0.2278
External policy "total greedy" average reward: 0.6758, min: 0.1574, max: 1.4074, stdev: 0.2248
New network won 62 and tied 172 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 352 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.29 seconds
Training examples lengths: [64738, 64606, 64749, 64630, 64904, 64773, 64968, 64855, 64605, 64648]
Total value: 470504.88
Training on 647476 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2644 (value: 0.0017, weighted value: 0.0872, policy: 0.1772, weighted policy: 0.1772), Train Mean Max: 0.9314
Epoch 2/10, Train Loss: 0.2441 (value: 0.0015, weighted value: 0.0751, policy: 0.1690, weighted policy: 0.1690), Train Mean Max: 0.9323
Epoch 3/10, Train Loss: 0.2316 (value: 0.0014, weighted value: 0.0688, policy: 0.1628, weighted policy: 0.1628), Train Mean Max: 0.9328
Epoch 4/10, Train Loss: 0.2236 (value: 0.0013, weighted value: 0.0638, policy: 0.1598, weighted policy: 0.1598), Train Mean Max: 0.9332
Epoch 5/10, Train Loss: 0.2181 (value: 0.0012, weighted value: 0.0604, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9336
Epoch 6/10, Train Loss: 0.2148 (value: 0.0012, weighted value: 0.0581, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9338
Epoch 7/10, Train Loss: 0.2103 (value: 0.0011, weighted value: 0.0541, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9340
Epoch 8/10, Train Loss: 0.2080 (value: 0.0011, weighted value: 0.0537, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9343
Epoch 9/10, Train Loss: 0.2064 (value: 0.0010, weighted value: 0.0516, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9344
Epoch 10/10, Train Loss: 0.2031 (value: 0.0010, weighted value: 0.0488, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9348
..training done in 65.24 seconds
..evaluation done in 22.66 seconds
Old network+MCTS average reward: 0.6931, min: 0.1296, max: 1.4074, stdev: 0.2291
New network+MCTS average reward: 0.6932, min: 0.1759, max: 1.3426, stdev: 0.2279
Old bare network average reward: 0.6462, min: 0.1296, max: 1.3148, stdev: 0.2361
New bare network average reward: 0.6484, min: 0.1481, max: 1.2778, stdev: 0.2297
External policy "random" average reward: 0.2472, min: -0.3981, max: 0.8426, stdev: 0.2226
External policy "individual greedy" average reward: 0.5188, min: -0.1019, max: 1.1204, stdev: 0.2154
External policy "total greedy" average reward: 0.6311, min: 0.1019, max: 1.2963, stdev: 0.2110
New network won 75 and tied 143 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 353 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.93 seconds
Training examples lengths: [64606, 64749, 64630, 64904, 64773, 64968, 64855, 64605, 64648, 65147]
Total value: 471045.13
Training on 647885 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2830 (value: 0.0020, weighted value: 0.1007, policy: 0.1823, weighted policy: 0.1823), Train Mean Max: 0.9305
Epoch 2/10, Train Loss: 0.2565 (value: 0.0017, weighted value: 0.0846, policy: 0.1719, weighted policy: 0.1719), Train Mean Max: 0.9317
Epoch 3/10, Train Loss: 0.2409 (value: 0.0015, weighted value: 0.0756, policy: 0.1654, weighted policy: 0.1654), Train Mean Max: 0.9324
Epoch 4/10, Train Loss: 0.2316 (value: 0.0014, weighted value: 0.0696, policy: 0.1620, weighted policy: 0.1620), Train Mean Max: 0.9327
Epoch 5/10, Train Loss: 0.2267 (value: 0.0013, weighted value: 0.0662, policy: 0.1605, weighted policy: 0.1605), Train Mean Max: 0.9330
Epoch 6/10, Train Loss: 0.2196 (value: 0.0012, weighted value: 0.0615, policy: 0.1580, weighted policy: 0.1580), Train Mean Max: 0.9333
Epoch 7/10, Train Loss: 0.2160 (value: 0.0012, weighted value: 0.0590, policy: 0.1569, weighted policy: 0.1569), Train Mean Max: 0.9335
Epoch 8/10, Train Loss: 0.2125 (value: 0.0011, weighted value: 0.0564, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9340
Epoch 9/10, Train Loss: 0.2093 (value: 0.0011, weighted value: 0.0540, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9341
Epoch 10/10, Train Loss: 0.2066 (value: 0.0010, weighted value: 0.0519, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9345
..training done in 68.95 seconds
..evaluation done in 18.66 seconds
Old network+MCTS average reward: 0.7424, min: 0.1481, max: 1.3333, stdev: 0.2202
New network+MCTS average reward: 0.7439, min: 0.1574, max: 1.3426, stdev: 0.2222
Old bare network average reward: 0.6981, min: 0.1481, max: 1.2870, stdev: 0.2251
New bare network average reward: 0.6984, min: 0.1481, max: 1.3426, stdev: 0.2243
External policy "random" average reward: 0.2841, min: -0.3333, max: 0.8796, stdev: 0.2247
External policy "individual greedy" average reward: 0.5634, min: -0.0463, max: 1.1111, stdev: 0.2116
External policy "total greedy" average reward: 0.6754, min: 0.0833, max: 1.3889, stdev: 0.2093
New network won 77 and tied 151 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 354 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.44 seconds
Training examples lengths: [64749, 64630, 64904, 64773, 64968, 64855, 64605, 64648, 65147, 64632]
Total value: 471348.31
Training on 647911 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2266 (value: 0.0013, weighted value: 0.0646, policy: 0.1620, weighted policy: 0.1620), Train Mean Max: 0.9333
Epoch 2/10, Train Loss: 0.2179 (value: 0.0012, weighted value: 0.0585, policy: 0.1594, weighted policy: 0.1594), Train Mean Max: 0.9340
Epoch 3/10, Train Loss: 0.2118 (value: 0.0011, weighted value: 0.0554, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9345
Epoch 4/10, Train Loss: 0.2076 (value: 0.0011, weighted value: 0.0528, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9346
Epoch 5/10, Train Loss: 0.2046 (value: 0.0010, weighted value: 0.0518, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9349
Epoch 6/10, Train Loss: 0.2017 (value: 0.0010, weighted value: 0.0486, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9351
Epoch 7/10, Train Loss: 0.2001 (value: 0.0009, weighted value: 0.0473, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9354
Epoch 8/10, Train Loss: 0.1991 (value: 0.0009, weighted value: 0.0469, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9355
Epoch 9/10, Train Loss: 0.1953 (value: 0.0009, weighted value: 0.0438, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9358
Epoch 10/10, Train Loss: 0.1947 (value: 0.0009, weighted value: 0.0435, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9359
..training done in 59.19 seconds
..evaluation done in 18.13 seconds
Old network+MCTS average reward: 0.7138, min: 0.1852, max: 1.3611, stdev: 0.2201
New network+MCTS average reward: 0.7122, min: 0.1019, max: 1.3611, stdev: 0.2198
Old bare network average reward: 0.6738, min: 0.0463, max: 1.3611, stdev: 0.2228
New bare network average reward: 0.6754, min: 0.0556, max: 1.3611, stdev: 0.2259
External policy "random" average reward: 0.2603, min: -0.3611, max: 1.0093, stdev: 0.2067
External policy "individual greedy" average reward: 0.5341, min: -0.0741, max: 1.0463, stdev: 0.2103
External policy "total greedy" average reward: 0.6470, min: 0.1111, max: 1.2222, stdev: 0.2100
New network won 58 and tied 180 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 355 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.45 seconds
Training examples lengths: [64630, 64904, 64773, 64968, 64855, 64605, 64648, 65147, 64632, 64726]
Total value: 470681.04
Training on 647888 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2468 (value: 0.0016, weighted value: 0.0775, policy: 0.1693, weighted policy: 0.1693), Train Mean Max: 0.9320
Epoch 2/10, Train Loss: 0.2333 (value: 0.0014, weighted value: 0.0692, policy: 0.1640, weighted policy: 0.1640), Train Mean Max: 0.9330
Epoch 3/10, Train Loss: 0.2226 (value: 0.0013, weighted value: 0.0632, policy: 0.1594, weighted policy: 0.1594), Train Mean Max: 0.9335
Epoch 4/10, Train Loss: 0.2179 (value: 0.0012, weighted value: 0.0606, policy: 0.1573, weighted policy: 0.1573), Train Mean Max: 0.9336
Epoch 5/10, Train Loss: 0.2142 (value: 0.0012, weighted value: 0.0577, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9340
Epoch 6/10, Train Loss: 0.2098 (value: 0.0011, weighted value: 0.0540, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9343
Epoch 7/10, Train Loss: 0.2070 (value: 0.0010, weighted value: 0.0522, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9347
Epoch 8/10, Train Loss: 0.2045 (value: 0.0010, weighted value: 0.0504, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9348
Epoch 9/10, Train Loss: 0.2023 (value: 0.0010, weighted value: 0.0490, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9350
Epoch 10/10, Train Loss: 0.1992 (value: 0.0009, weighted value: 0.0466, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9353
..training done in 58.78 seconds
..evaluation done in 18.56 seconds
Old network+MCTS average reward: 0.7398, min: 0.1481, max: 1.4352, stdev: 0.2166
New network+MCTS average reward: 0.7406, min: 0.1481, max: 1.4352, stdev: 0.2178
Old bare network average reward: 0.6982, min: 0.0556, max: 1.4352, stdev: 0.2238
New bare network average reward: 0.6992, min: 0.0556, max: 1.2778, stdev: 0.2221
External policy "random" average reward: 0.2791, min: -0.2315, max: 0.9815, stdev: 0.2245
External policy "individual greedy" average reward: 0.5577, min: -0.0741, max: 1.1389, stdev: 0.2219
External policy "total greedy" average reward: 0.6775, min: 0.1481, max: 1.3333, stdev: 0.2150
New network won 72 and tied 165 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 356 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.15 seconds
Training examples lengths: [64904, 64773, 64968, 64855, 64605, 64648, 65147, 64632, 64726, 64753]
Total value: 471332.14
Training on 648011 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2222 (value: 0.0012, weighted value: 0.0607, policy: 0.1615, weighted policy: 0.1615), Train Mean Max: 0.9339
Epoch 2/10, Train Loss: 0.2129 (value: 0.0011, weighted value: 0.0549, policy: 0.1580, weighted policy: 0.1580), Train Mean Max: 0.9347
Epoch 3/10, Train Loss: 0.2076 (value: 0.0011, weighted value: 0.0529, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9351
Epoch 4/10, Train Loss: 0.2041 (value: 0.0010, weighted value: 0.0508, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9353
Epoch 5/10, Train Loss: 0.1996 (value: 0.0009, weighted value: 0.0472, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9355
Epoch 6/10, Train Loss: 0.1981 (value: 0.0009, weighted value: 0.0461, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9357
Epoch 7/10, Train Loss: 0.1971 (value: 0.0009, weighted value: 0.0454, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9358
Epoch 8/10, Train Loss: 0.1954 (value: 0.0009, weighted value: 0.0444, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9360
Epoch 9/10, Train Loss: 0.1935 (value: 0.0008, weighted value: 0.0422, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9361
Epoch 10/10, Train Loss: 0.1935 (value: 0.0008, weighted value: 0.0422, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9363
..training done in 59.88 seconds
..evaluation done in 19.10 seconds
Old network+MCTS average reward: 0.7036, min: 0.0556, max: 1.4722, stdev: 0.2423
New network+MCTS average reward: 0.7098, min: 0.1204, max: 1.4352, stdev: 0.2447
Old bare network average reward: 0.6657, min: 0.0556, max: 1.4074, stdev: 0.2436
New bare network average reward: 0.6652, min: 0.0278, max: 1.4352, stdev: 0.2476
External policy "random" average reward: 0.2543, min: -0.3611, max: 1.0741, stdev: 0.2328
External policy "individual greedy" average reward: 0.5341, min: -0.0463, max: 1.2407, stdev: 0.2429
External policy "total greedy" average reward: 0.6421, min: 0.0463, max: 1.3796, stdev: 0.2229
New network won 85 and tied 159 out of 300 games (54.83% wins where ties are half wins)
Keeping the new network

Training iteration 357 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.74 seconds
Training examples lengths: [64773, 64968, 64855, 64605, 64648, 65147, 64632, 64726, 64753, 64717]
Total value: 470542.53
Training on 647824 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2141 (value: 0.0011, weighted value: 0.0553, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9352
Epoch 2/10, Train Loss: 0.2067 (value: 0.0010, weighted value: 0.0509, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9356
Epoch 3/10, Train Loss: 0.2009 (value: 0.0010, weighted value: 0.0477, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9359
Epoch 4/10, Train Loss: 0.1991 (value: 0.0009, weighted value: 0.0474, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9362
Epoch 5/10, Train Loss: 0.1964 (value: 0.0009, weighted value: 0.0448, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9364
Epoch 6/10, Train Loss: 0.1939 (value: 0.0009, weighted value: 0.0430, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9365
Epoch 7/10, Train Loss: 0.1938 (value: 0.0009, weighted value: 0.0431, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9366
Epoch 8/10, Train Loss: 0.1902 (value: 0.0008, weighted value: 0.0404, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9368
Epoch 9/10, Train Loss: 0.1906 (value: 0.0008, weighted value: 0.0411, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9370
Epoch 10/10, Train Loss: 0.1877 (value: 0.0008, weighted value: 0.0375, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9370
..training done in 60.05 seconds
..evaluation done in 19.06 seconds
Old network+MCTS average reward: 0.7208, min: -0.0556, max: 1.3426, stdev: 0.2439
New network+MCTS average reward: 0.7221, min: -0.1111, max: 1.3519, stdev: 0.2443
Old bare network average reward: 0.6823, min: -0.0926, max: 1.3519, stdev: 0.2512
New bare network average reward: 0.6810, min: -0.1204, max: 1.3426, stdev: 0.2483
External policy "random" average reward: 0.2687, min: -0.3611, max: 1.0556, stdev: 0.2457
External policy "individual greedy" average reward: 0.5453, min: 0.0093, max: 1.2037, stdev: 0.2384
External policy "total greedy" average reward: 0.6551, min: 0.0185, max: 1.2870, stdev: 0.2338
New network won 69 and tied 152 out of 300 games (48.33% wins where ties are half wins)
Reverting to the old network

Training iteration 358 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.86 seconds
Training examples lengths: [64968, 64855, 64605, 64648, 65147, 64632, 64726, 64753, 64717, 64629]
Total value: 469811.90
Training on 647680 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2379 (value: 0.0014, weighted value: 0.0702, policy: 0.1676, weighted policy: 0.1676), Train Mean Max: 0.9336
Epoch 2/10, Train Loss: 0.2217 (value: 0.0012, weighted value: 0.0608, policy: 0.1609, weighted policy: 0.1609), Train Mean Max: 0.9346
Epoch 3/10, Train Loss: 0.2159 (value: 0.0012, weighted value: 0.0593, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9350
Epoch 4/10, Train Loss: 0.2072 (value: 0.0011, weighted value: 0.0531, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9353
Epoch 5/10, Train Loss: 0.2037 (value: 0.0010, weighted value: 0.0504, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9354
Epoch 6/10, Train Loss: 0.2021 (value: 0.0010, weighted value: 0.0495, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9358
Epoch 7/10, Train Loss: 0.1999 (value: 0.0010, weighted value: 0.0485, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9360
Epoch 8/10, Train Loss: 0.1972 (value: 0.0009, weighted value: 0.0456, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9360
Epoch 9/10, Train Loss: 0.1951 (value: 0.0009, weighted value: 0.0434, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9362
Epoch 10/10, Train Loss: 0.1947 (value: 0.0009, weighted value: 0.0439, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9366
..training done in 59.75 seconds
..evaluation done in 18.69 seconds
Old network+MCTS average reward: 0.7166, min: 0.1204, max: 1.3426, stdev: 0.2263
New network+MCTS average reward: 0.7199, min: 0.1204, max: 1.2963, stdev: 0.2245
Old bare network average reward: 0.6792, min: 0.0741, max: 1.3426, stdev: 0.2292
New bare network average reward: 0.6820, min: 0.1204, max: 1.2963, stdev: 0.2286
External policy "random" average reward: 0.2498, min: -0.3611, max: 0.9259, stdev: 0.2017
External policy "individual greedy" average reward: 0.5337, min: 0.0093, max: 1.0833, stdev: 0.1993
External policy "total greedy" average reward: 0.6494, min: 0.1481, max: 1.2222, stdev: 0.2062
New network won 72 and tied 171 out of 300 games (52.50% wins where ties are half wins)
Keeping the new network

Training iteration 359 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.55 seconds
Training examples lengths: [64855, 64605, 64648, 65147, 64632, 64726, 64753, 64717, 64629, 64786]
Total value: 470033.84
Training on 647498 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2171 (value: 0.0012, weighted value: 0.0577, policy: 0.1594, weighted policy: 0.1594), Train Mean Max: 0.9352
Epoch 2/10, Train Loss: 0.2096 (value: 0.0011, weighted value: 0.0529, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9357
Epoch 3/10, Train Loss: 0.2017 (value: 0.0010, weighted value: 0.0487, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9361
Epoch 4/10, Train Loss: 0.1997 (value: 0.0009, weighted value: 0.0475, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9363
Epoch 5/10, Train Loss: 0.1973 (value: 0.0009, weighted value: 0.0462, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9364
Epoch 6/10, Train Loss: 0.1957 (value: 0.0009, weighted value: 0.0454, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9366
Epoch 7/10, Train Loss: 0.1914 (value: 0.0008, weighted value: 0.0417, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9369
Epoch 8/10, Train Loss: 0.1915 (value: 0.0008, weighted value: 0.0423, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9370
Epoch 9/10, Train Loss: 0.1901 (value: 0.0008, weighted value: 0.0409, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9370
Epoch 10/10, Train Loss: 0.1892 (value: 0.0008, weighted value: 0.0398, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9372
..training done in 59.51 seconds
..evaluation done in 18.34 seconds
Old network+MCTS average reward: 0.7190, min: 0.2407, max: 1.3704, stdev: 0.2282
New network+MCTS average reward: 0.7146, min: 0.2407, max: 1.3241, stdev: 0.2217
Old bare network average reward: 0.6749, min: 0.1111, max: 1.3426, stdev: 0.2323
New bare network average reward: 0.6784, min: 0.1111, max: 1.3889, stdev: 0.2320
External policy "random" average reward: 0.2604, min: -0.3796, max: 1.0000, stdev: 0.2160
External policy "individual greedy" average reward: 0.5340, min: 0.0926, max: 1.1389, stdev: 0.2239
External policy "total greedy" average reward: 0.6469, min: 0.1667, max: 1.3241, stdev: 0.2196
New network won 62 and tied 153 out of 300 games (46.17% wins where ties are half wins)
Reverting to the old network

Training iteration 360 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.68 seconds
Training examples lengths: [64605, 64648, 65147, 64632, 64726, 64753, 64717, 64629, 64786, 64820]
Total value: 469879.67
Training on 647463 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2392 (value: 0.0014, weighted value: 0.0713, policy: 0.1679, weighted policy: 0.1679), Train Mean Max: 0.9339
Epoch 2/10, Train Loss: 0.2230 (value: 0.0012, weighted value: 0.0623, policy: 0.1607, weighted policy: 0.1607), Train Mean Max: 0.9347
Epoch 3/10, Train Loss: 0.2137 (value: 0.0011, weighted value: 0.0572, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9353
Epoch 4/10, Train Loss: 0.2103 (value: 0.0011, weighted value: 0.0554, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9355
Epoch 5/10, Train Loss: 0.2043 (value: 0.0010, weighted value: 0.0512, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9358
Epoch 6/10, Train Loss: 0.2027 (value: 0.0010, weighted value: 0.0497, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9358
Epoch 7/10, Train Loss: 0.1998 (value: 0.0010, weighted value: 0.0481, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9362
Epoch 8/10, Train Loss: 0.1975 (value: 0.0009, weighted value: 0.0461, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9362
Epoch 9/10, Train Loss: 0.1955 (value: 0.0009, weighted value: 0.0447, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9365
Epoch 10/10, Train Loss: 0.1943 (value: 0.0009, weighted value: 0.0443, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9367
..training done in 63.27 seconds
..evaluation done in 18.21 seconds
Old network+MCTS average reward: 0.7177, min: 0.1852, max: 1.4537, stdev: 0.2326
New network+MCTS average reward: 0.7163, min: 0.1852, max: 1.4815, stdev: 0.2326
Old bare network average reward: 0.6784, min: 0.1481, max: 1.5833, stdev: 0.2383
New bare network average reward: 0.6826, min: 0.1852, max: 1.5185, stdev: 0.2324
External policy "random" average reward: 0.2584, min: -0.2407, max: 0.9259, stdev: 0.2209
External policy "individual greedy" average reward: 0.5311, min: -0.0741, max: 1.4630, stdev: 0.2287
External policy "total greedy" average reward: 0.6499, min: 0.0833, max: 1.3889, stdev: 0.2183
New network won 75 and tied 152 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_360

Training iteration 361 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.34 seconds
Training examples lengths: [64648, 65147, 64632, 64726, 64753, 64717, 64629, 64786, 64820, 64744]
Total value: 470570.07
Training on 647602 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2170 (value: 0.0011, weighted value: 0.0568, policy: 0.1601, weighted policy: 0.1601), Train Mean Max: 0.9353
Epoch 2/10, Train Loss: 0.2095 (value: 0.0011, weighted value: 0.0527, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9358
Epoch 3/10, Train Loss: 0.2022 (value: 0.0010, weighted value: 0.0487, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9362
Epoch 4/10, Train Loss: 0.1985 (value: 0.0009, weighted value: 0.0467, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9365
Epoch 5/10, Train Loss: 0.1974 (value: 0.0009, weighted value: 0.0461, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9367
Epoch 6/10, Train Loss: 0.1937 (value: 0.0009, weighted value: 0.0435, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9369
Epoch 7/10, Train Loss: 0.1918 (value: 0.0008, weighted value: 0.0421, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9370
Epoch 8/10, Train Loss: 0.1919 (value: 0.0008, weighted value: 0.0418, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9371
Epoch 9/10, Train Loss: 0.1902 (value: 0.0008, weighted value: 0.0407, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9373
Epoch 10/10, Train Loss: 0.1895 (value: 0.0008, weighted value: 0.0403, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9374
..training done in 60.54 seconds
..evaluation done in 19.03 seconds
Old network+MCTS average reward: 0.7313, min: 0.1667, max: 1.4907, stdev: 0.2201
New network+MCTS average reward: 0.7315, min: 0.2037, max: 1.4907, stdev: 0.2174
Old bare network average reward: 0.6999, min: 0.1296, max: 1.4907, stdev: 0.2256
New bare network average reward: 0.6953, min: 0.1111, max: 1.4907, stdev: 0.2268
External policy "random" average reward: 0.2777, min: -0.4444, max: 1.0648, stdev: 0.2340
External policy "individual greedy" average reward: 0.5605, min: -0.0278, max: 1.5185, stdev: 0.2191
External policy "total greedy" average reward: 0.6689, min: 0.0741, max: 1.4907, stdev: 0.2052
New network won 71 and tied 154 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 362 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.66 seconds
Training examples lengths: [65147, 64632, 64726, 64753, 64717, 64629, 64786, 64820, 64744, 64791]
Total value: 470617.63
Training on 647745 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2391 (value: 0.0014, weighted value: 0.0705, policy: 0.1687, weighted policy: 0.1687), Train Mean Max: 0.9339
Epoch 2/10, Train Loss: 0.2254 (value: 0.0013, weighted value: 0.0633, policy: 0.1621, weighted policy: 0.1621), Train Mean Max: 0.9348
Epoch 3/10, Train Loss: 0.2149 (value: 0.0011, weighted value: 0.0566, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9352
Epoch 4/10, Train Loss: 0.2095 (value: 0.0011, weighted value: 0.0549, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9356
Epoch 5/10, Train Loss: 0.2052 (value: 0.0010, weighted value: 0.0514, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9357
Epoch 6/10, Train Loss: 0.2017 (value: 0.0010, weighted value: 0.0499, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9361
Epoch 7/10, Train Loss: 0.1993 (value: 0.0010, weighted value: 0.0476, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9362
Epoch 8/10, Train Loss: 0.1972 (value: 0.0009, weighted value: 0.0464, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9365
Epoch 9/10, Train Loss: 0.1951 (value: 0.0009, weighted value: 0.0442, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9366
Epoch 10/10, Train Loss: 0.1941 (value: 0.0009, weighted value: 0.0434, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9367
..training done in 60.27 seconds
..evaluation done in 19.65 seconds
Old network+MCTS average reward: 0.7173, min: 0.1852, max: 1.2778, stdev: 0.2148
New network+MCTS average reward: 0.7156, min: 0.1296, max: 1.2778, stdev: 0.2128
Old bare network average reward: 0.6742, min: 0.1111, max: 1.2778, stdev: 0.2147
New bare network average reward: 0.6743, min: 0.1111, max: 1.2778, stdev: 0.2147
External policy "random" average reward: 0.2565, min: -0.4537, max: 0.9259, stdev: 0.2372
External policy "individual greedy" average reward: 0.5259, min: -0.0370, max: 1.0926, stdev: 0.2128
External policy "total greedy" average reward: 0.6458, min: -0.0185, max: 1.1204, stdev: 0.2073
New network won 70 and tied 151 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 363 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.12 seconds
Training examples lengths: [64632, 64726, 64753, 64717, 64629, 64786, 64820, 64744, 64791, 64586]
Total value: 470001.52
Training on 647184 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2633 (value: 0.0017, weighted value: 0.0851, policy: 0.1782, weighted policy: 0.1782), Train Mean Max: 0.9325
Epoch 2/10, Train Loss: 0.2709 (value: 0.0016, weighted value: 0.0796, policy: 0.1913, weighted policy: 0.1913), Train Mean Max: 0.9325
Epoch 3/10, Train Loss: 0.2309 (value: 0.0013, weighted value: 0.0661, policy: 0.1648, weighted policy: 0.1648), Train Mean Max: 0.9342
Epoch 4/10, Train Loss: 0.2302 (value: 0.0013, weighted value: 0.0626, policy: 0.1677, weighted policy: 0.1677), Train Mean Max: 0.9340
Epoch 5/10, Train Loss: 0.2202 (value: 0.0012, weighted value: 0.0608, policy: 0.1594, weighted policy: 0.1594), Train Mean Max: 0.9348
Epoch 6/10, Train Loss: 0.2108 (value: 0.0011, weighted value: 0.0573, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9350
Epoch 7/10, Train Loss: 0.2140 (value: 0.0011, weighted value: 0.0530, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9346
Epoch 8/10, Train Loss: 0.2092 (value: 0.0011, weighted value: 0.0550, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9351
Epoch 9/10, Train Loss: 0.2027 (value: 0.0010, weighted value: 0.0501, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9354
Epoch 10/10, Train Loss: 0.2047 (value: 0.0010, weighted value: 0.0480, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9352
..training done in 59.35 seconds
..evaluation done in 18.47 seconds
Old network+MCTS average reward: 0.7027, min: -0.0648, max: 1.3796, stdev: 0.2293
New network+MCTS average reward: 0.7067, min: -0.0926, max: 1.3796, stdev: 0.2314
Old bare network average reward: 0.6586, min: -0.1019, max: 1.3796, stdev: 0.2363
New bare network average reward: 0.6635, min: -0.1296, max: 1.2778, stdev: 0.2325
External policy "random" average reward: 0.2512, min: -0.7685, max: 0.8241, stdev: 0.2213
External policy "individual greedy" average reward: 0.5359, min: -0.2407, max: 1.1481, stdev: 0.2214
External policy "total greedy" average reward: 0.6515, min: -0.2037, max: 1.3704, stdev: 0.2164
New network won 96 and tied 127 out of 300 games (53.17% wins where ties are half wins)
Keeping the new network

Training iteration 364 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.25 seconds
Training examples lengths: [64726, 64753, 64717, 64629, 64786, 64820, 64744, 64791, 64586, 64851]
Total value: 470217.85
Training on 647403 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2304 (value: 0.0012, weighted value: 0.0621, policy: 0.1683, weighted policy: 0.1683), Train Mean Max: 0.9339
Epoch 2/10, Train Loss: 0.2156 (value: 0.0011, weighted value: 0.0554, policy: 0.1602, weighted policy: 0.1602), Train Mean Max: 0.9347
Epoch 3/10, Train Loss: 0.2100 (value: 0.0010, weighted value: 0.0523, policy: 0.1577, weighted policy: 0.1577), Train Mean Max: 0.9349
Epoch 4/10, Train Loss: 0.2062 (value: 0.0010, weighted value: 0.0502, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9350
Epoch 5/10, Train Loss: 0.2029 (value: 0.0010, weighted value: 0.0485, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9354
Epoch 6/10, Train Loss: 0.2005 (value: 0.0009, weighted value: 0.0466, policy: 0.1539, weighted policy: 0.1539), Train Mean Max: 0.9353
Epoch 7/10, Train Loss: 0.1990 (value: 0.0009, weighted value: 0.0453, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9356
Epoch 8/10, Train Loss: 0.1958 (value: 0.0009, weighted value: 0.0433, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9358
Epoch 9/10, Train Loss: 0.1957 (value: 0.0009, weighted value: 0.0429, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9360
Epoch 10/10, Train Loss: 0.1940 (value: 0.0008, weighted value: 0.0420, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9362
..training done in 63.65 seconds
..evaluation done in 17.70 seconds
Old network+MCTS average reward: 0.7127, min: 0.1296, max: 1.4259, stdev: 0.2309
New network+MCTS average reward: 0.7156, min: 0.1296, max: 1.4444, stdev: 0.2278
Old bare network average reward: 0.6770, min: 0.1019, max: 1.4444, stdev: 0.2311
New bare network average reward: 0.6739, min: -0.0556, max: 1.4074, stdev: 0.2337
External policy "random" average reward: 0.2543, min: -0.4815, max: 0.9815, stdev: 0.2241
External policy "individual greedy" average reward: 0.5317, min: -0.0926, max: 1.1944, stdev: 0.2233
External policy "total greedy" average reward: 0.6426, min: 0.0926, max: 1.2500, stdev: 0.2221
New network won 88 and tied 148 out of 300 games (54.00% wins where ties are half wins)
Keeping the new network

Training iteration 365 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.83 seconds
Training examples lengths: [64753, 64717, 64629, 64786, 64820, 64744, 64791, 64586, 64851, 64718]
Total value: 470015.44
Training on 647395 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2159 (value: 0.0011, weighted value: 0.0540, policy: 0.1619, weighted policy: 0.1619), Train Mean Max: 0.9347
Epoch 2/10, Train Loss: 0.2096 (value: 0.0010, weighted value: 0.0513, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9351
Epoch 3/10, Train Loss: 0.2026 (value: 0.0009, weighted value: 0.0472, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9357
Epoch 4/10, Train Loss: 0.2003 (value: 0.0009, weighted value: 0.0457, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9358
Epoch 5/10, Train Loss: 0.1962 (value: 0.0009, weighted value: 0.0441, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9363
Epoch 6/10, Train Loss: 0.1950 (value: 0.0009, weighted value: 0.0429, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9361
Epoch 7/10, Train Loss: 0.1925 (value: 0.0008, weighted value: 0.0407, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9363
Epoch 8/10, Train Loss: 0.1920 (value: 0.0008, weighted value: 0.0410, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9364
Epoch 9/10, Train Loss: 0.1907 (value: 0.0008, weighted value: 0.0402, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9367
Epoch 10/10, Train Loss: 0.1888 (value: 0.0008, weighted value: 0.0384, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9368
..training done in 64.18 seconds
..evaluation done in 18.74 seconds
Old network+MCTS average reward: 0.6985, min: -0.0093, max: 1.3426, stdev: 0.2380
New network+MCTS average reward: 0.6980, min: -0.0093, max: 1.3148, stdev: 0.2363
Old bare network average reward: 0.6620, min: 0.0000, max: 1.2778, stdev: 0.2381
New bare network average reward: 0.6580, min: -0.0741, max: 1.3426, stdev: 0.2414
External policy "random" average reward: 0.2499, min: -0.5185, max: 0.9074, stdev: 0.2287
External policy "individual greedy" average reward: 0.5208, min: -0.1667, max: 1.1389, stdev: 0.2368
External policy "total greedy" average reward: 0.6355, min: 0.0185, max: 1.1944, stdev: 0.2236
New network won 66 and tied 162 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 366 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.51 seconds
Training examples lengths: [64717, 64629, 64786, 64820, 64744, 64791, 64586, 64851, 64718, 64734]
Total value: 470304.44
Training on 647376 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2410 (value: 0.0014, weighted value: 0.0707, policy: 0.1704, weighted policy: 0.1704), Train Mean Max: 0.9331
Epoch 2/10, Train Loss: 0.2241 (value: 0.0012, weighted value: 0.0600, policy: 0.1640, weighted policy: 0.1640), Train Mean Max: 0.9341
Epoch 3/10, Train Loss: 0.2157 (value: 0.0011, weighted value: 0.0564, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9345
Epoch 4/10, Train Loss: 0.2107 (value: 0.0010, weighted value: 0.0525, policy: 0.1582, weighted policy: 0.1582), Train Mean Max: 0.9346
Epoch 5/10, Train Loss: 0.2086 (value: 0.0011, weighted value: 0.0531, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9348
Epoch 6/10, Train Loss: 0.2035 (value: 0.0010, weighted value: 0.0490, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9351
Epoch 7/10, Train Loss: 0.2007 (value: 0.0009, weighted value: 0.0468, policy: 0.1539, weighted policy: 0.1539), Train Mean Max: 0.9354
Epoch 8/10, Train Loss: 0.1994 (value: 0.0009, weighted value: 0.0459, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9355
Epoch 9/10, Train Loss: 0.1961 (value: 0.0009, weighted value: 0.0432, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9357
Epoch 10/10, Train Loss: 0.1965 (value: 0.0009, weighted value: 0.0438, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9359
..training done in 64.70 seconds
..evaluation done in 19.07 seconds
Old network+MCTS average reward: 0.7363, min: 0.1389, max: 1.3611, stdev: 0.2479
New network+MCTS average reward: 0.7362, min: 0.1111, max: 1.3519, stdev: 0.2449
Old bare network average reward: 0.6958, min: 0.0000, max: 1.3519, stdev: 0.2496
New bare network average reward: 0.6930, min: 0.0278, max: 1.3148, stdev: 0.2495
External policy "random" average reward: 0.2841, min: -0.3889, max: 1.2315, stdev: 0.2319
External policy "individual greedy" average reward: 0.5579, min: -0.1481, max: 1.3148, stdev: 0.2307
External policy "total greedy" average reward: 0.6643, min: 0.0185, max: 1.3796, stdev: 0.2322
New network won 79 and tied 139 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 367 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.42 seconds
Training examples lengths: [64629, 64786, 64820, 64744, 64791, 64586, 64851, 64718, 64734, 64831]
Total value: 470145.17
Training on 647490 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2602 (value: 0.0016, weighted value: 0.0820, policy: 0.1782, weighted policy: 0.1782), Train Mean Max: 0.9319
Epoch 2/10, Train Loss: 0.2398 (value: 0.0014, weighted value: 0.0706, policy: 0.1692, weighted policy: 0.1692), Train Mean Max: 0.9327
Epoch 3/10, Train Loss: 0.2268 (value: 0.0013, weighted value: 0.0638, policy: 0.1630, weighted policy: 0.1630), Train Mean Max: 0.9334
Epoch 4/10, Train Loss: 0.2205 (value: 0.0012, weighted value: 0.0606, policy: 0.1599, weighted policy: 0.1599), Train Mean Max: 0.9336
Epoch 5/10, Train Loss: 0.2151 (value: 0.0011, weighted value: 0.0567, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9339
Epoch 6/10, Train Loss: 0.2120 (value: 0.0011, weighted value: 0.0544, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9339
Epoch 7/10, Train Loss: 0.2075 (value: 0.0010, weighted value: 0.0510, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9344
Epoch 8/10, Train Loss: 0.2055 (value: 0.0010, weighted value: 0.0508, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9346
Epoch 9/10, Train Loss: 0.2041 (value: 0.0010, weighted value: 0.0491, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9347
Epoch 10/10, Train Loss: 0.2003 (value: 0.0009, weighted value: 0.0453, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9349
..training done in 65.79 seconds
..evaluation done in 18.99 seconds
Old network+MCTS average reward: 0.7281, min: 0.0370, max: 1.4630, stdev: 0.2503
New network+MCTS average reward: 0.7292, min: 0.0370, max: 1.4630, stdev: 0.2502
Old bare network average reward: 0.6880, min: -0.0093, max: 1.4630, stdev: 0.2542
New bare network average reward: 0.6886, min: -0.0093, max: 1.4630, stdev: 0.2508
External policy "random" average reward: 0.2680, min: -0.2870, max: 0.9907, stdev: 0.2361
External policy "individual greedy" average reward: 0.5543, min: -0.1019, max: 1.3704, stdev: 0.2452
External policy "total greedy" average reward: 0.6758, min: 0.0833, max: 1.4722, stdev: 0.2329
New network won 73 and tied 156 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 368 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.16 seconds
Training examples lengths: [64786, 64820, 64744, 64791, 64586, 64851, 64718, 64734, 64831, 64822]
Total value: 471116.48
Training on 647683 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2226 (value: 0.0012, weighted value: 0.0597, policy: 0.1629, weighted policy: 0.1629), Train Mean Max: 0.9337
Epoch 2/10, Train Loss: 0.2129 (value: 0.0011, weighted value: 0.0532, policy: 0.1597, weighted policy: 0.1597), Train Mean Max: 0.9341
Epoch 3/10, Train Loss: 0.2066 (value: 0.0010, weighted value: 0.0502, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9345
Epoch 4/10, Train Loss: 0.2048 (value: 0.0010, weighted value: 0.0491, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9347
Epoch 5/10, Train Loss: 0.2006 (value: 0.0009, weighted value: 0.0465, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9350
Epoch 6/10, Train Loss: 0.1994 (value: 0.0009, weighted value: 0.0452, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9351
Epoch 7/10, Train Loss: 0.1966 (value: 0.0009, weighted value: 0.0438, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9354
Epoch 8/10, Train Loss: 0.1951 (value: 0.0009, weighted value: 0.0426, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9354
Epoch 9/10, Train Loss: 0.1946 (value: 0.0008, weighted value: 0.0420, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9357
Epoch 10/10, Train Loss: 0.1922 (value: 0.0008, weighted value: 0.0398, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9359
..training done in 64.84 seconds
..evaluation done in 18.64 seconds
Old network+MCTS average reward: 0.7400, min: 0.2222, max: 1.4907, stdev: 0.2199
New network+MCTS average reward: 0.7365, min: 0.2222, max: 1.4722, stdev: 0.2167
Old bare network average reward: 0.6995, min: 0.2222, max: 1.4167, stdev: 0.2192
New bare network average reward: 0.6972, min: 0.1944, max: 1.4074, stdev: 0.2189
External policy "random" average reward: 0.2849, min: -0.3426, max: 1.0833, stdev: 0.2201
External policy "individual greedy" average reward: 0.5579, min: 0.0741, max: 1.3704, stdev: 0.2233
External policy "total greedy" average reward: 0.6704, min: 0.1296, max: 1.3148, stdev: 0.2101
New network won 64 and tied 158 out of 300 games (47.67% wins where ties are half wins)
Reverting to the old network

Training iteration 369 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.23 seconds
Training examples lengths: [64820, 64744, 64791, 64586, 64851, 64718, 64734, 64831, 64822, 65236]
Total value: 471459.77
Training on 648133 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2445 (value: 0.0015, weighted value: 0.0738, policy: 0.1707, weighted policy: 0.1707), Train Mean Max: 0.9322
Epoch 2/10, Train Loss: 0.2260 (value: 0.0013, weighted value: 0.0629, policy: 0.1631, weighted policy: 0.1631), Train Mean Max: 0.9331
Epoch 3/10, Train Loss: 0.2181 (value: 0.0012, weighted value: 0.0581, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9335
Epoch 4/10, Train Loss: 0.2143 (value: 0.0011, weighted value: 0.0563, policy: 0.1580, weighted policy: 0.1580), Train Mean Max: 0.9337
Epoch 5/10, Train Loss: 0.2092 (value: 0.0011, weighted value: 0.0527, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9341
Epoch 6/10, Train Loss: 0.2068 (value: 0.0010, weighted value: 0.0510, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9343
Epoch 7/10, Train Loss: 0.2041 (value: 0.0010, weighted value: 0.0495, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9345
Epoch 8/10, Train Loss: 0.2020 (value: 0.0009, weighted value: 0.0472, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9347
Epoch 9/10, Train Loss: 0.1999 (value: 0.0009, weighted value: 0.0457, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9348
Epoch 10/10, Train Loss: 0.1971 (value: 0.0009, weighted value: 0.0437, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9351
..training done in 60.06 seconds
..evaluation done in 18.83 seconds
Old network+MCTS average reward: 0.7474, min: 0.1019, max: 1.5463, stdev: 0.2415
New network+MCTS average reward: 0.7439, min: 0.1019, max: 1.5463, stdev: 0.2408
Old bare network average reward: 0.7066, min: 0.1019, max: 1.4259, stdev: 0.2472
New bare network average reward: 0.7043, min: 0.0741, max: 1.5463, stdev: 0.2507
External policy "random" average reward: 0.2828, min: -0.2963, max: 1.1019, stdev: 0.2255
External policy "individual greedy" average reward: 0.5552, min: -0.0185, max: 1.3241, stdev: 0.2409
External policy "total greedy" average reward: 0.6665, min: 0.1204, max: 1.3519, stdev: 0.2321
New network won 60 and tied 168 out of 300 games (48.00% wins where ties are half wins)
Reverting to the old network

Training iteration 370 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.85 seconds
Training examples lengths: [64744, 64791, 64586, 64851, 64718, 64734, 64831, 64822, 65236, 64914]
Total value: 472520.44
Training on 648227 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2630 (value: 0.0017, weighted value: 0.0854, policy: 0.1776, weighted policy: 0.1776), Train Mean Max: 0.9309
Epoch 2/10, Train Loss: 0.2417 (value: 0.0015, weighted value: 0.0740, policy: 0.1677, weighted policy: 0.1677), Train Mean Max: 0.9320
Epoch 3/10, Train Loss: 0.2302 (value: 0.0013, weighted value: 0.0671, policy: 0.1631, weighted policy: 0.1631), Train Mean Max: 0.9325
Epoch 4/10, Train Loss: 0.2241 (value: 0.0013, weighted value: 0.0636, policy: 0.1605, weighted policy: 0.1605), Train Mean Max: 0.9329
Epoch 5/10, Train Loss: 0.2174 (value: 0.0012, weighted value: 0.0594, policy: 0.1581, weighted policy: 0.1581), Train Mean Max: 0.9332
Epoch 6/10, Train Loss: 0.2146 (value: 0.0011, weighted value: 0.0568, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9334
Epoch 7/10, Train Loss: 0.2101 (value: 0.0011, weighted value: 0.0537, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9337
Epoch 8/10, Train Loss: 0.2069 (value: 0.0010, weighted value: 0.0515, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9340
Epoch 9/10, Train Loss: 0.2047 (value: 0.0010, weighted value: 0.0492, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9341
Epoch 10/10, Train Loss: 0.2033 (value: 0.0010, weighted value: 0.0481, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9343
..training done in 59.93 seconds
..evaluation done in 18.16 seconds
Old network+MCTS average reward: 0.7298, min: 0.1852, max: 1.4259, stdev: 0.2296
New network+MCTS average reward: 0.7352, min: 0.2037, max: 1.4630, stdev: 0.2334
Old bare network average reward: 0.6892, min: 0.1574, max: 1.4259, stdev: 0.2332
New bare network average reward: 0.6921, min: 0.1111, max: 1.3333, stdev: 0.2323
External policy "random" average reward: 0.2629, min: -0.3611, max: 0.9815, stdev: 0.2283
External policy "individual greedy" average reward: 0.5474, min: -0.0278, max: 1.2222, stdev: 0.2296
External policy "total greedy" average reward: 0.6588, min: 0.0833, max: 1.2407, stdev: 0.2300
New network won 80 and tied 156 out of 300 games (52.67% wins where ties are half wins)
Keeping the new network

Training iteration 371 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.91 seconds
Training examples lengths: [64791, 64586, 64851, 64718, 64734, 64831, 64822, 65236, 64914, 64558]
Total value: 472670.66
Training on 648041 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2236 (value: 0.0012, weighted value: 0.0606, policy: 0.1631, weighted policy: 0.1631), Train Mean Max: 0.9332
Epoch 2/10, Train Loss: 0.2140 (value: 0.0011, weighted value: 0.0556, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9340
Epoch 3/10, Train Loss: 0.2078 (value: 0.0010, weighted value: 0.0513, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9342
Epoch 4/10, Train Loss: 0.2044 (value: 0.0010, weighted value: 0.0494, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9345
Epoch 5/10, Train Loss: 0.2030 (value: 0.0010, weighted value: 0.0487, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9348
Epoch 6/10, Train Loss: 0.2013 (value: 0.0009, weighted value: 0.0463, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9348
Epoch 7/10, Train Loss: 0.1967 (value: 0.0009, weighted value: 0.0441, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9352
Epoch 8/10, Train Loss: 0.1971 (value: 0.0009, weighted value: 0.0441, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9353
Epoch 9/10, Train Loss: 0.1945 (value: 0.0009, weighted value: 0.0425, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9355
Epoch 10/10, Train Loss: 0.1931 (value: 0.0008, weighted value: 0.0410, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9357
..training done in 58.73 seconds
..evaluation done in 19.23 seconds
Old network+MCTS average reward: 0.7109, min: 0.1296, max: 1.5000, stdev: 0.2402
New network+MCTS average reward: 0.7125, min: 0.1296, max: 1.3611, stdev: 0.2348
Old bare network average reward: 0.6679, min: 0.0833, max: 1.3611, stdev: 0.2403
New bare network average reward: 0.6682, min: 0.0833, max: 1.3611, stdev: 0.2420
External policy "random" average reward: 0.2551, min: -0.4352, max: 0.9630, stdev: 0.2349
External policy "individual greedy" average reward: 0.5302, min: -0.0556, max: 1.1019, stdev: 0.2329
External policy "total greedy" average reward: 0.6437, min: 0.0463, max: 1.2685, stdev: 0.2278
New network won 72 and tied 166 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 372 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.53 seconds
Training examples lengths: [64586, 64851, 64718, 64734, 64831, 64822, 65236, 64914, 64558, 64701]
Total value: 473123.21
Training on 647951 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2148 (value: 0.0011, weighted value: 0.0541, policy: 0.1607, weighted policy: 0.1607), Train Mean Max: 0.9344
Epoch 2/10, Train Loss: 0.2070 (value: 0.0010, weighted value: 0.0496, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9349
Epoch 3/10, Train Loss: 0.2012 (value: 0.0009, weighted value: 0.0468, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9352
Epoch 4/10, Train Loss: 0.1989 (value: 0.0009, weighted value: 0.0450, policy: 0.1539, weighted policy: 0.1539), Train Mean Max: 0.9354
Epoch 5/10, Train Loss: 0.1970 (value: 0.0009, weighted value: 0.0438, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9356
Epoch 6/10, Train Loss: 0.1937 (value: 0.0008, weighted value: 0.0423, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9358
Epoch 7/10, Train Loss: 0.1931 (value: 0.0008, weighted value: 0.0413, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9359
Epoch 8/10, Train Loss: 0.1908 (value: 0.0008, weighted value: 0.0397, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9362
Epoch 9/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0387, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9362
Epoch 10/10, Train Loss: 0.1891 (value: 0.0008, weighted value: 0.0386, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9364
..training done in 64.44 seconds
..evaluation done in 19.12 seconds
Old network+MCTS average reward: 0.7080, min: -0.0648, max: 1.5370, stdev: 0.2398
New network+MCTS average reward: 0.7073, min: -0.0741, max: 1.5648, stdev: 0.2422
Old bare network average reward: 0.6699, min: -0.0185, max: 1.5093, stdev: 0.2436
New bare network average reward: 0.6665, min: -0.0370, max: 1.5833, stdev: 0.2462
External policy "random" average reward: 0.2599, min: -0.3056, max: 1.1944, stdev: 0.2138
External policy "individual greedy" average reward: 0.5340, min: -0.0741, max: 1.4537, stdev: 0.2240
External policy "total greedy" average reward: 0.6448, min: 0.0556, max: 1.4907, stdev: 0.2282
New network won 58 and tied 175 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 373 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.54 seconds
Training examples lengths: [64851, 64718, 64734, 64831, 64822, 65236, 64914, 64558, 64701, 64882]
Total value: 473753.44
Training on 648247 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2357 (value: 0.0013, weighted value: 0.0670, policy: 0.1687, weighted policy: 0.1687), Train Mean Max: 0.9329
Epoch 2/10, Train Loss: 0.2220 (value: 0.0012, weighted value: 0.0592, policy: 0.1628, weighted policy: 0.1628), Train Mean Max: 0.9338
Epoch 3/10, Train Loss: 0.2125 (value: 0.0011, weighted value: 0.0549, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9343
Epoch 4/10, Train Loss: 0.2088 (value: 0.0011, weighted value: 0.0528, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9345
Epoch 5/10, Train Loss: 0.2040 (value: 0.0010, weighted value: 0.0491, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9347
Epoch 6/10, Train Loss: 0.2012 (value: 0.0009, weighted value: 0.0475, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9350
Epoch 7/10, Train Loss: 0.2000 (value: 0.0009, weighted value: 0.0461, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9352
Epoch 8/10, Train Loss: 0.1975 (value: 0.0009, weighted value: 0.0446, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9353
Epoch 9/10, Train Loss: 0.1963 (value: 0.0009, weighted value: 0.0434, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9355
Epoch 10/10, Train Loss: 0.1931 (value: 0.0008, weighted value: 0.0416, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9358
..training done in 64.74 seconds
..evaluation done in 18.54 seconds
Old network+MCTS average reward: 0.7083, min: 0.0093, max: 1.6481, stdev: 0.2472
New network+MCTS average reward: 0.7074, min: 0.0093, max: 1.6759, stdev: 0.2472
Old bare network average reward: 0.6721, min: -0.1019, max: 1.5926, stdev: 0.2488
New bare network average reward: 0.6667, min: -0.0185, max: 1.5926, stdev: 0.2489
External policy "random" average reward: 0.2646, min: -0.3796, max: 1.2778, stdev: 0.2417
External policy "individual greedy" average reward: 0.5338, min: 0.0185, max: 1.4815, stdev: 0.2351
External policy "total greedy" average reward: 0.6409, min: 0.0370, max: 1.4722, stdev: 0.2380
New network won 77 and tied 162 out of 300 games (52.67% wins where ties are half wins)
Keeping the new network

Training iteration 374 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.68 seconds
Training examples lengths: [64718, 64734, 64831, 64822, 65236, 64914, 64558, 64701, 64882, 64776]
Total value: 473749.12
Training on 648172 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2150 (value: 0.0011, weighted value: 0.0550, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9345
Epoch 2/10, Train Loss: 0.2069 (value: 0.0010, weighted value: 0.0493, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9349
Epoch 3/10, Train Loss: 0.2016 (value: 0.0009, weighted value: 0.0472, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9354
Epoch 4/10, Train Loss: 0.1979 (value: 0.0009, weighted value: 0.0451, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9357
Epoch 5/10, Train Loss: 0.1966 (value: 0.0009, weighted value: 0.0435, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9355
Epoch 6/10, Train Loss: 0.1948 (value: 0.0009, weighted value: 0.0431, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9359
Epoch 7/10, Train Loss: 0.1917 (value: 0.0008, weighted value: 0.0404, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9363
Epoch 8/10, Train Loss: 0.1916 (value: 0.0008, weighted value: 0.0409, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9363
Epoch 9/10, Train Loss: 0.1905 (value: 0.0008, weighted value: 0.0388, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9363
Epoch 10/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0386, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9366
..training done in 64.88 seconds
..evaluation done in 18.52 seconds
Old network+MCTS average reward: 0.7000, min: 0.1111, max: 1.4630, stdev: 0.2426
New network+MCTS average reward: 0.7015, min: 0.1759, max: 1.4630, stdev: 0.2416
Old bare network average reward: 0.6599, min: 0.0833, max: 1.3519, stdev: 0.2464
New bare network average reward: 0.6608, min: 0.0833, max: 1.4630, stdev: 0.2425
External policy "random" average reward: 0.2434, min: -0.2870, max: 1.0185, stdev: 0.2215
External policy "individual greedy" average reward: 0.5302, min: -0.0463, max: 1.3333, stdev: 0.2313
External policy "total greedy" average reward: 0.6373, min: 0.1667, max: 1.3056, stdev: 0.2230
New network won 78 and tied 150 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 375 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.59 seconds
Training examples lengths: [64734, 64831, 64822, 65236, 64914, 64558, 64701, 64882, 64776, 64970]
Total value: 474644.48
Training on 648424 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2104 (value: 0.0010, weighted value: 0.0514, policy: 0.1590, weighted policy: 0.1590), Train Mean Max: 0.9354
Epoch 2/10, Train Loss: 0.2023 (value: 0.0009, weighted value: 0.0463, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9359
Epoch 3/10, Train Loss: 0.2004 (value: 0.0009, weighted value: 0.0464, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9360
Epoch 4/10, Train Loss: 0.1942 (value: 0.0008, weighted value: 0.0419, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9365
Epoch 5/10, Train Loss: 0.1938 (value: 0.0008, weighted value: 0.0425, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9364
Epoch 6/10, Train Loss: 0.1907 (value: 0.0008, weighted value: 0.0403, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9366
Epoch 7/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0391, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9368
Epoch 8/10, Train Loss: 0.1875 (value: 0.0008, weighted value: 0.0375, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9370
Epoch 9/10, Train Loss: 0.1868 (value: 0.0007, weighted value: 0.0372, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9371
Epoch 10/10, Train Loss: 0.1872 (value: 0.0008, weighted value: 0.0376, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9371
..training done in 64.11 seconds
..evaluation done in 18.45 seconds
Old network+MCTS average reward: 0.7311, min: 0.0926, max: 1.3241, stdev: 0.2384
New network+MCTS average reward: 0.7327, min: 0.1296, max: 1.3241, stdev: 0.2393
Old bare network average reward: 0.6918, min: 0.0741, max: 1.2963, stdev: 0.2359
New bare network average reward: 0.6918, min: 0.0185, max: 1.3241, stdev: 0.2384
External policy "random" average reward: 0.2735, min: -0.3426, max: 0.9352, stdev: 0.2236
External policy "individual greedy" average reward: 0.5434, min: -0.0833, max: 1.2315, stdev: 0.2245
External policy "total greedy" average reward: 0.6553, min: 0.0093, max: 1.4259, stdev: 0.2229
New network won 71 and tied 162 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 376 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.01 seconds
Training examples lengths: [64831, 64822, 65236, 64914, 64558, 64701, 64882, 64776, 64970, 65024]
Total value: 474615.68
Training on 648714 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2089 (value: 0.0010, weighted value: 0.0501, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9357
Epoch 2/10, Train Loss: 0.2006 (value: 0.0009, weighted value: 0.0455, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9362
Epoch 3/10, Train Loss: 0.1958 (value: 0.0009, weighted value: 0.0434, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9367
Epoch 4/10, Train Loss: 0.1936 (value: 0.0009, weighted value: 0.0425, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9368
Epoch 5/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0398, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9370
Epoch 6/10, Train Loss: 0.1885 (value: 0.0008, weighted value: 0.0391, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9370
Epoch 7/10, Train Loss: 0.1878 (value: 0.0008, weighted value: 0.0380, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9371
Epoch 8/10, Train Loss: 0.1863 (value: 0.0008, weighted value: 0.0376, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1846 (value: 0.0007, weighted value: 0.0360, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9375
Epoch 10/10, Train Loss: 0.1846 (value: 0.0007, weighted value: 0.0360, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9376
..training done in 65.52 seconds
..evaluation done in 18.78 seconds
Old network+MCTS average reward: 0.6964, min: 0.0000, max: 1.3889, stdev: 0.2430
New network+MCTS average reward: 0.7001, min: -0.0185, max: 1.3889, stdev: 0.2446
Old bare network average reward: 0.6561, min: 0.0000, max: 1.3426, stdev: 0.2458
New bare network average reward: 0.6593, min: 0.0000, max: 1.3889, stdev: 0.2488
External policy "random" average reward: 0.2567, min: -0.3241, max: 0.8796, stdev: 0.2350
External policy "individual greedy" average reward: 0.5252, min: -0.0463, max: 1.1944, stdev: 0.2323
External policy "total greedy" average reward: 0.6411, min: 0.0463, max: 1.2870, stdev: 0.2267
New network won 87 and tied 145 out of 300 games (53.17% wins where ties are half wins)
Keeping the new network

Training iteration 377 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.60 seconds
Training examples lengths: [64822, 65236, 64914, 64558, 64701, 64882, 64776, 64970, 65024, 64556]
Total value: 474511.81
Training on 648439 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2067 (value: 0.0010, weighted value: 0.0489, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9361
Epoch 2/10, Train Loss: 0.2005 (value: 0.0009, weighted value: 0.0447, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9367
Epoch 3/10, Train Loss: 0.1940 (value: 0.0008, weighted value: 0.0422, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9371
Epoch 4/10, Train Loss: 0.1912 (value: 0.0008, weighted value: 0.0414, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9372
Epoch 5/10, Train Loss: 0.1876 (value: 0.0008, weighted value: 0.0386, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9373
Epoch 6/10, Train Loss: 0.1864 (value: 0.0007, weighted value: 0.0375, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9375
Epoch 7/10, Train Loss: 0.1862 (value: 0.0007, weighted value: 0.0374, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9375
Epoch 8/10, Train Loss: 0.1853 (value: 0.0007, weighted value: 0.0367, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9376
Epoch 9/10, Train Loss: 0.1844 (value: 0.0007, weighted value: 0.0357, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9378
Epoch 10/10, Train Loss: 0.1826 (value: 0.0007, weighted value: 0.0345, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9379
..training done in 65.48 seconds
..evaluation done in 18.13 seconds
Old network+MCTS average reward: 0.7381, min: 0.1389, max: 1.3796, stdev: 0.2405
New network+MCTS average reward: 0.7380, min: 0.1389, max: 1.3796, stdev: 0.2434
Old bare network average reward: 0.6969, min: 0.0648, max: 1.3796, stdev: 0.2494
New bare network average reward: 0.7003, min: 0.0185, max: 1.3704, stdev: 0.2481
External policy "random" average reward: 0.2840, min: -0.3426, max: 1.2130, stdev: 0.2372
External policy "individual greedy" average reward: 0.5668, min: -0.0741, max: 1.3148, stdev: 0.2282
External policy "total greedy" average reward: 0.6775, min: 0.1759, max: 1.3796, stdev: 0.2250
New network won 57 and tied 176 out of 300 games (48.33% wins where ties are half wins)
Reverting to the old network

Training iteration 378 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.56 seconds
Training examples lengths: [65236, 64914, 64558, 64701, 64882, 64776, 64970, 65024, 64556, 64642]
Total value: 474619.66
Training on 648259 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2295 (value: 0.0012, weighted value: 0.0621, policy: 0.1674, weighted policy: 0.1674), Train Mean Max: 0.9346
Epoch 2/10, Train Loss: 0.2144 (value: 0.0011, weighted value: 0.0550, policy: 0.1594, weighted policy: 0.1594), Train Mean Max: 0.9357
Epoch 3/10, Train Loss: 0.2061 (value: 0.0010, weighted value: 0.0519, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9362
Epoch 4/10, Train Loss: 0.2008 (value: 0.0010, weighted value: 0.0484, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9363
Epoch 5/10, Train Loss: 0.1976 (value: 0.0009, weighted value: 0.0461, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9365
Epoch 6/10, Train Loss: 0.1960 (value: 0.0009, weighted value: 0.0452, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9366
Epoch 7/10, Train Loss: 0.1919 (value: 0.0008, weighted value: 0.0418, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9369
Epoch 8/10, Train Loss: 0.1926 (value: 0.0009, weighted value: 0.0426, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9369
Epoch 9/10, Train Loss: 0.1881 (value: 0.0008, weighted value: 0.0393, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9371
Epoch 10/10, Train Loss: 0.1875 (value: 0.0008, weighted value: 0.0387, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9373
..training done in 63.74 seconds
..evaluation done in 19.26 seconds
Old network+MCTS average reward: 0.7352, min: 0.2963, max: 1.8704, stdev: 0.2293
New network+MCTS average reward: 0.7373, min: 0.2500, max: 1.8704, stdev: 0.2303
Old bare network average reward: 0.7005, min: 0.2315, max: 1.8704, stdev: 0.2393
New bare network average reward: 0.7023, min: 0.2315, max: 1.8704, stdev: 0.2377
External policy "random" average reward: 0.2769, min: -0.2870, max: 1.0463, stdev: 0.2340
External policy "individual greedy" average reward: 0.5583, min: -0.1296, max: 1.7778, stdev: 0.2428
External policy "total greedy" average reward: 0.6765, min: 0.0741, max: 1.7222, stdev: 0.2359
New network won 74 and tied 160 out of 300 games (51.33% wins where ties are half wins)
Keeping the new network

Training iteration 379 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.80 seconds
Training examples lengths: [64914, 64558, 64701, 64882, 64776, 64970, 65024, 64556, 64642, 64567]
Total value: 474395.22
Training on 647590 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2118 (value: 0.0011, weighted value: 0.0535, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9357
Epoch 2/10, Train Loss: 0.2024 (value: 0.0009, weighted value: 0.0474, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9364
Epoch 3/10, Train Loss: 0.1986 (value: 0.0009, weighted value: 0.0460, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9367
Epoch 4/10, Train Loss: 0.1946 (value: 0.0009, weighted value: 0.0434, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9369
Epoch 5/10, Train Loss: 0.1922 (value: 0.0008, weighted value: 0.0422, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9372
Epoch 6/10, Train Loss: 0.1905 (value: 0.0008, weighted value: 0.0409, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9372
Epoch 7/10, Train Loss: 0.1875 (value: 0.0008, weighted value: 0.0389, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9375
Epoch 8/10, Train Loss: 0.1872 (value: 0.0008, weighted value: 0.0386, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9376
Epoch 9/10, Train Loss: 0.1857 (value: 0.0008, weighted value: 0.0377, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1841 (value: 0.0007, weighted value: 0.0359, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9378
..training done in 66.38 seconds
..evaluation done in 19.44 seconds
Old network+MCTS average reward: 0.7031, min: 0.1481, max: 1.4815, stdev: 0.2271
New network+MCTS average reward: 0.7012, min: 0.1111, max: 1.4167, stdev: 0.2299
Old bare network average reward: 0.6652, min: 0.1204, max: 1.4537, stdev: 0.2331
New bare network average reward: 0.6607, min: 0.1111, max: 1.4537, stdev: 0.2334
External policy "random" average reward: 0.2444, min: -0.3519, max: 0.9352, stdev: 0.2296
External policy "individual greedy" average reward: 0.5156, min: -0.1019, max: 1.1389, stdev: 0.2285
External policy "total greedy" average reward: 0.6393, min: 0.1019, max: 1.3241, stdev: 0.2278
New network won 64 and tied 163 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 380 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.07 seconds
Training examples lengths: [64558, 64701, 64882, 64776, 64970, 65024, 64556, 64642, 64567, 65012]
Total value: 473911.28
Training on 647688 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2341 (value: 0.0014, weighted value: 0.0681, policy: 0.1660, weighted policy: 0.1660), Train Mean Max: 0.9342
Epoch 2/10, Train Loss: 0.2197 (value: 0.0012, weighted value: 0.0580, policy: 0.1617, weighted policy: 0.1617), Train Mean Max: 0.9349
Epoch 3/10, Train Loss: 0.2097 (value: 0.0011, weighted value: 0.0538, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9355
Epoch 4/10, Train Loss: 0.2061 (value: 0.0010, weighted value: 0.0521, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9357
Epoch 5/10, Train Loss: 0.2006 (value: 0.0010, weighted value: 0.0485, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9361
Epoch 6/10, Train Loss: 0.1988 (value: 0.0009, weighted value: 0.0475, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9363
Epoch 7/10, Train Loss: 0.1971 (value: 0.0009, weighted value: 0.0458, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9362
Epoch 8/10, Train Loss: 0.1933 (value: 0.0009, weighted value: 0.0429, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9366
Epoch 9/10, Train Loss: 0.1921 (value: 0.0008, weighted value: 0.0423, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9368
Epoch 10/10, Train Loss: 0.1898 (value: 0.0008, weighted value: 0.0404, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9368
..training done in 69.66 seconds
..evaluation done in 18.87 seconds
Old network+MCTS average reward: 0.7339, min: 0.1389, max: 1.3519, stdev: 0.2377
New network+MCTS average reward: 0.7334, min: 0.1296, max: 1.3519, stdev: 0.2377
Old bare network average reward: 0.6940, min: 0.1389, max: 1.3519, stdev: 0.2408
New bare network average reward: 0.6983, min: 0.1204, max: 1.3426, stdev: 0.2431
External policy "random" average reward: 0.2797, min: -0.1667, max: 0.9537, stdev: 0.2265
External policy "individual greedy" average reward: 0.5651, min: -0.0463, max: 1.2315, stdev: 0.2366
External policy "total greedy" average reward: 0.6732, min: 0.1296, max: 1.3704, stdev: 0.2336
New network won 75 and tied 162 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_380

Training iteration 381 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.49 seconds
Training examples lengths: [64701, 64882, 64776, 64970, 65024, 64556, 64642, 64567, 65012, 64846]
Total value: 473756.49
Training on 647976 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2154 (value: 0.0011, weighted value: 0.0544, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9352
Epoch 2/10, Train Loss: 0.2058 (value: 0.0010, weighted value: 0.0503, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9359
Epoch 3/10, Train Loss: 0.1991 (value: 0.0009, weighted value: 0.0461, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9363
Epoch 4/10, Train Loss: 0.1987 (value: 0.0009, weighted value: 0.0456, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9362
Epoch 5/10, Train Loss: 0.1937 (value: 0.0008, weighted value: 0.0424, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9366
Epoch 6/10, Train Loss: 0.1926 (value: 0.0008, weighted value: 0.0418, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9367
Epoch 7/10, Train Loss: 0.1913 (value: 0.0008, weighted value: 0.0409, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9368
Epoch 8/10, Train Loss: 0.1894 (value: 0.0008, weighted value: 0.0393, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9370
Epoch 9/10, Train Loss: 0.1867 (value: 0.0008, weighted value: 0.0378, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9372
Epoch 10/10, Train Loss: 0.1875 (value: 0.0008, weighted value: 0.0379, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9372
..training done in 59.28 seconds
..evaluation done in 18.42 seconds
Old network+MCTS average reward: 0.7126, min: 0.1019, max: 1.4074, stdev: 0.2430
New network+MCTS average reward: 0.7123, min: 0.0833, max: 1.4352, stdev: 0.2429
Old bare network average reward: 0.6708, min: 0.0000, max: 1.4259, stdev: 0.2423
New bare network average reward: 0.6713, min: 0.0926, max: 1.3981, stdev: 0.2435
External policy "random" average reward: 0.2583, min: -0.3981, max: 0.9074, stdev: 0.2272
External policy "individual greedy" average reward: 0.5312, min: -0.1111, max: 1.1759, stdev: 0.2339
External policy "total greedy" average reward: 0.6436, min: 0.0463, max: 1.2870, stdev: 0.2225
New network won 67 and tied 165 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 382 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.97 seconds
Training examples lengths: [64882, 64776, 64970, 65024, 64556, 64642, 64567, 65012, 64846, 64997]
Total value: 473869.84
Training on 648272 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2398 (value: 0.0014, weighted value: 0.0702, policy: 0.1696, weighted policy: 0.1696), Train Mean Max: 0.9336
Epoch 2/10, Train Loss: 0.2211 (value: 0.0012, weighted value: 0.0592, policy: 0.1618, weighted policy: 0.1618), Train Mean Max: 0.9347
Epoch 3/10, Train Loss: 0.2131 (value: 0.0011, weighted value: 0.0557, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9349
Epoch 4/10, Train Loss: 0.2089 (value: 0.0011, weighted value: 0.0533, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9353
Epoch 5/10, Train Loss: 0.2035 (value: 0.0010, weighted value: 0.0499, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9355
Epoch 6/10, Train Loss: 0.1998 (value: 0.0010, weighted value: 0.0476, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9358
Epoch 7/10, Train Loss: 0.1984 (value: 0.0009, weighted value: 0.0462, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9357
Epoch 8/10, Train Loss: 0.1970 (value: 0.0009, weighted value: 0.0448, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9359
Epoch 9/10, Train Loss: 0.1950 (value: 0.0009, weighted value: 0.0434, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9362
Epoch 10/10, Train Loss: 0.1928 (value: 0.0008, weighted value: 0.0419, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9365
..training done in 63.60 seconds
..evaluation done in 19.53 seconds
Old network+MCTS average reward: 0.7278, min: 0.1944, max: 1.7315, stdev: 0.2298
New network+MCTS average reward: 0.7272, min: 0.1481, max: 1.7315, stdev: 0.2304
Old bare network average reward: 0.6910, min: 0.1481, max: 1.7315, stdev: 0.2405
New bare network average reward: 0.6882, min: 0.1389, max: 1.7315, stdev: 0.2372
External policy "random" average reward: 0.2676, min: -0.4630, max: 0.9074, stdev: 0.2228
External policy "individual greedy" average reward: 0.5467, min: 0.0093, max: 1.5926, stdev: 0.2343
External policy "total greedy" average reward: 0.6632, min: 0.1574, max: 1.7037, stdev: 0.2187
New network won 75 and tied 152 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 383 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.67 seconds
Training examples lengths: [64776, 64970, 65024, 64556, 64642, 64567, 65012, 64846, 64997, 64892]
Total value: 473894.14
Training on 648282 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2163 (value: 0.0011, weighted value: 0.0553, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9348
Epoch 2/10, Train Loss: 0.2068 (value: 0.0010, weighted value: 0.0506, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9355
Epoch 3/10, Train Loss: 0.2015 (value: 0.0009, weighted value: 0.0469, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9357
Epoch 4/10, Train Loss: 0.1995 (value: 0.0009, weighted value: 0.0467, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9359
Epoch 5/10, Train Loss: 0.1950 (value: 0.0009, weighted value: 0.0439, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9362
Epoch 6/10, Train Loss: 0.1925 (value: 0.0008, weighted value: 0.0416, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9362
Epoch 7/10, Train Loss: 0.1911 (value: 0.0008, weighted value: 0.0408, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9366
Epoch 8/10, Train Loss: 0.1907 (value: 0.0008, weighted value: 0.0407, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9366
Epoch 9/10, Train Loss: 0.1899 (value: 0.0008, weighted value: 0.0393, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9366
Epoch 10/10, Train Loss: 0.1877 (value: 0.0008, weighted value: 0.0380, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9369
..training done in 64.85 seconds
..evaluation done in 19.15 seconds
Old network+MCTS average reward: 0.7412, min: 0.1667, max: 1.4537, stdev: 0.2303
New network+MCTS average reward: 0.7422, min: 0.2037, max: 1.4630, stdev: 0.2307
Old bare network average reward: 0.7016, min: 0.0926, max: 1.4444, stdev: 0.2385
New bare network average reward: 0.7020, min: 0.0926, max: 1.4630, stdev: 0.2352
External policy "random" average reward: 0.2705, min: -0.2500, max: 0.9722, stdev: 0.2199
External policy "individual greedy" average reward: 0.5615, min: -0.0741, max: 1.3611, stdev: 0.2352
External policy "total greedy" average reward: 0.6775, min: 0.2037, max: 1.3426, stdev: 0.2202
New network won 73 and tied 155 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 384 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.31 seconds
Training examples lengths: [64970, 65024, 64556, 64642, 64567, 65012, 64846, 64997, 64892, 64708]
Total value: 474331.68
Training on 648214 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2117 (value: 0.0010, weighted value: 0.0516, policy: 0.1601, weighted policy: 0.1601), Train Mean Max: 0.9353
Epoch 2/10, Train Loss: 0.2029 (value: 0.0009, weighted value: 0.0474, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9360
Epoch 3/10, Train Loss: 0.1984 (value: 0.0009, weighted value: 0.0450, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9363
Epoch 4/10, Train Loss: 0.1951 (value: 0.0009, weighted value: 0.0434, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9366
Epoch 5/10, Train Loss: 0.1921 (value: 0.0008, weighted value: 0.0417, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9366
Epoch 6/10, Train Loss: 0.1907 (value: 0.0008, weighted value: 0.0404, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9368
Epoch 7/10, Train Loss: 0.1879 (value: 0.0008, weighted value: 0.0390, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9370
Epoch 8/10, Train Loss: 0.1862 (value: 0.0007, weighted value: 0.0370, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9371
Epoch 9/10, Train Loss: 0.1882 (value: 0.0008, weighted value: 0.0378, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9371
Epoch 10/10, Train Loss: 0.1857 (value: 0.0007, weighted value: 0.0363, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9374
..training done in 59.82 seconds
..evaluation done in 18.43 seconds
Old network+MCTS average reward: 0.7200, min: 0.1204, max: 1.4352, stdev: 0.2268
New network+MCTS average reward: 0.7208, min: 0.1667, max: 1.4537, stdev: 0.2290
Old bare network average reward: 0.6817, min: -0.0463, max: 1.4259, stdev: 0.2275
New bare network average reward: 0.6812, min: -0.0463, max: 1.4074, stdev: 0.2348
External policy "random" average reward: 0.2558, min: -0.3704, max: 0.9259, stdev: 0.2141
External policy "individual greedy" average reward: 0.5287, min: -0.0833, max: 1.2407, stdev: 0.2167
External policy "total greedy" average reward: 0.6580, min: 0.1019, max: 1.2870, stdev: 0.2125
New network won 66 and tied 170 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 385 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.84 seconds
Training examples lengths: [65024, 64556, 64642, 64567, 65012, 64846, 64997, 64892, 64708, 64907]
Total value: 474893.45
Training on 648151 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2085 (value: 0.0010, weighted value: 0.0499, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9360
Epoch 2/10, Train Loss: 0.2013 (value: 0.0009, weighted value: 0.0469, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9365
Epoch 3/10, Train Loss: 0.1962 (value: 0.0009, weighted value: 0.0439, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9368
Epoch 4/10, Train Loss: 0.1924 (value: 0.0008, weighted value: 0.0411, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9371
Epoch 5/10, Train Loss: 0.1904 (value: 0.0008, weighted value: 0.0401, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9371
Epoch 6/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0401, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9372
Epoch 7/10, Train Loss: 0.1866 (value: 0.0008, weighted value: 0.0379, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9374
Epoch 8/10, Train Loss: 0.1858 (value: 0.0008, weighted value: 0.0376, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9376
Epoch 9/10, Train Loss: 0.1836 (value: 0.0007, weighted value: 0.0353, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9376
Epoch 10/10, Train Loss: 0.1842 (value: 0.0007, weighted value: 0.0355, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9378
..training done in 60.05 seconds
..evaluation done in 22.16 seconds
Old network+MCTS average reward: 0.7152, min: 0.1111, max: 1.3889, stdev: 0.2368
New network+MCTS average reward: 0.7160, min: 0.1111, max: 1.4259, stdev: 0.2357
Old bare network average reward: 0.6784, min: 0.1111, max: 1.3241, stdev: 0.2408
New bare network average reward: 0.6777, min: 0.0741, max: 1.3241, stdev: 0.2395
External policy "random" average reward: 0.2631, min: -0.3333, max: 0.9630, stdev: 0.2257
External policy "individual greedy" average reward: 0.5316, min: -0.0833, max: 1.1481, stdev: 0.2312
External policy "total greedy" average reward: 0.6480, min: 0.1296, max: 1.1944, stdev: 0.2235
New network won 79 and tied 145 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 386 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.59 seconds
Training examples lengths: [64556, 64642, 64567, 65012, 64846, 64997, 64892, 64708, 64907, 64658]
Total value: 474519.99
Training on 647785 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2088 (value: 0.0010, weighted value: 0.0504, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9360
Epoch 2/10, Train Loss: 0.1992 (value: 0.0009, weighted value: 0.0440, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9367
Epoch 3/10, Train Loss: 0.1938 (value: 0.0008, weighted value: 0.0424, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9370
Epoch 4/10, Train Loss: 0.1924 (value: 0.0008, weighted value: 0.0411, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9370
Epoch 5/10, Train Loss: 0.1885 (value: 0.0008, weighted value: 0.0392, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9373
Epoch 6/10, Train Loss: 0.1882 (value: 0.0008, weighted value: 0.0391, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9373
Epoch 7/10, Train Loss: 0.1844 (value: 0.0007, weighted value: 0.0362, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9376
Epoch 8/10, Train Loss: 0.1855 (value: 0.0007, weighted value: 0.0360, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9376
Epoch 9/10, Train Loss: 0.1848 (value: 0.0007, weighted value: 0.0362, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9378
Epoch 10/10, Train Loss: 0.1833 (value: 0.0007, weighted value: 0.0350, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9378
..training done in 66.93 seconds
..evaluation done in 18.85 seconds
Old network+MCTS average reward: 0.7212, min: 0.0370, max: 1.4352, stdev: 0.2449
New network+MCTS average reward: 0.7210, min: 0.0741, max: 1.3981, stdev: 0.2419
Old bare network average reward: 0.6793, min: -0.0741, max: 1.4722, stdev: 0.2493
New bare network average reward: 0.6802, min: 0.0648, max: 1.3981, stdev: 0.2474
External policy "random" average reward: 0.2570, min: -0.3704, max: 0.8148, stdev: 0.2237
External policy "individual greedy" average reward: 0.5390, min: -0.0463, max: 1.2963, stdev: 0.2346
External policy "total greedy" average reward: 0.6522, min: 0.0463, max: 1.2870, stdev: 0.2328
New network won 64 and tied 178 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 387 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.95 seconds
Training examples lengths: [64642, 64567, 65012, 64846, 64997, 64892, 64708, 64907, 64658, 64679]
Total value: 475478.55
Training on 647908 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2071 (value: 0.0010, weighted value: 0.0495, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9363
Epoch 2/10, Train Loss: 0.1989 (value: 0.0009, weighted value: 0.0446, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9368
Epoch 3/10, Train Loss: 0.1927 (value: 0.0008, weighted value: 0.0412, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9372
Epoch 4/10, Train Loss: 0.1915 (value: 0.0008, weighted value: 0.0407, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9373
Epoch 5/10, Train Loss: 0.1891 (value: 0.0008, weighted value: 0.0387, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9374
Epoch 6/10, Train Loss: 0.1864 (value: 0.0008, weighted value: 0.0378, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9377
Epoch 7/10, Train Loss: 0.1847 (value: 0.0007, weighted value: 0.0371, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9378
Epoch 8/10, Train Loss: 0.1834 (value: 0.0007, weighted value: 0.0350, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9377
Epoch 9/10, Train Loss: 0.1844 (value: 0.0007, weighted value: 0.0357, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9378
Epoch 10/10, Train Loss: 0.1819 (value: 0.0007, weighted value: 0.0341, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9380
..training done in 62.54 seconds
..evaluation done in 18.35 seconds
Old network+MCTS average reward: 0.7150, min: -0.0278, max: 1.4907, stdev: 0.2372
New network+MCTS average reward: 0.7138, min: -0.0556, max: 1.5370, stdev: 0.2423
Old bare network average reward: 0.6760, min: -0.0926, max: 1.4907, stdev: 0.2407
New bare network average reward: 0.6774, min: -0.0093, max: 1.4907, stdev: 0.2399
External policy "random" average reward: 0.2514, min: -0.2685, max: 1.1296, stdev: 0.2220
External policy "individual greedy" average reward: 0.5255, min: 0.0000, max: 1.2870, stdev: 0.2321
External policy "total greedy" average reward: 0.6406, min: 0.0556, max: 1.3981, stdev: 0.2323
New network won 70 and tied 163 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 388 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.52 seconds
Training examples lengths: [64567, 65012, 64846, 64997, 64892, 64708, 64907, 64658, 64679, 65067]
Total value: 475909.10
Training on 648333 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2077 (value: 0.0010, weighted value: 0.0491, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9363
Epoch 2/10, Train Loss: 0.1991 (value: 0.0009, weighted value: 0.0442, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9367
Epoch 3/10, Train Loss: 0.1931 (value: 0.0008, weighted value: 0.0417, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9372
Epoch 4/10, Train Loss: 0.1910 (value: 0.0008, weighted value: 0.0401, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9372
Epoch 5/10, Train Loss: 0.1897 (value: 0.0008, weighted value: 0.0405, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9374
Epoch 6/10, Train Loss: 0.1870 (value: 0.0008, weighted value: 0.0377, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9374
Epoch 7/10, Train Loss: 0.1848 (value: 0.0007, weighted value: 0.0363, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9377
Epoch 8/10, Train Loss: 0.1847 (value: 0.0007, weighted value: 0.0364, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1834 (value: 0.0007, weighted value: 0.0350, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1825 (value: 0.0007, weighted value: 0.0342, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9378
..training done in 64.94 seconds
..evaluation done in 18.57 seconds
Old network+MCTS average reward: 0.7315, min: 0.2130, max: 1.4074, stdev: 0.2221
New network+MCTS average reward: 0.7316, min: 0.2870, max: 1.4074, stdev: 0.2237
Old bare network average reward: 0.6965, min: 0.2500, max: 1.3981, stdev: 0.2244
New bare network average reward: 0.6989, min: 0.2130, max: 1.3981, stdev: 0.2242
External policy "random" average reward: 0.2697, min: -0.2500, max: 0.9630, stdev: 0.2175
External policy "individual greedy" average reward: 0.5447, min: 0.0463, max: 1.2407, stdev: 0.2199
External policy "total greedy" average reward: 0.6591, min: 0.2130, max: 1.2685, stdev: 0.2102
New network won 67 and tied 169 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 389 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.46 seconds
Training examples lengths: [65012, 64846, 64997, 64892, 64708, 64907, 64658, 64679, 65067, 64630]
Total value: 476554.22
Training on 648396 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2063 (value: 0.0010, weighted value: 0.0488, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9364
Epoch 2/10, Train Loss: 0.1985 (value: 0.0009, weighted value: 0.0443, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9371
Epoch 3/10, Train Loss: 0.1925 (value: 0.0008, weighted value: 0.0413, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9373
Epoch 4/10, Train Loss: 0.1915 (value: 0.0008, weighted value: 0.0414, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9374
Epoch 5/10, Train Loss: 0.1862 (value: 0.0007, weighted value: 0.0375, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9377
Epoch 6/10, Train Loss: 0.1863 (value: 0.0008, weighted value: 0.0375, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9376
Epoch 7/10, Train Loss: 0.1849 (value: 0.0007, weighted value: 0.0366, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9378
Epoch 8/10, Train Loss: 0.1834 (value: 0.0007, weighted value: 0.0355, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9379
Epoch 9/10, Train Loss: 0.1824 (value: 0.0007, weighted value: 0.0345, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9380
Epoch 10/10, Train Loss: 0.1827 (value: 0.0007, weighted value: 0.0349, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9380
..training done in 59.68 seconds
..evaluation done in 18.12 seconds
Old network+MCTS average reward: 0.7346, min: 0.2315, max: 1.4815, stdev: 0.2266
New network+MCTS average reward: 0.7305, min: 0.2315, max: 1.4815, stdev: 0.2305
Old bare network average reward: 0.6942, min: 0.1111, max: 1.4815, stdev: 0.2316
New bare network average reward: 0.6967, min: 0.1759, max: 1.4815, stdev: 0.2293
External policy "random" average reward: 0.2740, min: -0.2500, max: 1.0463, stdev: 0.2147
External policy "individual greedy" average reward: 0.5502, min: -0.0370, max: 1.2315, stdev: 0.2102
External policy "total greedy" average reward: 0.6607, min: 0.1019, max: 1.3519, stdev: 0.2103
New network won 49 and tied 175 out of 300 games (45.50% wins where ties are half wins)
Reverting to the old network

Training iteration 390 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.70 seconds
Training examples lengths: [64846, 64997, 64892, 64708, 64907, 64658, 64679, 65067, 64630, 64654]
Total value: 475994.85
Training on 648038 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2289 (value: 0.0012, weighted value: 0.0624, policy: 0.1665, weighted policy: 0.1665), Train Mean Max: 0.9347
Epoch 2/10, Train Loss: 0.2152 (value: 0.0011, weighted value: 0.0554, policy: 0.1599, weighted policy: 0.1599), Train Mean Max: 0.9358
Epoch 3/10, Train Loss: 0.2057 (value: 0.0010, weighted value: 0.0510, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9362
Epoch 4/10, Train Loss: 0.1990 (value: 0.0009, weighted value: 0.0471, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9364
Epoch 5/10, Train Loss: 0.1959 (value: 0.0009, weighted value: 0.0451, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9367
Epoch 6/10, Train Loss: 0.1948 (value: 0.0009, weighted value: 0.0443, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9368
Epoch 7/10, Train Loss: 0.1916 (value: 0.0008, weighted value: 0.0418, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9371
Epoch 8/10, Train Loss: 0.1903 (value: 0.0008, weighted value: 0.0410, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9372
Epoch 9/10, Train Loss: 0.1888 (value: 0.0008, weighted value: 0.0394, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9373
Epoch 10/10, Train Loss: 0.1872 (value: 0.0008, weighted value: 0.0381, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9375
..training done in 60.59 seconds
..evaluation done in 18.59 seconds
Old network+MCTS average reward: 0.7468, min: 0.1481, max: 1.4907, stdev: 0.2253
New network+MCTS average reward: 0.7487, min: 0.1481, max: 1.4907, stdev: 0.2255
Old bare network average reward: 0.7127, min: 0.1481, max: 1.5278, stdev: 0.2329
New bare network average reward: 0.7100, min: 0.0463, max: 1.5278, stdev: 0.2353
External policy "random" average reward: 0.2644, min: -0.3426, max: 1.1759, stdev: 0.2286
External policy "individual greedy" average reward: 0.5547, min: 0.0648, max: 1.2037, stdev: 0.2280
External policy "total greedy" average reward: 0.6795, min: 0.1296, max: 1.4630, stdev: 0.2195
New network won 72 and tied 170 out of 300 games (52.33% wins where ties are half wins)
Keeping the new network

Training iteration 391 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.52 seconds
Training examples lengths: [64997, 64892, 64708, 64907, 64658, 64679, 65067, 64630, 64654, 64926]
Total value: 475922.75
Training on 648118 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2102 (value: 0.0011, weighted value: 0.0527, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9359
Epoch 2/10, Train Loss: 0.2017 (value: 0.0009, weighted value: 0.0467, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9366
Epoch 3/10, Train Loss: 0.1964 (value: 0.0009, weighted value: 0.0451, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9369
Epoch 4/10, Train Loss: 0.1923 (value: 0.0008, weighted value: 0.0425, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9371
Epoch 5/10, Train Loss: 0.1898 (value: 0.0008, weighted value: 0.0408, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9374
Epoch 6/10, Train Loss: 0.1886 (value: 0.0008, weighted value: 0.0402, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9374
Epoch 7/10, Train Loss: 0.1863 (value: 0.0008, weighted value: 0.0379, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9377
Epoch 8/10, Train Loss: 0.1859 (value: 0.0007, weighted value: 0.0374, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9377
Epoch 9/10, Train Loss: 0.1849 (value: 0.0007, weighted value: 0.0368, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1840 (value: 0.0007, weighted value: 0.0367, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9379
..training done in 65.30 seconds
..evaluation done in 18.91 seconds
Old network+MCTS average reward: 0.7355, min: -0.2315, max: 1.5926, stdev: 0.2443
New network+MCTS average reward: 0.7336, min: -0.1944, max: 1.5926, stdev: 0.2430
Old bare network average reward: 0.6948, min: -0.2963, max: 1.4074, stdev: 0.2427
New bare network average reward: 0.6953, min: -0.1667, max: 1.4074, stdev: 0.2433
External policy "random" average reward: 0.2805, min: -0.4815, max: 0.9352, stdev: 0.2260
External policy "individual greedy" average reward: 0.5537, min: -0.3056, max: 1.2222, stdev: 0.2283
External policy "total greedy" average reward: 0.6652, min: -0.1204, max: 1.3704, stdev: 0.2242
New network won 66 and tied 168 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 392 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 60.05 seconds
Training examples lengths: [64892, 64708, 64907, 64658, 64679, 65067, 64630, 64654, 64926, 64625]
Total value: 475525.64
Training on 647746 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2080 (value: 0.0010, weighted value: 0.0504, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9365
Epoch 2/10, Train Loss: 0.1999 (value: 0.0009, weighted value: 0.0456, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9370
Epoch 3/10, Train Loss: 0.1945 (value: 0.0009, weighted value: 0.0434, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9374
Epoch 4/10, Train Loss: 0.1906 (value: 0.0008, weighted value: 0.0411, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1890 (value: 0.0008, weighted value: 0.0406, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9378
Epoch 6/10, Train Loss: 0.1873 (value: 0.0008, weighted value: 0.0400, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9380
Epoch 7/10, Train Loss: 0.1844 (value: 0.0007, weighted value: 0.0368, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9381
Epoch 8/10, Train Loss: 0.1833 (value: 0.0007, weighted value: 0.0366, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9382
Epoch 9/10, Train Loss: 0.1841 (value: 0.0007, weighted value: 0.0369, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9382
Epoch 10/10, Train Loss: 0.1825 (value: 0.0007, weighted value: 0.0347, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9383
..training done in 65.46 seconds
..evaluation done in 18.26 seconds
Old network+MCTS average reward: 0.7037, min: 0.1296, max: 1.3704, stdev: 0.2219
New network+MCTS average reward: 0.7029, min: 0.1389, max: 1.3426, stdev: 0.2227
Old bare network average reward: 0.6634, min: 0.1204, max: 1.3056, stdev: 0.2297
New bare network average reward: 0.6640, min: 0.1389, max: 1.3056, stdev: 0.2295
External policy "random" average reward: 0.2516, min: -0.2778, max: 0.9630, stdev: 0.2157
External policy "individual greedy" average reward: 0.5185, min: -0.0278, max: 1.3796, stdev: 0.2196
External policy "total greedy" average reward: 0.6365, min: 0.1019, max: 1.2593, stdev: 0.2236
New network won 61 and tied 177 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 393 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.13 seconds
Training examples lengths: [64708, 64907, 64658, 64679, 65067, 64630, 64654, 64926, 64625, 64555]
Total value: 475329.06
Training on 647409 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2305 (value: 0.0013, weighted value: 0.0647, policy: 0.1658, weighted policy: 0.1658), Train Mean Max: 0.9349
Epoch 2/10, Train Loss: 0.2153 (value: 0.0011, weighted value: 0.0562, policy: 0.1592, weighted policy: 0.1592), Train Mean Max: 0.9358
Epoch 3/10, Train Loss: 0.2074 (value: 0.0010, weighted value: 0.0519, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9362
Epoch 4/10, Train Loss: 0.2012 (value: 0.0010, weighted value: 0.0489, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9367
Epoch 5/10, Train Loss: 0.1958 (value: 0.0009, weighted value: 0.0456, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9370
Epoch 6/10, Train Loss: 0.1957 (value: 0.0009, weighted value: 0.0452, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9370
Epoch 7/10, Train Loss: 0.1933 (value: 0.0009, weighted value: 0.0434, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9371
Epoch 8/10, Train Loss: 0.1905 (value: 0.0008, weighted value: 0.0417, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0404, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9374
Epoch 10/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0393, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9376
..training done in 58.86 seconds
..evaluation done in 18.48 seconds
Old network+MCTS average reward: 0.7469, min: 0.2130, max: 1.4907, stdev: 0.2270
New network+MCTS average reward: 0.7462, min: 0.2963, max: 1.4907, stdev: 0.2255
Old bare network average reward: 0.7068, min: 0.1204, max: 1.4907, stdev: 0.2318
New bare network average reward: 0.7056, min: 0.1389, max: 1.4907, stdev: 0.2325
External policy "random" average reward: 0.2821, min: -0.2130, max: 1.0278, stdev: 0.2295
External policy "individual greedy" average reward: 0.5554, min: -0.0370, max: 1.4352, stdev: 0.2262
External policy "total greedy" average reward: 0.6678, min: 0.0833, max: 1.5370, stdev: 0.2195
New network won 62 and tied 173 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 394 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.23 seconds
Training examples lengths: [64907, 64658, 64679, 65067, 64630, 64654, 64926, 64625, 64555, 65141]
Total value: 475649.95
Training on 647842 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2538 (value: 0.0016, weighted value: 0.0798, policy: 0.1739, weighted policy: 0.1739), Train Mean Max: 0.9334
Epoch 2/10, Train Loss: 0.2287 (value: 0.0013, weighted value: 0.0656, policy: 0.1630, weighted policy: 0.1630), Train Mean Max: 0.9349
Epoch 3/10, Train Loss: 0.2175 (value: 0.0012, weighted value: 0.0600, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9353
Epoch 4/10, Train Loss: 0.2111 (value: 0.0011, weighted value: 0.0570, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9357
Epoch 5/10, Train Loss: 0.2068 (value: 0.0011, weighted value: 0.0539, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9358
Epoch 6/10, Train Loss: 0.2027 (value: 0.0010, weighted value: 0.0508, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9360
Epoch 7/10, Train Loss: 0.1998 (value: 0.0010, weighted value: 0.0487, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9364
Epoch 8/10, Train Loss: 0.1967 (value: 0.0009, weighted value: 0.0465, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9366
Epoch 9/10, Train Loss: 0.1962 (value: 0.0009, weighted value: 0.0446, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9366
Epoch 10/10, Train Loss: 0.1941 (value: 0.0009, weighted value: 0.0446, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9369
..training done in 60.52 seconds
..evaluation done in 19.01 seconds
Old network+MCTS average reward: 0.7306, min: 0.2037, max: 1.4907, stdev: 0.2297
New network+MCTS average reward: 0.7324, min: 0.1944, max: 1.4907, stdev: 0.2309
Old bare network average reward: 0.6959, min: 0.0556, max: 1.4907, stdev: 0.2410
New bare network average reward: 0.6925, min: 0.0370, max: 1.4907, stdev: 0.2420
External policy "random" average reward: 0.2861, min: -0.2315, max: 0.9815, stdev: 0.2218
External policy "individual greedy" average reward: 0.5494, min: 0.0185, max: 1.1759, stdev: 0.2358
External policy "total greedy" average reward: 0.6660, min: 0.1389, max: 1.2500, stdev: 0.2203
New network won 77 and tied 153 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 395 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.04 seconds
Training examples lengths: [64658, 64679, 65067, 64630, 64654, 64926, 64625, 64555, 65141, 64809]
Total value: 475292.17
Training on 647744 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2158 (value: 0.0011, weighted value: 0.0573, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9355
Epoch 2/10, Train Loss: 0.2059 (value: 0.0010, weighted value: 0.0509, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9359
Epoch 3/10, Train Loss: 0.2011 (value: 0.0010, weighted value: 0.0487, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9363
Epoch 4/10, Train Loss: 0.1973 (value: 0.0009, weighted value: 0.0466, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9366
Epoch 5/10, Train Loss: 0.1949 (value: 0.0009, weighted value: 0.0449, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9368
Epoch 6/10, Train Loss: 0.1933 (value: 0.0009, weighted value: 0.0436, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9369
Epoch 7/10, Train Loss: 0.1907 (value: 0.0008, weighted value: 0.0415, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9371
Epoch 8/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0412, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9373
Epoch 9/10, Train Loss: 0.1874 (value: 0.0008, weighted value: 0.0392, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9374
Epoch 10/10, Train Loss: 0.1893 (value: 0.0008, weighted value: 0.0400, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9375
..training done in 60.24 seconds
..evaluation done in 19.09 seconds
Old network+MCTS average reward: 0.7318, min: 0.1111, max: 1.4722, stdev: 0.2350
New network+MCTS average reward: 0.7283, min: 0.1111, max: 1.5463, stdev: 0.2337
Old bare network average reward: 0.6873, min: 0.1111, max: 1.4907, stdev: 0.2397
New bare network average reward: 0.6889, min: 0.1111, max: 1.5093, stdev: 0.2389
External policy "random" average reward: 0.2544, min: -0.5093, max: 0.9444, stdev: 0.2349
External policy "individual greedy" average reward: 0.5335, min: -0.0926, max: 1.2315, stdev: 0.2293
External policy "total greedy" average reward: 0.6476, min: -0.0278, max: 1.2778, stdev: 0.2200
New network won 75 and tied 139 out of 300 games (48.17% wins where ties are half wins)
Reverting to the old network

Training iteration 396 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.29 seconds
Training examples lengths: [64679, 65067, 64630, 64654, 64926, 64625, 64555, 65141, 64809, 65019]
Total value: 475675.29
Training on 648105 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2352 (value: 0.0014, weighted value: 0.0697, policy: 0.1655, weighted policy: 0.1655), Train Mean Max: 0.9339
Epoch 2/10, Train Loss: 0.2219 (value: 0.0012, weighted value: 0.0616, policy: 0.1603, weighted policy: 0.1603), Train Mean Max: 0.9346
Epoch 3/10, Train Loss: 0.2125 (value: 0.0011, weighted value: 0.0566, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9353
Epoch 4/10, Train Loss: 0.2090 (value: 0.0011, weighted value: 0.0549, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9356
Epoch 5/10, Train Loss: 0.2038 (value: 0.0010, weighted value: 0.0517, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9358
Epoch 6/10, Train Loss: 0.2005 (value: 0.0010, weighted value: 0.0483, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9361
Epoch 7/10, Train Loss: 0.1966 (value: 0.0009, weighted value: 0.0461, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9363
Epoch 8/10, Train Loss: 0.1956 (value: 0.0009, weighted value: 0.0454, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9365
Epoch 9/10, Train Loss: 0.1943 (value: 0.0009, weighted value: 0.0444, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9366
Epoch 10/10, Train Loss: 0.1924 (value: 0.0009, weighted value: 0.0428, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9367
..training done in 66.61 seconds
..evaluation done in 19.24 seconds
Old network+MCTS average reward: 0.7358, min: 0.1019, max: 1.4722, stdev: 0.2408
New network+MCTS average reward: 0.7368, min: 0.1019, max: 1.5185, stdev: 0.2399
Old bare network average reward: 0.6996, min: 0.0000, max: 1.4537, stdev: 0.2460
New bare network average reward: 0.7031, min: -0.0926, max: 1.4815, stdev: 0.2452
External policy "random" average reward: 0.2682, min: -0.3981, max: 0.9537, stdev: 0.2238
External policy "individual greedy" average reward: 0.5504, min: -0.1852, max: 1.2870, stdev: 0.2230
External policy "total greedy" average reward: 0.6652, min: -0.0926, max: 1.3519, stdev: 0.2162
New network won 71 and tied 165 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 397 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.62 seconds
Training examples lengths: [65067, 64630, 64654, 64926, 64625, 64555, 65141, 64809, 65019, 64863]
Total value: 475468.19
Training on 648289 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2146 (value: 0.0011, weighted value: 0.0554, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9354
Epoch 2/10, Train Loss: 0.2055 (value: 0.0010, weighted value: 0.0503, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9359
Epoch 3/10, Train Loss: 0.1997 (value: 0.0010, weighted value: 0.0477, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9365
Epoch 4/10, Train Loss: 0.1968 (value: 0.0009, weighted value: 0.0459, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9366
Epoch 5/10, Train Loss: 0.1948 (value: 0.0009, weighted value: 0.0450, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9368
Epoch 6/10, Train Loss: 0.1919 (value: 0.0009, weighted value: 0.0426, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9369
Epoch 7/10, Train Loss: 0.1917 (value: 0.0008, weighted value: 0.0422, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9372
Epoch 8/10, Train Loss: 0.1885 (value: 0.0008, weighted value: 0.0399, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9373
Epoch 9/10, Train Loss: 0.1877 (value: 0.0008, weighted value: 0.0389, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9374
Epoch 10/10, Train Loss: 0.1857 (value: 0.0008, weighted value: 0.0375, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9376
..training done in 65.98 seconds
..evaluation done in 18.87 seconds
Old network+MCTS average reward: 0.7259, min: 0.1759, max: 1.4537, stdev: 0.2392
New network+MCTS average reward: 0.7229, min: 0.1759, max: 1.4537, stdev: 0.2387
Old bare network average reward: 0.6877, min: 0.1759, max: 1.4537, stdev: 0.2425
New bare network average reward: 0.6869, min: 0.1111, max: 1.4537, stdev: 0.2426
External policy "random" average reward: 0.2538, min: -0.4167, max: 0.8796, stdev: 0.2370
External policy "individual greedy" average reward: 0.5397, min: -0.0926, max: 1.2963, stdev: 0.2416
External policy "total greedy" average reward: 0.6558, min: 0.1111, max: 1.4259, stdev: 0.2315
New network won 62 and tied 159 out of 300 games (47.17% wins where ties are half wins)
Reverting to the old network

Training iteration 398 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.70 seconds
Training examples lengths: [64630, 64654, 64926, 64625, 64555, 65141, 64809, 65019, 64863, 65031]
Total value: 475687.12
Training on 648253 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2361 (value: 0.0014, weighted value: 0.0694, policy: 0.1668, weighted policy: 0.1668), Train Mean Max: 0.9341
Epoch 2/10, Train Loss: 0.2195 (value: 0.0012, weighted value: 0.0603, policy: 0.1592, weighted policy: 0.1592), Train Mean Max: 0.9349
Epoch 3/10, Train Loss: 0.2116 (value: 0.0011, weighted value: 0.0559, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9354
Epoch 4/10, Train Loss: 0.2061 (value: 0.0011, weighted value: 0.0528, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9357
Epoch 5/10, Train Loss: 0.2023 (value: 0.0010, weighted value: 0.0503, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9361
Epoch 6/10, Train Loss: 0.2008 (value: 0.0010, weighted value: 0.0493, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9362
Epoch 7/10, Train Loss: 0.1962 (value: 0.0009, weighted value: 0.0456, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9364
Epoch 8/10, Train Loss: 0.1948 (value: 0.0009, weighted value: 0.0448, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9368
Epoch 9/10, Train Loss: 0.1935 (value: 0.0009, weighted value: 0.0445, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9368
Epoch 10/10, Train Loss: 0.1913 (value: 0.0008, weighted value: 0.0420, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9371
..training done in 64.01 seconds
..evaluation done in 18.53 seconds
Old network+MCTS average reward: 0.7191, min: 0.1111, max: 1.5093, stdev: 0.2475
New network+MCTS average reward: 0.7227, min: 0.1111, max: 1.4815, stdev: 0.2437
Old bare network average reward: 0.6791, min: 0.0833, max: 1.5093, stdev: 0.2520
New bare network average reward: 0.6827, min: 0.0833, max: 1.3889, stdev: 0.2532
External policy "random" average reward: 0.2463, min: -0.5926, max: 0.8426, stdev: 0.2246
External policy "individual greedy" average reward: 0.5225, min: -0.0278, max: 1.2870, stdev: 0.2333
External policy "total greedy" average reward: 0.6356, min: 0.0370, max: 1.2778, stdev: 0.2338
New network won 73 and tied 166 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network

Training iteration 399 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.89 seconds
Training examples lengths: [64654, 64926, 64625, 64555, 65141, 64809, 65019, 64863, 65031, 64817]
Total value: 475771.84
Training on 648440 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2123 (value: 0.0011, weighted value: 0.0542, policy: 0.1581, weighted policy: 0.1581), Train Mean Max: 0.9358
Epoch 2/10, Train Loss: 0.2047 (value: 0.0010, weighted value: 0.0505, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9362
Epoch 3/10, Train Loss: 0.1998 (value: 0.0010, weighted value: 0.0478, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9366
Epoch 4/10, Train Loss: 0.1955 (value: 0.0009, weighted value: 0.0447, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9369
Epoch 5/10, Train Loss: 0.1941 (value: 0.0009, weighted value: 0.0442, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1911 (value: 0.0008, weighted value: 0.0420, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9372
Epoch 7/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0408, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9374
Epoch 8/10, Train Loss: 0.1881 (value: 0.0008, weighted value: 0.0400, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9376
Epoch 9/10, Train Loss: 0.1870 (value: 0.0008, weighted value: 0.0393, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1857 (value: 0.0008, weighted value: 0.0379, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9379
..training done in 60.65 seconds
..evaluation done in 17.86 seconds
Old network+MCTS average reward: 0.7180, min: 0.0648, max: 1.4815, stdev: 0.2236
New network+MCTS average reward: 0.7189, min: 0.0648, max: 1.5000, stdev: 0.2221
Old bare network average reward: 0.6822, min: 0.0463, max: 1.4815, stdev: 0.2294
New bare network average reward: 0.6852, min: 0.0463, max: 1.5000, stdev: 0.2258
External policy "random" average reward: 0.2645, min: -0.4167, max: 1.0556, stdev: 0.2203
External policy "individual greedy" average reward: 0.5381, min: -0.0648, max: 1.1759, stdev: 0.2101
External policy "total greedy" average reward: 0.6552, min: 0.0093, max: 1.3241, stdev: 0.2138
New network won 62 and tied 173 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 400 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.55 seconds
Training examples lengths: [64926, 64625, 64555, 65141, 64809, 65019, 64863, 65031, 64817, 64589]
Total value: 476353.28
Training on 648375 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2359 (value: 0.0014, weighted value: 0.0685, policy: 0.1673, weighted policy: 0.1673), Train Mean Max: 0.9342
Epoch 2/10, Train Loss: 0.2199 (value: 0.0012, weighted value: 0.0597, policy: 0.1602, weighted policy: 0.1602), Train Mean Max: 0.9351
Epoch 3/10, Train Loss: 0.2120 (value: 0.0011, weighted value: 0.0562, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9357
Epoch 4/10, Train Loss: 0.2065 (value: 0.0011, weighted value: 0.0532, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9358
Epoch 5/10, Train Loss: 0.2023 (value: 0.0010, weighted value: 0.0502, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9360
Epoch 6/10, Train Loss: 0.1977 (value: 0.0009, weighted value: 0.0466, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9362
Epoch 7/10, Train Loss: 0.1979 (value: 0.0009, weighted value: 0.0471, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9364
Epoch 8/10, Train Loss: 0.1948 (value: 0.0009, weighted value: 0.0446, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9367
Epoch 9/10, Train Loss: 0.1921 (value: 0.0009, weighted value: 0.0429, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9368
Epoch 10/10, Train Loss: 0.1924 (value: 0.0009, weighted value: 0.0426, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9369
..training done in 60.59 seconds
..evaluation done in 18.68 seconds
Old network+MCTS average reward: 0.7420, min: 0.1019, max: 1.6481, stdev: 0.2375
New network+MCTS average reward: 0.7492, min: 0.1019, max: 1.6481, stdev: 0.2377
Old bare network average reward: 0.7037, min: 0.0833, max: 1.4815, stdev: 0.2335
New bare network average reward: 0.7025, min: 0.0833, max: 1.4815, stdev: 0.2310
External policy "random" average reward: 0.2663, min: -0.3519, max: 0.9630, stdev: 0.2205
External policy "individual greedy" average reward: 0.5485, min: -0.0741, max: 1.3704, stdev: 0.2242
External policy "total greedy" average reward: 0.6704, min: 0.1296, max: 1.5093, stdev: 0.2166
New network won 89 and tied 159 out of 300 games (56.17% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_400

Training iteration 401 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 61.78 seconds
Training examples lengths: [64625, 64555, 65141, 64809, 65019, 64863, 65031, 64817, 64589, 64750]
Total value: 477277.85
Training on 648199 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2129 (value: 0.0011, weighted value: 0.0549, policy: 0.1580, weighted policy: 0.1580), Train Mean Max: 0.9357
Epoch 2/10, Train Loss: 0.2037 (value: 0.0010, weighted value: 0.0495, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9363
Epoch 3/10, Train Loss: 0.1989 (value: 0.0009, weighted value: 0.0469, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9365
Epoch 4/10, Train Loss: 0.1960 (value: 0.0009, weighted value: 0.0450, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9368
Epoch 5/10, Train Loss: 0.1938 (value: 0.0009, weighted value: 0.0437, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9371
Epoch 6/10, Train Loss: 0.1931 (value: 0.0009, weighted value: 0.0435, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9372
Epoch 7/10, Train Loss: 0.1882 (value: 0.0008, weighted value: 0.0399, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9375
Epoch 8/10, Train Loss: 0.1883 (value: 0.0008, weighted value: 0.0399, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9376
Epoch 9/10, Train Loss: 0.1875 (value: 0.0008, weighted value: 0.0397, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1848 (value: 0.0007, weighted value: 0.0373, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9379
..training done in 66.25 seconds
..evaluation done in 32.46 seconds
Old network+MCTS average reward: 0.7177, min: 0.0463, max: 1.3704, stdev: 0.2358
New network+MCTS average reward: 0.7175, min: 0.0556, max: 1.3704, stdev: 0.2365
Old bare network average reward: 0.6814, min: 0.0370, max: 1.3704, stdev: 0.2367
New bare network average reward: 0.6832, min: -0.0278, max: 1.3704, stdev: 0.2376
External policy "random" average reward: 0.2596, min: -0.2870, max: 1.0278, stdev: 0.2238
External policy "individual greedy" average reward: 0.5286, min: -0.0833, max: 1.2963, stdev: 0.2226
External policy "total greedy" average reward: 0.6431, min: 0.0463, max: 1.3426, stdev: 0.2146
New network won 61 and tied 171 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 402 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.40 seconds
Training examples lengths: [64555, 65141, 64809, 65019, 64863, 65031, 64817, 64589, 64750, 64797]
Total value: 478057.93
Training on 648371 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2334 (value: 0.0013, weighted value: 0.0671, policy: 0.1663, weighted policy: 0.1663), Train Mean Max: 0.9342
Epoch 2/10, Train Loss: 0.2208 (value: 0.0012, weighted value: 0.0609, policy: 0.1598, weighted policy: 0.1598), Train Mean Max: 0.9352
Epoch 3/10, Train Loss: 0.2093 (value: 0.0011, weighted value: 0.0547, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9358
Epoch 4/10, Train Loss: 0.2046 (value: 0.0010, weighted value: 0.0517, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9360
Epoch 5/10, Train Loss: 0.2019 (value: 0.0010, weighted value: 0.0496, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9363
Epoch 6/10, Train Loss: 0.1979 (value: 0.0010, weighted value: 0.0476, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9366
Epoch 7/10, Train Loss: 0.1956 (value: 0.0009, weighted value: 0.0455, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9366
Epoch 8/10, Train Loss: 0.1928 (value: 0.0009, weighted value: 0.0436, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9369
Epoch 9/10, Train Loss: 0.1920 (value: 0.0009, weighted value: 0.0432, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9370
Epoch 10/10, Train Loss: 0.1904 (value: 0.0008, weighted value: 0.0411, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9373
..training done in 62.55 seconds
..evaluation done in 19.04 seconds
Old network+MCTS average reward: 0.7257, min: 0.0000, max: 1.3981, stdev: 0.2292
New network+MCTS average reward: 0.7259, min: 0.0648, max: 1.3981, stdev: 0.2325
Old bare network average reward: 0.6906, min: 0.0093, max: 1.3704, stdev: 0.2330
New bare network average reward: 0.6890, min: 0.0278, max: 1.3796, stdev: 0.2352
External policy "random" average reward: 0.2669, min: -0.3981, max: 0.8519, stdev: 0.2285
External policy "individual greedy" average reward: 0.5432, min: -0.0648, max: 1.1574, stdev: 0.2246
External policy "total greedy" average reward: 0.6546, min: 0.0463, max: 1.4722, stdev: 0.2280
New network won 75 and tied 148 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 403 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.53 seconds
Training examples lengths: [65141, 64809, 65019, 64863, 65031, 64817, 64589, 64750, 64797, 64849]
Total value: 478321.77
Training on 648665 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2538 (value: 0.0016, weighted value: 0.0809, policy: 0.1729, weighted policy: 0.1729), Train Mean Max: 0.9329
Epoch 2/10, Train Loss: 0.2333 (value: 0.0014, weighted value: 0.0688, policy: 0.1645, weighted policy: 0.1645), Train Mean Max: 0.9341
Epoch 3/10, Train Loss: 0.2208 (value: 0.0012, weighted value: 0.0622, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9347
Epoch 4/10, Train Loss: 0.2147 (value: 0.0012, weighted value: 0.0588, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9351
Epoch 5/10, Train Loss: 0.2086 (value: 0.0011, weighted value: 0.0546, policy: 0.1539, weighted policy: 0.1539), Train Mean Max: 0.9354
Epoch 6/10, Train Loss: 0.2058 (value: 0.0011, weighted value: 0.0527, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9356
Epoch 7/10, Train Loss: 0.2025 (value: 0.0010, weighted value: 0.0503, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9357
Epoch 8/10, Train Loss: 0.1999 (value: 0.0010, weighted value: 0.0484, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9360
Epoch 9/10, Train Loss: 0.1984 (value: 0.0010, weighted value: 0.0476, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9362
Epoch 10/10, Train Loss: 0.1950 (value: 0.0009, weighted value: 0.0444, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9364
..training done in 62.79 seconds
..evaluation done in 18.74 seconds
Old network+MCTS average reward: 0.7273, min: 0.0278, max: 1.7685, stdev: 0.2465
New network+MCTS average reward: 0.7292, min: 0.0648, max: 1.7778, stdev: 0.2477
Old bare network average reward: 0.6898, min: -0.0185, max: 1.7685, stdev: 0.2489
New bare network average reward: 0.6932, min: -0.0185, max: 1.7778, stdev: 0.2499
External policy "random" average reward: 0.2471, min: -0.4074, max: 1.2130, stdev: 0.2367
External policy "individual greedy" average reward: 0.5375, min: -0.1574, max: 1.5370, stdev: 0.2456
External policy "total greedy" average reward: 0.6448, min: -0.0648, max: 1.6389, stdev: 0.2397
New network won 66 and tied 174 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 404 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.20 seconds
Training examples lengths: [64809, 65019, 64863, 65031, 64817, 64589, 64750, 64797, 64849, 64779]
Total value: 477750.33
Training on 648303 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2136 (value: 0.0011, weighted value: 0.0563, policy: 0.1573, weighted policy: 0.1573), Train Mean Max: 0.9352
Epoch 2/10, Train Loss: 0.2064 (value: 0.0010, weighted value: 0.0516, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9359
Epoch 3/10, Train Loss: 0.2013 (value: 0.0010, weighted value: 0.0485, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9361
Epoch 4/10, Train Loss: 0.1983 (value: 0.0010, weighted value: 0.0477, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9364
Epoch 5/10, Train Loss: 0.1949 (value: 0.0009, weighted value: 0.0450, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9366
Epoch 6/10, Train Loss: 0.1930 (value: 0.0009, weighted value: 0.0436, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9368
Epoch 7/10, Train Loss: 0.1915 (value: 0.0008, weighted value: 0.0422, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9370
Epoch 8/10, Train Loss: 0.1899 (value: 0.0008, weighted value: 0.0411, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9372
Epoch 9/10, Train Loss: 0.1890 (value: 0.0008, weighted value: 0.0405, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9374
Epoch 10/10, Train Loss: 0.1868 (value: 0.0008, weighted value: 0.0388, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9376
..training done in 60.58 seconds
..evaluation done in 18.70 seconds
Old network+MCTS average reward: 0.7264, min: 0.0370, max: 1.4907, stdev: 0.2487
New network+MCTS average reward: 0.7296, min: 0.0185, max: 1.4907, stdev: 0.2497
Old bare network average reward: 0.6940, min: 0.0278, max: 1.4907, stdev: 0.2557
New bare network average reward: 0.6898, min: 0.0278, max: 1.4907, stdev: 0.2551
External policy "random" average reward: 0.2631, min: -0.3426, max: 0.8704, stdev: 0.2289
External policy "individual greedy" average reward: 0.5498, min: -0.0370, max: 1.3889, stdev: 0.2341
External policy "total greedy" average reward: 0.6561, min: 0.0093, max: 1.4074, stdev: 0.2273
New network won 68 and tied 164 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 405 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.21 seconds
Training examples lengths: [65019, 64863, 65031, 64817, 64589, 64750, 64797, 64849, 64779, 64895]
Total value: 477684.31
Training on 648389 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2096 (value: 0.0010, weighted value: 0.0520, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9362
Epoch 2/10, Train Loss: 0.2017 (value: 0.0009, weighted value: 0.0470, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9366
Epoch 3/10, Train Loss: 0.1961 (value: 0.0009, weighted value: 0.0448, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9370
Epoch 4/10, Train Loss: 0.1938 (value: 0.0009, weighted value: 0.0436, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9373
Epoch 5/10, Train Loss: 0.1904 (value: 0.0008, weighted value: 0.0418, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9375
Epoch 6/10, Train Loss: 0.1883 (value: 0.0008, weighted value: 0.0399, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9376
Epoch 7/10, Train Loss: 0.1886 (value: 0.0008, weighted value: 0.0400, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9375
Epoch 8/10, Train Loss: 0.1867 (value: 0.0008, weighted value: 0.0393, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9379
Epoch 9/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0360, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9381
Epoch 10/10, Train Loss: 0.1837 (value: 0.0007, weighted value: 0.0363, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9381
..training done in 64.43 seconds
..evaluation done in 18.77 seconds
Old network+MCTS average reward: 0.7252, min: 0.2130, max: 1.2593, stdev: 0.2119
New network+MCTS average reward: 0.7228, min: 0.2130, max: 1.2593, stdev: 0.2069
Old bare network average reward: 0.6898, min: 0.1574, max: 1.2500, stdev: 0.2108
New bare network average reward: 0.6896, min: 0.1759, max: 1.2500, stdev: 0.2104
External policy "random" average reward: 0.2603, min: -0.4352, max: 0.8704, stdev: 0.2179
External policy "individual greedy" average reward: 0.5379, min: -0.0926, max: 1.0556, stdev: 0.2159
External policy "total greedy" average reward: 0.6542, min: 0.1667, max: 1.2315, stdev: 0.2055
New network won 64 and tied 173 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 406 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.15 seconds
Training examples lengths: [64863, 65031, 64817, 64589, 64750, 64797, 64849, 64779, 64895, 64648]
Total value: 477883.94
Training on 648018 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2079 (value: 0.0010, weighted value: 0.0507, policy: 0.1572, weighted policy: 0.1572), Train Mean Max: 0.9365
Epoch 2/10, Train Loss: 0.1993 (value: 0.0009, weighted value: 0.0458, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9371
Epoch 3/10, Train Loss: 0.1959 (value: 0.0009, weighted value: 0.0450, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9373
Epoch 4/10, Train Loss: 0.1902 (value: 0.0008, weighted value: 0.0411, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1881 (value: 0.0008, weighted value: 0.0397, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1861 (value: 0.0008, weighted value: 0.0384, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9379
Epoch 7/10, Train Loss: 0.1869 (value: 0.0008, weighted value: 0.0390, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1834 (value: 0.0007, weighted value: 0.0368, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1828 (value: 0.0007, weighted value: 0.0359, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9383
Epoch 10/10, Train Loss: 0.1820 (value: 0.0007, weighted value: 0.0345, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9385
..training done in 64.96 seconds
..evaluation done in 18.88 seconds
Old network+MCTS average reward: 0.7360, min: 0.0926, max: 1.4444, stdev: 0.2197
New network+MCTS average reward: 0.7427, min: 0.0926, max: 1.4444, stdev: 0.2195
Old bare network average reward: 0.7020, min: 0.0926, max: 1.3333, stdev: 0.2227
New bare network average reward: 0.7026, min: 0.0463, max: 1.3611, stdev: 0.2268
External policy "random" average reward: 0.2595, min: -0.3519, max: 0.7870, stdev: 0.2187
External policy "individual greedy" average reward: 0.5393, min: -0.0648, max: 1.1019, stdev: 0.1976
External policy "total greedy" average reward: 0.6587, min: 0.0556, max: 1.2407, stdev: 0.2027
New network won 89 and tied 159 out of 300 games (56.17% wins where ties are half wins)
Keeping the new network

Training iteration 407 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.91 seconds
Training examples lengths: [65031, 64817, 64589, 64750, 64797, 64849, 64779, 64895, 64648, 64725]
Total value: 478097.89
Training on 647880 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2081 (value: 0.0010, weighted value: 0.0508, policy: 0.1573, weighted policy: 0.1573), Train Mean Max: 0.9367
Epoch 2/10, Train Loss: 0.1980 (value: 0.0009, weighted value: 0.0450, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9374
Epoch 3/10, Train Loss: 0.1912 (value: 0.0008, weighted value: 0.0413, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9378
Epoch 4/10, Train Loss: 0.1898 (value: 0.0008, weighted value: 0.0407, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9379
Epoch 5/10, Train Loss: 0.1874 (value: 0.0008, weighted value: 0.0396, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9382
Epoch 6/10, Train Loss: 0.1848 (value: 0.0007, weighted value: 0.0374, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9382
Epoch 7/10, Train Loss: 0.1843 (value: 0.0007, weighted value: 0.0368, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9383
Epoch 8/10, Train Loss: 0.1825 (value: 0.0007, weighted value: 0.0362, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9386
Epoch 9/10, Train Loss: 0.1811 (value: 0.0007, weighted value: 0.0348, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9386
Epoch 10/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0348, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9386
..training done in 64.67 seconds
..evaluation done in 18.65 seconds
Old network+MCTS average reward: 0.7572, min: 0.1481, max: 1.4444, stdev: 0.2317
New network+MCTS average reward: 0.7541, min: 0.1296, max: 1.4444, stdev: 0.2325
Old bare network average reward: 0.7151, min: 0.0370, max: 1.4074, stdev: 0.2415
New bare network average reward: 0.7151, min: 0.0370, max: 1.4074, stdev: 0.2366
External policy "random" average reward: 0.2919, min: -0.3519, max: 1.0463, stdev: 0.2173
External policy "individual greedy" average reward: 0.5644, min: 0.0370, max: 1.2037, stdev: 0.2163
External policy "total greedy" average reward: 0.6767, min: 0.0370, max: 1.3148, stdev: 0.2212
New network won 62 and tied 169 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 408 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.87 seconds
Training examples lengths: [64817, 64589, 64750, 64797, 64849, 64779, 64895, 64648, 64725, 65031]
Total value: 478523.32
Training on 647880 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2290 (value: 0.0013, weighted value: 0.0640, policy: 0.1651, weighted policy: 0.1651), Train Mean Max: 0.9353
Epoch 2/10, Train Loss: 0.2144 (value: 0.0011, weighted value: 0.0549, policy: 0.1596, weighted policy: 0.1596), Train Mean Max: 0.9360
Epoch 3/10, Train Loss: 0.2048 (value: 0.0010, weighted value: 0.0516, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9367
Epoch 4/10, Train Loss: 0.1990 (value: 0.0009, weighted value: 0.0474, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9369
Epoch 5/10, Train Loss: 0.1971 (value: 0.0009, weighted value: 0.0465, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9370
Epoch 6/10, Train Loss: 0.1940 (value: 0.0009, weighted value: 0.0446, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9372
Epoch 7/10, Train Loss: 0.1907 (value: 0.0008, weighted value: 0.0414, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9374
Epoch 8/10, Train Loss: 0.1899 (value: 0.0008, weighted value: 0.0408, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1885 (value: 0.0008, weighted value: 0.0400, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1868 (value: 0.0008, weighted value: 0.0385, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9378
..training done in 59.69 seconds
..evaluation done in 18.70 seconds
Old network+MCTS average reward: 0.7441, min: 0.1111, max: 1.5278, stdev: 0.2344
New network+MCTS average reward: 0.7425, min: 0.1111, max: 1.5000, stdev: 0.2316
Old bare network average reward: 0.7043, min: 0.0463, max: 1.4352, stdev: 0.2300
New bare network average reward: 0.6998, min: 0.1111, max: 1.4722, stdev: 0.2298
External policy "random" average reward: 0.2744, min: -0.2778, max: 1.1296, stdev: 0.2319
External policy "individual greedy" average reward: 0.5516, min: 0.0000, max: 1.2222, stdev: 0.2379
External policy "total greedy" average reward: 0.6705, min: 0.1019, max: 1.4444, stdev: 0.2287
New network won 69 and tied 149 out of 300 games (47.83% wins where ties are half wins)
Reverting to the old network

Training iteration 409 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.63 seconds
Training examples lengths: [64589, 64750, 64797, 64849, 64779, 64895, 64648, 64725, 65031, 64750]
Total value: 477616.06
Training on 647813 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2503 (value: 0.0015, weighted value: 0.0772, policy: 0.1731, weighted policy: 0.1731), Train Mean Max: 0.9339
Epoch 2/10, Train Loss: 0.2287 (value: 0.0013, weighted value: 0.0647, policy: 0.1640, weighted policy: 0.1640), Train Mean Max: 0.9350
Epoch 3/10, Train Loss: 0.2178 (value: 0.0012, weighted value: 0.0608, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9356
Epoch 4/10, Train Loss: 0.2091 (value: 0.0011, weighted value: 0.0546, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9358
Epoch 5/10, Train Loss: 0.2052 (value: 0.0010, weighted value: 0.0521, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9360
Epoch 6/10, Train Loss: 0.2009 (value: 0.0010, weighted value: 0.0494, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9364
Epoch 7/10, Train Loss: 0.1987 (value: 0.0009, weighted value: 0.0470, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9365
Epoch 8/10, Train Loss: 0.1969 (value: 0.0009, weighted value: 0.0462, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9366
Epoch 9/10, Train Loss: 0.1940 (value: 0.0009, weighted value: 0.0437, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9369
Epoch 10/10, Train Loss: 0.1911 (value: 0.0008, weighted value: 0.0421, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9372
..training done in 59.84 seconds
..evaluation done in 17.98 seconds
Old network+MCTS average reward: 0.7156, min: 0.1852, max: 1.3611, stdev: 0.2227
New network+MCTS average reward: 0.7167, min: 0.2500, max: 1.3611, stdev: 0.2204
Old bare network average reward: 0.6793, min: 0.1852, max: 1.3241, stdev: 0.2237
New bare network average reward: 0.6815, min: 0.1852, max: 1.3241, stdev: 0.2210
External policy "random" average reward: 0.2533, min: -0.3333, max: 0.8889, stdev: 0.2210
External policy "individual greedy" average reward: 0.5355, min: 0.0370, max: 1.2222, stdev: 0.2299
External policy "total greedy" average reward: 0.6427, min: 0.0556, max: 1.2685, stdev: 0.2314
New network won 68 and tied 173 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 410 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.03 seconds
Training examples lengths: [64750, 64797, 64849, 64779, 64895, 64648, 64725, 65031, 64750, 64729]
Total value: 477732.06
Training on 647953 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2140 (value: 0.0011, weighted value: 0.0555, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9357
Epoch 2/10, Train Loss: 0.2041 (value: 0.0010, weighted value: 0.0491, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9363
Epoch 3/10, Train Loss: 0.1984 (value: 0.0009, weighted value: 0.0465, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9367
Epoch 4/10, Train Loss: 0.1950 (value: 0.0009, weighted value: 0.0450, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9369
Epoch 5/10, Train Loss: 0.1931 (value: 0.0009, weighted value: 0.0436, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9370
Epoch 6/10, Train Loss: 0.1910 (value: 0.0008, weighted value: 0.0420, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9372
Epoch 7/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0399, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9375
Epoch 8/10, Train Loss: 0.1882 (value: 0.0008, weighted value: 0.0390, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1882 (value: 0.0008, weighted value: 0.0392, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1857 (value: 0.0008, weighted value: 0.0380, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9380
..training done in 64.32 seconds
..evaluation done in 18.83 seconds
Old network+MCTS average reward: 0.7264, min: 0.1111, max: 1.5000, stdev: 0.2357
New network+MCTS average reward: 0.7269, min: 0.1481, max: 1.4537, stdev: 0.2303
Old bare network average reward: 0.6909, min: 0.0556, max: 1.4722, stdev: 0.2360
New bare network average reward: 0.6931, min: 0.0648, max: 1.4537, stdev: 0.2344
External policy "random" average reward: 0.2602, min: -0.3889, max: 0.9352, stdev: 0.2191
External policy "individual greedy" average reward: 0.5408, min: -0.0370, max: 1.2315, stdev: 0.2282
External policy "total greedy" average reward: 0.6440, min: 0.1019, max: 1.2315, stdev: 0.2186
New network won 62 and tied 164 out of 300 games (48.00% wins where ties are half wins)
Reverting to the old network

Training iteration 411 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.49 seconds
Training examples lengths: [64797, 64849, 64779, 64895, 64648, 64725, 65031, 64750, 64729, 64604]
Total value: 477387.09
Training on 647807 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2343 (value: 0.0014, weighted value: 0.0688, policy: 0.1655, weighted policy: 0.1655), Train Mean Max: 0.9341
Epoch 2/10, Train Loss: 0.2189 (value: 0.0012, weighted value: 0.0584, policy: 0.1606, weighted policy: 0.1606), Train Mean Max: 0.9351
Epoch 3/10, Train Loss: 0.2105 (value: 0.0011, weighted value: 0.0554, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9356
Epoch 4/10, Train Loss: 0.2041 (value: 0.0010, weighted value: 0.0507, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9359
Epoch 5/10, Train Loss: 0.2028 (value: 0.0010, weighted value: 0.0505, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9362
Epoch 6/10, Train Loss: 0.1981 (value: 0.0009, weighted value: 0.0471, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9364
Epoch 7/10, Train Loss: 0.1964 (value: 0.0009, weighted value: 0.0455, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9365
Epoch 8/10, Train Loss: 0.1941 (value: 0.0009, weighted value: 0.0440, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9368
Epoch 9/10, Train Loss: 0.1927 (value: 0.0008, weighted value: 0.0423, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9369
Epoch 10/10, Train Loss: 0.1909 (value: 0.0008, weighted value: 0.0412, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9370
..training done in 59.50 seconds
..evaluation done in 18.42 seconds
Old network+MCTS average reward: 0.7308, min: 0.0926, max: 1.4815, stdev: 0.2327
New network+MCTS average reward: 0.7275, min: 0.1389, max: 1.4815, stdev: 0.2301
Old bare network average reward: 0.6887, min: 0.0926, max: 1.4815, stdev: 0.2329
New bare network average reward: 0.6867, min: 0.0556, max: 1.4815, stdev: 0.2321
External policy "random" average reward: 0.2595, min: -0.2500, max: 0.9167, stdev: 0.2265
External policy "individual greedy" average reward: 0.5430, min: -0.0370, max: 1.4074, stdev: 0.2312
External policy "total greedy" average reward: 0.6516, min: 0.1019, max: 1.4630, stdev: 0.2228
New network won 66 and tied 162 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 412 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.69 seconds
Training examples lengths: [64849, 64779, 64895, 64648, 64725, 65031, 64750, 64729, 64604, 64965]
Total value: 477555.82
Training on 647975 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2537 (value: 0.0016, weighted value: 0.0798, policy: 0.1739, weighted policy: 0.1739), Train Mean Max: 0.9328
Epoch 2/10, Train Loss: 0.2338 (value: 0.0014, weighted value: 0.0688, policy: 0.1649, weighted policy: 0.1649), Train Mean Max: 0.9338
Epoch 3/10, Train Loss: 0.2218 (value: 0.0013, weighted value: 0.0631, policy: 0.1587, weighted policy: 0.1587), Train Mean Max: 0.9345
Epoch 4/10, Train Loss: 0.2133 (value: 0.0011, weighted value: 0.0573, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9350
Epoch 5/10, Train Loss: 0.2092 (value: 0.0011, weighted value: 0.0550, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9352
Epoch 6/10, Train Loss: 0.2047 (value: 0.0010, weighted value: 0.0518, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9354
Epoch 7/10, Train Loss: 0.2029 (value: 0.0010, weighted value: 0.0499, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9355
Epoch 8/10, Train Loss: 0.2000 (value: 0.0010, weighted value: 0.0479, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9359
Epoch 9/10, Train Loss: 0.1972 (value: 0.0009, weighted value: 0.0460, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9361
Epoch 10/10, Train Loss: 0.1941 (value: 0.0009, weighted value: 0.0440, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9363
..training done in 67.07 seconds
..evaluation done in 18.65 seconds
Old network+MCTS average reward: 0.7330, min: 0.1852, max: 1.4352, stdev: 0.2396
New network+MCTS average reward: 0.7339, min: 0.1852, max: 1.4907, stdev: 0.2409
Old bare network average reward: 0.6917, min: 0.0833, max: 1.4352, stdev: 0.2465
New bare network average reward: 0.6943, min: 0.0833, max: 1.4352, stdev: 0.2474
External policy "random" average reward: 0.2660, min: -0.3519, max: 0.9537, stdev: 0.2160
External policy "individual greedy" average reward: 0.5481, min: -0.0185, max: 1.2315, stdev: 0.2192
External policy "total greedy" average reward: 0.6625, min: 0.1389, max: 1.4630, stdev: 0.2226
New network won 71 and tied 164 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 413 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.11 seconds
Training examples lengths: [64779, 64895, 64648, 64725, 65031, 64750, 64729, 64604, 64965, 64672]
Total value: 477891.35
Training on 647798 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2161 (value: 0.0012, weighted value: 0.0576, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9348
Epoch 2/10, Train Loss: 0.2077 (value: 0.0010, weighted value: 0.0525, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9357
Epoch 3/10, Train Loss: 0.2017 (value: 0.0010, weighted value: 0.0483, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9358
Epoch 4/10, Train Loss: 0.1983 (value: 0.0009, weighted value: 0.0467, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9362
Epoch 5/10, Train Loss: 0.1950 (value: 0.0009, weighted value: 0.0447, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9365
Epoch 6/10, Train Loss: 0.1936 (value: 0.0009, weighted value: 0.0434, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9366
Epoch 7/10, Train Loss: 0.1925 (value: 0.0008, weighted value: 0.0424, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9368
Epoch 8/10, Train Loss: 0.1895 (value: 0.0008, weighted value: 0.0405, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9370
Epoch 9/10, Train Loss: 0.1894 (value: 0.0008, weighted value: 0.0405, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9372
Epoch 10/10, Train Loss: 0.1872 (value: 0.0008, weighted value: 0.0382, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9374
..training done in 59.64 seconds
..evaluation done in 19.12 seconds
Old network+MCTS average reward: 0.7308, min: 0.1296, max: 1.4444, stdev: 0.2261
New network+MCTS average reward: 0.7282, min: 0.1296, max: 1.3333, stdev: 0.2261
Old bare network average reward: 0.6883, min: -0.0093, max: 1.3796, stdev: 0.2327
New bare network average reward: 0.6949, min: 0.0833, max: 1.3796, stdev: 0.2345
External policy "random" average reward: 0.2505, min: -0.2963, max: 1.0463, stdev: 0.2217
External policy "individual greedy" average reward: 0.5413, min: 0.0278, max: 1.1759, stdev: 0.2146
External policy "total greedy" average reward: 0.6560, min: 0.1574, max: 1.3056, stdev: 0.2181
New network won 67 and tied 153 out of 300 games (47.83% wins where ties are half wins)
Reverting to the old network

Training iteration 414 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.72 seconds
Training examples lengths: [64895, 64648, 64725, 65031, 64750, 64729, 64604, 64965, 64672, 64828]
Total value: 478649.58
Training on 647847 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2367 (value: 0.0014, weighted value: 0.0696, policy: 0.1671, weighted policy: 0.1671), Train Mean Max: 0.9334
Epoch 2/10, Train Loss: 0.2201 (value: 0.0012, weighted value: 0.0597, policy: 0.1604, weighted policy: 0.1604), Train Mean Max: 0.9343
Epoch 3/10, Train Loss: 0.2141 (value: 0.0011, weighted value: 0.0573, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9348
Epoch 4/10, Train Loss: 0.2089 (value: 0.0011, weighted value: 0.0537, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9349
Epoch 5/10, Train Loss: 0.2037 (value: 0.0010, weighted value: 0.0504, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9354
Epoch 6/10, Train Loss: 0.1989 (value: 0.0010, weighted value: 0.0476, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9356
Epoch 7/10, Train Loss: 0.1985 (value: 0.0009, weighted value: 0.0467, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9357
Epoch 8/10, Train Loss: 0.1973 (value: 0.0009, weighted value: 0.0465, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9360
Epoch 9/10, Train Loss: 0.1941 (value: 0.0009, weighted value: 0.0435, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9362
Epoch 10/10, Train Loss: 0.1920 (value: 0.0008, weighted value: 0.0420, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9365
..training done in 60.18 seconds
..evaluation done in 19.00 seconds
Old network+MCTS average reward: 0.7366, min: 0.1296, max: 1.4630, stdev: 0.2380
New network+MCTS average reward: 0.7369, min: 0.1667, max: 1.4630, stdev: 0.2392
Old bare network average reward: 0.6998, min: 0.1296, max: 1.4630, stdev: 0.2427
New bare network average reward: 0.6978, min: 0.1481, max: 1.4630, stdev: 0.2400
External policy "random" average reward: 0.2643, min: -0.2963, max: 1.0185, stdev: 0.2195
External policy "individual greedy" average reward: 0.5412, min: 0.0278, max: 1.3241, stdev: 0.2305
External policy "total greedy" average reward: 0.6493, min: 0.1574, max: 1.4352, stdev: 0.2268
New network won 62 and tied 172 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 415 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.20 seconds
Training examples lengths: [64648, 64725, 65031, 64750, 64729, 64604, 64965, 64672, 64828, 64916]
Total value: 479195.63
Training on 647868 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2564 (value: 0.0016, weighted value: 0.0810, policy: 0.1754, weighted policy: 0.1754), Train Mean Max: 0.9321
Epoch 2/10, Train Loss: 0.2330 (value: 0.0014, weighted value: 0.0685, policy: 0.1645, weighted policy: 0.1645), Train Mean Max: 0.9337
Epoch 3/10, Train Loss: 0.2224 (value: 0.0013, weighted value: 0.0639, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9339
Epoch 4/10, Train Loss: 0.2149 (value: 0.0012, weighted value: 0.0584, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9344
Epoch 5/10, Train Loss: 0.2108 (value: 0.0011, weighted value: 0.0562, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9347
Epoch 6/10, Train Loss: 0.2068 (value: 0.0011, weighted value: 0.0531, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9349
Epoch 7/10, Train Loss: 0.2029 (value: 0.0010, weighted value: 0.0500, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9353
Epoch 8/10, Train Loss: 0.2004 (value: 0.0010, weighted value: 0.0489, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9355
Epoch 9/10, Train Loss: 0.1976 (value: 0.0009, weighted value: 0.0465, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9358
Epoch 10/10, Train Loss: 0.1964 (value: 0.0009, weighted value: 0.0454, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9360
..training done in 59.81 seconds
..evaluation done in 18.13 seconds
Old network+MCTS average reward: 0.7152, min: 0.0556, max: 1.4537, stdev: 0.2256
New network+MCTS average reward: 0.7130, min: 0.0556, max: 1.4630, stdev: 0.2236
Old bare network average reward: 0.6766, min: 0.0556, max: 1.3704, stdev: 0.2298
New bare network average reward: 0.6760, min: -0.0370, max: 1.3611, stdev: 0.2276
External policy "random" average reward: 0.2524, min: -0.5463, max: 0.9259, stdev: 0.2143
External policy "individual greedy" average reward: 0.5112, min: -0.0370, max: 1.1667, stdev: 0.2177
External policy "total greedy" average reward: 0.6423, min: 0.0185, max: 1.2685, stdev: 0.2127
New network won 71 and tied 153 out of 300 games (49.17% wins where ties are half wins)
Reverting to the old network

Training iteration 416 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.52 seconds
Training examples lengths: [64725, 65031, 64750, 64729, 64604, 64965, 64672, 64828, 64916, 64860]
Total value: 479475.60
Training on 648080 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2722 (value: 0.0018, weighted value: 0.0923, policy: 0.1798, weighted policy: 0.1798), Train Mean Max: 0.9311
Epoch 2/10, Train Loss: 0.2473 (value: 0.0016, weighted value: 0.0788, policy: 0.1685, weighted policy: 0.1685), Train Mean Max: 0.9325
Epoch 3/10, Train Loss: 0.2306 (value: 0.0014, weighted value: 0.0694, policy: 0.1612, weighted policy: 0.1612), Train Mean Max: 0.9331
Epoch 4/10, Train Loss: 0.2228 (value: 0.0013, weighted value: 0.0645, policy: 0.1583, weighted policy: 0.1583), Train Mean Max: 0.9336
Epoch 5/10, Train Loss: 0.2175 (value: 0.0012, weighted value: 0.0612, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9339
Epoch 6/10, Train Loss: 0.2127 (value: 0.0012, weighted value: 0.0579, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9342
Epoch 7/10, Train Loss: 0.2080 (value: 0.0011, weighted value: 0.0540, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9346
Epoch 8/10, Train Loss: 0.2052 (value: 0.0010, weighted value: 0.0521, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9349
Epoch 9/10, Train Loss: 0.2017 (value: 0.0010, weighted value: 0.0498, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9352
Epoch 10/10, Train Loss: 0.1999 (value: 0.0010, weighted value: 0.0484, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9355
..training done in 63.70 seconds
..evaluation done in 22.79 seconds
Old network+MCTS average reward: 0.7116, min: 0.2037, max: 1.4907, stdev: 0.2176
New network+MCTS average reward: 0.7094, min: 0.2037, max: 1.4907, stdev: 0.2210
Old bare network average reward: 0.6719, min: 0.1204, max: 1.4907, stdev: 0.2188
New bare network average reward: 0.6761, min: 0.1204, max: 1.4907, stdev: 0.2233
External policy "random" average reward: 0.2417, min: -0.3426, max: 0.8796, stdev: 0.2063
External policy "individual greedy" average reward: 0.5240, min: -0.0370, max: 1.1852, stdev: 0.2172
External policy "total greedy" average reward: 0.6329, min: 0.0463, max: 1.1852, stdev: 0.2025
New network won 67 and tied 158 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 417 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.85 seconds
Training examples lengths: [65031, 64750, 64729, 64604, 64965, 64672, 64828, 64916, 64860, 64877]
Total value: 478804.44
Training on 648232 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2902 (value: 0.0021, weighted value: 0.1041, policy: 0.1862, weighted policy: 0.1862), Train Mean Max: 0.9301
Epoch 2/10, Train Loss: 0.2568 (value: 0.0017, weighted value: 0.0854, policy: 0.1714, weighted policy: 0.1714), Train Mean Max: 0.9316
Epoch 3/10, Train Loss: 0.2418 (value: 0.0015, weighted value: 0.0768, policy: 0.1651, weighted policy: 0.1651), Train Mean Max: 0.9322
Epoch 4/10, Train Loss: 0.2311 (value: 0.0014, weighted value: 0.0704, policy: 0.1607, weighted policy: 0.1607), Train Mean Max: 0.9328
Epoch 5/10, Train Loss: 0.2249 (value: 0.0013, weighted value: 0.0661, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9330
Epoch 6/10, Train Loss: 0.2177 (value: 0.0012, weighted value: 0.0609, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9336
Epoch 7/10, Train Loss: 0.2142 (value: 0.0012, weighted value: 0.0585, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9338
Epoch 8/10, Train Loss: 0.2099 (value: 0.0011, weighted value: 0.0551, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9341
Epoch 9/10, Train Loss: 0.2080 (value: 0.0011, weighted value: 0.0539, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9344
Epoch 10/10, Train Loss: 0.2040 (value: 0.0010, weighted value: 0.0505, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9346
..training done in 76.27 seconds
..evaluation done in 18.70 seconds
Old network+MCTS average reward: 0.7375, min: 0.0926, max: 1.3981, stdev: 0.2378
New network+MCTS average reward: 0.7353, min: 0.0926, max: 1.3889, stdev: 0.2385
Old bare network average reward: 0.7016, min: 0.0556, max: 1.3889, stdev: 0.2414
New bare network average reward: 0.7008, min: 0.0556, max: 1.3889, stdev: 0.2424
External policy "random" average reward: 0.2737, min: -0.3611, max: 0.8796, stdev: 0.2291
External policy "individual greedy" average reward: 0.5392, min: -0.0556, max: 1.2500, stdev: 0.2357
External policy "total greedy" average reward: 0.6453, min: 0.1111, max: 1.3148, stdev: 0.2284
New network won 64 and tied 165 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 418 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.18 seconds
Training examples lengths: [64750, 64729, 64604, 64965, 64672, 64828, 64916, 64860, 64877, 65155]
Total value: 478785.53
Training on 648356 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.3042 (value: 0.0023, weighted value: 0.1133, policy: 0.1909, weighted policy: 0.1909), Train Mean Max: 0.9292
Epoch 2/10, Train Loss: 0.2680 (value: 0.0018, weighted value: 0.0923, policy: 0.1757, weighted policy: 0.1757), Train Mean Max: 0.9309
Epoch 3/10, Train Loss: 0.2486 (value: 0.0016, weighted value: 0.0821, policy: 0.1665, weighted policy: 0.1665), Train Mean Max: 0.9315
Epoch 4/10, Train Loss: 0.2401 (value: 0.0016, weighted value: 0.0776, policy: 0.1626, weighted policy: 0.1626), Train Mean Max: 0.9320
Epoch 5/10, Train Loss: 0.2286 (value: 0.0014, weighted value: 0.0687, policy: 0.1599, weighted policy: 0.1599), Train Mean Max: 0.9325
Epoch 6/10, Train Loss: 0.2227 (value: 0.0013, weighted value: 0.0651, policy: 0.1577, weighted policy: 0.1577), Train Mean Max: 0.9329
Epoch 7/10, Train Loss: 0.2196 (value: 0.0013, weighted value: 0.0626, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9332
Epoch 8/10, Train Loss: 0.2137 (value: 0.0012, weighted value: 0.0585, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9337
Epoch 9/10, Train Loss: 0.2097 (value: 0.0011, weighted value: 0.0552, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9338
Epoch 10/10, Train Loss: 0.2074 (value: 0.0011, weighted value: 0.0538, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9343
..training done in 65.16 seconds
..evaluation done in 18.93 seconds
Old network+MCTS average reward: 0.7257, min: 0.0648, max: 1.4074, stdev: 0.2311
New network+MCTS average reward: 0.7292, min: 0.1481, max: 1.4444, stdev: 0.2274
Old bare network average reward: 0.6907, min: 0.0556, max: 1.4074, stdev: 0.2337
New bare network average reward: 0.6912, min: 0.0556, max: 1.4074, stdev: 0.2325
External policy "random" average reward: 0.2613, min: -0.2500, max: 1.0000, stdev: 0.2099
External policy "individual greedy" average reward: 0.5336, min: 0.0463, max: 1.4167, stdev: 0.2267
External policy "total greedy" average reward: 0.6468, min: 0.1481, max: 1.5926, stdev: 0.2100
New network won 77 and tied 161 out of 300 games (52.50% wins where ties are half wins)
Keeping the new network

Training iteration 419 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.19 seconds
Training examples lengths: [64729, 64604, 64965, 64672, 64828, 64916, 64860, 64877, 65155, 64637]
Total value: 479518.06
Training on 648243 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2233 (value: 0.0013, weighted value: 0.0629, policy: 0.1604, weighted policy: 0.1604), Train Mean Max: 0.9333
Epoch 2/10, Train Loss: 0.2168 (value: 0.0012, weighted value: 0.0594, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9338
Epoch 3/10, Train Loss: 0.2095 (value: 0.0011, weighted value: 0.0543, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9344
Epoch 4/10, Train Loss: 0.2058 (value: 0.0010, weighted value: 0.0522, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9346
Epoch 5/10, Train Loss: 0.2019 (value: 0.0010, weighted value: 0.0498, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9351
Epoch 6/10, Train Loss: 0.1998 (value: 0.0010, weighted value: 0.0478, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9350
Epoch 7/10, Train Loss: 0.1986 (value: 0.0009, weighted value: 0.0472, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9355
Epoch 8/10, Train Loss: 0.1960 (value: 0.0009, weighted value: 0.0453, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9358
Epoch 9/10, Train Loss: 0.1923 (value: 0.0009, weighted value: 0.0429, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9362
Epoch 10/10, Train Loss: 0.1921 (value: 0.0009, weighted value: 0.0426, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9361
..training done in 64.41 seconds
..evaluation done in 18.79 seconds
Old network+MCTS average reward: 0.7088, min: 0.1389, max: 1.3333, stdev: 0.2358
New network+MCTS average reward: 0.7094, min: 0.1389, max: 1.3333, stdev: 0.2350
Old bare network average reward: 0.6703, min: 0.0926, max: 1.3333, stdev: 0.2328
New bare network average reward: 0.6686, min: 0.0926, max: 1.3333, stdev: 0.2355
External policy "random" average reward: 0.2435, min: -0.3611, max: 0.8611, stdev: 0.2335
External policy "individual greedy" average reward: 0.5289, min: -0.0278, max: 1.2315, stdev: 0.2348
External policy "total greedy" average reward: 0.6369, min: 0.1296, max: 1.1759, stdev: 0.2188
New network won 65 and tied 162 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 420 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.68 seconds
Training examples lengths: [64604, 64965, 64672, 64828, 64916, 64860, 64877, 65155, 64637, 64906]
Total value: 479975.28
Training on 648420 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2412 (value: 0.0015, weighted value: 0.0748, policy: 0.1664, weighted policy: 0.1664), Train Mean Max: 0.9321
Epoch 2/10, Train Loss: 0.2278 (value: 0.0013, weighted value: 0.0661, policy: 0.1617, weighted policy: 0.1617), Train Mean Max: 0.9331
Epoch 3/10, Train Loss: 0.2190 (value: 0.0012, weighted value: 0.0613, policy: 0.1577, weighted policy: 0.1577), Train Mean Max: 0.9335
Epoch 4/10, Train Loss: 0.2134 (value: 0.0011, weighted value: 0.0572, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9340
Epoch 5/10, Train Loss: 0.2084 (value: 0.0011, weighted value: 0.0546, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9344
Epoch 6/10, Train Loss: 0.2060 (value: 0.0010, weighted value: 0.0521, policy: 0.1539, weighted policy: 0.1539), Train Mean Max: 0.9346
Epoch 7/10, Train Loss: 0.2039 (value: 0.0010, weighted value: 0.0514, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9350
Epoch 8/10, Train Loss: 0.1988 (value: 0.0009, weighted value: 0.0473, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9353
Epoch 9/10, Train Loss: 0.1972 (value: 0.0009, weighted value: 0.0464, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9354
Epoch 10/10, Train Loss: 0.1959 (value: 0.0009, weighted value: 0.0450, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9356
..training done in 64.45 seconds
..evaluation done in 18.03 seconds
Old network+MCTS average reward: 0.7150, min: 0.1204, max: 1.4074, stdev: 0.2305
New network+MCTS average reward: 0.7121, min: 0.0926, max: 1.4074, stdev: 0.2298
Old bare network average reward: 0.6754, min: 0.1204, max: 1.4074, stdev: 0.2268
New bare network average reward: 0.6768, min: 0.0741, max: 1.3426, stdev: 0.2315
External policy "random" average reward: 0.2603, min: -0.5093, max: 1.0648, stdev: 0.2432
External policy "individual greedy" average reward: 0.5199, min: -0.1111, max: 1.2685, stdev: 0.2366
External policy "total greedy" average reward: 0.6288, min: -0.0185, max: 1.3519, stdev: 0.2265
New network won 64 and tied 155 out of 300 games (47.17% wins where ties are half wins)
Reverting to the old network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_420

Training iteration 421 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.06 seconds
Training examples lengths: [64965, 64672, 64828, 64916, 64860, 64877, 65155, 64637, 64906, 64767]
Total value: 480095.81
Training on 648583 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2587 (value: 0.0017, weighted value: 0.0854, policy: 0.1734, weighted policy: 0.1734), Train Mean Max: 0.9314
Epoch 2/10, Train Loss: 0.2392 (value: 0.0015, weighted value: 0.0741, policy: 0.1651, weighted policy: 0.1651), Train Mean Max: 0.9323
Epoch 3/10, Train Loss: 0.2285 (value: 0.0014, weighted value: 0.0679, policy: 0.1606, weighted policy: 0.1606), Train Mean Max: 0.9328
Epoch 4/10, Train Loss: 0.2212 (value: 0.0013, weighted value: 0.0639, policy: 0.1573, weighted policy: 0.1573), Train Mean Max: 0.9333
Epoch 5/10, Train Loss: 0.2155 (value: 0.0012, weighted value: 0.0595, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9336
Epoch 6/10, Train Loss: 0.2113 (value: 0.0011, weighted value: 0.0565, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9339
Epoch 7/10, Train Loss: 0.2074 (value: 0.0011, weighted value: 0.0541, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9343
Epoch 8/10, Train Loss: 0.2043 (value: 0.0010, weighted value: 0.0518, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9346
Epoch 9/10, Train Loss: 0.2024 (value: 0.0010, weighted value: 0.0491, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9348
Epoch 10/10, Train Loss: 0.2006 (value: 0.0010, weighted value: 0.0484, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9352
..training done in 64.87 seconds
..evaluation done in 18.49 seconds
Old network+MCTS average reward: 0.7123, min: 0.1574, max: 1.4074, stdev: 0.2252
New network+MCTS average reward: 0.7140, min: 0.1759, max: 1.4352, stdev: 0.2302
Old bare network average reward: 0.6716, min: 0.1481, max: 1.4074, stdev: 0.2243
New bare network average reward: 0.6773, min: 0.1481, max: 1.4074, stdev: 0.2304
External policy "random" average reward: 0.2499, min: -0.3519, max: 1.0000, stdev: 0.2319
External policy "individual greedy" average reward: 0.5238, min: -0.0370, max: 1.4352, stdev: 0.2283
External policy "total greedy" average reward: 0.6402, min: 0.1481, max: 1.3889, stdev: 0.2288
New network won 79 and tied 158 out of 300 games (52.67% wins where ties are half wins)
Keeping the new network

Training iteration 422 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.40 seconds
Training examples lengths: [64672, 64828, 64916, 64860, 64877, 65155, 64637, 64906, 64767, 64998]
Total value: 480629.97
Training on 648616 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2173 (value: 0.0012, weighted value: 0.0588, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9342
Epoch 2/10, Train Loss: 0.2092 (value: 0.0011, weighted value: 0.0536, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9347
Epoch 3/10, Train Loss: 0.2031 (value: 0.0010, weighted value: 0.0501, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9351
Epoch 4/10, Train Loss: 0.2005 (value: 0.0010, weighted value: 0.0488, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9354
Epoch 5/10, Train Loss: 0.1987 (value: 0.0010, weighted value: 0.0480, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9356
Epoch 6/10, Train Loss: 0.1952 (value: 0.0009, weighted value: 0.0451, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9360
Epoch 7/10, Train Loss: 0.1929 (value: 0.0009, weighted value: 0.0431, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9362
Epoch 8/10, Train Loss: 0.1921 (value: 0.0009, weighted value: 0.0428, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9364
Epoch 9/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0408, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9368
Epoch 10/10, Train Loss: 0.1879 (value: 0.0008, weighted value: 0.0394, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9369
..training done in 64.79 seconds
..evaluation done in 18.38 seconds
Old network+MCTS average reward: 0.7246, min: 0.1389, max: 1.4907, stdev: 0.2466
New network+MCTS average reward: 0.7241, min: 0.0741, max: 1.5741, stdev: 0.2456
Old bare network average reward: 0.6911, min: 0.0741, max: 1.4074, stdev: 0.2533
New bare network average reward: 0.6875, min: 0.0741, max: 1.5741, stdev: 0.2539
External policy "random" average reward: 0.2526, min: -0.3333, max: 1.1944, stdev: 0.2375
External policy "individual greedy" average reward: 0.5324, min: -0.0278, max: 1.2963, stdev: 0.2400
External policy "total greedy" average reward: 0.6456, min: 0.1111, max: 1.4444, stdev: 0.2334
New network won 57 and tied 170 out of 300 games (47.33% wins where ties are half wins)
Reverting to the old network

Training iteration 423 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.61 seconds
Training examples lengths: [64828, 64916, 64860, 64877, 65155, 64637, 64906, 64767, 64998, 64809]
Total value: 480901.99
Training on 648753 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2366 (value: 0.0014, weighted value: 0.0707, policy: 0.1659, weighted policy: 0.1659), Train Mean Max: 0.9331
Epoch 2/10, Train Loss: 0.2201 (value: 0.0012, weighted value: 0.0607, policy: 0.1594, weighted policy: 0.1594), Train Mean Max: 0.9341
Epoch 3/10, Train Loss: 0.2130 (value: 0.0011, weighted value: 0.0568, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9345
Epoch 4/10, Train Loss: 0.2085 (value: 0.0011, weighted value: 0.0549, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9347
Epoch 5/10, Train Loss: 0.2041 (value: 0.0010, weighted value: 0.0512, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9349
Epoch 6/10, Train Loss: 0.2004 (value: 0.0010, weighted value: 0.0487, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9353
Epoch 7/10, Train Loss: 0.1986 (value: 0.0010, weighted value: 0.0477, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9356
Epoch 8/10, Train Loss: 0.1962 (value: 0.0009, weighted value: 0.0459, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9359
Epoch 9/10, Train Loss: 0.1939 (value: 0.0009, weighted value: 0.0438, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9362
Epoch 10/10, Train Loss: 0.1926 (value: 0.0009, weighted value: 0.0426, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9363
..training done in 64.65 seconds
..evaluation done in 18.15 seconds
Old network+MCTS average reward: 0.7423, min: 0.2222, max: 1.3889, stdev: 0.2266
New network+MCTS average reward: 0.7416, min: 0.1019, max: 1.3981, stdev: 0.2286
Old bare network average reward: 0.7065, min: 0.0556, max: 1.3426, stdev: 0.2304
New bare network average reward: 0.7066, min: 0.0556, max: 1.3426, stdev: 0.2304
External policy "random" average reward: 0.2709, min: -0.3148, max: 1.0278, stdev: 0.2305
External policy "individual greedy" average reward: 0.5451, min: -0.0463, max: 1.2500, stdev: 0.2230
External policy "total greedy" average reward: 0.6540, min: 0.1019, max: 1.3148, stdev: 0.2279
New network won 66 and tied 160 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 424 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.80 seconds
Training examples lengths: [64916, 64860, 64877, 65155, 64637, 64906, 64767, 64998, 64809, 64572]
Total value: 480202.10
Training on 648497 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2519 (value: 0.0016, weighted value: 0.0798, policy: 0.1721, weighted policy: 0.1721), Train Mean Max: 0.9321
Epoch 2/10, Train Loss: 0.2337 (value: 0.0014, weighted value: 0.0701, policy: 0.1636, weighted policy: 0.1636), Train Mean Max: 0.9333
Epoch 3/10, Train Loss: 0.2213 (value: 0.0013, weighted value: 0.0626, policy: 0.1587, weighted policy: 0.1587), Train Mean Max: 0.9337
Epoch 4/10, Train Loss: 0.2135 (value: 0.0012, weighted value: 0.0584, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9342
Epoch 5/10, Train Loss: 0.2112 (value: 0.0012, weighted value: 0.0575, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9347
Epoch 6/10, Train Loss: 0.2047 (value: 0.0010, weighted value: 0.0521, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9350
Epoch 7/10, Train Loss: 0.2031 (value: 0.0010, weighted value: 0.0508, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9352
Epoch 8/10, Train Loss: 0.2004 (value: 0.0010, weighted value: 0.0487, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9356
Epoch 9/10, Train Loss: 0.1981 (value: 0.0009, weighted value: 0.0473, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9358
Epoch 10/10, Train Loss: 0.1955 (value: 0.0009, weighted value: 0.0458, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9360
..training done in 59.73 seconds
..evaluation done in 18.71 seconds
Old network+MCTS average reward: 0.7395, min: 0.0926, max: 1.3981, stdev: 0.2334
New network+MCTS average reward: 0.7400, min: 0.0926, max: 1.3981, stdev: 0.2365
Old bare network average reward: 0.7000, min: 0.0833, max: 1.3981, stdev: 0.2425
New bare network average reward: 0.6986, min: 0.0741, max: 1.3889, stdev: 0.2390
External policy "random" average reward: 0.2660, min: -0.3056, max: 0.8241, stdev: 0.2161
External policy "individual greedy" average reward: 0.5456, min: 0.0556, max: 1.2593, stdev: 0.2185
External policy "total greedy" average reward: 0.6604, min: 0.0370, max: 1.2037, stdev: 0.2175
New network won 80 and tied 146 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 425 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.49 seconds
Training examples lengths: [64860, 64877, 65155, 64637, 64906, 64767, 64998, 64809, 64572, 64808]
Total value: 480109.25
Training on 648389 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2143 (value: 0.0011, weighted value: 0.0563, policy: 0.1580, weighted policy: 0.1580), Train Mean Max: 0.9349
Epoch 2/10, Train Loss: 0.2059 (value: 0.0010, weighted value: 0.0519, policy: 0.1539, weighted policy: 0.1539), Train Mean Max: 0.9356
Epoch 3/10, Train Loss: 0.2008 (value: 0.0010, weighted value: 0.0492, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9360
Epoch 4/10, Train Loss: 0.1962 (value: 0.0009, weighted value: 0.0455, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9362
Epoch 5/10, Train Loss: 0.1948 (value: 0.0009, weighted value: 0.0445, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9364
Epoch 6/10, Train Loss: 0.1936 (value: 0.0009, weighted value: 0.0444, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9368
Epoch 7/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0415, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9370
Epoch 8/10, Train Loss: 0.1889 (value: 0.0008, weighted value: 0.0403, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9370
Epoch 9/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0401, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9374
Epoch 10/10, Train Loss: 0.1868 (value: 0.0008, weighted value: 0.0391, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9375
..training done in 60.04 seconds
..evaluation done in 19.25 seconds
Old network+MCTS average reward: 0.7304, min: 0.0926, max: 1.4630, stdev: 0.2374
New network+MCTS average reward: 0.7319, min: 0.0926, max: 1.4630, stdev: 0.2357
Old bare network average reward: 0.6939, min: 0.0741, max: 1.3796, stdev: 0.2441
New bare network average reward: 0.6935, min: 0.0926, max: 1.3889, stdev: 0.2405
External policy "random" average reward: 0.2461, min: -0.4167, max: 0.9630, stdev: 0.2289
External policy "individual greedy" average reward: 0.5348, min: -0.2685, max: 1.3704, stdev: 0.2364
External policy "total greedy" average reward: 0.6446, min: 0.1204, max: 1.3333, stdev: 0.2217
New network won 75 and tied 166 out of 300 games (52.67% wins where ties are half wins)
Keeping the new network

Training iteration 426 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.77 seconds
Training examples lengths: [64877, 65155, 64637, 64906, 64767, 64998, 64809, 64572, 64808, 64985]
Total value: 480511.93
Training on 648514 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2075 (value: 0.0010, weighted value: 0.0511, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9361
Epoch 2/10, Train Loss: 0.1980 (value: 0.0009, weighted value: 0.0458, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9369
Epoch 3/10, Train Loss: 0.1950 (value: 0.0009, weighted value: 0.0448, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9371
Epoch 4/10, Train Loss: 0.1909 (value: 0.0008, weighted value: 0.0417, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9374
Epoch 5/10, Train Loss: 0.1889 (value: 0.0008, weighted value: 0.0413, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9377
Epoch 6/10, Train Loss: 0.1867 (value: 0.0008, weighted value: 0.0398, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9379
Epoch 7/10, Train Loss: 0.1855 (value: 0.0008, weighted value: 0.0385, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9380
Epoch 8/10, Train Loss: 0.1851 (value: 0.0008, weighted value: 0.0378, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1833 (value: 0.0007, weighted value: 0.0371, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9383
Epoch 10/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0356, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9383
..training done in 68.74 seconds
..evaluation done in 19.38 seconds
Old network+MCTS average reward: 0.7457, min: -0.1111, max: 1.4259, stdev: 0.2385
New network+MCTS average reward: 0.7459, min: -0.0463, max: 1.3796, stdev: 0.2400
Old bare network average reward: 0.7104, min: -0.1111, max: 1.3796, stdev: 0.2432
New bare network average reward: 0.7122, min: -0.0463, max: 1.3519, stdev: 0.2424
External policy "random" average reward: 0.2823, min: -0.3426, max: 0.8796, stdev: 0.2335
External policy "individual greedy" average reward: 0.5452, min: -0.0833, max: 1.3148, stdev: 0.2416
External policy "total greedy" average reward: 0.6633, min: 0.0741, max: 1.3981, stdev: 0.2320
New network won 75 and tied 162 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network

Training iteration 427 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 66.20 seconds
Training examples lengths: [65155, 64637, 64906, 64767, 64998, 64809, 64572, 64808, 64985, 64640]
Total value: 481193.51
Training on 648277 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2043 (value: 0.0010, weighted value: 0.0485, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9370
Epoch 2/10, Train Loss: 0.1963 (value: 0.0009, weighted value: 0.0448, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9376
Epoch 3/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0406, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9379
Epoch 4/10, Train Loss: 0.1885 (value: 0.0008, weighted value: 0.0415, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9382
Epoch 5/10, Train Loss: 0.1862 (value: 0.0008, weighted value: 0.0396, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9383
Epoch 6/10, Train Loss: 0.1833 (value: 0.0007, weighted value: 0.0367, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9385
Epoch 7/10, Train Loss: 0.1833 (value: 0.0007, weighted value: 0.0371, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9385
Epoch 8/10, Train Loss: 0.1823 (value: 0.0007, weighted value: 0.0366, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9386
Epoch 9/10, Train Loss: 0.1809 (value: 0.0007, weighted value: 0.0355, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9388
Epoch 10/10, Train Loss: 0.1785 (value: 0.0007, weighted value: 0.0329, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9390
..training done in 74.45 seconds
..evaluation done in 18.02 seconds
Old network+MCTS average reward: 0.7164, min: 0.1389, max: 1.2963, stdev: 0.2390
New network+MCTS average reward: 0.7177, min: 0.1019, max: 1.3333, stdev: 0.2415
Old bare network average reward: 0.6841, min: 0.0556, max: 1.2778, stdev: 0.2404
New bare network average reward: 0.6802, min: 0.1111, max: 1.2685, stdev: 0.2431
External policy "random" average reward: 0.2683, min: -0.2963, max: 0.8704, stdev: 0.2246
External policy "individual greedy" average reward: 0.5412, min: 0.0370, max: 1.2315, stdev: 0.2193
External policy "total greedy" average reward: 0.6460, min: 0.1389, max: 1.3519, stdev: 0.2186
New network won 71 and tied 168 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 428 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.44 seconds
Training examples lengths: [64637, 64906, 64767, 64998, 64809, 64572, 64808, 64985, 64640, 64733]
Total value: 481109.00
Training on 647855 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2020 (value: 0.0009, weighted value: 0.0472, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9372
Epoch 2/10, Train Loss: 0.1957 (value: 0.0009, weighted value: 0.0440, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9378
Epoch 3/10, Train Loss: 0.1893 (value: 0.0008, weighted value: 0.0406, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9382
Epoch 4/10, Train Loss: 0.1850 (value: 0.0008, weighted value: 0.0379, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9384
Epoch 5/10, Train Loss: 0.1849 (value: 0.0008, weighted value: 0.0386, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9386
Epoch 6/10, Train Loss: 0.1830 (value: 0.0007, weighted value: 0.0370, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9387
Epoch 7/10, Train Loss: 0.1822 (value: 0.0007, weighted value: 0.0364, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9388
Epoch 8/10, Train Loss: 0.1803 (value: 0.0007, weighted value: 0.0344, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9389
Epoch 9/10, Train Loss: 0.1795 (value: 0.0007, weighted value: 0.0343, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9391
Epoch 10/10, Train Loss: 0.1789 (value: 0.0007, weighted value: 0.0335, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9392
..training done in 63.02 seconds
..evaluation done in 26.11 seconds
Old network+MCTS average reward: 0.7320, min: 0.1667, max: 1.5185, stdev: 0.2242
New network+MCTS average reward: 0.7298, min: 0.1944, max: 1.5185, stdev: 0.2200
Old bare network average reward: 0.6902, min: 0.1389, max: 1.4722, stdev: 0.2222
New bare network average reward: 0.6932, min: 0.0926, max: 1.4259, stdev: 0.2225
External policy "random" average reward: 0.2496, min: -0.3241, max: 0.9907, stdev: 0.2287
External policy "individual greedy" average reward: 0.5201, min: 0.0093, max: 1.3148, stdev: 0.2238
External policy "total greedy" average reward: 0.6358, min: 0.0093, max: 1.4259, stdev: 0.2161
New network won 54 and tied 182 out of 300 games (48.33% wins where ties are half wins)
Reverting to the old network

Training iteration 429 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 61.44 seconds
Training examples lengths: [64906, 64767, 64998, 64809, 64572, 64808, 64985, 64640, 64733, 64518]
Total value: 481085.03
Training on 647736 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2233 (value: 0.0012, weighted value: 0.0601, policy: 0.1632, weighted policy: 0.1632), Train Mean Max: 0.9359
Epoch 2/10, Train Loss: 0.2095 (value: 0.0011, weighted value: 0.0530, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9368
Epoch 3/10, Train Loss: 0.1998 (value: 0.0010, weighted value: 0.0483, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9374
Epoch 4/10, Train Loss: 0.1958 (value: 0.0009, weighted value: 0.0466, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9375
Epoch 5/10, Train Loss: 0.1923 (value: 0.0009, weighted value: 0.0440, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9378
Epoch 6/10, Train Loss: 0.1905 (value: 0.0009, weighted value: 0.0425, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9379
Epoch 7/10, Train Loss: 0.1878 (value: 0.0008, weighted value: 0.0403, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9381
Epoch 8/10, Train Loss: 0.1874 (value: 0.0008, weighted value: 0.0399, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9382
Epoch 9/10, Train Loss: 0.1834 (value: 0.0007, weighted value: 0.0373, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9386
Epoch 10/10, Train Loss: 0.1853 (value: 0.0008, weighted value: 0.0389, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9385
..training done in 74.60 seconds
..evaluation done in 23.38 seconds
Old network+MCTS average reward: 0.7402, min: 0.2037, max: 1.4074, stdev: 0.2368
New network+MCTS average reward: 0.7363, min: 0.2130, max: 1.4167, stdev: 0.2366
Old bare network average reward: 0.6995, min: 0.0926, max: 1.4074, stdev: 0.2407
New bare network average reward: 0.6955, min: 0.0926, max: 1.4167, stdev: 0.2420
External policy "random" average reward: 0.2706, min: -0.3889, max: 0.9259, stdev: 0.2200
External policy "individual greedy" average reward: 0.5501, min: 0.0093, max: 1.2315, stdev: 0.2237
External policy "total greedy" average reward: 0.6727, min: 0.1019, max: 1.4074, stdev: 0.2234
New network won 68 and tied 153 out of 300 games (48.17% wins where ties are half wins)
Reverting to the old network

Training iteration 430 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 60.04 seconds
Training examples lengths: [64767, 64998, 64809, 64572, 64808, 64985, 64640, 64733, 64518, 65205]
Total value: 481962.78
Training on 648035 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2448 (value: 0.0015, weighted value: 0.0746, policy: 0.1702, weighted policy: 0.1702), Train Mean Max: 0.9344
Epoch 2/10, Train Loss: 0.2214 (value: 0.0012, weighted value: 0.0614, policy: 0.1599, weighted policy: 0.1599), Train Mean Max: 0.9357
Epoch 3/10, Train Loss: 0.2108 (value: 0.0011, weighted value: 0.0568, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9364
Epoch 4/10, Train Loss: 0.2057 (value: 0.0011, weighted value: 0.0534, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9365
Epoch 5/10, Train Loss: 0.2008 (value: 0.0010, weighted value: 0.0499, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9368
Epoch 6/10, Train Loss: 0.1973 (value: 0.0010, weighted value: 0.0476, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1936 (value: 0.0009, weighted value: 0.0455, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9376
Epoch 8/10, Train Loss: 0.1926 (value: 0.0009, weighted value: 0.0452, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1897 (value: 0.0008, weighted value: 0.0417, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9378
Epoch 10/10, Train Loss: 0.1890 (value: 0.0008, weighted value: 0.0418, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9380
..training done in 73.35 seconds
..evaluation done in 18.74 seconds
Old network+MCTS average reward: 0.7384, min: 0.1667, max: 1.4444, stdev: 0.2545
New network+MCTS average reward: 0.7388, min: 0.1204, max: 1.4444, stdev: 0.2526
Old bare network average reward: 0.7025, min: 0.0833, max: 1.4259, stdev: 0.2576
New bare network average reward: 0.7030, min: 0.0833, max: 1.4259, stdev: 0.2569
External policy "random" average reward: 0.2639, min: -0.5556, max: 0.9722, stdev: 0.2291
External policy "individual greedy" average reward: 0.5352, min: -0.1574, max: 1.2500, stdev: 0.2433
External policy "total greedy" average reward: 0.6581, min: -0.0648, max: 1.3148, stdev: 0.2405
New network won 61 and tied 182 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 431 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 63.87 seconds
Training examples lengths: [64998, 64809, 64572, 64808, 64985, 64640, 64733, 64518, 65205, 64759]
Total value: 482181.65
Training on 648027 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2091 (value: 0.0011, weighted value: 0.0537, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9365
Epoch 2/10, Train Loss: 0.2005 (value: 0.0010, weighted value: 0.0485, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9372
Epoch 3/10, Train Loss: 0.1958 (value: 0.0009, weighted value: 0.0457, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9376
Epoch 4/10, Train Loss: 0.1930 (value: 0.0009, weighted value: 0.0446, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9377
Epoch 5/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0424, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9379
Epoch 6/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0420, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9379
Epoch 7/10, Train Loss: 0.1860 (value: 0.0008, weighted value: 0.0401, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9383
Epoch 8/10, Train Loss: 0.1840 (value: 0.0008, weighted value: 0.0377, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1841 (value: 0.0008, weighted value: 0.0377, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9385
Epoch 10/10, Train Loss: 0.1842 (value: 0.0008, weighted value: 0.0375, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9386
..training done in 69.30 seconds
..evaluation done in 18.43 seconds
Old network+MCTS average reward: 0.7365, min: 0.2315, max: 1.3981, stdev: 0.2304
New network+MCTS average reward: 0.7402, min: 0.2130, max: 1.4074, stdev: 0.2306
Old bare network average reward: 0.7075, min: 0.1296, max: 1.4352, stdev: 0.2390
New bare network average reward: 0.7075, min: 0.1944, max: 1.3981, stdev: 0.2355
External policy "random" average reward: 0.2586, min: -0.1759, max: 0.8704, stdev: 0.2138
External policy "individual greedy" average reward: 0.5501, min: 0.0000, max: 1.3333, stdev: 0.2283
External policy "total greedy" average reward: 0.6676, min: 0.0648, max: 1.4444, stdev: 0.2201
New network won 76 and tied 167 out of 300 games (53.17% wins where ties are half wins)
Keeping the new network

Training iteration 432 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.19 seconds
Training examples lengths: [64809, 64572, 64808, 64985, 64640, 64733, 64518, 65205, 64759, 64685]
Total value: 481407.59
Training on 647714 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2048 (value: 0.0010, weighted value: 0.0485, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9371
Epoch 2/10, Train Loss: 0.1971 (value: 0.0009, weighted value: 0.0452, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9378
Epoch 3/10, Train Loss: 0.1923 (value: 0.0009, weighted value: 0.0431, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9380
Epoch 4/10, Train Loss: 0.1879 (value: 0.0008, weighted value: 0.0401, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9384
Epoch 5/10, Train Loss: 0.1862 (value: 0.0008, weighted value: 0.0392, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9383
Epoch 6/10, Train Loss: 0.1857 (value: 0.0008, weighted value: 0.0395, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9385
Epoch 7/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0368, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9387
Epoch 8/10, Train Loss: 0.1823 (value: 0.0007, weighted value: 0.0358, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9387
Epoch 9/10, Train Loss: 0.1814 (value: 0.0007, weighted value: 0.0361, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9390
Epoch 10/10, Train Loss: 0.1798 (value: 0.0007, weighted value: 0.0342, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9390
..training done in 68.43 seconds
..evaluation done in 20.12 seconds
Old network+MCTS average reward: 0.7295, min: 0.1481, max: 1.5278, stdev: 0.2422
New network+MCTS average reward: 0.7323, min: 0.1759, max: 1.5278, stdev: 0.2416
Old bare network average reward: 0.6895, min: 0.1296, max: 1.4074, stdev: 0.2428
New bare network average reward: 0.6946, min: 0.1296, max: 1.4630, stdev: 0.2476
External policy "random" average reward: 0.2481, min: -0.3148, max: 1.1296, stdev: 0.2437
External policy "individual greedy" average reward: 0.5289, min: 0.0185, max: 1.2037, stdev: 0.2426
External policy "total greedy" average reward: 0.6502, min: 0.0926, max: 1.3519, stdev: 0.2293
New network won 82 and tied 155 out of 300 games (53.17% wins where ties are half wins)
Keeping the new network

Training iteration 433 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.47 seconds
Training examples lengths: [64572, 64808, 64985, 64640, 64733, 64518, 65205, 64759, 64685, 64582]
Total value: 481228.49
Training on 647487 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2051 (value: 0.0010, weighted value: 0.0497, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9373
Epoch 2/10, Train Loss: 0.1946 (value: 0.0009, weighted value: 0.0436, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9380
Epoch 3/10, Train Loss: 0.1910 (value: 0.0008, weighted value: 0.0419, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9383
Epoch 4/10, Train Loss: 0.1855 (value: 0.0008, weighted value: 0.0392, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9386
Epoch 5/10, Train Loss: 0.1856 (value: 0.0008, weighted value: 0.0390, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9387
Epoch 6/10, Train Loss: 0.1849 (value: 0.0008, weighted value: 0.0385, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9388
Epoch 7/10, Train Loss: 0.1816 (value: 0.0007, weighted value: 0.0360, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9390
Epoch 8/10, Train Loss: 0.1804 (value: 0.0007, weighted value: 0.0350, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9391
Epoch 9/10, Train Loss: 0.1794 (value: 0.0007, weighted value: 0.0344, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9392
Epoch 10/10, Train Loss: 0.1790 (value: 0.0007, weighted value: 0.0335, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9393
..training done in 59.89 seconds
..evaluation done in 25.93 seconds
Old network+MCTS average reward: 0.7466, min: 0.1944, max: 1.5000, stdev: 0.2366
New network+MCTS average reward: 0.7537, min: 0.1944, max: 1.5000, stdev: 0.2322
Old bare network average reward: 0.7128, min: 0.1389, max: 1.5000, stdev: 0.2392
New bare network average reward: 0.7144, min: 0.1759, max: 1.5000, stdev: 0.2438
External policy "random" average reward: 0.2821, min: -0.4722, max: 1.1019, stdev: 0.2314
External policy "individual greedy" average reward: 0.5643, min: 0.0926, max: 1.4352, stdev: 0.2309
External policy "total greedy" average reward: 0.6715, min: 0.1019, max: 1.5093, stdev: 0.2327
New network won 81 and tied 172 out of 300 games (55.67% wins where ties are half wins)
Keeping the new network

Training iteration 434 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.57 seconds
Training examples lengths: [64808, 64985, 64640, 64733, 64518, 65205, 64759, 64685, 64582, 64777]
Total value: 481894.03
Training on 647692 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2038 (value: 0.0009, weighted value: 0.0475, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9375
Epoch 2/10, Train Loss: 0.1942 (value: 0.0008, weighted value: 0.0424, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9382
Epoch 3/10, Train Loss: 0.1897 (value: 0.0008, weighted value: 0.0409, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9385
Epoch 4/10, Train Loss: 0.1860 (value: 0.0008, weighted value: 0.0389, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9386
Epoch 5/10, Train Loss: 0.1834 (value: 0.0008, weighted value: 0.0375, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9388
Epoch 6/10, Train Loss: 0.1826 (value: 0.0007, weighted value: 0.0366, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9388
Epoch 7/10, Train Loss: 0.1810 (value: 0.0007, weighted value: 0.0351, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9390
Epoch 8/10, Train Loss: 0.1794 (value: 0.0007, weighted value: 0.0341, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9392
Epoch 9/10, Train Loss: 0.1799 (value: 0.0007, weighted value: 0.0342, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9392
Epoch 10/10, Train Loss: 0.1787 (value: 0.0007, weighted value: 0.0334, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9394
..training done in 67.07 seconds
..evaluation done in 19.60 seconds
Old network+MCTS average reward: 0.7418, min: -0.0185, max: 1.4444, stdev: 0.2315
New network+MCTS average reward: 0.7433, min: -0.0185, max: 1.4444, stdev: 0.2338
Old bare network average reward: 0.7057, min: -0.0185, max: 1.3796, stdev: 0.2320
New bare network average reward: 0.7069, min: -0.0185, max: 1.4074, stdev: 0.2340
External policy "random" average reward: 0.2765, min: -0.3148, max: 0.9630, stdev: 0.2270
External policy "individual greedy" average reward: 0.5599, min: -0.0648, max: 1.1852, stdev: 0.2240
External policy "total greedy" average reward: 0.6644, min: 0.0648, max: 1.2407, stdev: 0.2121
New network won 71 and tied 166 out of 300 games (51.33% wins where ties are half wins)
Keeping the new network

Training iteration 435 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.34 seconds
Training examples lengths: [64985, 64640, 64733, 64518, 65205, 64759, 64685, 64582, 64777, 64578]
Total value: 481938.31
Training on 647462 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2007 (value: 0.0009, weighted value: 0.0468, policy: 0.1539, weighted policy: 0.1539), Train Mean Max: 0.9376
Epoch 2/10, Train Loss: 0.1930 (value: 0.0008, weighted value: 0.0421, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9383
Epoch 3/10, Train Loss: 0.1871 (value: 0.0008, weighted value: 0.0396, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9386
Epoch 4/10, Train Loss: 0.1851 (value: 0.0008, weighted value: 0.0375, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9387
Epoch 5/10, Train Loss: 0.1837 (value: 0.0007, weighted value: 0.0369, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9389
Epoch 6/10, Train Loss: 0.1819 (value: 0.0007, weighted value: 0.0359, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9390
Epoch 7/10, Train Loss: 0.1807 (value: 0.0007, weighted value: 0.0354, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9391
Epoch 8/10, Train Loss: 0.1791 (value: 0.0007, weighted value: 0.0343, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9393
Epoch 9/10, Train Loss: 0.1775 (value: 0.0007, weighted value: 0.0330, policy: 0.1446, weighted policy: 0.1446), Train Mean Max: 0.9393
Epoch 10/10, Train Loss: 0.1773 (value: 0.0007, weighted value: 0.0331, policy: 0.1442, weighted policy: 0.1442), Train Mean Max: 0.9395
..training done in 61.41 seconds
..evaluation done in 18.37 seconds
Old network+MCTS average reward: 0.7298, min: 0.1481, max: 1.5741, stdev: 0.2470
New network+MCTS average reward: 0.7292, min: 0.1481, max: 1.5648, stdev: 0.2513
Old bare network average reward: 0.6935, min: 0.1389, max: 1.4444, stdev: 0.2511
New bare network average reward: 0.6960, min: 0.0833, max: 1.4537, stdev: 0.2511
External policy "random" average reward: 0.2595, min: -0.2778, max: 0.9630, stdev: 0.2333
External policy "individual greedy" average reward: 0.5452, min: -0.0278, max: 1.3333, stdev: 0.2361
External policy "total greedy" average reward: 0.6518, min: 0.1389, max: 1.5000, stdev: 0.2274
New network won 67 and tied 156 out of 300 games (48.33% wins where ties are half wins)
Reverting to the old network

Training iteration 436 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.69 seconds
Training examples lengths: [64640, 64733, 64518, 65205, 64759, 64685, 64582, 64777, 64578, 65080]
Total value: 481800.07
Training on 647557 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2228 (value: 0.0012, weighted value: 0.0598, policy: 0.1630, weighted policy: 0.1630), Train Mean Max: 0.9360
Epoch 2/10, Train Loss: 0.2092 (value: 0.0010, weighted value: 0.0524, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9368
Epoch 3/10, Train Loss: 0.1994 (value: 0.0009, weighted value: 0.0474, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9374
Epoch 4/10, Train Loss: 0.1958 (value: 0.0009, weighted value: 0.0458, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1921 (value: 0.0009, weighted value: 0.0435, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9379
Epoch 6/10, Train Loss: 0.1897 (value: 0.0008, weighted value: 0.0419, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9378
Epoch 7/10, Train Loss: 0.1872 (value: 0.0008, weighted value: 0.0405, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9382
Epoch 8/10, Train Loss: 0.1848 (value: 0.0008, weighted value: 0.0382, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1848 (value: 0.0008, weighted value: 0.0378, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9382
Epoch 10/10, Train Loss: 0.1836 (value: 0.0007, weighted value: 0.0372, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9387
..training done in 68.63 seconds
..evaluation done in 19.18 seconds
Old network+MCTS average reward: 0.7287, min: 0.2037, max: 1.4537, stdev: 0.2263
New network+MCTS average reward: 0.7267, min: 0.1944, max: 1.4630, stdev: 0.2226
Old bare network average reward: 0.6928, min: 0.1481, max: 1.4537, stdev: 0.2258
New bare network average reward: 0.6935, min: 0.1667, max: 1.3796, stdev: 0.2251
External policy "random" average reward: 0.2506, min: -0.3519, max: 1.0000, stdev: 0.2054
External policy "individual greedy" average reward: 0.5382, min: 0.0000, max: 1.2037, stdev: 0.2091
External policy "total greedy" average reward: 0.6475, min: 0.0278, max: 1.4444, stdev: 0.2091
New network won 68 and tied 155 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 437 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.99 seconds
Training examples lengths: [64733, 64518, 65205, 64759, 64685, 64582, 64777, 64578, 65080, 64746]
Total value: 481883.73
Training on 647663 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2423 (value: 0.0015, weighted value: 0.0726, policy: 0.1696, weighted policy: 0.1696), Train Mean Max: 0.9347
Epoch 2/10, Train Loss: 0.2238 (value: 0.0013, weighted value: 0.0628, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9357
Epoch 3/10, Train Loss: 0.2115 (value: 0.0011, weighted value: 0.0566, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9364
Epoch 4/10, Train Loss: 0.2038 (value: 0.0010, weighted value: 0.0514, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9367
Epoch 5/10, Train Loss: 0.1999 (value: 0.0010, weighted value: 0.0495, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9370
Epoch 6/10, Train Loss: 0.1970 (value: 0.0009, weighted value: 0.0473, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1947 (value: 0.0009, weighted value: 0.0456, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1914 (value: 0.0009, weighted value: 0.0435, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0413, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9376
Epoch 10/10, Train Loss: 0.1880 (value: 0.0008, weighted value: 0.0401, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9377
..training done in 60.59 seconds
..evaluation done in 19.12 seconds
Old network+MCTS average reward: 0.7462, min: 0.1759, max: 1.4722, stdev: 0.2263
New network+MCTS average reward: 0.7457, min: 0.1759, max: 1.4722, stdev: 0.2294
Old bare network average reward: 0.7067, min: 0.1204, max: 1.4722, stdev: 0.2258
New bare network average reward: 0.7031, min: 0.0926, max: 1.4722, stdev: 0.2271
External policy "random" average reward: 0.2641, min: -0.3981, max: 1.0926, stdev: 0.2278
External policy "individual greedy" average reward: 0.5488, min: -0.0093, max: 1.3704, stdev: 0.2232
External policy "total greedy" average reward: 0.6657, min: 0.1667, max: 1.3889, stdev: 0.2089
New network won 79 and tied 152 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 438 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.18 seconds
Training examples lengths: [64518, 65205, 64759, 64685, 64582, 64777, 64578, 65080, 64746, 64884]
Total value: 482052.38
Training on 647814 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2102 (value: 0.0011, weighted value: 0.0527, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9362
Epoch 2/10, Train Loss: 0.2013 (value: 0.0010, weighted value: 0.0484, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9371
Epoch 3/10, Train Loss: 0.1953 (value: 0.0009, weighted value: 0.0452, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9372
Epoch 4/10, Train Loss: 0.1925 (value: 0.0009, weighted value: 0.0436, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9375
Epoch 5/10, Train Loss: 0.1906 (value: 0.0008, weighted value: 0.0421, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9375
Epoch 6/10, Train Loss: 0.1895 (value: 0.0008, weighted value: 0.0415, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9377
Epoch 7/10, Train Loss: 0.1859 (value: 0.0008, weighted value: 0.0386, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9381
Epoch 8/10, Train Loss: 0.1847 (value: 0.0008, weighted value: 0.0382, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9383
Epoch 9/10, Train Loss: 0.1830 (value: 0.0007, weighted value: 0.0367, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9382
Epoch 10/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0365, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9385
..training done in 71.05 seconds
..evaluation done in 20.29 seconds
Old network+MCTS average reward: 0.7391, min: 0.1296, max: 1.5370, stdev: 0.2303
New network+MCTS average reward: 0.7387, min: 0.1296, max: 1.5093, stdev: 0.2304
Old bare network average reward: 0.6997, min: 0.0463, max: 1.4259, stdev: 0.2298
New bare network average reward: 0.7008, min: 0.0463, max: 1.5093, stdev: 0.2324
External policy "random" average reward: 0.2526, min: -0.4630, max: 0.8611, stdev: 0.2304
External policy "individual greedy" average reward: 0.5228, min: -0.2037, max: 1.2963, stdev: 0.2322
External policy "total greedy" average reward: 0.6425, min: 0.0648, max: 1.2500, stdev: 0.2158
New network won 65 and tied 160 out of 300 games (48.33% wins where ties are half wins)
Reverting to the old network

Training iteration 439 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.69 seconds
Training examples lengths: [65205, 64759, 64685, 64582, 64777, 64578, 65080, 64746, 64884, 64907]
Total value: 482408.04
Training on 648203 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2301 (value: 0.0013, weighted value: 0.0661, policy: 0.1640, weighted policy: 0.1640), Train Mean Max: 0.9347
Epoch 2/10, Train Loss: 0.2160 (value: 0.0011, weighted value: 0.0574, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9356
Epoch 3/10, Train Loss: 0.2093 (value: 0.0011, weighted value: 0.0543, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9360
Epoch 4/10, Train Loss: 0.2016 (value: 0.0010, weighted value: 0.0498, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9365
Epoch 5/10, Train Loss: 0.1992 (value: 0.0010, weighted value: 0.0485, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9366
Epoch 6/10, Train Loss: 0.1958 (value: 0.0009, weighted value: 0.0460, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9369
Epoch 7/10, Train Loss: 0.1928 (value: 0.0009, weighted value: 0.0433, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9369
Epoch 8/10, Train Loss: 0.1923 (value: 0.0009, weighted value: 0.0438, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9372
Epoch 9/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0415, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9374
Epoch 10/10, Train Loss: 0.1880 (value: 0.0008, weighted value: 0.0397, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9375
..training done in 63.36 seconds
..evaluation done in 18.61 seconds
Old network+MCTS average reward: 0.7521, min: 0.0926, max: 1.5648, stdev: 0.2311
New network+MCTS average reward: 0.7481, min: 0.1111, max: 1.4815, stdev: 0.2300
Old bare network average reward: 0.7099, min: 0.0278, max: 1.4815, stdev: 0.2305
New bare network average reward: 0.7131, min: 0.0556, max: 1.5093, stdev: 0.2321
External policy "random" average reward: 0.2702, min: -0.2685, max: 0.9074, stdev: 0.2161
External policy "individual greedy" average reward: 0.5546, min: 0.0370, max: 1.2778, stdev: 0.2254
External policy "total greedy" average reward: 0.6624, min: 0.1019, max: 1.2500, stdev: 0.2143
New network won 69 and tied 157 out of 300 games (49.17% wins where ties are half wins)
Reverting to the old network

Training iteration 440 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.38 seconds
Training examples lengths: [64759, 64685, 64582, 64777, 64578, 65080, 64746, 64884, 64907, 65009]
Total value: 482249.10
Training on 648007 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2526 (value: 0.0016, weighted value: 0.0795, policy: 0.1730, weighted policy: 0.1730), Train Mean Max: 0.9332
Epoch 2/10, Train Loss: 0.2304 (value: 0.0013, weighted value: 0.0672, policy: 0.1632, weighted policy: 0.1632), Train Mean Max: 0.9345
Epoch 3/10, Train Loss: 0.2203 (value: 0.0013, weighted value: 0.0625, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9349
Epoch 4/10, Train Loss: 0.2116 (value: 0.0011, weighted value: 0.0569, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9353
Epoch 5/10, Train Loss: 0.2060 (value: 0.0011, weighted value: 0.0530, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9355
Epoch 6/10, Train Loss: 0.2037 (value: 0.0010, weighted value: 0.0517, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9358
Epoch 7/10, Train Loss: 0.1996 (value: 0.0010, weighted value: 0.0485, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9361
Epoch 8/10, Train Loss: 0.1989 (value: 0.0010, weighted value: 0.0481, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9362
Epoch 9/10, Train Loss: 0.1959 (value: 0.0009, weighted value: 0.0458, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9364
Epoch 10/10, Train Loss: 0.1926 (value: 0.0009, weighted value: 0.0433, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9366
..training done in 59.64 seconds
..evaluation done in 18.93 seconds
Old network+MCTS average reward: 0.7471, min: 0.1481, max: 1.4815, stdev: 0.2396
New network+MCTS average reward: 0.7512, min: 0.1481, max: 1.5370, stdev: 0.2393
Old bare network average reward: 0.7113, min: 0.1481, max: 1.5370, stdev: 0.2447
New bare network average reward: 0.7097, min: 0.1481, max: 1.5370, stdev: 0.2436
External policy "random" average reward: 0.2522, min: -0.3519, max: 0.9074, stdev: 0.2217
External policy "individual greedy" average reward: 0.5460, min: -0.0370, max: 1.3148, stdev: 0.2265
External policy "total greedy" average reward: 0.6685, min: 0.0926, max: 1.4630, stdev: 0.2250
New network won 75 and tied 160 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_440

Training iteration 441 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.56 seconds
Training examples lengths: [64685, 64582, 64777, 64578, 65080, 64746, 64884, 64907, 65009, 64759]
Total value: 482173.13
Training on 648007 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2143 (value: 0.0011, weighted value: 0.0552, policy: 0.1591, weighted policy: 0.1591), Train Mean Max: 0.9351
Epoch 2/10, Train Loss: 0.2065 (value: 0.0010, weighted value: 0.0519, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9359
Epoch 3/10, Train Loss: 0.2005 (value: 0.0010, weighted value: 0.0480, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9361
Epoch 4/10, Train Loss: 0.1966 (value: 0.0009, weighted value: 0.0459, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9366
Epoch 5/10, Train Loss: 0.1942 (value: 0.0009, weighted value: 0.0438, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9366
Epoch 6/10, Train Loss: 0.1927 (value: 0.0009, weighted value: 0.0429, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9368
Epoch 7/10, Train Loss: 0.1902 (value: 0.0008, weighted value: 0.0419, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9371
Epoch 8/10, Train Loss: 0.1876 (value: 0.0008, weighted value: 0.0391, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9372
Epoch 9/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0399, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9373
Epoch 10/10, Train Loss: 0.1858 (value: 0.0008, weighted value: 0.0380, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9375
..training done in 65.14 seconds
..evaluation done in 18.88 seconds
Old network+MCTS average reward: 0.7394, min: -0.0648, max: 1.6574, stdev: 0.2408
New network+MCTS average reward: 0.7392, min: -0.0278, max: 1.5741, stdev: 0.2334
Old bare network average reward: 0.7032, min: -0.1389, max: 1.6574, stdev: 0.2403
New bare network average reward: 0.7031, min: -0.1019, max: 1.5741, stdev: 0.2397
External policy "random" average reward: 0.2677, min: -0.3426, max: 1.1204, stdev: 0.2387
External policy "individual greedy" average reward: 0.5450, min: -0.1481, max: 1.3056, stdev: 0.2357
External policy "total greedy" average reward: 0.6629, min: -0.0556, max: 1.2778, stdev: 0.2277
New network won 85 and tied 135 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 442 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.53 seconds
Training examples lengths: [64582, 64777, 64578, 65080, 64746, 64884, 64907, 65009, 64759, 64533]
Total value: 481739.94
Training on 647855 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2093 (value: 0.0010, weighted value: 0.0507, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9359
Epoch 2/10, Train Loss: 0.2000 (value: 0.0009, weighted value: 0.0457, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9365
Epoch 3/10, Train Loss: 0.1954 (value: 0.0009, weighted value: 0.0439, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9369
Epoch 4/10, Train Loss: 0.1932 (value: 0.0009, weighted value: 0.0435, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9370
Epoch 5/10, Train Loss: 0.1886 (value: 0.0008, weighted value: 0.0397, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9373
Epoch 6/10, Train Loss: 0.1881 (value: 0.0008, weighted value: 0.0398, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9373
Epoch 7/10, Train Loss: 0.1858 (value: 0.0007, weighted value: 0.0375, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9375
Epoch 8/10, Train Loss: 0.1859 (value: 0.0007, weighted value: 0.0374, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9377
Epoch 9/10, Train Loss: 0.1833 (value: 0.0007, weighted value: 0.0355, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1823 (value: 0.0007, weighted value: 0.0353, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9381
..training done in 65.64 seconds
..evaluation done in 18.80 seconds
Old network+MCTS average reward: 0.7330, min: 0.1111, max: 1.3981, stdev: 0.2311
New network+MCTS average reward: 0.7349, min: 0.1111, max: 1.3981, stdev: 0.2286
Old bare network average reward: 0.6973, min: 0.1019, max: 1.3796, stdev: 0.2393
New bare network average reward: 0.7008, min: 0.0926, max: 1.3796, stdev: 0.2335
External policy "random" average reward: 0.2571, min: -0.3796, max: 1.0463, stdev: 0.2263
External policy "individual greedy" average reward: 0.5424, min: 0.0185, max: 1.2778, stdev: 0.2279
External policy "total greedy" average reward: 0.6554, min: 0.1111, max: 1.3704, stdev: 0.2240
New network won 70 and tied 165 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 443 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.84 seconds
Training examples lengths: [64777, 64578, 65080, 64746, 64884, 64907, 65009, 64759, 64533, 64874]
Total value: 482070.04
Training on 648147 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2057 (value: 0.0010, weighted value: 0.0491, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9364
Epoch 2/10, Train Loss: 0.1966 (value: 0.0009, weighted value: 0.0436, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9370
Epoch 3/10, Train Loss: 0.1917 (value: 0.0008, weighted value: 0.0417, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9373
Epoch 4/10, Train Loss: 0.1892 (value: 0.0008, weighted value: 0.0404, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1859 (value: 0.0008, weighted value: 0.0379, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9377
Epoch 6/10, Train Loss: 0.1849 (value: 0.0007, weighted value: 0.0375, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9378
Epoch 7/10, Train Loss: 0.1845 (value: 0.0007, weighted value: 0.0372, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9380
Epoch 8/10, Train Loss: 0.1835 (value: 0.0007, weighted value: 0.0356, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1817 (value: 0.0007, weighted value: 0.0355, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9383
Epoch 10/10, Train Loss: 0.1797 (value: 0.0007, weighted value: 0.0339, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9383
..training done in 60.54 seconds
..evaluation done in 18.37 seconds
Old network+MCTS average reward: 0.7325, min: 0.1204, max: 1.6481, stdev: 0.2256
New network+MCTS average reward: 0.7323, min: 0.0741, max: 1.6481, stdev: 0.2287
Old bare network average reward: 0.6956, min: -0.0278, max: 1.6481, stdev: 0.2316
New bare network average reward: 0.6953, min: -0.0185, max: 1.6481, stdev: 0.2358
External policy "random" average reward: 0.2615, min: -0.3889, max: 0.8611, stdev: 0.2355
External policy "individual greedy" average reward: 0.5488, min: -0.0648, max: 1.3056, stdev: 0.2275
External policy "total greedy" average reward: 0.6585, min: 0.1111, max: 1.4907, stdev: 0.2227
New network won 66 and tied 168 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 444 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.99 seconds
Training examples lengths: [64578, 65080, 64746, 64884, 64907, 65009, 64759, 64533, 64874, 64697]
Total value: 481924.33
Training on 648067 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2042 (value: 0.0010, weighted value: 0.0477, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9367
Epoch 2/10, Train Loss: 0.1957 (value: 0.0009, weighted value: 0.0427, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9373
Epoch 3/10, Train Loss: 0.1917 (value: 0.0008, weighted value: 0.0417, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9376
Epoch 4/10, Train Loss: 0.1873 (value: 0.0008, weighted value: 0.0383, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9378
Epoch 5/10, Train Loss: 0.1865 (value: 0.0008, weighted value: 0.0383, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9378
Epoch 6/10, Train Loss: 0.1842 (value: 0.0007, weighted value: 0.0370, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9380
Epoch 7/10, Train Loss: 0.1832 (value: 0.0007, weighted value: 0.0359, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9381
Epoch 8/10, Train Loss: 0.1822 (value: 0.0007, weighted value: 0.0351, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9383
Epoch 9/10, Train Loss: 0.1811 (value: 0.0007, weighted value: 0.0346, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9383
Epoch 10/10, Train Loss: 0.1795 (value: 0.0007, weighted value: 0.0326, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9384
..training done in 59.71 seconds
..evaluation done in 18.74 seconds
Old network+MCTS average reward: 0.7228, min: 0.2037, max: 1.5000, stdev: 0.2162
New network+MCTS average reward: 0.7238, min: 0.2037, max: 1.4907, stdev: 0.2130
Old bare network average reward: 0.6905, min: 0.1574, max: 1.4074, stdev: 0.2177
New bare network average reward: 0.6882, min: 0.1204, max: 1.4907, stdev: 0.2171
External policy "random" average reward: 0.2755, min: -0.2963, max: 0.9537, stdev: 0.2025
External policy "individual greedy" average reward: 0.5258, min: -0.0556, max: 1.2315, stdev: 0.2157
External policy "total greedy" average reward: 0.6503, min: -0.0833, max: 1.3704, stdev: 0.2116
New network won 74 and tied 164 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network

Training iteration 445 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.33 seconds
Training examples lengths: [65080, 64746, 64884, 64907, 65009, 64759, 64533, 64874, 64697, 64584]
Total value: 481955.36
Training on 648073 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2040 (value: 0.0009, weighted value: 0.0472, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9368
Epoch 2/10, Train Loss: 0.1940 (value: 0.0008, weighted value: 0.0416, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9374
Epoch 3/10, Train Loss: 0.1899 (value: 0.0008, weighted value: 0.0404, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9377
Epoch 4/10, Train Loss: 0.1868 (value: 0.0008, weighted value: 0.0383, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9380
Epoch 5/10, Train Loss: 0.1846 (value: 0.0007, weighted value: 0.0368, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9380
Epoch 6/10, Train Loss: 0.1827 (value: 0.0007, weighted value: 0.0352, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9382
Epoch 7/10, Train Loss: 0.1832 (value: 0.0007, weighted value: 0.0362, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9383
Epoch 8/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0344, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9385
Epoch 9/10, Train Loss: 0.1796 (value: 0.0007, weighted value: 0.0332, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9386
Epoch 10/10, Train Loss: 0.1787 (value: 0.0006, weighted value: 0.0323, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9386
..training done in 59.66 seconds
..evaluation done in 18.46 seconds
Old network+MCTS average reward: 0.7196, min: 0.1204, max: 1.3981, stdev: 0.2344
New network+MCTS average reward: 0.7226, min: 0.1019, max: 1.3981, stdev: 0.2351
Old bare network average reward: 0.6827, min: 0.1019, max: 1.3889, stdev: 0.2366
New bare network average reward: 0.6852, min: 0.1019, max: 1.3611, stdev: 0.2340
External policy "random" average reward: 0.2514, min: -0.3611, max: 0.8704, stdev: 0.2176
External policy "individual greedy" average reward: 0.5328, min: -0.0093, max: 1.2130, stdev: 0.2251
External policy "total greedy" average reward: 0.6461, min: 0.1759, max: 1.3333, stdev: 0.2234
New network won 64 and tied 167 out of 300 games (49.17% wins where ties are half wins)
Reverting to the old network

Training iteration 446 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.66 seconds
Training examples lengths: [64746, 64884, 64907, 65009, 64759, 64533, 64874, 64697, 64584, 64801]
Total value: 481783.75
Training on 647794 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2244 (value: 0.0012, weighted value: 0.0601, policy: 0.1643, weighted policy: 0.1643), Train Mean Max: 0.9355
Epoch 2/10, Train Loss: 0.2094 (value: 0.0010, weighted value: 0.0520, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9365
Epoch 3/10, Train Loss: 0.2015 (value: 0.0010, weighted value: 0.0485, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9368
Epoch 4/10, Train Loss: 0.1967 (value: 0.0009, weighted value: 0.0456, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9371
Epoch 5/10, Train Loss: 0.1933 (value: 0.0009, weighted value: 0.0432, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9372
Epoch 6/10, Train Loss: 0.1911 (value: 0.0008, weighted value: 0.0415, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9374
Epoch 7/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0399, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9377
Epoch 8/10, Train Loss: 0.1869 (value: 0.0008, weighted value: 0.0392, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1859 (value: 0.0008, weighted value: 0.0377, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9378
Epoch 10/10, Train Loss: 0.1842 (value: 0.0007, weighted value: 0.0374, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9381
..training done in 60.27 seconds
..evaluation done in 19.80 seconds
Old network+MCTS average reward: 0.7642, min: 0.1574, max: 1.3889, stdev: 0.2201
New network+MCTS average reward: 0.7653, min: 0.1574, max: 1.4352, stdev: 0.2195
Old bare network average reward: 0.7309, min: 0.1574, max: 1.3889, stdev: 0.2245
New bare network average reward: 0.7322, min: 0.1574, max: 1.3889, stdev: 0.2282
External policy "random" average reward: 0.2698, min: -0.3333, max: 1.1296, stdev: 0.2312
External policy "individual greedy" average reward: 0.5588, min: -0.0370, max: 1.1944, stdev: 0.2157
External policy "total greedy" average reward: 0.6755, min: 0.1574, max: 1.2500, stdev: 0.2119
New network won 75 and tied 165 out of 300 games (52.50% wins where ties are half wins)
Keeping the new network

Training iteration 447 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.12 seconds
Training examples lengths: [64884, 64907, 65009, 64759, 64533, 64874, 64697, 64584, 64801, 64651]
Total value: 482135.73
Training on 647699 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2065 (value: 0.0010, weighted value: 0.0497, policy: 0.1569, weighted policy: 0.1569), Train Mean Max: 0.9365
Epoch 2/10, Train Loss: 0.1998 (value: 0.0009, weighted value: 0.0458, policy: 0.1539, weighted policy: 0.1539), Train Mean Max: 0.9370
Epoch 3/10, Train Loss: 0.1922 (value: 0.0008, weighted value: 0.0414, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9375
Epoch 4/10, Train Loss: 0.1913 (value: 0.0008, weighted value: 0.0424, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0398, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9377
Epoch 6/10, Train Loss: 0.1859 (value: 0.0008, weighted value: 0.0376, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9379
Epoch 7/10, Train Loss: 0.1848 (value: 0.0007, weighted value: 0.0373, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9381
Epoch 8/10, Train Loss: 0.1842 (value: 0.0007, weighted value: 0.0375, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1819 (value: 0.0007, weighted value: 0.0353, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1809 (value: 0.0007, weighted value: 0.0342, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9383
..training done in 82.14 seconds
..evaluation done in 22.63 seconds
Old network+MCTS average reward: 0.7422, min: 0.0185, max: 1.4444, stdev: 0.2483
New network+MCTS average reward: 0.7413, min: 0.0185, max: 1.4815, stdev: 0.2506
Old bare network average reward: 0.7042, min: -0.0648, max: 1.4444, stdev: 0.2567
New bare network average reward: 0.7054, min: -0.0648, max: 1.4815, stdev: 0.2536
External policy "random" average reward: 0.2517, min: -0.4537, max: 1.0741, stdev: 0.2429
External policy "individual greedy" average reward: 0.5368, min: -0.1481, max: 1.3426, stdev: 0.2376
External policy "total greedy" average reward: 0.6485, min: 0.0000, max: 1.4815, stdev: 0.2339
New network won 60 and tied 169 out of 300 games (48.17% wins where ties are half wins)
Reverting to the old network

Training iteration 448 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.83 seconds
Training examples lengths: [64907, 65009, 64759, 64533, 64874, 64697, 64584, 64801, 64651, 64919]
Total value: 481736.64
Training on 647734 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2282 (value: 0.0012, weighted value: 0.0618, policy: 0.1664, weighted policy: 0.1664), Train Mean Max: 0.9348
Epoch 2/10, Train Loss: 0.2146 (value: 0.0011, weighted value: 0.0556, policy: 0.1590, weighted policy: 0.1590), Train Mean Max: 0.9359
Epoch 3/10, Train Loss: 0.2054 (value: 0.0010, weighted value: 0.0513, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9363
Epoch 4/10, Train Loss: 0.1988 (value: 0.0009, weighted value: 0.0469, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9365
Epoch 5/10, Train Loss: 0.1966 (value: 0.0009, weighted value: 0.0462, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9368
Epoch 6/10, Train Loss: 0.1944 (value: 0.0009, weighted value: 0.0435, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9368
Epoch 7/10, Train Loss: 0.1927 (value: 0.0009, weighted value: 0.0426, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9370
Epoch 8/10, Train Loss: 0.1885 (value: 0.0008, weighted value: 0.0398, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1879 (value: 0.0008, weighted value: 0.0395, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9374
Epoch 10/10, Train Loss: 0.1874 (value: 0.0008, weighted value: 0.0388, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9377
..training done in 64.16 seconds
..evaluation done in 19.83 seconds
Old network+MCTS average reward: 0.7254, min: 0.1204, max: 1.4167, stdev: 0.2378
New network+MCTS average reward: 0.7256, min: 0.1204, max: 1.3611, stdev: 0.2394
Old bare network average reward: 0.6896, min: 0.0741, max: 1.3611, stdev: 0.2407
New bare network average reward: 0.6885, min: 0.0463, max: 1.3241, stdev: 0.2398
External policy "random" average reward: 0.2592, min: -0.3148, max: 1.0093, stdev: 0.2220
External policy "individual greedy" average reward: 0.5322, min: 0.0093, max: 1.2407, stdev: 0.2275
External policy "total greedy" average reward: 0.6422, min: 0.0370, max: 1.2500, stdev: 0.2269
New network won 68 and tied 173 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 449 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.10 seconds
Training examples lengths: [65009, 64759, 64533, 64874, 64697, 64584, 64801, 64651, 64919, 64950]
Total value: 481680.87
Training on 647777 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2094 (value: 0.0010, weighted value: 0.0524, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9362
Epoch 2/10, Train Loss: 0.1990 (value: 0.0009, weighted value: 0.0455, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9368
Epoch 3/10, Train Loss: 0.1951 (value: 0.0009, weighted value: 0.0437, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9370
Epoch 4/10, Train Loss: 0.1921 (value: 0.0008, weighted value: 0.0419, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9372
Epoch 5/10, Train Loss: 0.1902 (value: 0.0008, weighted value: 0.0410, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9374
Epoch 6/10, Train Loss: 0.1883 (value: 0.0008, weighted value: 0.0392, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9376
Epoch 7/10, Train Loss: 0.1863 (value: 0.0008, weighted value: 0.0382, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9377
Epoch 8/10, Train Loss: 0.1845 (value: 0.0007, weighted value: 0.0373, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9380
Epoch 9/10, Train Loss: 0.1834 (value: 0.0007, weighted value: 0.0356, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1848 (value: 0.0007, weighted value: 0.0365, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9380
..training done in 67.51 seconds
..evaluation done in 18.90 seconds
Old network+MCTS average reward: 0.7176, min: 0.1296, max: 1.2778, stdev: 0.2342
New network+MCTS average reward: 0.7143, min: 0.1759, max: 1.2500, stdev: 0.2339
Old bare network average reward: 0.6815, min: 0.1389, max: 1.2315, stdev: 0.2357
New bare network average reward: 0.6795, min: 0.1296, max: 1.2315, stdev: 0.2340
External policy "random" average reward: 0.2450, min: -0.3889, max: 1.0093, stdev: 0.2084
External policy "individual greedy" average reward: 0.5185, min: -0.1019, max: 1.0741, stdev: 0.2214
External policy "total greedy" average reward: 0.6331, min: 0.1019, max: 1.2222, stdev: 0.2193
New network won 64 and tied 160 out of 300 games (48.00% wins where ties are half wins)
Reverting to the old network

Training iteration 450 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.51 seconds
Training examples lengths: [64759, 64533, 64874, 64697, 64584, 64801, 64651, 64919, 64950, 64663]
Total value: 480935.43
Training on 647431 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2290 (value: 0.0013, weighted value: 0.0638, policy: 0.1652, weighted policy: 0.1652), Train Mean Max: 0.9345
Epoch 2/10, Train Loss: 0.2164 (value: 0.0011, weighted value: 0.0562, policy: 0.1602, weighted policy: 0.1602), Train Mean Max: 0.9353
Epoch 3/10, Train Loss: 0.2061 (value: 0.0010, weighted value: 0.0511, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9361
Epoch 4/10, Train Loss: 0.2023 (value: 0.0010, weighted value: 0.0493, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9362
Epoch 5/10, Train Loss: 0.1980 (value: 0.0009, weighted value: 0.0462, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9365
Epoch 6/10, Train Loss: 0.1954 (value: 0.0009, weighted value: 0.0446, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9367
Epoch 7/10, Train Loss: 0.1931 (value: 0.0009, weighted value: 0.0426, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9366
Epoch 8/10, Train Loss: 0.1922 (value: 0.0008, weighted value: 0.0423, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9369
Epoch 9/10, Train Loss: 0.1886 (value: 0.0008, weighted value: 0.0397, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9373
Epoch 10/10, Train Loss: 0.1890 (value: 0.0008, weighted value: 0.0396, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9372
..training done in 59.00 seconds
..evaluation done in 18.94 seconds
Old network+MCTS average reward: 0.7284, min: 0.1667, max: 1.4722, stdev: 0.2349
New network+MCTS average reward: 0.7290, min: 0.0833, max: 1.3889, stdev: 0.2353
Old bare network average reward: 0.6897, min: 0.1296, max: 1.4259, stdev: 0.2341
New bare network average reward: 0.6928, min: 0.0833, max: 1.4259, stdev: 0.2338
External policy "random" average reward: 0.2638, min: -0.3889, max: 0.9259, stdev: 0.2324
External policy "individual greedy" average reward: 0.5501, min: -0.0833, max: 1.2963, stdev: 0.2259
External policy "total greedy" average reward: 0.6594, min: 0.0556, max: 1.3426, stdev: 0.2273
New network won 71 and tied 150 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 451 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.49 seconds
Training examples lengths: [64533, 64874, 64697, 64584, 64801, 64651, 64919, 64950, 64663, 64818]
Total value: 480791.79
Training on 647490 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2494 (value: 0.0015, weighted value: 0.0772, policy: 0.1722, weighted policy: 0.1722), Train Mean Max: 0.9332
Epoch 2/10, Train Loss: 0.2284 (value: 0.0013, weighted value: 0.0652, policy: 0.1632, weighted policy: 0.1632), Train Mean Max: 0.9344
Epoch 3/10, Train Loss: 0.2181 (value: 0.0012, weighted value: 0.0597, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9349
Epoch 4/10, Train Loss: 0.2109 (value: 0.0011, weighted value: 0.0557, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9352
Epoch 5/10, Train Loss: 0.2070 (value: 0.0011, weighted value: 0.0526, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9353
Epoch 6/10, Train Loss: 0.2019 (value: 0.0010, weighted value: 0.0490, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9356
Epoch 7/10, Train Loss: 0.2000 (value: 0.0010, weighted value: 0.0480, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9359
Epoch 8/10, Train Loss: 0.1965 (value: 0.0009, weighted value: 0.0451, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9362
Epoch 9/10, Train Loss: 0.1953 (value: 0.0009, weighted value: 0.0447, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9364
Epoch 10/10, Train Loss: 0.1933 (value: 0.0009, weighted value: 0.0426, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9366
..training done in 69.89 seconds
..evaluation done in 19.48 seconds
Old network+MCTS average reward: 0.7173, min: 0.1111, max: 1.5093, stdev: 0.2398
New network+MCTS average reward: 0.7180, min: 0.1111, max: 1.5093, stdev: 0.2411
Old bare network average reward: 0.6789, min: 0.1111, max: 1.4537, stdev: 0.2420
New bare network average reward: 0.6785, min: 0.1111, max: 1.4537, stdev: 0.2442
External policy "random" average reward: 0.2399, min: -0.4352, max: 0.9815, stdev: 0.2242
External policy "individual greedy" average reward: 0.5164, min: 0.0185, max: 1.2685, stdev: 0.2241
External policy "total greedy" average reward: 0.6337, min: 0.0926, max: 1.4537, stdev: 0.2310
New network won 65 and tied 172 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 452 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.71 seconds
Training examples lengths: [64874, 64697, 64584, 64801, 64651, 64919, 64950, 64663, 64818, 65064]
Total value: 482085.97
Training on 648021 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2134 (value: 0.0011, weighted value: 0.0545, policy: 0.1589, weighted policy: 0.1589), Train Mean Max: 0.9351
Epoch 2/10, Train Loss: 0.2046 (value: 0.0010, weighted value: 0.0499, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9357
Epoch 3/10, Train Loss: 0.1986 (value: 0.0009, weighted value: 0.0463, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9361
Epoch 4/10, Train Loss: 0.1973 (value: 0.0009, weighted value: 0.0462, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9363
Epoch 5/10, Train Loss: 0.1938 (value: 0.0009, weighted value: 0.0435, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9365
Epoch 6/10, Train Loss: 0.1917 (value: 0.0008, weighted value: 0.0419, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9367
Epoch 7/10, Train Loss: 0.1904 (value: 0.0008, weighted value: 0.0406, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9369
Epoch 8/10, Train Loss: 0.1885 (value: 0.0008, weighted value: 0.0392, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9371
Epoch 9/10, Train Loss: 0.1878 (value: 0.0008, weighted value: 0.0385, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9372
Epoch 10/10, Train Loss: 0.1861 (value: 0.0007, weighted value: 0.0370, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9372
..training done in 60.24 seconds
..evaluation done in 18.45 seconds
Old network+MCTS average reward: 0.7278, min: 0.1111, max: 1.5370, stdev: 0.2491
New network+MCTS average reward: 0.7264, min: 0.1389, max: 1.5370, stdev: 0.2437
Old bare network average reward: 0.6955, min: 0.0648, max: 1.5370, stdev: 0.2522
New bare network average reward: 0.6918, min: 0.1019, max: 1.5370, stdev: 0.2511
External policy "random" average reward: 0.2777, min: -0.3148, max: 1.3704, stdev: 0.2347
External policy "individual greedy" average reward: 0.5377, min: -0.0185, max: 1.4444, stdev: 0.2420
External policy "total greedy" average reward: 0.6548, min: 0.1481, max: 1.5463, stdev: 0.2346
New network won 63 and tied 174 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 453 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.74 seconds
Training examples lengths: [64697, 64584, 64801, 64651, 64919, 64950, 64663, 64818, 65064, 64602]
Total value: 481669.06
Training on 647749 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2069 (value: 0.0010, weighted value: 0.0498, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9358
Epoch 2/10, Train Loss: 0.1993 (value: 0.0009, weighted value: 0.0448, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9365
Epoch 3/10, Train Loss: 0.1934 (value: 0.0008, weighted value: 0.0423, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9367
Epoch 4/10, Train Loss: 0.1913 (value: 0.0008, weighted value: 0.0416, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9369
Epoch 5/10, Train Loss: 0.1895 (value: 0.0008, weighted value: 0.0397, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9370
Epoch 6/10, Train Loss: 0.1874 (value: 0.0008, weighted value: 0.0382, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9372
Epoch 7/10, Train Loss: 0.1861 (value: 0.0007, weighted value: 0.0375, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1847 (value: 0.0007, weighted value: 0.0360, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1846 (value: 0.0007, weighted value: 0.0361, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9376
Epoch 10/10, Train Loss: 0.1816 (value: 0.0007, weighted value: 0.0341, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9379
..training done in 59.83 seconds
..evaluation done in 18.80 seconds
Old network+MCTS average reward: 0.7373, min: 0.2685, max: 1.3519, stdev: 0.2257
New network+MCTS average reward: 0.7352, min: 0.2685, max: 1.3519, stdev: 0.2254
Old bare network average reward: 0.7056, min: 0.2500, max: 1.3519, stdev: 0.2311
New bare network average reward: 0.7013, min: 0.2222, max: 1.3519, stdev: 0.2328
External policy "random" average reward: 0.2589, min: -0.4167, max: 0.9167, stdev: 0.2393
External policy "individual greedy" average reward: 0.5424, min: -0.0278, max: 1.2037, stdev: 0.2206
External policy "total greedy" average reward: 0.6538, min: 0.1574, max: 1.3056, stdev: 0.2228
New network won 57 and tied 173 out of 300 games (47.83% wins where ties are half wins)
Reverting to the old network

Training iteration 454 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.87 seconds
Training examples lengths: [64584, 64801, 64651, 64919, 64950, 64663, 64818, 65064, 64602, 64754]
Total value: 482216.54
Training on 647806 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2263 (value: 0.0012, weighted value: 0.0624, policy: 0.1640, weighted policy: 0.1640), Train Mean Max: 0.9345
Epoch 2/10, Train Loss: 0.2139 (value: 0.0011, weighted value: 0.0549, policy: 0.1590, weighted policy: 0.1590), Train Mean Max: 0.9354
Epoch 3/10, Train Loss: 0.2059 (value: 0.0010, weighted value: 0.0514, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9359
Epoch 4/10, Train Loss: 0.1992 (value: 0.0009, weighted value: 0.0474, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9361
Epoch 5/10, Train Loss: 0.1968 (value: 0.0009, weighted value: 0.0454, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9363
Epoch 6/10, Train Loss: 0.1937 (value: 0.0009, weighted value: 0.0434, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9365
Epoch 7/10, Train Loss: 0.1924 (value: 0.0009, weighted value: 0.0426, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9369
Epoch 8/10, Train Loss: 0.1912 (value: 0.0008, weighted value: 0.0413, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9369
Epoch 9/10, Train Loss: 0.1881 (value: 0.0008, weighted value: 0.0393, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9371
Epoch 10/10, Train Loss: 0.1863 (value: 0.0008, weighted value: 0.0378, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9373
..training done in 60.30 seconds
..evaluation done in 18.61 seconds
Old network+MCTS average reward: 0.7210, min: 0.1759, max: 1.2593, stdev: 0.2228
New network+MCTS average reward: 0.7204, min: 0.1111, max: 1.3056, stdev: 0.2259
Old bare network average reward: 0.6893, min: -0.0278, max: 1.2593, stdev: 0.2300
New bare network average reward: 0.6879, min: -0.0278, max: 1.3148, stdev: 0.2348
External policy "random" average reward: 0.2415, min: -0.3519, max: 0.8148, stdev: 0.2263
External policy "individual greedy" average reward: 0.5211, min: -0.1759, max: 1.0648, stdev: 0.2176
External policy "total greedy" average reward: 0.6369, min: 0.0278, max: 1.3241, stdev: 0.2198
New network won 71 and tied 151 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 455 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.02 seconds
Training examples lengths: [64801, 64651, 64919, 64950, 64663, 64818, 65064, 64602, 64754, 64721]
Total value: 482561.96
Training on 647943 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2461 (value: 0.0015, weighted value: 0.0749, policy: 0.1712, weighted policy: 0.1712), Train Mean Max: 0.9333
Epoch 2/10, Train Loss: 0.2249 (value: 0.0012, weighted value: 0.0623, policy: 0.1626, weighted policy: 0.1626), Train Mean Max: 0.9345
Epoch 3/10, Train Loss: 0.2154 (value: 0.0012, weighted value: 0.0579, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9350
Epoch 4/10, Train Loss: 0.2093 (value: 0.0011, weighted value: 0.0548, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9353
Epoch 5/10, Train Loss: 0.2035 (value: 0.0010, weighted value: 0.0507, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9356
Epoch 6/10, Train Loss: 0.1999 (value: 0.0010, weighted value: 0.0477, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9359
Epoch 7/10, Train Loss: 0.1980 (value: 0.0009, weighted value: 0.0469, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9360
Epoch 8/10, Train Loss: 0.1952 (value: 0.0009, weighted value: 0.0445, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9362
Epoch 9/10, Train Loss: 0.1952 (value: 0.0009, weighted value: 0.0447, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9364
Epoch 10/10, Train Loss: 0.1901 (value: 0.0008, weighted value: 0.0410, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9367
..training done in 59.34 seconds
..evaluation done in 19.17 seconds
Old network+MCTS average reward: 0.7243, min: 0.1296, max: 1.6389, stdev: 0.2195
New network+MCTS average reward: 0.7239, min: 0.1852, max: 1.6389, stdev: 0.2209
Old bare network average reward: 0.6908, min: 0.1759, max: 1.5648, stdev: 0.2189
New bare network average reward: 0.6898, min: 0.0741, max: 1.5185, stdev: 0.2213
External policy "random" average reward: 0.2618, min: -0.3056, max: 0.8981, stdev: 0.2199
External policy "individual greedy" average reward: 0.5340, min: 0.0463, max: 1.1481, stdev: 0.2164
External policy "total greedy" average reward: 0.6457, min: 0.2037, max: 1.3796, stdev: 0.2049
New network won 61 and tied 176 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 456 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.46 seconds
Training examples lengths: [64651, 64919, 64950, 64663, 64818, 65064, 64602, 64754, 64721, 64647]
Total value: 482862.34
Training on 647789 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2647 (value: 0.0017, weighted value: 0.0873, policy: 0.1774, weighted policy: 0.1774), Train Mean Max: 0.9321
Epoch 2/10, Train Loss: 0.2399 (value: 0.0014, weighted value: 0.0723, policy: 0.1676, weighted policy: 0.1676), Train Mean Max: 0.9333
Epoch 3/10, Train Loss: 0.2244 (value: 0.0013, weighted value: 0.0644, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9341
Epoch 4/10, Train Loss: 0.2175 (value: 0.0012, weighted value: 0.0607, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9344
Epoch 5/10, Train Loss: 0.2114 (value: 0.0011, weighted value: 0.0564, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9348
Epoch 6/10, Train Loss: 0.2073 (value: 0.0011, weighted value: 0.0533, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9349
Epoch 7/10, Train Loss: 0.2034 (value: 0.0010, weighted value: 0.0508, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9353
Epoch 8/10, Train Loss: 0.1998 (value: 0.0010, weighted value: 0.0481, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9355
Epoch 9/10, Train Loss: 0.2001 (value: 0.0010, weighted value: 0.0486, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9357
Epoch 10/10, Train Loss: 0.1941 (value: 0.0009, weighted value: 0.0436, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9360
..training done in 60.31 seconds
..evaluation done in 18.65 seconds
Old network+MCTS average reward: 0.7255, min: 0.1296, max: 1.4167, stdev: 0.2316
New network+MCTS average reward: 0.7256, min: 0.1296, max: 1.3519, stdev: 0.2331
Old bare network average reward: 0.6876, min: 0.1296, max: 1.4167, stdev: 0.2372
New bare network average reward: 0.6848, min: 0.1019, max: 1.2963, stdev: 0.2391
External policy "random" average reward: 0.2489, min: -0.2778, max: 0.9722, stdev: 0.2266
External policy "individual greedy" average reward: 0.5385, min: -0.1111, max: 1.1759, stdev: 0.2359
External policy "total greedy" average reward: 0.6535, min: 0.1111, max: 1.2593, stdev: 0.2265
New network won 74 and tied 177 out of 300 games (54.17% wins where ties are half wins)
Keeping the new network

Training iteration 457 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 105.99 seconds
Training examples lengths: [64919, 64950, 64663, 64818, 65064, 64602, 64754, 64721, 64647, 64521]
Total value: 482759.81
Training on 647659 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2146 (value: 0.0011, weighted value: 0.0558, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9345
Epoch 2/10, Train Loss: 0.2092 (value: 0.0011, weighted value: 0.0531, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9352
Epoch 3/10, Train Loss: 0.2007 (value: 0.0009, weighted value: 0.0474, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9356
Epoch 4/10, Train Loss: 0.1986 (value: 0.0009, weighted value: 0.0464, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9358
Epoch 5/10, Train Loss: 0.1965 (value: 0.0009, weighted value: 0.0449, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9360
Epoch 6/10, Train Loss: 0.1922 (value: 0.0008, weighted value: 0.0421, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9363
Epoch 7/10, Train Loss: 0.1914 (value: 0.0008, weighted value: 0.0415, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9364
Epoch 8/10, Train Loss: 0.1916 (value: 0.0008, weighted value: 0.0418, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9366
Epoch 9/10, Train Loss: 0.1879 (value: 0.0008, weighted value: 0.0385, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9368
Epoch 10/10, Train Loss: 0.1868 (value: 0.0008, weighted value: 0.0382, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9371
..training done in 75.48 seconds
..evaluation done in 19.28 seconds
Old network+MCTS average reward: 0.7548, min: 0.2222, max: 1.7407, stdev: 0.2468
New network+MCTS average reward: 0.7565, min: 0.1944, max: 1.6944, stdev: 0.2445
Old bare network average reward: 0.7234, min: 0.1852, max: 1.6944, stdev: 0.2466
New bare network average reward: 0.7206, min: 0.1944, max: 1.6667, stdev: 0.2489
External policy "random" average reward: 0.2794, min: -0.2500, max: 1.0833, stdev: 0.2313
External policy "individual greedy" average reward: 0.5664, min: -0.0185, max: 1.5463, stdev: 0.2405
External policy "total greedy" average reward: 0.6804, min: 0.1574, max: 1.5741, stdev: 0.2362
New network won 74 and tied 168 out of 300 games (52.67% wins where ties are half wins)
Keeping the new network

Training iteration 458 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.35 seconds
Training examples lengths: [64950, 64663, 64818, 65064, 64602, 64754, 64721, 64647, 64521, 64704]
Total value: 482272.42
Training on 647444 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2091 (value: 0.0010, weighted value: 0.0505, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9355
Epoch 2/10, Train Loss: 0.2012 (value: 0.0009, weighted value: 0.0469, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9361
Epoch 3/10, Train Loss: 0.1946 (value: 0.0009, weighted value: 0.0426, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9365
Epoch 4/10, Train Loss: 0.1950 (value: 0.0009, weighted value: 0.0428, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9364
Epoch 5/10, Train Loss: 0.1901 (value: 0.0008, weighted value: 0.0405, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9368
Epoch 6/10, Train Loss: 0.1887 (value: 0.0008, weighted value: 0.0391, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9370
Epoch 7/10, Train Loss: 0.1883 (value: 0.0008, weighted value: 0.0391, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9372
Epoch 8/10, Train Loss: 0.1855 (value: 0.0007, weighted value: 0.0365, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9373
Epoch 9/10, Train Loss: 0.1854 (value: 0.0007, weighted value: 0.0365, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9374
Epoch 10/10, Train Loss: 0.1837 (value: 0.0007, weighted value: 0.0351, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9374
..training done in 65.70 seconds
..evaluation done in 18.31 seconds
Old network+MCTS average reward: 0.7321, min: 0.1667, max: 1.4630, stdev: 0.2329
New network+MCTS average reward: 0.7326, min: 0.1667, max: 1.4537, stdev: 0.2346
Old bare network average reward: 0.6935, min: 0.0741, max: 1.4630, stdev: 0.2382
New bare network average reward: 0.6933, min: 0.0741, max: 1.4444, stdev: 0.2402
External policy "random" average reward: 0.2419, min: -0.4815, max: 0.8796, stdev: 0.2127
External policy "individual greedy" average reward: 0.5390, min: 0.0093, max: 1.2963, stdev: 0.2210
External policy "total greedy" average reward: 0.6507, min: 0.0926, max: 1.2315, stdev: 0.2155
New network won 70 and tied 163 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 459 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.40 seconds
Training examples lengths: [64663, 64818, 65064, 64602, 64754, 64721, 64647, 64521, 64704, 64859]
Total value: 482794.21
Training on 647353 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2050 (value: 0.0010, weighted value: 0.0478, policy: 0.1572, weighted policy: 0.1572), Train Mean Max: 0.9363
Epoch 2/10, Train Loss: 0.1985 (value: 0.0009, weighted value: 0.0439, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9369
Epoch 3/10, Train Loss: 0.1932 (value: 0.0008, weighted value: 0.0418, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9371
Epoch 4/10, Train Loss: 0.1890 (value: 0.0008, weighted value: 0.0398, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1869 (value: 0.0008, weighted value: 0.0383, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1859 (value: 0.0007, weighted value: 0.0371, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9376
Epoch 7/10, Train Loss: 0.1839 (value: 0.0007, weighted value: 0.0360, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1839 (value: 0.0007, weighted value: 0.0360, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9380
Epoch 9/10, Train Loss: 0.1814 (value: 0.0007, weighted value: 0.0341, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9381
Epoch 10/10, Train Loss: 0.1809 (value: 0.0007, weighted value: 0.0335, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9383
..training done in 65.26 seconds
..evaluation done in 18.26 seconds
Old network+MCTS average reward: 0.7295, min: 0.0000, max: 1.6111, stdev: 0.2344
New network+MCTS average reward: 0.7267, min: 0.0000, max: 1.5926, stdev: 0.2359
Old bare network average reward: 0.6983, min: 0.0000, max: 1.5926, stdev: 0.2373
New bare network average reward: 0.6985, min: 0.0000, max: 1.5926, stdev: 0.2337
External policy "random" average reward: 0.2598, min: -0.3981, max: 0.9815, stdev: 0.2139
External policy "individual greedy" average reward: 0.5329, min: -0.0926, max: 1.2685, stdev: 0.2201
External policy "total greedy" average reward: 0.6474, min: 0.0648, max: 1.3611, stdev: 0.2176
New network won 67 and tied 157 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 460 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.98 seconds
Training examples lengths: [64818, 65064, 64602, 64754, 64721, 64647, 64521, 64704, 64859, 65104]
Total value: 483551.54
Training on 647794 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2256 (value: 0.0012, weighted value: 0.0604, policy: 0.1653, weighted policy: 0.1653), Train Mean Max: 0.9350
Epoch 2/10, Train Loss: 0.2113 (value: 0.0011, weighted value: 0.0535, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9359
Epoch 3/10, Train Loss: 0.2022 (value: 0.0010, weighted value: 0.0490, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9365
Epoch 4/10, Train Loss: 0.1978 (value: 0.0009, weighted value: 0.0466, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9367
Epoch 5/10, Train Loss: 0.1947 (value: 0.0009, weighted value: 0.0443, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1917 (value: 0.0008, weighted value: 0.0418, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9370
Epoch 7/10, Train Loss: 0.1901 (value: 0.0008, weighted value: 0.0412, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1883 (value: 0.0008, weighted value: 0.0400, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1865 (value: 0.0008, weighted value: 0.0386, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9376
Epoch 10/10, Train Loss: 0.1846 (value: 0.0007, weighted value: 0.0370, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9377
..training done in 66.39 seconds
..evaluation done in 18.63 seconds
Old network+MCTS average reward: 0.7050, min: 0.1204, max: 1.4352, stdev: 0.2319
New network+MCTS average reward: 0.7041, min: 0.1389, max: 1.4537, stdev: 0.2312
Old bare network average reward: 0.6690, min: 0.1204, max: 1.4537, stdev: 0.2363
New bare network average reward: 0.6712, min: 0.0185, max: 1.4537, stdev: 0.2365
External policy "random" average reward: 0.2303, min: -0.2500, max: 0.9907, stdev: 0.2233
External policy "individual greedy" average reward: 0.5202, min: 0.0093, max: 1.4259, stdev: 0.2294
External policy "total greedy" average reward: 0.6311, min: 0.1111, max: 1.3611, stdev: 0.2217
New network won 65 and tied 165 out of 300 games (49.17% wins where ties are half wins)
Reverting to the old network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_460

Training iteration 461 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.19 seconds
Training examples lengths: [65064, 64602, 64754, 64721, 64647, 64521, 64704, 64859, 65104, 64693]
Total value: 483782.88
Training on 647669 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2462 (value: 0.0015, weighted value: 0.0741, policy: 0.1721, weighted policy: 0.1721), Train Mean Max: 0.9337
Epoch 2/10, Train Loss: 0.2265 (value: 0.0013, weighted value: 0.0633, policy: 0.1631, weighted policy: 0.1631), Train Mean Max: 0.9349
Epoch 3/10, Train Loss: 0.2140 (value: 0.0011, weighted value: 0.0573, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9355
Epoch 4/10, Train Loss: 0.2067 (value: 0.0011, weighted value: 0.0528, policy: 0.1539, weighted policy: 0.1539), Train Mean Max: 0.9358
Epoch 5/10, Train Loss: 0.2017 (value: 0.0010, weighted value: 0.0498, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9362
Epoch 6/10, Train Loss: 0.1992 (value: 0.0010, weighted value: 0.0482, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9363
Epoch 7/10, Train Loss: 0.1962 (value: 0.0009, weighted value: 0.0459, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9365
Epoch 8/10, Train Loss: 0.1929 (value: 0.0009, weighted value: 0.0437, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9367
Epoch 9/10, Train Loss: 0.1919 (value: 0.0009, weighted value: 0.0429, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9369
Epoch 10/10, Train Loss: 0.1909 (value: 0.0008, weighted value: 0.0418, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9370
..training done in 62.93 seconds
..evaluation done in 18.94 seconds
Old network+MCTS average reward: 0.7384, min: 0.1019, max: 1.2870, stdev: 0.2248
New network+MCTS average reward: 0.7397, min: 0.1019, max: 1.2778, stdev: 0.2248
Old bare network average reward: 0.7044, min: 0.0093, max: 1.2870, stdev: 0.2292
New bare network average reward: 0.7102, min: 0.0093, max: 1.2685, stdev: 0.2254
External policy "random" average reward: 0.2720, min: -0.5093, max: 0.9630, stdev: 0.2374
External policy "individual greedy" average reward: 0.5431, min: -0.0741, max: 1.1574, stdev: 0.2248
External policy "total greedy" average reward: 0.6625, min: 0.0741, max: 1.2963, stdev: 0.2218
New network won 71 and tied 161 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 462 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.78 seconds
Training examples lengths: [64602, 64754, 64721, 64647, 64521, 64704, 64859, 65104, 64693, 64391]
Total value: 482671.10
Training on 646996 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2096 (value: 0.0011, weighted value: 0.0528, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9358
Epoch 2/10, Train Loss: 0.2016 (value: 0.0010, weighted value: 0.0478, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9364
Epoch 3/10, Train Loss: 0.1965 (value: 0.0009, weighted value: 0.0453, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9368
Epoch 4/10, Train Loss: 0.1939 (value: 0.0009, weighted value: 0.0447, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9371
Epoch 5/10, Train Loss: 0.1907 (value: 0.0008, weighted value: 0.0420, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9372
Epoch 6/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0403, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9373
Epoch 7/10, Train Loss: 0.1876 (value: 0.0008, weighted value: 0.0397, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9375
Epoch 8/10, Train Loss: 0.1855 (value: 0.0008, weighted value: 0.0379, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9377
Epoch 9/10, Train Loss: 0.1846 (value: 0.0008, weighted value: 0.0377, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1828 (value: 0.0007, weighted value: 0.0359, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9379
..training done in 67.34 seconds
..evaluation done in 19.67 seconds
Old network+MCTS average reward: 0.7662, min: -0.1574, max: 1.7407, stdev: 0.2497
New network+MCTS average reward: 0.7692, min: -0.0278, max: 1.7500, stdev: 0.2478
Old bare network average reward: 0.7318, min: -0.0741, max: 1.7407, stdev: 0.2470
New bare network average reward: 0.7293, min: -0.0278, max: 1.6111, stdev: 0.2465
External policy "random" average reward: 0.2905, min: -0.3519, max: 1.0926, stdev: 0.2429
External policy "individual greedy" average reward: 0.5723, min: -0.0741, max: 1.3426, stdev: 0.2451
External policy "total greedy" average reward: 0.6951, min: 0.0926, max: 1.6296, stdev: 0.2387
New network won 78 and tied 156 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network

Training iteration 463 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.90 seconds
Training examples lengths: [64754, 64721, 64647, 64521, 64704, 64859, 65104, 64693, 64391, 64622]
Total value: 482653.57
Training on 647016 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2057 (value: 0.0010, weighted value: 0.0493, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9367
Epoch 2/10, Train Loss: 0.1964 (value: 0.0009, weighted value: 0.0437, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9372
Epoch 3/10, Train Loss: 0.1928 (value: 0.0009, weighted value: 0.0427, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9375
Epoch 4/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0398, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9377
Epoch 5/10, Train Loss: 0.1869 (value: 0.0008, weighted value: 0.0387, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9380
Epoch 6/10, Train Loss: 0.1858 (value: 0.0008, weighted value: 0.0379, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9380
Epoch 7/10, Train Loss: 0.1840 (value: 0.0007, weighted value: 0.0366, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9382
Epoch 8/10, Train Loss: 0.1823 (value: 0.0007, weighted value: 0.0356, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9383
Epoch 9/10, Train Loss: 0.1824 (value: 0.0007, weighted value: 0.0357, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9385
Epoch 10/10, Train Loss: 0.1800 (value: 0.0007, weighted value: 0.0336, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9386
..training done in 67.61 seconds
..evaluation done in 20.73 seconds
Old network+MCTS average reward: 0.7321, min: 0.1667, max: 1.3796, stdev: 0.2243
New network+MCTS average reward: 0.7325, min: 0.1944, max: 1.3704, stdev: 0.2230
Old bare network average reward: 0.6966, min: 0.0926, max: 1.3796, stdev: 0.2263
New bare network average reward: 0.6992, min: 0.0926, max: 1.3796, stdev: 0.2247
External policy "random" average reward: 0.2617, min: -0.3519, max: 0.8426, stdev: 0.2240
External policy "individual greedy" average reward: 0.5352, min: 0.0000, max: 1.0833, stdev: 0.2212
External policy "total greedy" average reward: 0.6551, min: 0.1852, max: 1.2037, stdev: 0.2160
New network won 60 and tied 166 out of 300 games (47.67% wins where ties are half wins)
Reverting to the old network

Training iteration 464 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.97 seconds
Training examples lengths: [64721, 64647, 64521, 64704, 64859, 65104, 64693, 64391, 64622, 64507]
Total value: 481923.89
Training on 646769 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2265 (value: 0.0012, weighted value: 0.0613, policy: 0.1652, weighted policy: 0.1652), Train Mean Max: 0.9352
Epoch 2/10, Train Loss: 0.2115 (value: 0.0011, weighted value: 0.0527, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9362
Epoch 3/10, Train Loss: 0.2037 (value: 0.0010, weighted value: 0.0496, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9365
Epoch 4/10, Train Loss: 0.1992 (value: 0.0010, weighted value: 0.0476, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9368
Epoch 5/10, Train Loss: 0.1942 (value: 0.0009, weighted value: 0.0438, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1922 (value: 0.0009, weighted value: 0.0431, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1906 (value: 0.0008, weighted value: 0.0411, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9372
Epoch 8/10, Train Loss: 0.1887 (value: 0.0008, weighted value: 0.0401, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1868 (value: 0.0008, weighted value: 0.0392, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1852 (value: 0.0007, weighted value: 0.0374, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9378
..training done in 67.13 seconds
..evaluation done in 19.31 seconds
Old network+MCTS average reward: 0.7559, min: 0.1574, max: 1.3796, stdev: 0.2179
New network+MCTS average reward: 0.7523, min: 0.1574, max: 1.3796, stdev: 0.2180
Old bare network average reward: 0.7189, min: 0.0926, max: 1.3148, stdev: 0.2188
New bare network average reward: 0.7123, min: 0.0926, max: 1.3148, stdev: 0.2164
External policy "random" average reward: 0.2648, min: -0.3611, max: 0.9167, stdev: 0.2028
External policy "individual greedy" average reward: 0.5526, min: 0.0093, max: 1.2685, stdev: 0.2208
External policy "total greedy" average reward: 0.6657, min: 0.1111, max: 1.2778, stdev: 0.2144
New network won 61 and tied 159 out of 300 games (46.83% wins where ties are half wins)
Reverting to the old network

Training iteration 465 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 60.07 seconds
Training examples lengths: [64647, 64521, 64704, 64859, 65104, 64693, 64391, 64622, 64507, 64766]
Total value: 481756.17
Training on 646814 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2459 (value: 0.0015, weighted value: 0.0731, policy: 0.1728, weighted policy: 0.1728), Train Mean Max: 0.9338
Epoch 2/10, Train Loss: 0.2253 (value: 0.0012, weighted value: 0.0613, policy: 0.1640, weighted policy: 0.1640), Train Mean Max: 0.9348
Epoch 3/10, Train Loss: 0.2144 (value: 0.0011, weighted value: 0.0571, policy: 0.1573, weighted policy: 0.1573), Train Mean Max: 0.9354
Epoch 4/10, Train Loss: 0.2087 (value: 0.0011, weighted value: 0.0537, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9357
Epoch 5/10, Train Loss: 0.2023 (value: 0.0010, weighted value: 0.0495, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9359
Epoch 6/10, Train Loss: 0.2004 (value: 0.0010, weighted value: 0.0489, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9361
Epoch 7/10, Train Loss: 0.1962 (value: 0.0009, weighted value: 0.0453, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9363
Epoch 8/10, Train Loss: 0.1946 (value: 0.0009, weighted value: 0.0439, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9365
Epoch 9/10, Train Loss: 0.1922 (value: 0.0008, weighted value: 0.0422, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9367
Epoch 10/10, Train Loss: 0.1910 (value: 0.0008, weighted value: 0.0414, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9368
..training done in 67.15 seconds
..evaluation done in 18.86 seconds
Old network+MCTS average reward: 0.7301, min: 0.1667, max: 1.4815, stdev: 0.2383
New network+MCTS average reward: 0.7307, min: 0.1574, max: 1.4815, stdev: 0.2399
Old bare network average reward: 0.6970, min: 0.0741, max: 1.4167, stdev: 0.2413
New bare network average reward: 0.6967, min: 0.0556, max: 1.4352, stdev: 0.2437
External policy "random" average reward: 0.2459, min: -0.4167, max: 0.9630, stdev: 0.2297
External policy "individual greedy" average reward: 0.5281, min: -0.1759, max: 1.3056, stdev: 0.2327
External policy "total greedy" average reward: 0.6365, min: 0.0741, max: 1.3333, stdev: 0.2275
New network won 70 and tied 156 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 466 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.73 seconds
Training examples lengths: [64521, 64704, 64859, 65104, 64693, 64391, 64622, 64507, 64766, 64716]
Total value: 481538.28
Training on 646883 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2661 (value: 0.0017, weighted value: 0.0855, policy: 0.1806, weighted policy: 0.1806), Train Mean Max: 0.9323
Epoch 2/10, Train Loss: 0.2381 (value: 0.0014, weighted value: 0.0706, policy: 0.1675, weighted policy: 0.1675), Train Mean Max: 0.9338
Epoch 3/10, Train Loss: 0.2244 (value: 0.0013, weighted value: 0.0635, policy: 0.1609, weighted policy: 0.1609), Train Mean Max: 0.9343
Epoch 4/10, Train Loss: 0.2157 (value: 0.0012, weighted value: 0.0589, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9346
Epoch 5/10, Train Loss: 0.2110 (value: 0.0011, weighted value: 0.0554, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9348
Epoch 6/10, Train Loss: 0.2068 (value: 0.0011, weighted value: 0.0532, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9352
Epoch 7/10, Train Loss: 0.2038 (value: 0.0010, weighted value: 0.0503, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9353
Epoch 8/10, Train Loss: 0.1993 (value: 0.0009, weighted value: 0.0470, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9357
Epoch 9/10, Train Loss: 0.1979 (value: 0.0009, weighted value: 0.0464, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9358
Epoch 10/10, Train Loss: 0.1940 (value: 0.0009, weighted value: 0.0432, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9361
..training done in 64.66 seconds
..evaluation done in 18.73 seconds
Old network+MCTS average reward: 0.7244, min: 0.1574, max: 1.6481, stdev: 0.2370
New network+MCTS average reward: 0.7249, min: 0.1389, max: 1.5833, stdev: 0.2348
Old bare network average reward: 0.6869, min: 0.1574, max: 1.5833, stdev: 0.2397
New bare network average reward: 0.6902, min: 0.1111, max: 1.5833, stdev: 0.2382
External policy "random" average reward: 0.2559, min: -0.3333, max: 0.9537, stdev: 0.2220
External policy "individual greedy" average reward: 0.5280, min: 0.0463, max: 1.4074, stdev: 0.2234
External policy "total greedy" average reward: 0.6441, min: 0.1389, max: 1.4074, stdev: 0.2264
New network won 70 and tied 158 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 467 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.35 seconds
Training examples lengths: [64704, 64859, 65104, 64693, 64391, 64622, 64507, 64766, 64716, 64835]
Total value: 481822.70
Training on 647197 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2826 (value: 0.0019, weighted value: 0.0960, policy: 0.1866, weighted policy: 0.1866), Train Mean Max: 0.9312
Epoch 2/10, Train Loss: 0.2530 (value: 0.0016, weighted value: 0.0786, policy: 0.1743, weighted policy: 0.1743), Train Mean Max: 0.9326
Epoch 3/10, Train Loss: 0.2442 (value: 0.0015, weighted value: 0.0748, policy: 0.1694, weighted policy: 0.1694), Train Mean Max: 0.9330
Epoch 4/10, Train Loss: 0.2302 (value: 0.0013, weighted value: 0.0647, policy: 0.1655, weighted policy: 0.1655), Train Mean Max: 0.9336
Epoch 5/10, Train Loss: 0.2197 (value: 0.0012, weighted value: 0.0615, policy: 0.1581, weighted policy: 0.1581), Train Mean Max: 0.9341
Epoch 6/10, Train Loss: 0.2199 (value: 0.0012, weighted value: 0.0587, policy: 0.1613, weighted policy: 0.1613), Train Mean Max: 0.9339
Epoch 7/10, Train Loss: 0.2157 (value: 0.0011, weighted value: 0.0544, policy: 0.1614, weighted policy: 0.1614), Train Mean Max: 0.9342
Epoch 8/10, Train Loss: 0.2064 (value: 0.0010, weighted value: 0.0520, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9349
Epoch 9/10, Train Loss: 0.2051 (value: 0.0010, weighted value: 0.0504, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9349
Epoch 10/10, Train Loss: 0.2038 (value: 0.0010, weighted value: 0.0484, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9350
..training done in 63.87 seconds
..evaluation done in 18.38 seconds
Old network+MCTS average reward: 0.7223, min: 0.0556, max: 1.4537, stdev: 0.2461
New network+MCTS average reward: 0.7198, min: 0.0278, max: 1.4167, stdev: 0.2473
Old bare network average reward: 0.6839, min: -0.0185, max: 1.3426, stdev: 0.2487
New bare network average reward: 0.6844, min: 0.0556, max: 1.3333, stdev: 0.2471
External policy "random" average reward: 0.2616, min: -0.2870, max: 0.9444, stdev: 0.2323
External policy "individual greedy" average reward: 0.5340, min: 0.0370, max: 1.2222, stdev: 0.2297
External policy "total greedy" average reward: 0.6411, min: 0.0370, max: 1.4074, stdev: 0.2286
New network won 76 and tied 142 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 468 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.95 seconds
Training examples lengths: [64859, 65104, 64693, 64391, 64622, 64507, 64766, 64716, 64835, 64750]
Total value: 482468.15
Training on 647243 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2965 (value: 0.0021, weighted value: 0.1055, policy: 0.1910, weighted policy: 0.1910), Train Mean Max: 0.9303
Epoch 2/10, Train Loss: 0.2614 (value: 0.0017, weighted value: 0.0863, policy: 0.1750, weighted policy: 0.1750), Train Mean Max: 0.9319
Epoch 3/10, Train Loss: 0.2450 (value: 0.0015, weighted value: 0.0769, policy: 0.1681, weighted policy: 0.1681), Train Mean Max: 0.9325
Epoch 4/10, Train Loss: 0.2329 (value: 0.0014, weighted value: 0.0710, policy: 0.1619, weighted policy: 0.1619), Train Mean Max: 0.9330
Epoch 5/10, Train Loss: 0.2267 (value: 0.0013, weighted value: 0.0657, policy: 0.1609, weighted policy: 0.1609), Train Mean Max: 0.9332
Epoch 6/10, Train Loss: 0.2189 (value: 0.0012, weighted value: 0.0605, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9336
Epoch 7/10, Train Loss: 0.2190 (value: 0.0012, weighted value: 0.0589, policy: 0.1601, weighted policy: 0.1601), Train Mean Max: 0.9336
Epoch 8/10, Train Loss: 0.2107 (value: 0.0011, weighted value: 0.0545, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9343
Epoch 9/10, Train Loss: 0.2070 (value: 0.0010, weighted value: 0.0522, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9345
Epoch 10/10, Train Loss: 0.2059 (value: 0.0010, weighted value: 0.0511, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9345
..training done in 63.99 seconds
..evaluation done in 18.32 seconds
Old network+MCTS average reward: 0.7304, min: 0.1111, max: 1.3889, stdev: 0.2473
New network+MCTS average reward: 0.7318, min: 0.1389, max: 1.3889, stdev: 0.2438
Old bare network average reward: 0.6951, min: 0.0370, max: 1.3889, stdev: 0.2478
New bare network average reward: 0.6947, min: 0.0370, max: 1.3889, stdev: 0.2471
External policy "random" average reward: 0.2629, min: -0.4259, max: 1.0093, stdev: 0.2336
External policy "individual greedy" average reward: 0.5364, min: -0.1944, max: 1.1204, stdev: 0.2384
External policy "total greedy" average reward: 0.6518, min: 0.1204, max: 1.2130, stdev: 0.2249
New network won 86 and tied 138 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 469 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.32 seconds
Training examples lengths: [65104, 64693, 64391, 64622, 64507, 64766, 64716, 64835, 64750, 64936]
Total value: 482264.55
Training on 647320 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2254 (value: 0.0012, weighted value: 0.0598, policy: 0.1656, weighted policy: 0.1656), Train Mean Max: 0.9333
Epoch 2/10, Train Loss: 0.2144 (value: 0.0011, weighted value: 0.0548, policy: 0.1596, weighted policy: 0.1596), Train Mean Max: 0.9340
Epoch 3/10, Train Loss: 0.2087 (value: 0.0010, weighted value: 0.0524, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9345
Epoch 4/10, Train Loss: 0.2055 (value: 0.0010, weighted value: 0.0500, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9344
Epoch 5/10, Train Loss: 0.2017 (value: 0.0009, weighted value: 0.0470, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9349
Epoch 6/10, Train Loss: 0.1993 (value: 0.0009, weighted value: 0.0462, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9350
Epoch 7/10, Train Loss: 0.1978 (value: 0.0009, weighted value: 0.0438, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9352
Epoch 8/10, Train Loss: 0.1960 (value: 0.0009, weighted value: 0.0432, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9355
Epoch 9/10, Train Loss: 0.1941 (value: 0.0008, weighted value: 0.0422, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9358
Epoch 10/10, Train Loss: 0.1929 (value: 0.0008, weighted value: 0.0405, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9358
..training done in 65.66 seconds
..evaluation done in 19.05 seconds
Old network+MCTS average reward: 0.7301, min: 0.0093, max: 1.4907, stdev: 0.2370
New network+MCTS average reward: 0.7321, min: 0.0093, max: 1.4907, stdev: 0.2338
Old bare network average reward: 0.6929, min: 0.0093, max: 1.4167, stdev: 0.2398
New bare network average reward: 0.6973, min: 0.0093, max: 1.4167, stdev: 0.2363
External policy "random" average reward: 0.2506, min: -0.2778, max: 0.8704, stdev: 0.2058
External policy "individual greedy" average reward: 0.5265, min: -0.1389, max: 1.3519, stdev: 0.2197
External policy "total greedy" average reward: 0.6509, min: -0.0185, max: 1.3148, stdev: 0.2139
New network won 87 and tied 132 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 470 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.89 seconds
Training examples lengths: [64693, 64391, 64622, 64507, 64766, 64716, 64835, 64750, 64936, 64844]
Total value: 482250.09
Training on 647060 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2096 (value: 0.0010, weighted value: 0.0504, policy: 0.1592, weighted policy: 0.1592), Train Mean Max: 0.9346
Epoch 2/10, Train Loss: 0.2041 (value: 0.0010, weighted value: 0.0478, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9353
Epoch 3/10, Train Loss: 0.1974 (value: 0.0009, weighted value: 0.0446, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9357
Epoch 4/10, Train Loss: 0.1955 (value: 0.0009, weighted value: 0.0433, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9358
Epoch 5/10, Train Loss: 0.1929 (value: 0.0008, weighted value: 0.0417, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9361
Epoch 6/10, Train Loss: 0.1903 (value: 0.0008, weighted value: 0.0404, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9363
Epoch 7/10, Train Loss: 0.1891 (value: 0.0008, weighted value: 0.0392, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9364
Epoch 8/10, Train Loss: 0.1873 (value: 0.0008, weighted value: 0.0376, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9367
Epoch 9/10, Train Loss: 0.1871 (value: 0.0007, weighted value: 0.0372, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9366
Epoch 10/10, Train Loss: 0.1856 (value: 0.0007, weighted value: 0.0360, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9369
..training done in 59.03 seconds
..evaluation done in 18.38 seconds
Old network+MCTS average reward: 0.7523, min: 0.1759, max: 1.4815, stdev: 0.2288
New network+MCTS average reward: 0.7524, min: 0.1667, max: 1.4815, stdev: 0.2308
Old bare network average reward: 0.7160, min: 0.0556, max: 1.4444, stdev: 0.2366
New bare network average reward: 0.7188, min: 0.1389, max: 1.5185, stdev: 0.2295
External policy "random" average reward: 0.2763, min: -0.3333, max: 0.9630, stdev: 0.2292
External policy "individual greedy" average reward: 0.5480, min: -0.0278, max: 1.2130, stdev: 0.2190
External policy "total greedy" average reward: 0.6717, min: 0.1574, max: 1.3704, stdev: 0.2120
New network won 62 and tied 167 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 471 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.25 seconds
Training examples lengths: [64391, 64622, 64507, 64766, 64716, 64835, 64750, 64936, 64844, 64540]
Total value: 482017.44
Training on 646907 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2312 (value: 0.0013, weighted value: 0.0630, policy: 0.1682, weighted policy: 0.1682), Train Mean Max: 0.9333
Epoch 2/10, Train Loss: 0.2165 (value: 0.0011, weighted value: 0.0561, policy: 0.1604, weighted policy: 0.1604), Train Mean Max: 0.9342
Epoch 3/10, Train Loss: 0.2087 (value: 0.0010, weighted value: 0.0524, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9346
Epoch 4/10, Train Loss: 0.2038 (value: 0.0010, weighted value: 0.0492, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9348
Epoch 5/10, Train Loss: 0.2000 (value: 0.0009, weighted value: 0.0473, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9351
Epoch 6/10, Train Loss: 0.1976 (value: 0.0009, weighted value: 0.0451, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9352
Epoch 7/10, Train Loss: 0.1952 (value: 0.0009, weighted value: 0.0438, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9356
Epoch 8/10, Train Loss: 0.1933 (value: 0.0008, weighted value: 0.0425, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9357
Epoch 9/10, Train Loss: 0.1923 (value: 0.0008, weighted value: 0.0406, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9358
Epoch 10/10, Train Loss: 0.1903 (value: 0.0008, weighted value: 0.0391, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9362
..training done in 59.51 seconds
..evaluation done in 18.66 seconds
Old network+MCTS average reward: 0.7519, min: 0.1759, max: 1.4444, stdev: 0.2251
New network+MCTS average reward: 0.7516, min: 0.1944, max: 1.4444, stdev: 0.2274
Old bare network average reward: 0.7156, min: 0.1204, max: 1.4444, stdev: 0.2299
New bare network average reward: 0.7142, min: 0.1759, max: 1.4444, stdev: 0.2288
External policy "random" average reward: 0.2630, min: -0.4167, max: 0.8981, stdev: 0.2164
External policy "individual greedy" average reward: 0.5521, min: 0.0278, max: 1.3148, stdev: 0.2154
External policy "total greedy" average reward: 0.6574, min: 0.1296, max: 1.3333, stdev: 0.2128
New network won 73 and tied 151 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 472 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.23 seconds
Training examples lengths: [64622, 64507, 64766, 64716, 64835, 64750, 64936, 64844, 64540, 64804]
Total value: 482945.87
Training on 647320 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2501 (value: 0.0015, weighted value: 0.0758, policy: 0.1743, weighted policy: 0.1743), Train Mean Max: 0.9320
Epoch 2/10, Train Loss: 0.2295 (value: 0.0013, weighted value: 0.0645, policy: 0.1650, weighted policy: 0.1650), Train Mean Max: 0.9331
Epoch 3/10, Train Loss: 0.2185 (value: 0.0012, weighted value: 0.0585, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9337
Epoch 4/10, Train Loss: 0.2125 (value: 0.0011, weighted value: 0.0555, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9340
Epoch 5/10, Train Loss: 0.2083 (value: 0.0011, weighted value: 0.0528, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9343
Epoch 6/10, Train Loss: 0.2040 (value: 0.0010, weighted value: 0.0494, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9345
Epoch 7/10, Train Loss: 0.2013 (value: 0.0010, weighted value: 0.0476, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9348
Epoch 8/10, Train Loss: 0.1997 (value: 0.0009, weighted value: 0.0463, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9351
Epoch 9/10, Train Loss: 0.1982 (value: 0.0009, weighted value: 0.0445, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9352
Epoch 10/10, Train Loss: 0.1955 (value: 0.0009, weighted value: 0.0437, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9355
..training done in 67.48 seconds
..evaluation done in 19.21 seconds
Old network+MCTS average reward: 0.7144, min: 0.1481, max: 1.5185, stdev: 0.2393
New network+MCTS average reward: 0.7154, min: 0.1667, max: 1.5185, stdev: 0.2389
Old bare network average reward: 0.6823, min: 0.1111, max: 1.4352, stdev: 0.2410
New bare network average reward: 0.6820, min: 0.1296, max: 1.4537, stdev: 0.2363
External policy "random" average reward: 0.2446, min: -0.3796, max: 1.2870, stdev: 0.2301
External policy "individual greedy" average reward: 0.5208, min: -0.0556, max: 1.3148, stdev: 0.2324
External policy "total greedy" average reward: 0.6281, min: 0.0741, max: 1.4815, stdev: 0.2322
New network won 79 and tied 146 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 473 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.61 seconds
Training examples lengths: [64507, 64766, 64716, 64835, 64750, 64936, 64844, 64540, 64804, 64641]
Total value: 483637.89
Training on 647339 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2123 (value: 0.0010, weighted value: 0.0523, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9346
Epoch 2/10, Train Loss: 0.2068 (value: 0.0010, weighted value: 0.0494, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9350
Epoch 3/10, Train Loss: 0.1999 (value: 0.0009, weighted value: 0.0461, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9357
Epoch 4/10, Train Loss: 0.1968 (value: 0.0009, weighted value: 0.0446, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9357
Epoch 5/10, Train Loss: 0.1943 (value: 0.0008, weighted value: 0.0425, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9360
Epoch 6/10, Train Loss: 0.1927 (value: 0.0008, weighted value: 0.0408, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9362
Epoch 7/10, Train Loss: 0.1908 (value: 0.0008, weighted value: 0.0400, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9364
Epoch 8/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0397, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9366
Epoch 9/10, Train Loss: 0.1866 (value: 0.0007, weighted value: 0.0366, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9367
Epoch 10/10, Train Loss: 0.1881 (value: 0.0008, weighted value: 0.0380, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9370
..training done in 65.47 seconds
..evaluation done in 18.01 seconds
Old network+MCTS average reward: 0.7363, min: 0.2037, max: 1.4444, stdev: 0.2291
New network+MCTS average reward: 0.7370, min: 0.1481, max: 1.5000, stdev: 0.2290
Old bare network average reward: 0.7049, min: 0.1389, max: 1.4444, stdev: 0.2349
New bare network average reward: 0.7015, min: 0.1389, max: 1.4444, stdev: 0.2327
External policy "random" average reward: 0.2696, min: -0.2500, max: 0.9815, stdev: 0.2316
External policy "individual greedy" average reward: 0.5473, min: -0.0093, max: 1.2407, stdev: 0.2218
External policy "total greedy" average reward: 0.6571, min: 0.2130, max: 1.4444, stdev: 0.2121
New network won 72 and tied 160 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 474 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.56 seconds
Training examples lengths: [64766, 64716, 64835, 64750, 64936, 64844, 64540, 64804, 64641, 64745]
Total value: 484535.35
Training on 647577 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2081 (value: 0.0010, weighted value: 0.0489, policy: 0.1592, weighted policy: 0.1592), Train Mean Max: 0.9356
Epoch 2/10, Train Loss: 0.1991 (value: 0.0009, weighted value: 0.0445, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9363
Epoch 3/10, Train Loss: 0.1943 (value: 0.0008, weighted value: 0.0423, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9365
Epoch 4/10, Train Loss: 0.1901 (value: 0.0008, weighted value: 0.0401, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9367
Epoch 5/10, Train Loss: 0.1891 (value: 0.0008, weighted value: 0.0392, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1874 (value: 0.0008, weighted value: 0.0375, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9369
Epoch 7/10, Train Loss: 0.1862 (value: 0.0007, weighted value: 0.0369, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9371
Epoch 8/10, Train Loss: 0.1852 (value: 0.0007, weighted value: 0.0370, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9373
Epoch 9/10, Train Loss: 0.1841 (value: 0.0007, weighted value: 0.0347, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9374
Epoch 10/10, Train Loss: 0.1823 (value: 0.0007, weighted value: 0.0340, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9377
..training done in 65.97 seconds
..evaluation done in 18.69 seconds
Old network+MCTS average reward: 0.7265, min: 0.0926, max: 1.4444, stdev: 0.2404
New network+MCTS average reward: 0.7261, min: 0.0926, max: 1.4444, stdev: 0.2407
Old bare network average reward: 0.6902, min: 0.0926, max: 1.4444, stdev: 0.2431
New bare network average reward: 0.6917, min: 0.0926, max: 1.4444, stdev: 0.2456
External policy "random" average reward: 0.2557, min: -0.3056, max: 1.0463, stdev: 0.2272
External policy "individual greedy" average reward: 0.5198, min: -0.1667, max: 1.1296, stdev: 0.2354
External policy "total greedy" average reward: 0.6473, min: 0.0278, max: 1.2963, stdev: 0.2335
New network won 62 and tied 169 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 475 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.60 seconds
Training examples lengths: [64716, 64835, 64750, 64936, 64844, 64540, 64804, 64641, 64745, 64620]
Total value: 484307.62
Training on 647431 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2273 (value: 0.0012, weighted value: 0.0609, policy: 0.1664, weighted policy: 0.1664), Train Mean Max: 0.9344
Epoch 2/10, Train Loss: 0.2150 (value: 0.0011, weighted value: 0.0549, policy: 0.1601, weighted policy: 0.1601), Train Mean Max: 0.9352
Epoch 3/10, Train Loss: 0.2053 (value: 0.0010, weighted value: 0.0497, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9356
Epoch 4/10, Train Loss: 0.1998 (value: 0.0009, weighted value: 0.0464, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9360
Epoch 5/10, Train Loss: 0.1969 (value: 0.0009, weighted value: 0.0452, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9360
Epoch 6/10, Train Loss: 0.1941 (value: 0.0009, weighted value: 0.0427, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9365
Epoch 7/10, Train Loss: 0.1928 (value: 0.0008, weighted value: 0.0417, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9365
Epoch 8/10, Train Loss: 0.1909 (value: 0.0008, weighted value: 0.0409, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9368
Epoch 9/10, Train Loss: 0.1889 (value: 0.0008, weighted value: 0.0387, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9368
Epoch 10/10, Train Loss: 0.1868 (value: 0.0008, weighted value: 0.0378, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9371
..training done in 65.15 seconds
..evaluation done in 18.30 seconds
Old network+MCTS average reward: 0.7323, min: 0.1574, max: 1.5185, stdev: 0.2330
New network+MCTS average reward: 0.7290, min: 0.1759, max: 1.5185, stdev: 0.2363
Old bare network average reward: 0.6926, min: 0.1111, max: 1.5185, stdev: 0.2372
New bare network average reward: 0.6905, min: 0.0926, max: 1.5000, stdev: 0.2337
External policy "random" average reward: 0.2593, min: -0.3519, max: 0.9167, stdev: 0.2287
External policy "individual greedy" average reward: 0.5371, min: 0.0000, max: 1.2870, stdev: 0.2338
External policy "total greedy" average reward: 0.6466, min: 0.1204, max: 1.3611, stdev: 0.2300
New network won 78 and tied 148 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 476 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.96 seconds
Training examples lengths: [64835, 64750, 64936, 64844, 64540, 64804, 64641, 64745, 64620, 65091]
Total value: 485054.70
Training on 647806 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2081 (value: 0.0010, weighted value: 0.0502, policy: 0.1580, weighted policy: 0.1580), Train Mean Max: 0.9356
Epoch 2/10, Train Loss: 0.2011 (value: 0.0009, weighted value: 0.0459, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9362
Epoch 3/10, Train Loss: 0.1948 (value: 0.0009, weighted value: 0.0430, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9367
Epoch 4/10, Train Loss: 0.1917 (value: 0.0008, weighted value: 0.0409, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9369
Epoch 5/10, Train Loss: 0.1887 (value: 0.0008, weighted value: 0.0393, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9372
Epoch 6/10, Train Loss: 0.1880 (value: 0.0008, weighted value: 0.0391, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9372
Epoch 7/10, Train Loss: 0.1857 (value: 0.0007, weighted value: 0.0373, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9374
Epoch 8/10, Train Loss: 0.1841 (value: 0.0007, weighted value: 0.0359, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1839 (value: 0.0007, weighted value: 0.0356, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9375
Epoch 10/10, Train Loss: 0.1825 (value: 0.0007, weighted value: 0.0344, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9379
..training done in 62.69 seconds
..evaluation done in 19.40 seconds
Old network+MCTS average reward: 0.7454, min: 0.1019, max: 1.8611, stdev: 0.2369
New network+MCTS average reward: 0.7458, min: 0.1759, max: 1.8889, stdev: 0.2332
Old bare network average reward: 0.7137, min: 0.1389, max: 1.8611, stdev: 0.2400
New bare network average reward: 0.7106, min: 0.1019, max: 1.8981, stdev: 0.2389
External policy "random" average reward: 0.2660, min: -0.2315, max: 1.0093, stdev: 0.2222
External policy "individual greedy" average reward: 0.5414, min: -0.0185, max: 1.6481, stdev: 0.2252
External policy "total greedy" average reward: 0.6559, min: 0.1296, max: 1.6759, stdev: 0.2228
New network won 68 and tied 172 out of 300 games (51.33% wins where ties are half wins)
Keeping the new network

Training iteration 477 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.26 seconds
Training examples lengths: [64750, 64936, 64844, 64540, 64804, 64641, 64745, 64620, 65091, 64801]
Total value: 485059.94
Training on 647772 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2034 (value: 0.0009, weighted value: 0.0470, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9364
Epoch 2/10, Train Loss: 0.1955 (value: 0.0008, weighted value: 0.0425, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9370
Epoch 3/10, Train Loss: 0.1920 (value: 0.0008, weighted value: 0.0415, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9374
Epoch 4/10, Train Loss: 0.1868 (value: 0.0008, weighted value: 0.0382, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1850 (value: 0.0007, weighted value: 0.0370, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1847 (value: 0.0007, weighted value: 0.0365, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9378
Epoch 7/10, Train Loss: 0.1846 (value: 0.0007, weighted value: 0.0363, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9378
Epoch 8/10, Train Loss: 0.1805 (value: 0.0007, weighted value: 0.0336, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9382
Epoch 9/10, Train Loss: 0.1811 (value: 0.0007, weighted value: 0.0342, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9382
Epoch 10/10, Train Loss: 0.1798 (value: 0.0007, weighted value: 0.0331, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9382
..training done in 64.20 seconds
..evaluation done in 18.26 seconds
Old network+MCTS average reward: 0.7461, min: 0.1111, max: 1.4074, stdev: 0.2379
New network+MCTS average reward: 0.7414, min: 0.1111, max: 1.3796, stdev: 0.2365
Old bare network average reward: 0.7126, min: 0.0278, max: 1.4074, stdev: 0.2433
New bare network average reward: 0.7121, min: 0.0741, max: 1.3611, stdev: 0.2398
External policy "random" average reward: 0.2568, min: -0.3333, max: 0.9259, stdev: 0.2205
External policy "individual greedy" average reward: 0.5472, min: -0.0648, max: 1.1667, stdev: 0.2339
External policy "total greedy" average reward: 0.6626, min: 0.0463, max: 1.3241, stdev: 0.2235
New network won 55 and tied 165 out of 300 games (45.83% wins where ties are half wins)
Reverting to the old network

Training iteration 478 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.24 seconds
Training examples lengths: [64936, 64844, 64540, 64804, 64641, 64745, 64620, 65091, 64801, 65099]
Total value: 485849.41
Training on 648121 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2251 (value: 0.0012, weighted value: 0.0592, policy: 0.1659, weighted policy: 0.1659), Train Mean Max: 0.9350
Epoch 2/10, Train Loss: 0.2103 (value: 0.0011, weighted value: 0.0525, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9360
Epoch 3/10, Train Loss: 0.2004 (value: 0.0009, weighted value: 0.0472, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9364
Epoch 4/10, Train Loss: 0.1963 (value: 0.0009, weighted value: 0.0457, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9367
Epoch 5/10, Train Loss: 0.1923 (value: 0.0009, weighted value: 0.0428, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1908 (value: 0.0008, weighted value: 0.0416, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9370
Epoch 7/10, Train Loss: 0.1898 (value: 0.0008, weighted value: 0.0404, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9372
Epoch 8/10, Train Loss: 0.1883 (value: 0.0008, weighted value: 0.0397, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1847 (value: 0.0007, weighted value: 0.0373, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1845 (value: 0.0007, weighted value: 0.0365, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9377
..training done in 64.52 seconds
..evaluation done in 19.30 seconds
Old network+MCTS average reward: 0.7480, min: 0.1759, max: 1.4352, stdev: 0.2286
New network+MCTS average reward: 0.7480, min: 0.1759, max: 1.5000, stdev: 0.2287
Old bare network average reward: 0.7131, min: 0.0185, max: 1.4352, stdev: 0.2347
New bare network average reward: 0.7123, min: 0.0185, max: 1.4352, stdev: 0.2333
External policy "random" average reward: 0.2650, min: -0.3148, max: 0.9907, stdev: 0.2358
External policy "individual greedy" average reward: 0.5562, min: -0.0278, max: 1.3519, stdev: 0.2180
External policy "total greedy" average reward: 0.6612, min: 0.0185, max: 1.3241, stdev: 0.2201
New network won 69 and tied 162 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 479 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.62 seconds
Training examples lengths: [64844, 64540, 64804, 64641, 64745, 64620, 65091, 64801, 65099, 65207]
Total value: 486390.19
Training on 648392 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2050 (value: 0.0010, weighted value: 0.0490, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9362
Epoch 2/10, Train Loss: 0.1981 (value: 0.0009, weighted value: 0.0448, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9368
Epoch 3/10, Train Loss: 0.1924 (value: 0.0009, weighted value: 0.0425, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9375
Epoch 4/10, Train Loss: 0.1888 (value: 0.0008, weighted value: 0.0401, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9374
Epoch 5/10, Train Loss: 0.1874 (value: 0.0008, weighted value: 0.0397, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1853 (value: 0.0008, weighted value: 0.0378, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9378
Epoch 7/10, Train Loss: 0.1835 (value: 0.0007, weighted value: 0.0361, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9380
Epoch 8/10, Train Loss: 0.1831 (value: 0.0007, weighted value: 0.0361, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9382
Epoch 9/10, Train Loss: 0.1814 (value: 0.0007, weighted value: 0.0349, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1808 (value: 0.0007, weighted value: 0.0346, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9384
..training done in 65.59 seconds
..evaluation done in 18.44 seconds
Old network+MCTS average reward: 0.7219, min: -0.0370, max: 1.4167, stdev: 0.2366
New network+MCTS average reward: 0.7255, min: -0.0463, max: 1.4537, stdev: 0.2391
Old bare network average reward: 0.6895, min: 0.0463, max: 1.3889, stdev: 0.2397
New bare network average reward: 0.6892, min: -0.1019, max: 1.4167, stdev: 0.2394
External policy "random" average reward: 0.2653, min: -0.3333, max: 0.9259, stdev: 0.2247
External policy "individual greedy" average reward: 0.5306, min: -0.2593, max: 1.2778, stdev: 0.2275
External policy "total greedy" average reward: 0.6362, min: -0.1481, max: 1.3241, stdev: 0.2279
New network won 73 and tied 167 out of 300 games (52.17% wins where ties are half wins)
Keeping the new network

Training iteration 480 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.21 seconds
Training examples lengths: [64540, 64804, 64641, 64745, 64620, 65091, 64801, 65099, 65207, 64679]
Total value: 486075.93
Training on 648227 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2021 (value: 0.0009, weighted value: 0.0457, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9367
Epoch 2/10, Train Loss: 0.1950 (value: 0.0008, weighted value: 0.0422, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9375
Epoch 3/10, Train Loss: 0.1908 (value: 0.0008, weighted value: 0.0403, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9376
Epoch 4/10, Train Loss: 0.1865 (value: 0.0008, weighted value: 0.0383, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9379
Epoch 5/10, Train Loss: 0.1843 (value: 0.0007, weighted value: 0.0370, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9380
Epoch 6/10, Train Loss: 0.1827 (value: 0.0007, weighted value: 0.0361, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9381
Epoch 7/10, Train Loss: 0.1815 (value: 0.0007, weighted value: 0.0346, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9382
Epoch 8/10, Train Loss: 0.1816 (value: 0.0007, weighted value: 0.0343, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9383
Epoch 9/10, Train Loss: 0.1796 (value: 0.0007, weighted value: 0.0330, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9385
Epoch 10/10, Train Loss: 0.1790 (value: 0.0007, weighted value: 0.0330, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9387
..training done in 63.44 seconds
..evaluation done in 19.52 seconds
Old network+MCTS average reward: 0.7399, min: 0.1481, max: 1.4074, stdev: 0.2382
New network+MCTS average reward: 0.7353, min: 0.1389, max: 1.3981, stdev: 0.2385
Old bare network average reward: 0.7037, min: -0.0648, max: 1.3981, stdev: 0.2393
New bare network average reward: 0.7031, min: 0.1389, max: 1.4074, stdev: 0.2418
External policy "random" average reward: 0.2674, min: -0.3426, max: 0.8981, stdev: 0.2336
External policy "individual greedy" average reward: 0.5456, min: -0.1481, max: 1.2963, stdev: 0.2318
External policy "total greedy" average reward: 0.6658, min: -0.0648, max: 1.3056, stdev: 0.2311
New network won 53 and tied 172 out of 300 games (46.33% wins where ties are half wins)
Reverting to the old network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_480

Training iteration 481 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.79 seconds
Training examples lengths: [64804, 64641, 64745, 64620, 65091, 64801, 65099, 65207, 64679, 64518]
Total value: 486253.33
Training on 648205 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2255 (value: 0.0012, weighted value: 0.0605, policy: 0.1650, weighted policy: 0.1650), Train Mean Max: 0.9352
Epoch 2/10, Train Loss: 0.2088 (value: 0.0010, weighted value: 0.0511, policy: 0.1577, weighted policy: 0.1577), Train Mean Max: 0.9363
Epoch 3/10, Train Loss: 0.2004 (value: 0.0009, weighted value: 0.0473, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9367
Epoch 4/10, Train Loss: 0.1968 (value: 0.0009, weighted value: 0.0458, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9370
Epoch 5/10, Train Loss: 0.1931 (value: 0.0009, weighted value: 0.0431, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9370
Epoch 6/10, Train Loss: 0.1903 (value: 0.0008, weighted value: 0.0412, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9372
Epoch 7/10, Train Loss: 0.1880 (value: 0.0008, weighted value: 0.0397, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9375
Epoch 8/10, Train Loss: 0.1868 (value: 0.0008, weighted value: 0.0384, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1863 (value: 0.0008, weighted value: 0.0386, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1834 (value: 0.0007, weighted value: 0.0352, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9379
..training done in 60.21 seconds
..evaluation done in 25.06 seconds
Old network+MCTS average reward: 0.7673, min: 0.1667, max: 1.5556, stdev: 0.2378
New network+MCTS average reward: 0.7665, min: 0.1759, max: 1.5278, stdev: 0.2315
Old bare network average reward: 0.7372, min: 0.1389, max: 1.5556, stdev: 0.2376
New bare network average reward: 0.7377, min: 0.1296, max: 1.5093, stdev: 0.2343
External policy "random" average reward: 0.2963, min: -0.2778, max: 0.8611, stdev: 0.2230
External policy "individual greedy" average reward: 0.5740, min: -0.1019, max: 1.2130, stdev: 0.2342
External policy "total greedy" average reward: 0.6746, min: 0.0648, max: 1.2685, stdev: 0.2346
New network won 70 and tied 160 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 482 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.82 seconds
Training examples lengths: [64641, 64745, 64620, 65091, 64801, 65099, 65207, 64679, 64518, 64799]
Total value: 486222.63
Training on 648200 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2043 (value: 0.0010, weighted value: 0.0476, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9364
Epoch 2/10, Train Loss: 0.1968 (value: 0.0009, weighted value: 0.0439, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9370
Epoch 3/10, Train Loss: 0.1911 (value: 0.0008, weighted value: 0.0409, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9373
Epoch 4/10, Train Loss: 0.1899 (value: 0.0008, weighted value: 0.0401, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9374
Epoch 5/10, Train Loss: 0.1870 (value: 0.0008, weighted value: 0.0382, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1849 (value: 0.0007, weighted value: 0.0368, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9377
Epoch 7/10, Train Loss: 0.1839 (value: 0.0007, weighted value: 0.0363, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0347, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1818 (value: 0.0007, weighted value: 0.0340, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1801 (value: 0.0007, weighted value: 0.0333, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9382
..training done in 66.54 seconds
..evaluation done in 18.87 seconds
Old network+MCTS average reward: 0.7399, min: 0.0648, max: 1.5648, stdev: 0.2328
New network+MCTS average reward: 0.7395, min: 0.0648, max: 1.5463, stdev: 0.2312
Old bare network average reward: 0.7055, min: 0.0648, max: 1.5648, stdev: 0.2347
New bare network average reward: 0.7036, min: 0.0648, max: 1.4722, stdev: 0.2352
External policy "random" average reward: 0.2654, min: -0.3611, max: 1.0833, stdev: 0.2327
External policy "individual greedy" average reward: 0.5465, min: -0.0648, max: 1.4444, stdev: 0.2172
External policy "total greedy" average reward: 0.6544, min: 0.0000, max: 1.5093, stdev: 0.2209
New network won 63 and tied 171 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 483 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.91 seconds
Training examples lengths: [64745, 64620, 65091, 64801, 65099, 65207, 64679, 64518, 64799, 64811]
Total value: 486648.58
Training on 648370 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2261 (value: 0.0012, weighted value: 0.0608, policy: 0.1653, weighted policy: 0.1653), Train Mean Max: 0.9347
Epoch 2/10, Train Loss: 0.2112 (value: 0.0011, weighted value: 0.0530, policy: 0.1582, weighted policy: 0.1582), Train Mean Max: 0.9357
Epoch 3/10, Train Loss: 0.2025 (value: 0.0010, weighted value: 0.0484, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9361
Epoch 4/10, Train Loss: 0.1989 (value: 0.0009, weighted value: 0.0472, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9363
Epoch 5/10, Train Loss: 0.1961 (value: 0.0009, weighted value: 0.0447, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9365
Epoch 6/10, Train Loss: 0.1916 (value: 0.0008, weighted value: 0.0411, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9366
Epoch 7/10, Train Loss: 0.1907 (value: 0.0008, weighted value: 0.0416, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9369
Epoch 8/10, Train Loss: 0.1876 (value: 0.0008, weighted value: 0.0392, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9371
Epoch 9/10, Train Loss: 0.1870 (value: 0.0008, weighted value: 0.0385, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9372
Epoch 10/10, Train Loss: 0.1853 (value: 0.0007, weighted value: 0.0371, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9374
..training done in 63.17 seconds
..evaluation done in 18.46 seconds
Old network+MCTS average reward: 0.7394, min: 0.1759, max: 1.5093, stdev: 0.2383
New network+MCTS average reward: 0.7398, min: 0.0926, max: 1.5093, stdev: 0.2382
Old bare network average reward: 0.7036, min: 0.0926, max: 1.5093, stdev: 0.2397
New bare network average reward: 0.7026, min: 0.0926, max: 1.5093, stdev: 0.2412
External policy "random" average reward: 0.2622, min: -0.2963, max: 0.8333, stdev: 0.2160
External policy "individual greedy" average reward: 0.5260, min: 0.0093, max: 1.2037, stdev: 0.2155
External policy "total greedy" average reward: 0.6467, min: 0.0556, max: 1.2407, stdev: 0.2104
New network won 69 and tied 164 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 484 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.57 seconds
Training examples lengths: [64620, 65091, 64801, 65099, 65207, 64679, 64518, 64799, 64811, 64875]
Total value: 486329.20
Training on 648500 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2062 (value: 0.0010, weighted value: 0.0495, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9361
Epoch 2/10, Train Loss: 0.1982 (value: 0.0009, weighted value: 0.0451, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9368
Epoch 3/10, Train Loss: 0.1926 (value: 0.0008, weighted value: 0.0421, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9371
Epoch 4/10, Train Loss: 0.1893 (value: 0.0008, weighted value: 0.0403, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9372
Epoch 5/10, Train Loss: 0.1881 (value: 0.0008, weighted value: 0.0393, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9373
Epoch 6/10, Train Loss: 0.1865 (value: 0.0008, weighted value: 0.0380, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9376
Epoch 7/10, Train Loss: 0.1843 (value: 0.0007, weighted value: 0.0367, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9377
Epoch 8/10, Train Loss: 0.1828 (value: 0.0007, weighted value: 0.0356, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1825 (value: 0.0007, weighted value: 0.0355, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9381
Epoch 10/10, Train Loss: 0.1821 (value: 0.0007, weighted value: 0.0345, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9381
..training done in 59.84 seconds
..evaluation done in 18.53 seconds
Old network+MCTS average reward: 0.7129, min: 0.2500, max: 1.2778, stdev: 0.2097
New network+MCTS average reward: 0.7154, min: 0.2130, max: 1.2593, stdev: 0.2121
Old bare network average reward: 0.6778, min: 0.1481, max: 1.2500, stdev: 0.2114
New bare network average reward: 0.6783, min: 0.1667, max: 1.2500, stdev: 0.2097
External policy "random" average reward: 0.2395, min: -0.3611, max: 0.8241, stdev: 0.2090
External policy "individual greedy" average reward: 0.5234, min: -0.0278, max: 1.1852, stdev: 0.2024
External policy "total greedy" average reward: 0.6373, min: 0.2130, max: 1.2037, stdev: 0.1948
New network won 71 and tied 165 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 485 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.47 seconds
Training examples lengths: [65091, 64801, 65099, 65207, 64679, 64518, 64799, 64811, 64875, 65024]
Total value: 487502.78
Training on 648904 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2022 (value: 0.0009, weighted value: 0.0458, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9367
Epoch 2/10, Train Loss: 0.1941 (value: 0.0009, weighted value: 0.0425, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9374
Epoch 3/10, Train Loss: 0.1894 (value: 0.0008, weighted value: 0.0404, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9377
Epoch 4/10, Train Loss: 0.1868 (value: 0.0008, weighted value: 0.0387, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9378
Epoch 5/10, Train Loss: 0.1843 (value: 0.0007, weighted value: 0.0366, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9380
Epoch 6/10, Train Loss: 0.1833 (value: 0.0007, weighted value: 0.0364, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9381
Epoch 7/10, Train Loss: 0.1830 (value: 0.0007, weighted value: 0.0363, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9383
Epoch 8/10, Train Loss: 0.1794 (value: 0.0007, weighted value: 0.0336, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1786 (value: 0.0007, weighted value: 0.0330, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9386
Epoch 10/10, Train Loss: 0.1792 (value: 0.0007, weighted value: 0.0328, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9385
..training done in 68.83 seconds
..evaluation done in 23.71 seconds
Old network+MCTS average reward: 0.7367, min: 0.1574, max: 1.6852, stdev: 0.2351
New network+MCTS average reward: 0.7398, min: 0.1574, max: 1.6852, stdev: 0.2374
Old bare network average reward: 0.6998, min: 0.0741, max: 1.6852, stdev: 0.2431
New bare network average reward: 0.6961, min: 0.1019, max: 1.6852, stdev: 0.2428
External policy "random" average reward: 0.2578, min: -0.2778, max: 0.8333, stdev: 0.2136
External policy "individual greedy" average reward: 0.5363, min: -0.0741, max: 1.4444, stdev: 0.2248
External policy "total greedy" average reward: 0.6475, min: 0.0833, max: 1.6019, stdev: 0.2298
New network won 82 and tied 158 out of 300 games (53.67% wins where ties are half wins)
Keeping the new network

Training iteration 486 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 69.54 seconds
Training examples lengths: [64801, 65099, 65207, 64679, 64518, 64799, 64811, 64875, 65024, 64633]
Total value: 486800.29
Training on 648446 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2012 (value: 0.0009, weighted value: 0.0454, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9370
Epoch 2/10, Train Loss: 0.1919 (value: 0.0008, weighted value: 0.0408, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9376
Epoch 3/10, Train Loss: 0.1867 (value: 0.0008, weighted value: 0.0381, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9379
Epoch 4/10, Train Loss: 0.1858 (value: 0.0008, weighted value: 0.0378, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9379
Epoch 5/10, Train Loss: 0.1836 (value: 0.0007, weighted value: 0.0361, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9381
Epoch 6/10, Train Loss: 0.1817 (value: 0.0007, weighted value: 0.0346, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9382
Epoch 7/10, Train Loss: 0.1809 (value: 0.0007, weighted value: 0.0342, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9384
Epoch 8/10, Train Loss: 0.1788 (value: 0.0007, weighted value: 0.0327, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9385
Epoch 9/10, Train Loss: 0.1777 (value: 0.0006, weighted value: 0.0316, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9387
Epoch 10/10, Train Loss: 0.1777 (value: 0.0006, weighted value: 0.0320, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9388
..training done in 73.06 seconds
..evaluation done in 21.24 seconds
Old network+MCTS average reward: 0.7527, min: 0.0926, max: 1.3241, stdev: 0.2385
New network+MCTS average reward: 0.7531, min: 0.0926, max: 1.4074, stdev: 0.2379
Old bare network average reward: 0.7184, min: 0.0370, max: 1.2963, stdev: 0.2433
New bare network average reward: 0.7189, min: 0.0370, max: 1.2963, stdev: 0.2446
External policy "random" average reward: 0.2783, min: -0.3796, max: 0.8704, stdev: 0.2204
External policy "individual greedy" average reward: 0.5445, min: -0.1111, max: 1.2222, stdev: 0.2282
External policy "total greedy" average reward: 0.6731, min: -0.0833, max: 1.2407, stdev: 0.2235
New network won 63 and tied 176 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 487 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.20 seconds
Training examples lengths: [65099, 65207, 64679, 64518, 64799, 64811, 64875, 65024, 64633, 64943]
Total value: 487156.31
Training on 648588 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2001 (value: 0.0009, weighted value: 0.0440, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9371
Epoch 2/10, Train Loss: 0.1926 (value: 0.0008, weighted value: 0.0405, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9379
Epoch 3/10, Train Loss: 0.1873 (value: 0.0008, weighted value: 0.0382, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9381
Epoch 4/10, Train Loss: 0.1837 (value: 0.0007, weighted value: 0.0362, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9383
Epoch 5/10, Train Loss: 0.1827 (value: 0.0007, weighted value: 0.0359, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9383
Epoch 6/10, Train Loss: 0.1799 (value: 0.0007, weighted value: 0.0339, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9384
Epoch 7/10, Train Loss: 0.1795 (value: 0.0007, weighted value: 0.0339, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9386
Epoch 8/10, Train Loss: 0.1783 (value: 0.0007, weighted value: 0.0327, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9386
Epoch 9/10, Train Loss: 0.1783 (value: 0.0006, weighted value: 0.0318, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9388
Epoch 10/10, Train Loss: 0.1774 (value: 0.0006, weighted value: 0.0312, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9388
..training done in 65.16 seconds
..evaluation done in 18.55 seconds
Old network+MCTS average reward: 0.7468, min: 0.2037, max: 1.4722, stdev: 0.2272
New network+MCTS average reward: 0.7469, min: 0.2037, max: 1.4722, stdev: 0.2295
Old bare network average reward: 0.7085, min: 0.1944, max: 1.4722, stdev: 0.2355
New bare network average reward: 0.7058, min: 0.0185, max: 1.4722, stdev: 0.2351
External policy "random" average reward: 0.2570, min: -0.3241, max: 1.1759, stdev: 0.2340
External policy "individual greedy" average reward: 0.5290, min: -0.0370, max: 1.2593, stdev: 0.2350
External policy "total greedy" average reward: 0.6437, min: 0.1296, max: 1.3333, stdev: 0.2172
New network won 64 and tied 178 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 488 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.86 seconds
Training examples lengths: [65207, 64679, 64518, 64799, 64811, 64875, 65024, 64633, 64943, 64699]
Total value: 486872.53
Training on 648188 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2002 (value: 0.0009, weighted value: 0.0440, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9370
Epoch 2/10, Train Loss: 0.1912 (value: 0.0008, weighted value: 0.0402, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9379
Epoch 3/10, Train Loss: 0.1869 (value: 0.0008, weighted value: 0.0382, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9379
Epoch 4/10, Train Loss: 0.1837 (value: 0.0007, weighted value: 0.0359, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9381
Epoch 5/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0345, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9382
Epoch 6/10, Train Loss: 0.1808 (value: 0.0007, weighted value: 0.0345, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9384
Epoch 7/10, Train Loss: 0.1793 (value: 0.0007, weighted value: 0.0327, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9383
Epoch 8/10, Train Loss: 0.1792 (value: 0.0007, weighted value: 0.0327, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9385
Epoch 9/10, Train Loss: 0.1770 (value: 0.0006, weighted value: 0.0309, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9387
Epoch 10/10, Train Loss: 0.1780 (value: 0.0006, weighted value: 0.0314, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9386
..training done in 78.18 seconds
..evaluation done in 24.40 seconds
Old network+MCTS average reward: 0.7638, min: 0.2685, max: 1.5833, stdev: 0.2462
New network+MCTS average reward: 0.7654, min: 0.2593, max: 1.5833, stdev: 0.2460
Old bare network average reward: 0.7403, min: 0.2315, max: 1.4630, stdev: 0.2474
New bare network average reward: 0.7394, min: 0.2315, max: 1.4630, stdev: 0.2473
External policy "random" average reward: 0.2862, min: -0.2130, max: 0.9815, stdev: 0.2264
External policy "individual greedy" average reward: 0.5679, min: 0.0185, max: 1.2870, stdev: 0.2331
External policy "total greedy" average reward: 0.6735, min: 0.2037, max: 1.5093, stdev: 0.2255
New network won 65 and tied 172 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 489 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.74 seconds
Training examples lengths: [64679, 64518, 64799, 64811, 64875, 65024, 64633, 64943, 64699, 64686]
Total value: 485794.69
Training on 647667 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1986 (value: 0.0009, weighted value: 0.0432, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9370
Epoch 2/10, Train Loss: 0.1926 (value: 0.0008, weighted value: 0.0394, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9376
Epoch 3/10, Train Loss: 0.1866 (value: 0.0007, weighted value: 0.0370, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9382
Epoch 4/10, Train Loss: 0.1841 (value: 0.0007, weighted value: 0.0362, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9380
Epoch 5/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0350, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9382
Epoch 6/10, Train Loss: 0.1808 (value: 0.0007, weighted value: 0.0330, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9382
Epoch 7/10, Train Loss: 0.1788 (value: 0.0006, weighted value: 0.0325, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9385
Epoch 8/10, Train Loss: 0.1787 (value: 0.0006, weighted value: 0.0319, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9385
Epoch 9/10, Train Loss: 0.1784 (value: 0.0006, weighted value: 0.0315, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9386
Epoch 10/10, Train Loss: 0.1769 (value: 0.0006, weighted value: 0.0306, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9387
..training done in 66.70 seconds
..evaluation done in 18.69 seconds
Old network+MCTS average reward: 0.7547, min: 0.1667, max: 1.5741, stdev: 0.2297
New network+MCTS average reward: 0.7553, min: 0.2407, max: 1.6204, stdev: 0.2276
Old bare network average reward: 0.7153, min: 0.1111, max: 1.5833, stdev: 0.2329
New bare network average reward: 0.7119, min: 0.1667, max: 1.6204, stdev: 0.2324
External policy "random" average reward: 0.2762, min: -0.3519, max: 0.9907, stdev: 0.2163
External policy "individual greedy" average reward: 0.5458, min: -0.0278, max: 1.1204, stdev: 0.2213
External policy "total greedy" average reward: 0.6634, min: -0.0370, max: 1.3981, stdev: 0.2196
New network won 61 and tied 174 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 490 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 60.77 seconds
Training examples lengths: [64518, 64799, 64811, 64875, 65024, 64633, 64943, 64699, 64686, 64850]
Total value: 486229.40
Training on 647838 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2198 (value: 0.0011, weighted value: 0.0567, policy: 0.1630, weighted policy: 0.1630), Train Mean Max: 0.9357
Epoch 2/10, Train Loss: 0.2054 (value: 0.0010, weighted value: 0.0484, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9365
Epoch 3/10, Train Loss: 0.1984 (value: 0.0009, weighted value: 0.0450, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9370
Epoch 4/10, Train Loss: 0.1931 (value: 0.0008, weighted value: 0.0421, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9372
Epoch 5/10, Train Loss: 0.1901 (value: 0.0008, weighted value: 0.0411, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9374
Epoch 6/10, Train Loss: 0.1877 (value: 0.0008, weighted value: 0.0393, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9375
Epoch 7/10, Train Loss: 0.1856 (value: 0.0007, weighted value: 0.0374, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9375
Epoch 8/10, Train Loss: 0.1836 (value: 0.0007, weighted value: 0.0358, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1830 (value: 0.0007, weighted value: 0.0356, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0339, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9380
..training done in 65.96 seconds
..evaluation done in 18.95 seconds
Old network+MCTS average reward: 0.7382, min: 0.1204, max: 1.4907, stdev: 0.2307
New network+MCTS average reward: 0.7376, min: 0.1667, max: 1.5185, stdev: 0.2288
Old bare network average reward: 0.6995, min: 0.1204, max: 1.4907, stdev: 0.2290
New bare network average reward: 0.6985, min: 0.1204, max: 1.4907, stdev: 0.2297
External policy "random" average reward: 0.2599, min: -0.2407, max: 0.9259, stdev: 0.2150
External policy "individual greedy" average reward: 0.5435, min: 0.0370, max: 1.2593, stdev: 0.2221
External policy "total greedy" average reward: 0.6520, min: 0.1204, max: 1.4167, stdev: 0.2182
New network won 62 and tied 177 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 491 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.22 seconds
Training examples lengths: [64799, 64811, 64875, 65024, 64633, 64943, 64699, 64686, 64850, 64897]
Total value: 486556.44
Training on 648217 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2062 (value: 0.0010, weighted value: 0.0477, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9363
Epoch 2/10, Train Loss: 0.1959 (value: 0.0009, weighted value: 0.0428, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9372
Epoch 3/10, Train Loss: 0.1895 (value: 0.0008, weighted value: 0.0391, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9375
Epoch 4/10, Train Loss: 0.1870 (value: 0.0008, weighted value: 0.0376, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1860 (value: 0.0007, weighted value: 0.0373, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1838 (value: 0.0007, weighted value: 0.0360, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9377
Epoch 7/10, Train Loss: 0.1820 (value: 0.0007, weighted value: 0.0346, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1802 (value: 0.0007, weighted value: 0.0337, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9380
Epoch 9/10, Train Loss: 0.1801 (value: 0.0007, weighted value: 0.0325, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9381
Epoch 10/10, Train Loss: 0.1796 (value: 0.0007, weighted value: 0.0326, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9383
..training done in 59.69 seconds
..evaluation done in 18.74 seconds
Old network+MCTS average reward: 0.7335, min: 0.0833, max: 1.5185, stdev: 0.2215
New network+MCTS average reward: 0.7365, min: 0.0741, max: 1.5741, stdev: 0.2237
Old bare network average reward: 0.7026, min: 0.0741, max: 1.6019, stdev: 0.2307
New bare network average reward: 0.7017, min: 0.0093, max: 1.6019, stdev: 0.2285
External policy "random" average reward: 0.2639, min: -0.4630, max: 1.0741, stdev: 0.2356
External policy "individual greedy" average reward: 0.5348, min: -0.0185, max: 1.3426, stdev: 0.2177
External policy "total greedy" average reward: 0.6491, min: 0.0741, max: 1.4722, stdev: 0.2219
New network won 78 and tied 164 out of 300 games (53.33% wins where ties are half wins)
Keeping the new network

Training iteration 492 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.01 seconds
Training examples lengths: [64811, 64875, 65024, 64633, 64943, 64699, 64686, 64850, 64897, 64898]
Total value: 486485.56
Training on 648316 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2000 (value: 0.0009, weighted value: 0.0446, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9369
Epoch 2/10, Train Loss: 0.1930 (value: 0.0008, weighted value: 0.0407, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9374
Epoch 3/10, Train Loss: 0.1878 (value: 0.0008, weighted value: 0.0382, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9376
Epoch 4/10, Train Loss: 0.1851 (value: 0.0007, weighted value: 0.0368, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9379
Epoch 5/10, Train Loss: 0.1826 (value: 0.0007, weighted value: 0.0349, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9380
Epoch 6/10, Train Loss: 0.1822 (value: 0.0007, weighted value: 0.0347, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9380
Epoch 7/10, Train Loss: 0.1784 (value: 0.0006, weighted value: 0.0321, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9384
Epoch 8/10, Train Loss: 0.1797 (value: 0.0007, weighted value: 0.0328, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1787 (value: 0.0006, weighted value: 0.0317, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1772 (value: 0.0006, weighted value: 0.0316, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9386
..training done in 69.09 seconds
..evaluation done in 24.20 seconds
Old network+MCTS average reward: 0.7355, min: 0.1667, max: 1.4259, stdev: 0.2203
New network+MCTS average reward: 0.7361, min: 0.1481, max: 1.4352, stdev: 0.2195
Old bare network average reward: 0.7045, min: 0.1204, max: 1.3889, stdev: 0.2262
New bare network average reward: 0.7019, min: 0.1204, max: 1.3519, stdev: 0.2230
External policy "random" average reward: 0.2441, min: -0.4444, max: 0.8241, stdev: 0.2105
External policy "individual greedy" average reward: 0.5274, min: -0.0648, max: 1.0185, stdev: 0.2122
External policy "total greedy" average reward: 0.6424, min: 0.1019, max: 1.2593, stdev: 0.2100
New network won 63 and tied 183 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 493 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.79 seconds
Training examples lengths: [64875, 65024, 64633, 64943, 64699, 64686, 64850, 64897, 64898, 64794]
Total value: 486147.62
Training on 648299 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1989 (value: 0.0009, weighted value: 0.0438, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9371
Epoch 2/10, Train Loss: 0.1921 (value: 0.0008, weighted value: 0.0401, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9377
Epoch 3/10, Train Loss: 0.1862 (value: 0.0007, weighted value: 0.0369, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9380
Epoch 4/10, Train Loss: 0.1852 (value: 0.0007, weighted value: 0.0371, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9382
Epoch 5/10, Train Loss: 0.1818 (value: 0.0007, weighted value: 0.0347, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9383
Epoch 6/10, Train Loss: 0.1795 (value: 0.0007, weighted value: 0.0334, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9384
Epoch 7/10, Train Loss: 0.1791 (value: 0.0007, weighted value: 0.0327, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9385
Epoch 8/10, Train Loss: 0.1791 (value: 0.0007, weighted value: 0.0328, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9386
Epoch 9/10, Train Loss: 0.1768 (value: 0.0006, weighted value: 0.0308, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9387
Epoch 10/10, Train Loss: 0.1770 (value: 0.0006, weighted value: 0.0309, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9388
..training done in 72.38 seconds
..evaluation done in 17.80 seconds
Old network+MCTS average reward: 0.7294, min: 0.1019, max: 1.5648, stdev: 0.2397
New network+MCTS average reward: 0.7301, min: 0.1111, max: 1.5648, stdev: 0.2399
Old bare network average reward: 0.6924, min: 0.1111, max: 1.5648, stdev: 0.2485
New bare network average reward: 0.6899, min: 0.1111, max: 1.5648, stdev: 0.2474
External policy "random" average reward: 0.2612, min: -0.4907, max: 1.0278, stdev: 0.2391
External policy "individual greedy" average reward: 0.5302, min: -0.0463, max: 1.4907, stdev: 0.2483
External policy "total greedy" average reward: 0.6406, min: 0.0648, max: 1.5463, stdev: 0.2340
New network won 67 and tied 172 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 494 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.12 seconds
Training examples lengths: [65024, 64633, 64943, 64699, 64686, 64850, 64897, 64898, 64794, 65052]
Total value: 486593.94
Training on 648476 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2000 (value: 0.0009, weighted value: 0.0428, policy: 0.1572, weighted policy: 0.1572), Train Mean Max: 0.9370
Epoch 2/10, Train Loss: 0.1916 (value: 0.0008, weighted value: 0.0394, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9378
Epoch 3/10, Train Loss: 0.1865 (value: 0.0007, weighted value: 0.0371, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9381
Epoch 4/10, Train Loss: 0.1833 (value: 0.0007, weighted value: 0.0354, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9383
Epoch 5/10, Train Loss: 0.1810 (value: 0.0007, weighted value: 0.0343, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9384
Epoch 6/10, Train Loss: 0.1804 (value: 0.0007, weighted value: 0.0335, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9384
Epoch 7/10, Train Loss: 0.1798 (value: 0.0007, weighted value: 0.0333, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9384
Epoch 8/10, Train Loss: 0.1775 (value: 0.0006, weighted value: 0.0311, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9387
Epoch 9/10, Train Loss: 0.1766 (value: 0.0006, weighted value: 0.0311, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9387
Epoch 10/10, Train Loss: 0.1759 (value: 0.0006, weighted value: 0.0302, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9388
..training done in 60.84 seconds
..evaluation done in 18.28 seconds
Old network+MCTS average reward: 0.7593, min: 0.1389, max: 1.5833, stdev: 0.2342
New network+MCTS average reward: 0.7600, min: 0.1389, max: 1.5833, stdev: 0.2325
Old bare network average reward: 0.7263, min: 0.0926, max: 1.5741, stdev: 0.2379
New bare network average reward: 0.7273, min: 0.0926, max: 1.5833, stdev: 0.2388
External policy "random" average reward: 0.2855, min: -0.2685, max: 1.2315, stdev: 0.2217
External policy "individual greedy" average reward: 0.5461, min: 0.0648, max: 1.5648, stdev: 0.2211
External policy "total greedy" average reward: 0.6653, min: 0.1481, max: 1.6019, stdev: 0.2202
New network won 64 and tied 169 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 495 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.11 seconds
Training examples lengths: [64633, 64943, 64699, 64686, 64850, 64897, 64898, 64794, 65052, 64904]
Total value: 485690.11
Training on 648356 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2205 (value: 0.0011, weighted value: 0.0555, policy: 0.1650, weighted policy: 0.1650), Train Mean Max: 0.9354
Epoch 2/10, Train Loss: 0.2077 (value: 0.0010, weighted value: 0.0499, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9363
Epoch 3/10, Train Loss: 0.1974 (value: 0.0009, weighted value: 0.0444, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9369
Epoch 4/10, Train Loss: 0.1936 (value: 0.0009, weighted value: 0.0428, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9370
Epoch 5/10, Train Loss: 0.1916 (value: 0.0008, weighted value: 0.0416, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9372
Epoch 6/10, Train Loss: 0.1862 (value: 0.0008, weighted value: 0.0378, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9376
Epoch 7/10, Train Loss: 0.1861 (value: 0.0008, weighted value: 0.0382, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9375
Epoch 8/10, Train Loss: 0.1844 (value: 0.0007, weighted value: 0.0358, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9376
Epoch 9/10, Train Loss: 0.1833 (value: 0.0007, weighted value: 0.0356, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0335, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9379
..training done in 59.53 seconds
..evaluation done in 18.44 seconds
Old network+MCTS average reward: 0.7192, min: 0.1111, max: 1.4167, stdev: 0.2244
New network+MCTS average reward: 0.7186, min: 0.1111, max: 1.4167, stdev: 0.2219
Old bare network average reward: 0.6924, min: 0.1111, max: 1.4167, stdev: 0.2228
New bare network average reward: 0.6873, min: 0.0370, max: 1.4167, stdev: 0.2253
External policy "random" average reward: 0.2431, min: -0.4907, max: 0.9259, stdev: 0.2265
External policy "individual greedy" average reward: 0.5266, min: -0.0741, max: 1.2315, stdev: 0.2269
External policy "total greedy" average reward: 0.6415, min: 0.0093, max: 1.2593, stdev: 0.2108
New network won 65 and tied 175 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 496 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.79 seconds
Training examples lengths: [64943, 64699, 64686, 64850, 64897, 64898, 64794, 65052, 64904, 64882]
Total value: 486565.74
Training on 648605 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2023 (value: 0.0009, weighted value: 0.0470, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9366
Epoch 2/10, Train Loss: 0.1962 (value: 0.0009, weighted value: 0.0436, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9371
Epoch 3/10, Train Loss: 0.1901 (value: 0.0008, weighted value: 0.0398, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9375
Epoch 4/10, Train Loss: 0.1865 (value: 0.0008, weighted value: 0.0379, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9377
Epoch 5/10, Train Loss: 0.1849 (value: 0.0007, weighted value: 0.0373, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9379
Epoch 6/10, Train Loss: 0.1841 (value: 0.0007, weighted value: 0.0362, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9378
Epoch 7/10, Train Loss: 0.1822 (value: 0.0007, weighted value: 0.0348, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9380
Epoch 8/10, Train Loss: 0.1800 (value: 0.0007, weighted value: 0.0333, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9383
Epoch 9/10, Train Loss: 0.1803 (value: 0.0007, weighted value: 0.0331, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9383
Epoch 10/10, Train Loss: 0.1787 (value: 0.0006, weighted value: 0.0323, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9385
..training done in 60.18 seconds
..evaluation done in 18.48 seconds
Old network+MCTS average reward: 0.7470, min: 0.1667, max: 1.4815, stdev: 0.2366
New network+MCTS average reward: 0.7452, min: 0.1667, max: 1.4815, stdev: 0.2350
Old bare network average reward: 0.7148, min: 0.1296, max: 1.4815, stdev: 0.2380
New bare network average reward: 0.7142, min: 0.1296, max: 1.4815, stdev: 0.2403
External policy "random" average reward: 0.2678, min: -0.3889, max: 0.8704, stdev: 0.2185
External policy "individual greedy" average reward: 0.5460, min: -0.0093, max: 1.1667, stdev: 0.2181
External policy "total greedy" average reward: 0.6609, min: 0.1759, max: 1.2778, stdev: 0.2114
New network won 52 and tied 191 out of 300 games (49.17% wins where ties are half wins)
Reverting to the old network

Training iteration 497 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.39 seconds
Training examples lengths: [64699, 64686, 64850, 64897, 64898, 64794, 65052, 64904, 64882, 64671]
Total value: 486941.67
Training on 648333 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2234 (value: 0.0012, weighted value: 0.0589, policy: 0.1645, weighted policy: 0.1645), Train Mean Max: 0.9350
Epoch 2/10, Train Loss: 0.2096 (value: 0.0010, weighted value: 0.0525, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9361
Epoch 3/10, Train Loss: 0.2018 (value: 0.0010, weighted value: 0.0481, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9363
Epoch 4/10, Train Loss: 0.1958 (value: 0.0009, weighted value: 0.0446, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9368
Epoch 5/10, Train Loss: 0.1939 (value: 0.0009, weighted value: 0.0436, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1895 (value: 0.0008, weighted value: 0.0409, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9372
Epoch 7/10, Train Loss: 0.1889 (value: 0.0008, weighted value: 0.0402, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9371
Epoch 8/10, Train Loss: 0.1879 (value: 0.0008, weighted value: 0.0384, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9373
Epoch 9/10, Train Loss: 0.1843 (value: 0.0007, weighted value: 0.0364, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1842 (value: 0.0007, weighted value: 0.0360, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9377
..training done in 60.66 seconds
..evaluation done in 19.13 seconds
Old network+MCTS average reward: 0.7469, min: 0.0833, max: 1.4352, stdev: 0.2289
New network+MCTS average reward: 0.7465, min: 0.0648, max: 1.3611, stdev: 0.2318
Old bare network average reward: 0.7060, min: 0.0370, max: 1.3241, stdev: 0.2352
New bare network average reward: 0.7094, min: 0.0370, max: 1.3056, stdev: 0.2332
External policy "random" average reward: 0.2584, min: -0.3426, max: 0.8611, stdev: 0.2278
External policy "individual greedy" average reward: 0.5457, min: -0.0463, max: 1.2222, stdev: 0.2337
External policy "total greedy" average reward: 0.6554, min: 0.0370, max: 1.3981, stdev: 0.2209
New network won 71 and tied 155 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 498 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.73 seconds
Training examples lengths: [64686, 64850, 64897, 64898, 64794, 65052, 64904, 64882, 64671, 65125]
Total value: 487222.29
Training on 648759 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2430 (value: 0.0014, weighted value: 0.0719, policy: 0.1711, weighted policy: 0.1711), Train Mean Max: 0.9336
Epoch 2/10, Train Loss: 0.2248 (value: 0.0013, weighted value: 0.0626, policy: 0.1622, weighted policy: 0.1622), Train Mean Max: 0.9347
Epoch 3/10, Train Loss: 0.2105 (value: 0.0011, weighted value: 0.0549, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9354
Epoch 4/10, Train Loss: 0.2064 (value: 0.0011, weighted value: 0.0526, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9355
Epoch 5/10, Train Loss: 0.2011 (value: 0.0010, weighted value: 0.0489, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9359
Epoch 6/10, Train Loss: 0.1971 (value: 0.0009, weighted value: 0.0461, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9362
Epoch 7/10, Train Loss: 0.1953 (value: 0.0009, weighted value: 0.0443, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9364
Epoch 8/10, Train Loss: 0.1932 (value: 0.0009, weighted value: 0.0427, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9365
Epoch 9/10, Train Loss: 0.1907 (value: 0.0008, weighted value: 0.0416, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9367
Epoch 10/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0394, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9370
..training done in 59.24 seconds
..evaluation done in 18.06 seconds
Old network+MCTS average reward: 0.7460, min: 0.2130, max: 1.5185, stdev: 0.2244
New network+MCTS average reward: 0.7468, min: 0.2130, max: 1.5741, stdev: 0.2230
Old bare network average reward: 0.7162, min: 0.1944, max: 1.4537, stdev: 0.2254
New bare network average reward: 0.7143, min: 0.2222, max: 1.4444, stdev: 0.2227
External policy "random" average reward: 0.2608, min: -0.3519, max: 1.0463, stdev: 0.2321
External policy "individual greedy" average reward: 0.5419, min: 0.0093, max: 1.3148, stdev: 0.2156
External policy "total greedy" average reward: 0.6558, min: 0.2130, max: 1.3704, stdev: 0.2087
New network won 69 and tied 160 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 499 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.68 seconds
Training examples lengths: [64850, 64897, 64898, 64794, 65052, 64904, 64882, 64671, 65125, 65102]
Total value: 487895.35
Training on 649175 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2628 (value: 0.0017, weighted value: 0.0856, policy: 0.1773, weighted policy: 0.1773), Train Mean Max: 0.9322
Epoch 2/10, Train Loss: 0.2346 (value: 0.0014, weighted value: 0.0689, policy: 0.1657, weighted policy: 0.1657), Train Mean Max: 0.9337
Epoch 3/10, Train Loss: 0.2216 (value: 0.0013, weighted value: 0.0627, policy: 0.1589, weighted policy: 0.1589), Train Mean Max: 0.9345
Epoch 4/10, Train Loss: 0.2138 (value: 0.0012, weighted value: 0.0577, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9348
Epoch 5/10, Train Loss: 0.2081 (value: 0.0011, weighted value: 0.0542, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9351
Epoch 6/10, Train Loss: 0.2038 (value: 0.0010, weighted value: 0.0507, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9354
Epoch 7/10, Train Loss: 0.2003 (value: 0.0010, weighted value: 0.0491, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9357
Epoch 8/10, Train Loss: 0.1989 (value: 0.0009, weighted value: 0.0471, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9357
Epoch 9/10, Train Loss: 0.1941 (value: 0.0009, weighted value: 0.0434, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9361
Epoch 10/10, Train Loss: 0.1941 (value: 0.0009, weighted value: 0.0439, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9363
..training done in 59.69 seconds
..evaluation done in 18.42 seconds
Old network+MCTS average reward: 0.7653, min: 0.1852, max: 1.4167, stdev: 0.2242
New network+MCTS average reward: 0.7648, min: 0.1944, max: 1.4167, stdev: 0.2276
Old bare network average reward: 0.7291, min: 0.1852, max: 1.3981, stdev: 0.2285
New bare network average reward: 0.7304, min: 0.1296, max: 1.4352, stdev: 0.2290
External policy "random" average reward: 0.2893, min: -0.1852, max: 1.0648, stdev: 0.2082
External policy "individual greedy" average reward: 0.5565, min: 0.0185, max: 1.3704, stdev: 0.2181
External policy "total greedy" average reward: 0.6753, min: 0.1574, max: 1.3889, stdev: 0.2182
New network won 66 and tied 166 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 500 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.16 seconds
Training examples lengths: [64897, 64898, 64794, 65052, 64904, 64882, 64671, 65125, 65102, 64707]
Total value: 487342.78
Training on 649032 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2786 (value: 0.0019, weighted value: 0.0962, policy: 0.1824, weighted policy: 0.1824), Train Mean Max: 0.9311
Epoch 2/10, Train Loss: 0.2475 (value: 0.0016, weighted value: 0.0784, policy: 0.1691, weighted policy: 0.1691), Train Mean Max: 0.9329
Epoch 3/10, Train Loss: 0.2332 (value: 0.0014, weighted value: 0.0705, policy: 0.1627, weighted policy: 0.1627), Train Mean Max: 0.9334
Epoch 4/10, Train Loss: 0.2208 (value: 0.0013, weighted value: 0.0629, policy: 0.1580, weighted policy: 0.1580), Train Mean Max: 0.9339
Epoch 5/10, Train Loss: 0.2175 (value: 0.0012, weighted value: 0.0614, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9341
Epoch 6/10, Train Loss: 0.2107 (value: 0.0011, weighted value: 0.0559, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9346
Epoch 7/10, Train Loss: 0.2062 (value: 0.0011, weighted value: 0.0526, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9347
Epoch 8/10, Train Loss: 0.2026 (value: 0.0010, weighted value: 0.0503, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9351
Epoch 9/10, Train Loss: 0.1996 (value: 0.0010, weighted value: 0.0476, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9353
Epoch 10/10, Train Loss: 0.1970 (value: 0.0009, weighted value: 0.0459, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9356
..training done in 60.36 seconds
..evaluation done in 18.25 seconds
Old network+MCTS average reward: 0.7494, min: 0.0741, max: 1.4722, stdev: 0.2440
New network+MCTS average reward: 0.7543, min: 0.0648, max: 1.4907, stdev: 0.2429
Old bare network average reward: 0.7162, min: 0.0741, max: 1.4722, stdev: 0.2460
New bare network average reward: 0.7181, min: 0.0556, max: 1.4630, stdev: 0.2462
External policy "random" average reward: 0.2673, min: -0.2593, max: 0.9259, stdev: 0.2192
External policy "individual greedy" average reward: 0.5497, min: 0.0185, max: 1.3056, stdev: 0.2309
External policy "total greedy" average reward: 0.6663, min: 0.0741, max: 1.3056, stdev: 0.2259
New network won 79 and tied 160 out of 300 games (53.00% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_500

Training iteration 501 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.19 seconds
Training examples lengths: [64898, 64794, 65052, 64904, 64882, 64671, 65125, 65102, 64707, 64917]
Total value: 487718.21
Training on 649052 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2167 (value: 0.0012, weighted value: 0.0576, policy: 0.1591, weighted policy: 0.1591), Train Mean Max: 0.9343
Epoch 2/10, Train Loss: 0.2075 (value: 0.0010, weighted value: 0.0517, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9350
Epoch 3/10, Train Loss: 0.2025 (value: 0.0010, weighted value: 0.0491, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9353
Epoch 4/10, Train Loss: 0.1986 (value: 0.0009, weighted value: 0.0466, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9355
Epoch 5/10, Train Loss: 0.1957 (value: 0.0009, weighted value: 0.0449, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9358
Epoch 6/10, Train Loss: 0.1940 (value: 0.0009, weighted value: 0.0436, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9360
Epoch 7/10, Train Loss: 0.1911 (value: 0.0008, weighted value: 0.0409, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9363
Epoch 8/10, Train Loss: 0.1909 (value: 0.0008, weighted value: 0.0407, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9365
Epoch 9/10, Train Loss: 0.1888 (value: 0.0008, weighted value: 0.0391, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9367
Epoch 10/10, Train Loss: 0.1858 (value: 0.0007, weighted value: 0.0375, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9370
..training done in 61.54 seconds
..evaluation done in 18.48 seconds
Old network+MCTS average reward: 0.7515, min: 0.1852, max: 1.3519, stdev: 0.2217
New network+MCTS average reward: 0.7540, min: 0.1481, max: 1.3519, stdev: 0.2233
Old bare network average reward: 0.7157, min: 0.0741, max: 1.3519, stdev: 0.2301
New bare network average reward: 0.7152, min: 0.0556, max: 1.3519, stdev: 0.2280
External policy "random" average reward: 0.2681, min: -0.3611, max: 0.8333, stdev: 0.2144
External policy "individual greedy" average reward: 0.5468, min: 0.0278, max: 1.3241, stdev: 0.2150
External policy "total greedy" average reward: 0.6601, min: 0.0926, max: 1.2778, stdev: 0.2101
New network won 67 and tied 174 out of 300 games (51.33% wins where ties are half wins)
Keeping the new network

Training iteration 502 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.54 seconds
Training examples lengths: [64794, 65052, 64904, 64882, 64671, 65125, 65102, 64707, 64917, 64659]
Total value: 487772.98
Training on 648813 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2081 (value: 0.0010, weighted value: 0.0506, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9353
Epoch 2/10, Train Loss: 0.1982 (value: 0.0009, weighted value: 0.0445, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9361
Epoch 3/10, Train Loss: 0.1938 (value: 0.0009, weighted value: 0.0426, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9364
Epoch 4/10, Train Loss: 0.1912 (value: 0.0008, weighted value: 0.0409, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9366
Epoch 5/10, Train Loss: 0.1883 (value: 0.0008, weighted value: 0.0394, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9368
Epoch 6/10, Train Loss: 0.1877 (value: 0.0008, weighted value: 0.0388, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9370
Epoch 7/10, Train Loss: 0.1867 (value: 0.0007, weighted value: 0.0375, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9371
Epoch 8/10, Train Loss: 0.1841 (value: 0.0007, weighted value: 0.0360, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1833 (value: 0.0007, weighted value: 0.0355, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9375
Epoch 10/10, Train Loss: 0.1825 (value: 0.0007, weighted value: 0.0350, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9377
..training done in 67.69 seconds
..evaluation done in 23.63 seconds
Old network+MCTS average reward: 0.7613, min: 0.1111, max: 1.3889, stdev: 0.2264
New network+MCTS average reward: 0.7586, min: 0.1852, max: 1.4259, stdev: 0.2265
Old bare network average reward: 0.7276, min: 0.1111, max: 1.3889, stdev: 0.2353
New bare network average reward: 0.7324, min: 0.0463, max: 1.3889, stdev: 0.2332
External policy "random" average reward: 0.2742, min: -0.3981, max: 0.9537, stdev: 0.2378
External policy "individual greedy" average reward: 0.5507, min: -0.0463, max: 1.3981, stdev: 0.2264
External policy "total greedy" average reward: 0.6690, min: 0.1019, max: 1.4352, stdev: 0.2180
New network won 60 and tied 175 out of 300 games (49.17% wins where ties are half wins)
Reverting to the old network

Training iteration 503 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 60.58 seconds
Training examples lengths: [65052, 64904, 64882, 64671, 65125, 65102, 64707, 64917, 64659, 64895]
Total value: 487553.00
Training on 648914 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2270 (value: 0.0012, weighted value: 0.0618, policy: 0.1652, weighted policy: 0.1652), Train Mean Max: 0.9339
Epoch 2/10, Train Loss: 0.2126 (value: 0.0011, weighted value: 0.0534, policy: 0.1592, weighted policy: 0.1592), Train Mean Max: 0.9348
Epoch 3/10, Train Loss: 0.2047 (value: 0.0010, weighted value: 0.0502, policy: 0.1545, weighted policy: 0.1545), Train Mean Max: 0.9354
Epoch 4/10, Train Loss: 0.1991 (value: 0.0009, weighted value: 0.0471, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9357
Epoch 5/10, Train Loss: 0.1975 (value: 0.0009, weighted value: 0.0457, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9359
Epoch 6/10, Train Loss: 0.1927 (value: 0.0009, weighted value: 0.0425, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9362
Epoch 7/10, Train Loss: 0.1929 (value: 0.0009, weighted value: 0.0432, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9363
Epoch 8/10, Train Loss: 0.1887 (value: 0.0008, weighted value: 0.0395, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9367
Epoch 9/10, Train Loss: 0.1881 (value: 0.0008, weighted value: 0.0389, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9368
Epoch 10/10, Train Loss: 0.1871 (value: 0.0008, weighted value: 0.0388, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9369
..training done in 69.59 seconds
..evaluation done in 18.63 seconds
Old network+MCTS average reward: 0.7285, min: 0.1944, max: 1.3981, stdev: 0.2113
New network+MCTS average reward: 0.7298, min: 0.1944, max: 1.4259, stdev: 0.2137
Old bare network average reward: 0.6933, min: 0.0000, max: 1.3889, stdev: 0.2236
New bare network average reward: 0.6969, min: 0.0000, max: 1.3611, stdev: 0.2219
External policy "random" average reward: 0.2432, min: -0.3426, max: 0.8519, stdev: 0.2150
External policy "individual greedy" average reward: 0.5232, min: -0.0370, max: 1.2315, stdev: 0.2190
External policy "total greedy" average reward: 0.6343, min: 0.1389, max: 1.2130, stdev: 0.2100
New network won 61 and tied 180 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 504 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.38 seconds
Training examples lengths: [64904, 64882, 64671, 65125, 65102, 64707, 64917, 64659, 64895, 64463]
Total value: 486772.06
Training on 648325 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2064 (value: 0.0010, weighted value: 0.0497, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9356
Epoch 2/10, Train Loss: 0.1986 (value: 0.0009, weighted value: 0.0444, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9362
Epoch 3/10, Train Loss: 0.1943 (value: 0.0008, weighted value: 0.0420, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9365
Epoch 4/10, Train Loss: 0.1925 (value: 0.0008, weighted value: 0.0423, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9368
Epoch 5/10, Train Loss: 0.1875 (value: 0.0008, weighted value: 0.0384, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9370
Epoch 6/10, Train Loss: 0.1866 (value: 0.0008, weighted value: 0.0381, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9372
Epoch 7/10, Train Loss: 0.1859 (value: 0.0008, weighted value: 0.0377, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1834 (value: 0.0007, weighted value: 0.0351, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1821 (value: 0.0007, weighted value: 0.0344, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9376
Epoch 10/10, Train Loss: 0.1827 (value: 0.0007, weighted value: 0.0351, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9377
..training done in 59.36 seconds
..evaluation done in 19.34 seconds
Old network+MCTS average reward: 0.7275, min: 0.0556, max: 1.3981, stdev: 0.2236
New network+MCTS average reward: 0.7255, min: 0.0556, max: 1.3704, stdev: 0.2227
Old bare network average reward: 0.6924, min: -0.0370, max: 1.3704, stdev: 0.2249
New bare network average reward: 0.6896, min: 0.0093, max: 1.3981, stdev: 0.2231
External policy "random" average reward: 0.2307, min: -0.3333, max: 0.7963, stdev: 0.2210
External policy "individual greedy" average reward: 0.5133, min: -0.0463, max: 1.1481, stdev: 0.2269
External policy "total greedy" average reward: 0.6352, min: 0.0926, max: 1.2130, stdev: 0.2189
New network won 66 and tied 163 out of 300 games (49.17% wins where ties are half wins)
Reverting to the old network

Training iteration 505 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.36 seconds
Training examples lengths: [64882, 64671, 65125, 65102, 64707, 64917, 64659, 64895, 64463, 64708]
Total value: 487063.37
Training on 648129 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2266 (value: 0.0012, weighted value: 0.0614, policy: 0.1653, weighted policy: 0.1653), Train Mean Max: 0.9341
Epoch 2/10, Train Loss: 0.2121 (value: 0.0011, weighted value: 0.0538, policy: 0.1582, weighted policy: 0.1582), Train Mean Max: 0.9353
Epoch 3/10, Train Loss: 0.2055 (value: 0.0010, weighted value: 0.0499, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9355
Epoch 4/10, Train Loss: 0.1993 (value: 0.0009, weighted value: 0.0467, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9359
Epoch 5/10, Train Loss: 0.1975 (value: 0.0009, weighted value: 0.0463, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9360
Epoch 6/10, Train Loss: 0.1930 (value: 0.0008, weighted value: 0.0422, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9363
Epoch 7/10, Train Loss: 0.1917 (value: 0.0008, weighted value: 0.0414, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9365
Epoch 8/10, Train Loss: 0.1889 (value: 0.0008, weighted value: 0.0396, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9366
Epoch 9/10, Train Loss: 0.1883 (value: 0.0008, weighted value: 0.0392, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9369
Epoch 10/10, Train Loss: 0.1867 (value: 0.0008, weighted value: 0.0377, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9371
..training done in 60.25 seconds
..evaluation done in 18.86 seconds
Old network+MCTS average reward: 0.7231, min: 0.1019, max: 1.5648, stdev: 0.2501
New network+MCTS average reward: 0.7246, min: 0.0741, max: 1.5648, stdev: 0.2498
Old bare network average reward: 0.6878, min: 0.0741, max: 1.5648, stdev: 0.2557
New bare network average reward: 0.6917, min: 0.0741, max: 1.4537, stdev: 0.2528
External policy "random" average reward: 0.2419, min: -0.3056, max: 0.9167, stdev: 0.2217
External policy "individual greedy" average reward: 0.5184, min: -0.0648, max: 1.2407, stdev: 0.2378
External policy "total greedy" average reward: 0.6265, min: 0.0833, max: 1.3426, stdev: 0.2416
New network won 68 and tied 171 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 506 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.52 seconds
Training examples lengths: [64671, 65125, 65102, 64707, 64917, 64659, 64895, 64463, 64708, 64880]
Total value: 486517.06
Training on 648127 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2059 (value: 0.0010, weighted value: 0.0493, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9358
Epoch 2/10, Train Loss: 0.1987 (value: 0.0009, weighted value: 0.0447, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9363
Epoch 3/10, Train Loss: 0.1945 (value: 0.0009, weighted value: 0.0430, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9368
Epoch 4/10, Train Loss: 0.1915 (value: 0.0008, weighted value: 0.0410, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9368
Epoch 5/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0399, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9370
Epoch 6/10, Train Loss: 0.1855 (value: 0.0007, weighted value: 0.0372, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9374
Epoch 7/10, Train Loss: 0.1862 (value: 0.0008, weighted value: 0.0378, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1822 (value: 0.0007, weighted value: 0.0346, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9376
Epoch 9/10, Train Loss: 0.1832 (value: 0.0007, weighted value: 0.0353, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1807 (value: 0.0007, weighted value: 0.0340, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9380
..training done in 59.83 seconds
..evaluation done in 18.78 seconds
Old network+MCTS average reward: 0.7502, min: 0.1296, max: 1.4815, stdev: 0.2295
New network+MCTS average reward: 0.7488, min: 0.1667, max: 1.4815, stdev: 0.2267
Old bare network average reward: 0.7132, min: 0.1204, max: 1.4352, stdev: 0.2303
New bare network average reward: 0.7176, min: 0.1204, max: 1.4352, stdev: 0.2276
External policy "random" average reward: 0.2813, min: -0.3056, max: 0.9630, stdev: 0.2176
External policy "individual greedy" average reward: 0.5493, min: 0.0278, max: 1.3426, stdev: 0.2185
External policy "total greedy" average reward: 0.6558, min: 0.1296, max: 1.3519, stdev: 0.2157
New network won 61 and tied 178 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 507 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.39 seconds
Training examples lengths: [65125, 65102, 64707, 64917, 64659, 64895, 64463, 64708, 64880, 64662]
Total value: 486152.52
Training on 648118 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2030 (value: 0.0009, weighted value: 0.0457, policy: 0.1572, weighted policy: 0.1572), Train Mean Max: 0.9362
Epoch 2/10, Train Loss: 0.1950 (value: 0.0008, weighted value: 0.0415, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9369
Epoch 3/10, Train Loss: 0.1910 (value: 0.0008, weighted value: 0.0408, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9374
Epoch 4/10, Train Loss: 0.1864 (value: 0.0007, weighted value: 0.0373, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1870 (value: 0.0008, weighted value: 0.0382, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9377
Epoch 6/10, Train Loss: 0.1818 (value: 0.0007, weighted value: 0.0340, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9380
Epoch 7/10, Train Loss: 0.1819 (value: 0.0007, weighted value: 0.0346, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9380
Epoch 8/10, Train Loss: 0.1825 (value: 0.0007, weighted value: 0.0347, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1793 (value: 0.0007, weighted value: 0.0329, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9383
Epoch 10/10, Train Loss: 0.1794 (value: 0.0007, weighted value: 0.0326, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9384
..training done in 60.77 seconds
..evaluation done in 18.85 seconds
Old network+MCTS average reward: 0.7510, min: 0.2315, max: 1.4722, stdev: 0.2354
New network+MCTS average reward: 0.7483, min: 0.2315, max: 1.5463, stdev: 0.2360
Old bare network average reward: 0.7142, min: 0.1389, max: 1.4907, stdev: 0.2419
New bare network average reward: 0.7133, min: 0.1481, max: 1.4907, stdev: 0.2414
External policy "random" average reward: 0.2509, min: -0.3611, max: 0.9815, stdev: 0.2377
External policy "individual greedy" average reward: 0.5313, min: -0.0833, max: 1.2407, stdev: 0.2316
External policy "total greedy" average reward: 0.6469, min: 0.1852, max: 1.5185, stdev: 0.2229
New network won 59 and tied 173 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 508 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.56 seconds
Training examples lengths: [65102, 64707, 64917, 64659, 64895, 64463, 64708, 64880, 64662, 64778]
Total value: 486073.77
Training on 647771 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2246 (value: 0.0012, weighted value: 0.0584, policy: 0.1662, weighted policy: 0.1662), Train Mean Max: 0.9346
Epoch 2/10, Train Loss: 0.2100 (value: 0.0010, weighted value: 0.0514, policy: 0.1587, weighted policy: 0.1587), Train Mean Max: 0.9360
Epoch 3/10, Train Loss: 0.2009 (value: 0.0009, weighted value: 0.0473, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9364
Epoch 4/10, Train Loss: 0.1964 (value: 0.0009, weighted value: 0.0452, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9367
Epoch 5/10, Train Loss: 0.1912 (value: 0.0008, weighted value: 0.0415, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1904 (value: 0.0008, weighted value: 0.0403, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1888 (value: 0.0008, weighted value: 0.0399, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1854 (value: 0.0007, weighted value: 0.0371, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1845 (value: 0.0007, weighted value: 0.0370, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9376
Epoch 10/10, Train Loss: 0.1843 (value: 0.0007, weighted value: 0.0367, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9378
..training done in 67.92 seconds
..evaluation done in 19.62 seconds
Old network+MCTS average reward: 0.7289, min: 0.1389, max: 1.3704, stdev: 0.2312
New network+MCTS average reward: 0.7311, min: 0.1296, max: 1.3704, stdev: 0.2305
Old bare network average reward: 0.6933, min: -0.0926, max: 1.3519, stdev: 0.2397
New bare network average reward: 0.6988, min: 0.0648, max: 1.3519, stdev: 0.2369
External policy "random" average reward: 0.2618, min: -0.5463, max: 1.0741, stdev: 0.2441
External policy "individual greedy" average reward: 0.5258, min: -0.1019, max: 1.1667, stdev: 0.2308
External policy "total greedy" average reward: 0.6484, min: 0.1111, max: 1.3056, stdev: 0.2255
New network won 61 and tied 174 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 509 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.01 seconds
Training examples lengths: [64707, 64917, 64659, 64895, 64463, 64708, 64880, 64662, 64778, 64851]
Total value: 486473.53
Training on 647520 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2411 (value: 0.0014, weighted value: 0.0689, policy: 0.1723, weighted policy: 0.1723), Train Mean Max: 0.9337
Epoch 2/10, Train Loss: 0.2212 (value: 0.0012, weighted value: 0.0592, policy: 0.1620, weighted policy: 0.1620), Train Mean Max: 0.9349
Epoch 3/10, Train Loss: 0.2103 (value: 0.0011, weighted value: 0.0538, policy: 0.1565, weighted policy: 0.1565), Train Mean Max: 0.9355
Epoch 4/10, Train Loss: 0.2031 (value: 0.0010, weighted value: 0.0497, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9358
Epoch 5/10, Train Loss: 0.1992 (value: 0.0010, weighted value: 0.0476, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9362
Epoch 6/10, Train Loss: 0.1963 (value: 0.0009, weighted value: 0.0452, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9365
Epoch 7/10, Train Loss: 0.1934 (value: 0.0009, weighted value: 0.0437, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9368
Epoch 8/10, Train Loss: 0.1906 (value: 0.0008, weighted value: 0.0417, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9369
Epoch 9/10, Train Loss: 0.1898 (value: 0.0008, weighted value: 0.0407, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9372
Epoch 10/10, Train Loss: 0.1877 (value: 0.0008, weighted value: 0.0385, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9372
..training done in 60.21 seconds
..evaluation done in 18.17 seconds
Old network+MCTS average reward: 0.7394, min: 0.0741, max: 1.4537, stdev: 0.2288
New network+MCTS average reward: 0.7409, min: 0.0926, max: 1.4444, stdev: 0.2281
Old bare network average reward: 0.7045, min: -0.0185, max: 1.4074, stdev: 0.2345
New bare network average reward: 0.7085, min: 0.0185, max: 1.4444, stdev: 0.2339
External policy "random" average reward: 0.2499, min: -0.3981, max: 0.9074, stdev: 0.2247
External policy "individual greedy" average reward: 0.5260, min: -0.0741, max: 1.3148, stdev: 0.2183
External policy "total greedy" average reward: 0.6485, min: -0.0278, max: 1.3519, stdev: 0.2176
New network won 80 and tied 146 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 510 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 61.30 seconds
Training examples lengths: [64917, 64659, 64895, 64463, 64708, 64880, 64662, 64778, 64851, 64823]
Total value: 487075.43
Training on 647636 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2074 (value: 0.0010, weighted value: 0.0498, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9359
Epoch 2/10, Train Loss: 0.2012 (value: 0.0009, weighted value: 0.0472, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9366
Epoch 3/10, Train Loss: 0.1935 (value: 0.0009, weighted value: 0.0425, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9370
Epoch 4/10, Train Loss: 0.1902 (value: 0.0008, weighted value: 0.0413, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9373
Epoch 5/10, Train Loss: 0.1888 (value: 0.0008, weighted value: 0.0402, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9373
Epoch 6/10, Train Loss: 0.1875 (value: 0.0008, weighted value: 0.0398, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9376
Epoch 7/10, Train Loss: 0.1847 (value: 0.0007, weighted value: 0.0368, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9378
Epoch 8/10, Train Loss: 0.1845 (value: 0.0007, weighted value: 0.0372, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9379
Epoch 9/10, Train Loss: 0.1818 (value: 0.0007, weighted value: 0.0344, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9381
Epoch 10/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0348, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9382
..training done in 71.69 seconds
..evaluation done in 19.60 seconds
Old network+MCTS average reward: 0.7484, min: 0.1019, max: 1.4907, stdev: 0.2193
New network+MCTS average reward: 0.7470, min: 0.2222, max: 1.5000, stdev: 0.2158
Old bare network average reward: 0.7084, min: 0.1759, max: 1.4074, stdev: 0.2208
New bare network average reward: 0.7086, min: 0.1944, max: 1.4074, stdev: 0.2168
External policy "random" average reward: 0.2697, min: -0.2685, max: 0.9444, stdev: 0.2172
External policy "individual greedy" average reward: 0.5400, min: 0.0185, max: 1.2500, stdev: 0.2096
External policy "total greedy" average reward: 0.6614, min: 0.1574, max: 1.2778, stdev: 0.2045
New network won 66 and tied 161 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 511 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.39 seconds
Training examples lengths: [64659, 64895, 64463, 64708, 64880, 64662, 64778, 64851, 64823, 64853]
Total value: 487132.70
Training on 647572 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2269 (value: 0.0012, weighted value: 0.0623, policy: 0.1646, weighted policy: 0.1646), Train Mean Max: 0.9345
Epoch 2/10, Train Loss: 0.2135 (value: 0.0011, weighted value: 0.0549, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9355
Epoch 3/10, Train Loss: 0.2037 (value: 0.0010, weighted value: 0.0501, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9361
Epoch 4/10, Train Loss: 0.1993 (value: 0.0010, weighted value: 0.0482, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9363
Epoch 5/10, Train Loss: 0.1959 (value: 0.0009, weighted value: 0.0455, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9365
Epoch 6/10, Train Loss: 0.1931 (value: 0.0009, weighted value: 0.0435, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9369
Epoch 7/10, Train Loss: 0.1914 (value: 0.0008, weighted value: 0.0425, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9370
Epoch 8/10, Train Loss: 0.1888 (value: 0.0008, weighted value: 0.0405, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9373
Epoch 9/10, Train Loss: 0.1872 (value: 0.0008, weighted value: 0.0391, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9374
Epoch 10/10, Train Loss: 0.1856 (value: 0.0008, weighted value: 0.0379, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9376
..training done in 63.69 seconds
..evaluation done in 23.57 seconds
Old network+MCTS average reward: 0.7485, min: 0.1852, max: 1.6389, stdev: 0.2379
New network+MCTS average reward: 0.7470, min: 0.1852, max: 1.6389, stdev: 0.2393
Old bare network average reward: 0.7131, min: 0.0741, max: 1.6389, stdev: 0.2410
New bare network average reward: 0.7145, min: 0.1667, max: 1.6389, stdev: 0.2371
External policy "random" average reward: 0.2669, min: -0.4815, max: 1.0000, stdev: 0.2219
External policy "individual greedy" average reward: 0.5494, min: -0.1019, max: 1.5833, stdev: 0.2313
External policy "total greedy" average reward: 0.6474, min: 0.0278, max: 1.4630, stdev: 0.2305
New network won 59 and tied 175 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 512 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 60.56 seconds
Training examples lengths: [64895, 64463, 64708, 64880, 64662, 64778, 64851, 64823, 64853, 64771]
Total value: 487150.44
Training on 647684 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2453 (value: 0.0015, weighted value: 0.0739, policy: 0.1714, weighted policy: 0.1714), Train Mean Max: 0.9332
Epoch 2/10, Train Loss: 0.2252 (value: 0.0013, weighted value: 0.0627, policy: 0.1626, weighted policy: 0.1626), Train Mean Max: 0.9345
Epoch 3/10, Train Loss: 0.2145 (value: 0.0012, weighted value: 0.0576, policy: 0.1569, weighted policy: 0.1569), Train Mean Max: 0.9351
Epoch 4/10, Train Loss: 0.2066 (value: 0.0011, weighted value: 0.0526, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9354
Epoch 5/10, Train Loss: 0.2020 (value: 0.0010, weighted value: 0.0500, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9358
Epoch 6/10, Train Loss: 0.2002 (value: 0.0010, weighted value: 0.0492, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9361
Epoch 7/10, Train Loss: 0.1947 (value: 0.0009, weighted value: 0.0445, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9363
Epoch 8/10, Train Loss: 0.1932 (value: 0.0009, weighted value: 0.0437, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9367
Epoch 9/10, Train Loss: 0.1908 (value: 0.0008, weighted value: 0.0415, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9368
Epoch 10/10, Train Loss: 0.1908 (value: 0.0008, weighted value: 0.0417, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9368
..training done in 74.44 seconds
..evaluation done in 27.42 seconds
Old network+MCTS average reward: 0.7777, min: 0.1204, max: 1.3981, stdev: 0.2332
New network+MCTS average reward: 0.7814, min: 0.1204, max: 1.3981, stdev: 0.2344
Old bare network average reward: 0.7459, min: 0.1204, max: 1.3981, stdev: 0.2370
New bare network average reward: 0.7430, min: 0.1204, max: 1.3981, stdev: 0.2430
External policy "random" average reward: 0.2839, min: -0.2500, max: 0.8611, stdev: 0.2115
External policy "individual greedy" average reward: 0.5657, min: -0.0833, max: 1.1574, stdev: 0.2272
External policy "total greedy" average reward: 0.6888, min: 0.0926, max: 1.3981, stdev: 0.2229
New network won 67 and tied 178 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network

Training iteration 513 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.73 seconds
Training examples lengths: [64463, 64708, 64880, 64662, 64778, 64851, 64823, 64853, 64771, 64816]
Total value: 487308.66
Training on 647605 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2091 (value: 0.0010, weighted value: 0.0516, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9355
Epoch 2/10, Train Loss: 0.2021 (value: 0.0010, weighted value: 0.0475, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9359
Epoch 3/10, Train Loss: 0.1955 (value: 0.0009, weighted value: 0.0440, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9365
Epoch 4/10, Train Loss: 0.1940 (value: 0.0009, weighted value: 0.0433, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9365
Epoch 5/10, Train Loss: 0.1908 (value: 0.0008, weighted value: 0.0408, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1875 (value: 0.0008, weighted value: 0.0389, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1873 (value: 0.0008, weighted value: 0.0391, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9372
Epoch 8/10, Train Loss: 0.1855 (value: 0.0008, weighted value: 0.0378, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1835 (value: 0.0007, weighted value: 0.0354, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9376
Epoch 10/10, Train Loss: 0.1831 (value: 0.0007, weighted value: 0.0359, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9379
..training done in 66.54 seconds
..evaluation done in 19.10 seconds
Old network+MCTS average reward: 0.7388, min: -0.0185, max: 1.6481, stdev: 0.2253
New network+MCTS average reward: 0.7375, min: -0.0278, max: 1.6019, stdev: 0.2278
Old bare network average reward: 0.7078, min: -0.0556, max: 1.6019, stdev: 0.2270
New bare network average reward: 0.7052, min: -0.0463, max: 1.6944, stdev: 0.2306
External policy "random" average reward: 0.2698, min: -0.3056, max: 1.1574, stdev: 0.2147
External policy "individual greedy" average reward: 0.5408, min: -0.1204, max: 1.5926, stdev: 0.2292
External policy "total greedy" average reward: 0.6585, min: -0.0093, max: 1.6111, stdev: 0.2181
New network won 60 and tied 159 out of 300 games (46.50% wins where ties are half wins)
Reverting to the old network

Training iteration 514 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.14 seconds
Training examples lengths: [64708, 64880, 64662, 64778, 64851, 64823, 64853, 64771, 64816, 65028]
Total value: 487991.82
Training on 648170 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2276 (value: 0.0013, weighted value: 0.0626, policy: 0.1650, weighted policy: 0.1650), Train Mean Max: 0.9341
Epoch 2/10, Train Loss: 0.2152 (value: 0.0011, weighted value: 0.0563, policy: 0.1589, weighted policy: 0.1589), Train Mean Max: 0.9352
Epoch 3/10, Train Loss: 0.2050 (value: 0.0010, weighted value: 0.0510, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9356
Epoch 4/10, Train Loss: 0.2008 (value: 0.0010, weighted value: 0.0483, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9359
Epoch 5/10, Train Loss: 0.1963 (value: 0.0009, weighted value: 0.0453, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9363
Epoch 6/10, Train Loss: 0.1948 (value: 0.0009, weighted value: 0.0445, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9364
Epoch 7/10, Train Loss: 0.1915 (value: 0.0008, weighted value: 0.0423, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9367
Epoch 8/10, Train Loss: 0.1901 (value: 0.0008, weighted value: 0.0406, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9367
Epoch 9/10, Train Loss: 0.1887 (value: 0.0008, weighted value: 0.0396, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9371
Epoch 10/10, Train Loss: 0.1866 (value: 0.0008, weighted value: 0.0382, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9372
..training done in 59.60 seconds
..evaluation done in 19.01 seconds
Old network+MCTS average reward: 0.7468, min: 0.1389, max: 1.4630, stdev: 0.2360
New network+MCTS average reward: 0.7467, min: 0.1481, max: 1.4630, stdev: 0.2359
Old bare network average reward: 0.7152, min: 0.1481, max: 1.4630, stdev: 0.2402
New bare network average reward: 0.7117, min: 0.1296, max: 1.4444, stdev: 0.2387
External policy "random" average reward: 0.2628, min: -0.5370, max: 1.3796, stdev: 0.2269
External policy "individual greedy" average reward: 0.5379, min: -0.0370, max: 1.3889, stdev: 0.2312
External policy "total greedy" average reward: 0.6538, min: 0.0463, max: 1.4444, stdev: 0.2227
New network won 78 and tied 162 out of 300 games (53.00% wins where ties are half wins)
Keeping the new network

Training iteration 515 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.12 seconds
Training examples lengths: [64880, 64662, 64778, 64851, 64823, 64853, 64771, 64816, 65028, 64861]
Total value: 488384.32
Training on 648323 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2056 (value: 0.0010, weighted value: 0.0492, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9359
Epoch 2/10, Train Loss: 0.1982 (value: 0.0009, weighted value: 0.0449, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9365
Epoch 3/10, Train Loss: 0.1926 (value: 0.0009, weighted value: 0.0428, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9369
Epoch 4/10, Train Loss: 0.1902 (value: 0.0008, weighted value: 0.0414, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9369
Epoch 5/10, Train Loss: 0.1868 (value: 0.0008, weighted value: 0.0389, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9374
Epoch 6/10, Train Loss: 0.1851 (value: 0.0007, weighted value: 0.0370, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9374
Epoch 7/10, Train Loss: 0.1853 (value: 0.0007, weighted value: 0.0374, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9376
Epoch 8/10, Train Loss: 0.1848 (value: 0.0007, weighted value: 0.0371, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1809 (value: 0.0007, weighted value: 0.0343, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9380
Epoch 10/10, Train Loss: 0.1804 (value: 0.0007, weighted value: 0.0339, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9381
..training done in 68.86 seconds
..evaluation done in 18.32 seconds
Old network+MCTS average reward: 0.7398, min: 0.1852, max: 1.3889, stdev: 0.2171
New network+MCTS average reward: 0.7427, min: 0.1852, max: 1.3889, stdev: 0.2166
Old bare network average reward: 0.7111, min: 0.1481, max: 1.3889, stdev: 0.2176
New bare network average reward: 0.7095, min: 0.1667, max: 1.3889, stdev: 0.2146
External policy "random" average reward: 0.2595, min: -0.3796, max: 0.8704, stdev: 0.2267
External policy "individual greedy" average reward: 0.5372, min: -0.0741, max: 1.2407, stdev: 0.2174
External policy "total greedy" average reward: 0.6540, min: -0.0463, max: 1.2685, stdev: 0.2093
New network won 70 and tied 179 out of 300 games (53.17% wins where ties are half wins)
Keeping the new network

Training iteration 516 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.47 seconds
Training examples lengths: [64662, 64778, 64851, 64823, 64853, 64771, 64816, 65028, 64861, 64542]
Total value: 487739.58
Training on 647985 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2018 (value: 0.0009, weighted value: 0.0455, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9364
Epoch 2/10, Train Loss: 0.1940 (value: 0.0008, weighted value: 0.0413, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9371
Epoch 3/10, Train Loss: 0.1903 (value: 0.0008, weighted value: 0.0404, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9375
Epoch 4/10, Train Loss: 0.1864 (value: 0.0008, weighted value: 0.0379, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1846 (value: 0.0007, weighted value: 0.0364, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9377
Epoch 6/10, Train Loss: 0.1827 (value: 0.0007, weighted value: 0.0354, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9381
Epoch 7/10, Train Loss: 0.1827 (value: 0.0007, weighted value: 0.0357, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9381
Epoch 8/10, Train Loss: 0.1797 (value: 0.0007, weighted value: 0.0330, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9382
Epoch 9/10, Train Loss: 0.1801 (value: 0.0007, weighted value: 0.0331, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1786 (value: 0.0006, weighted value: 0.0323, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9386
..training done in 59.89 seconds
..evaluation done in 17.96 seconds
Old network+MCTS average reward: 0.7326, min: 0.0278, max: 1.3704, stdev: 0.2340
New network+MCTS average reward: 0.7309, min: 0.0556, max: 1.3426, stdev: 0.2321
Old bare network average reward: 0.6959, min: -0.0093, max: 1.3426, stdev: 0.2351
New bare network average reward: 0.6985, min: -0.0093, max: 1.3426, stdev: 0.2339
External policy "random" average reward: 0.2610, min: -0.2870, max: 0.9444, stdev: 0.2408
External policy "individual greedy" average reward: 0.5318, min: -0.0556, max: 1.1574, stdev: 0.2308
External policy "total greedy" average reward: 0.6383, min: -0.0185, max: 1.2685, stdev: 0.2164
New network won 57 and tied 172 out of 300 games (47.67% wins where ties are half wins)
Reverting to the old network

Training iteration 517 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.93 seconds
Training examples lengths: [64778, 64851, 64823, 64853, 64771, 64816, 65028, 64861, 64542, 65006]
Total value: 488567.59
Training on 648329 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2212 (value: 0.0011, weighted value: 0.0563, policy: 0.1649, weighted policy: 0.1649), Train Mean Max: 0.9351
Epoch 2/10, Train Loss: 0.2079 (value: 0.0010, weighted value: 0.0509, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9362
Epoch 3/10, Train Loss: 0.2002 (value: 0.0009, weighted value: 0.0473, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9364
Epoch 4/10, Train Loss: 0.1943 (value: 0.0009, weighted value: 0.0433, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9367
Epoch 5/10, Train Loss: 0.1919 (value: 0.0008, weighted value: 0.0422, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9370
Epoch 6/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0403, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1880 (value: 0.0008, weighted value: 0.0389, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9374
Epoch 8/10, Train Loss: 0.1851 (value: 0.0007, weighted value: 0.0370, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1841 (value: 0.0007, weighted value: 0.0367, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9378
Epoch 10/10, Train Loss: 0.1824 (value: 0.0007, weighted value: 0.0351, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9379
..training done in 65.35 seconds
..evaluation done in 18.79 seconds
Old network+MCTS average reward: 0.7612, min: 0.2130, max: 1.7315, stdev: 0.2551
New network+MCTS average reward: 0.7618, min: 0.2130, max: 1.7037, stdev: 0.2556
Old bare network average reward: 0.7271, min: 0.1019, max: 1.6759, stdev: 0.2596
New bare network average reward: 0.7237, min: 0.2130, max: 1.6759, stdev: 0.2593
External policy "random" average reward: 0.2838, min: -0.3056, max: 1.1389, stdev: 0.2504
External policy "individual greedy" average reward: 0.5633, min: -0.1759, max: 1.5185, stdev: 0.2480
External policy "total greedy" average reward: 0.6751, min: 0.1111, max: 1.6759, stdev: 0.2444
New network won 64 and tied 177 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 518 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.46 seconds
Training examples lengths: [64851, 64823, 64853, 64771, 64816, 65028, 64861, 64542, 65006, 64776]
Total value: 488383.85
Training on 648327 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2033 (value: 0.0009, weighted value: 0.0465, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9363
Epoch 2/10, Train Loss: 0.1956 (value: 0.0008, weighted value: 0.0424, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9370
Epoch 3/10, Train Loss: 0.1903 (value: 0.0008, weighted value: 0.0398, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9375
Epoch 4/10, Train Loss: 0.1879 (value: 0.0008, weighted value: 0.0392, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1853 (value: 0.0007, weighted value: 0.0373, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9377
Epoch 6/10, Train Loss: 0.1826 (value: 0.0007, weighted value: 0.0354, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9379
Epoch 7/10, Train Loss: 0.1825 (value: 0.0007, weighted value: 0.0350, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1820 (value: 0.0007, weighted value: 0.0348, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1802 (value: 0.0007, weighted value: 0.0336, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9383
Epoch 10/10, Train Loss: 0.1788 (value: 0.0006, weighted value: 0.0324, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9384
..training done in 68.78 seconds
..evaluation done in 18.98 seconds
Old network+MCTS average reward: 0.7493, min: 0.1574, max: 1.5741, stdev: 0.2384
New network+MCTS average reward: 0.7517, min: 0.1204, max: 1.5741, stdev: 0.2405
Old bare network average reward: 0.7160, min: 0.0926, max: 1.5741, stdev: 0.2475
New bare network average reward: 0.7203, min: 0.1204, max: 1.5000, stdev: 0.2460
External policy "random" average reward: 0.2636, min: -0.4630, max: 1.1574, stdev: 0.2278
External policy "individual greedy" average reward: 0.5475, min: 0.0278, max: 1.4537, stdev: 0.2335
External policy "total greedy" average reward: 0.6592, min: 0.0741, max: 1.6204, stdev: 0.2313
New network won 71 and tied 177 out of 300 games (53.17% wins where ties are half wins)
Keeping the new network

Training iteration 519 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.76 seconds
Training examples lengths: [64823, 64853, 64771, 64816, 65028, 64861, 64542, 65006, 64776, 64738]
Total value: 487878.39
Training on 648214 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2000 (value: 0.0009, weighted value: 0.0439, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9366
Epoch 2/10, Train Loss: 0.1937 (value: 0.0008, weighted value: 0.0408, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9371
Epoch 3/10, Train Loss: 0.1886 (value: 0.0008, weighted value: 0.0384, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9376
Epoch 4/10, Train Loss: 0.1858 (value: 0.0007, weighted value: 0.0368, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9377
Epoch 5/10, Train Loss: 0.1845 (value: 0.0007, weighted value: 0.0365, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9380
Epoch 6/10, Train Loss: 0.1814 (value: 0.0007, weighted value: 0.0344, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9380
Epoch 7/10, Train Loss: 0.1801 (value: 0.0007, weighted value: 0.0334, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9382
Epoch 8/10, Train Loss: 0.1791 (value: 0.0006, weighted value: 0.0324, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9382
Epoch 9/10, Train Loss: 0.1785 (value: 0.0006, weighted value: 0.0318, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9382
Epoch 10/10, Train Loss: 0.1776 (value: 0.0006, weighted value: 0.0310, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9385
..training done in 58.81 seconds
..evaluation done in 17.53 seconds
Old network+MCTS average reward: 0.7465, min: 0.1852, max: 1.5370, stdev: 0.2338
New network+MCTS average reward: 0.7464, min: 0.2222, max: 1.5370, stdev: 0.2307
Old bare network average reward: 0.7151, min: 0.1574, max: 1.4907, stdev: 0.2326
New bare network average reward: 0.7162, min: 0.1574, max: 1.4907, stdev: 0.2325
External policy "random" average reward: 0.2585, min: -0.2593, max: 1.0741, stdev: 0.2165
External policy "individual greedy" average reward: 0.5364, min: -0.0741, max: 1.3519, stdev: 0.2230
External policy "total greedy" average reward: 0.6624, min: 0.1944, max: 1.4907, stdev: 0.2235
New network won 67 and tied 174 out of 300 games (51.33% wins where ties are half wins)
Keeping the new network

Training iteration 520 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.52 seconds
Training examples lengths: [64853, 64771, 64816, 65028, 64861, 64542, 65006, 64776, 64738, 64926]
Total value: 487688.99
Training on 648317 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2008 (value: 0.0009, weighted value: 0.0439, policy: 0.1569, weighted policy: 0.1569), Train Mean Max: 0.9369
Epoch 2/10, Train Loss: 0.1932 (value: 0.0008, weighted value: 0.0405, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9376
Epoch 3/10, Train Loss: 0.1861 (value: 0.0007, weighted value: 0.0368, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9380
Epoch 4/10, Train Loss: 0.1849 (value: 0.0007, weighted value: 0.0361, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9379
Epoch 5/10, Train Loss: 0.1822 (value: 0.0007, weighted value: 0.0351, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9381
Epoch 6/10, Train Loss: 0.1804 (value: 0.0007, weighted value: 0.0334, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9382
Epoch 7/10, Train Loss: 0.1800 (value: 0.0007, weighted value: 0.0330, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9384
Epoch 8/10, Train Loss: 0.1790 (value: 0.0006, weighted value: 0.0322, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1776 (value: 0.0006, weighted value: 0.0308, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9385
Epoch 10/10, Train Loss: 0.1774 (value: 0.0006, weighted value: 0.0313, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9386
..training done in 59.85 seconds
..evaluation done in 18.63 seconds
Old network+MCTS average reward: 0.7617, min: 0.2037, max: 1.5370, stdev: 0.2300
New network+MCTS average reward: 0.7604, min: 0.1759, max: 1.5648, stdev: 0.2337
Old bare network average reward: 0.7322, min: 0.1759, max: 1.5370, stdev: 0.2373
New bare network average reward: 0.7300, min: 0.1667, max: 1.5370, stdev: 0.2368
External policy "random" average reward: 0.2821, min: -0.3241, max: 1.0556, stdev: 0.2142
External policy "individual greedy" average reward: 0.5608, min: 0.0093, max: 1.3148, stdev: 0.2184
External policy "total greedy" average reward: 0.6720, min: 0.1019, max: 1.5093, stdev: 0.2116
New network won 68 and tied 175 out of 300 games (51.83% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_520

Training iteration 521 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.62 seconds
Training examples lengths: [64771, 64816, 65028, 64861, 64542, 65006, 64776, 64738, 64926, 64746]
Total value: 487468.19
Training on 648210 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1981 (value: 0.0009, weighted value: 0.0427, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9369
Epoch 2/10, Train Loss: 0.1924 (value: 0.0008, weighted value: 0.0393, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9374
Epoch 3/10, Train Loss: 0.1862 (value: 0.0007, weighted value: 0.0363, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9380
Epoch 4/10, Train Loss: 0.1833 (value: 0.0007, weighted value: 0.0355, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9382
Epoch 5/10, Train Loss: 0.1818 (value: 0.0007, weighted value: 0.0342, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9382
Epoch 6/10, Train Loss: 0.1797 (value: 0.0007, weighted value: 0.0331, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9383
Epoch 7/10, Train Loss: 0.1793 (value: 0.0007, weighted value: 0.0326, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9383
Epoch 8/10, Train Loss: 0.1783 (value: 0.0006, weighted value: 0.0319, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1762 (value: 0.0006, weighted value: 0.0296, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9386
Epoch 10/10, Train Loss: 0.1776 (value: 0.0006, weighted value: 0.0304, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9387
..training done in 59.83 seconds
..evaluation done in 18.30 seconds
Old network+MCTS average reward: 0.7316, min: 0.0278, max: 1.5000, stdev: 0.2402
New network+MCTS average reward: 0.7311, min: 0.0556, max: 1.5000, stdev: 0.2423
Old bare network average reward: 0.6947, min: 0.0278, max: 1.5000, stdev: 0.2449
New bare network average reward: 0.6941, min: 0.0278, max: 1.5000, stdev: 0.2454
External policy "random" average reward: 0.2501, min: -0.3981, max: 0.9259, stdev: 0.2167
External policy "individual greedy" average reward: 0.5247, min: -0.0278, max: 1.2593, stdev: 0.2258
External policy "total greedy" average reward: 0.6393, min: -0.0093, max: 1.4537, stdev: 0.2312
New network won 50 and tied 198 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 522 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.62 seconds
Training examples lengths: [64816, 65028, 64861, 64542, 65006, 64776, 64738, 64926, 64746, 64722]
Total value: 487649.56
Training on 648161 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2202 (value: 0.0011, weighted value: 0.0562, policy: 0.1639, weighted policy: 0.1639), Train Mean Max: 0.9354
Epoch 2/10, Train Loss: 0.2050 (value: 0.0009, weighted value: 0.0474, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9363
Epoch 3/10, Train Loss: 0.1984 (value: 0.0009, weighted value: 0.0452, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9368
Epoch 4/10, Train Loss: 0.1931 (value: 0.0009, weighted value: 0.0425, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9369
Epoch 5/10, Train Loss: 0.1901 (value: 0.0008, weighted value: 0.0409, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9371
Epoch 6/10, Train Loss: 0.1874 (value: 0.0008, weighted value: 0.0376, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9372
Epoch 7/10, Train Loss: 0.1859 (value: 0.0007, weighted value: 0.0370, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9374
Epoch 8/10, Train Loss: 0.1846 (value: 0.0007, weighted value: 0.0363, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9376
Epoch 9/10, Train Loss: 0.1823 (value: 0.0007, weighted value: 0.0347, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9378
Epoch 10/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0355, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9377
..training done in 60.35 seconds
..evaluation done in 20.69 seconds
Old network+MCTS average reward: 0.7045, min: 0.1481, max: 1.3241, stdev: 0.2240
New network+MCTS average reward: 0.7020, min: 0.0185, max: 1.3241, stdev: 0.2267
Old bare network average reward: 0.6697, min: 0.0185, max: 1.2685, stdev: 0.2303
New bare network average reward: 0.6696, min: 0.0185, max: 1.2685, stdev: 0.2282
External policy "random" average reward: 0.2322, min: -0.3981, max: 0.7778, stdev: 0.2278
External policy "individual greedy" average reward: 0.5175, min: -0.0463, max: 1.0556, stdev: 0.2213
External policy "total greedy" average reward: 0.6276, min: 0.1389, max: 1.2222, stdev: 0.2049
New network won 60 and tied 171 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 523 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.13 seconds
Training examples lengths: [65028, 64861, 64542, 65006, 64776, 64738, 64926, 64746, 64722, 64589]
Total value: 487866.26
Training on 647934 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2380 (value: 0.0014, weighted value: 0.0675, policy: 0.1705, weighted policy: 0.1705), Train Mean Max: 0.9341
Epoch 2/10, Train Loss: 0.2200 (value: 0.0011, weighted value: 0.0573, policy: 0.1628, weighted policy: 0.1628), Train Mean Max: 0.9352
Epoch 3/10, Train Loss: 0.2075 (value: 0.0010, weighted value: 0.0518, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9358
Epoch 4/10, Train Loss: 0.2011 (value: 0.0010, weighted value: 0.0484, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9362
Epoch 5/10, Train Loss: 0.1960 (value: 0.0009, weighted value: 0.0448, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9363
Epoch 6/10, Train Loss: 0.1935 (value: 0.0009, weighted value: 0.0434, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9366
Epoch 7/10, Train Loss: 0.1919 (value: 0.0008, weighted value: 0.0418, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9367
Epoch 8/10, Train Loss: 0.1899 (value: 0.0008, weighted value: 0.0396, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9369
Epoch 9/10, Train Loss: 0.1881 (value: 0.0008, weighted value: 0.0388, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9372
Epoch 10/10, Train Loss: 0.1854 (value: 0.0007, weighted value: 0.0373, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9373
..training done in 78.51 seconds
..evaluation done in 18.61 seconds
Old network+MCTS average reward: 0.7312, min: 0.1389, max: 1.4167, stdev: 0.2285
New network+MCTS average reward: 0.7317, min: 0.1204, max: 1.4167, stdev: 0.2290
Old bare network average reward: 0.6978, min: 0.1019, max: 1.3889, stdev: 0.2322
New bare network average reward: 0.6954, min: 0.1019, max: 1.4167, stdev: 0.2289
External policy "random" average reward: 0.2674, min: -0.3426, max: 0.8981, stdev: 0.2208
External policy "individual greedy" average reward: 0.5315, min: -0.1204, max: 1.1667, stdev: 0.2303
External policy "total greedy" average reward: 0.6438, min: -0.0556, max: 1.3426, stdev: 0.2256
New network won 74 and tied 164 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network

Training iteration 524 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.07 seconds
Training examples lengths: [64861, 64542, 65006, 64776, 64738, 64926, 64746, 64722, 64589, 64577]
Total value: 487849.39
Training on 647483 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2051 (value: 0.0010, weighted value: 0.0488, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9358
Epoch 2/10, Train Loss: 0.1981 (value: 0.0009, weighted value: 0.0440, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9365
Epoch 3/10, Train Loss: 0.1937 (value: 0.0008, weighted value: 0.0421, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9370
Epoch 4/10, Train Loss: 0.1912 (value: 0.0008, weighted value: 0.0414, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9371
Epoch 5/10, Train Loss: 0.1867 (value: 0.0008, weighted value: 0.0382, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9373
Epoch 6/10, Train Loss: 0.1853 (value: 0.0007, weighted value: 0.0375, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9374
Epoch 7/10, Train Loss: 0.1840 (value: 0.0007, weighted value: 0.0360, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9376
Epoch 8/10, Train Loss: 0.1821 (value: 0.0007, weighted value: 0.0342, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1832 (value: 0.0007, weighted value: 0.0352, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9378
Epoch 10/10, Train Loss: 0.1804 (value: 0.0007, weighted value: 0.0335, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9381
..training done in 60.26 seconds
..evaluation done in 19.14 seconds
Old network+MCTS average reward: 0.7430, min: -0.0185, max: 1.5093, stdev: 0.2401
New network+MCTS average reward: 0.7436, min: 0.2222, max: 1.5463, stdev: 0.2394
Old bare network average reward: 0.7097, min: -0.0185, max: 1.5093, stdev: 0.2483
New bare network average reward: 0.7071, min: 0.1481, max: 1.5185, stdev: 0.2476
External policy "random" average reward: 0.2450, min: -0.1852, max: 0.8241, stdev: 0.2045
External policy "individual greedy" average reward: 0.5247, min: 0.0370, max: 1.2778, stdev: 0.2207
External policy "total greedy" average reward: 0.6453, min: 0.1667, max: 1.4352, stdev: 0.2252
New network won 55 and tied 184 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 525 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.89 seconds
Training examples lengths: [64542, 65006, 64776, 64738, 64926, 64746, 64722, 64589, 64577, 65028]
Total value: 488135.12
Training on 647650 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2248 (value: 0.0012, weighted value: 0.0602, policy: 0.1646, weighted policy: 0.1646), Train Mean Max: 0.9344
Epoch 2/10, Train Loss: 0.2120 (value: 0.0010, weighted value: 0.0524, policy: 0.1597, weighted policy: 0.1597), Train Mean Max: 0.9354
Epoch 3/10, Train Loss: 0.2025 (value: 0.0010, weighted value: 0.0490, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9361
Epoch 4/10, Train Loss: 0.1972 (value: 0.0009, weighted value: 0.0461, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9364
Epoch 5/10, Train Loss: 0.1938 (value: 0.0009, weighted value: 0.0434, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9365
Epoch 6/10, Train Loss: 0.1912 (value: 0.0008, weighted value: 0.0418, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9368
Epoch 7/10, Train Loss: 0.1906 (value: 0.0008, weighted value: 0.0412, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9368
Epoch 8/10, Train Loss: 0.1872 (value: 0.0008, weighted value: 0.0387, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9371
Epoch 9/10, Train Loss: 0.1860 (value: 0.0007, weighted value: 0.0373, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9372
Epoch 10/10, Train Loss: 0.1856 (value: 0.0008, weighted value: 0.0375, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9375
..training done in 59.78 seconds
..evaluation done in 18.70 seconds
Old network+MCTS average reward: 0.7363, min: 0.1852, max: 1.5093, stdev: 0.2351
New network+MCTS average reward: 0.7337, min: 0.1296, max: 1.5093, stdev: 0.2353
Old bare network average reward: 0.7061, min: 0.1852, max: 1.5093, stdev: 0.2354
New bare network average reward: 0.7012, min: 0.0833, max: 1.4444, stdev: 0.2365
External policy "random" average reward: 0.2538, min: -0.4259, max: 0.8519, stdev: 0.2197
External policy "individual greedy" average reward: 0.5229, min: 0.0093, max: 1.1944, stdev: 0.2224
External policy "total greedy" average reward: 0.6371, min: 0.0833, max: 1.3426, stdev: 0.2103
New network won 65 and tied 161 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 526 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.84 seconds
Training examples lengths: [65006, 64776, 64738, 64926, 64746, 64722, 64589, 64577, 65028, 65031]
Total value: 489523.30
Training on 648139 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2431 (value: 0.0014, weighted value: 0.0719, policy: 0.1712, weighted policy: 0.1712), Train Mean Max: 0.9333
Epoch 2/10, Train Loss: 0.2227 (value: 0.0012, weighted value: 0.0601, policy: 0.1626, weighted policy: 0.1626), Train Mean Max: 0.9347
Epoch 3/10, Train Loss: 0.2132 (value: 0.0011, weighted value: 0.0562, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9352
Epoch 4/10, Train Loss: 0.2057 (value: 0.0010, weighted value: 0.0520, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9355
Epoch 5/10, Train Loss: 0.2014 (value: 0.0010, weighted value: 0.0488, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9358
Epoch 6/10, Train Loss: 0.1967 (value: 0.0009, weighted value: 0.0458, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9360
Epoch 7/10, Train Loss: 0.1942 (value: 0.0009, weighted value: 0.0441, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9362
Epoch 8/10, Train Loss: 0.1931 (value: 0.0009, weighted value: 0.0431, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9364
Epoch 9/10, Train Loss: 0.1902 (value: 0.0008, weighted value: 0.0405, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9367
Epoch 10/10, Train Loss: 0.1887 (value: 0.0008, weighted value: 0.0398, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9368
..training done in 68.38 seconds
..evaluation done in 21.82 seconds
Old network+MCTS average reward: 0.7531, min: 0.1944, max: 1.3796, stdev: 0.2406
New network+MCTS average reward: 0.7521, min: 0.1944, max: 1.3796, stdev: 0.2409
Old bare network average reward: 0.7217, min: 0.1852, max: 1.3611, stdev: 0.2434
New bare network average reward: 0.7210, min: 0.1204, max: 1.3611, stdev: 0.2417
External policy "random" average reward: 0.2697, min: -0.3704, max: 0.9630, stdev: 0.2306
External policy "individual greedy" average reward: 0.5409, min: -0.0370, max: 1.1111, stdev: 0.2252
External policy "total greedy" average reward: 0.6586, min: 0.1574, max: 1.3056, stdev: 0.2140
New network won 63 and tied 169 out of 300 games (49.17% wins where ties are half wins)
Reverting to the old network

Training iteration 527 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 60.28 seconds
Training examples lengths: [64776, 64738, 64926, 64746, 64722, 64589, 64577, 65028, 65031, 64904]
Total value: 489225.52
Training on 648037 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2607 (value: 0.0017, weighted value: 0.0826, policy: 0.1781, weighted policy: 0.1781), Train Mean Max: 0.9320
Epoch 2/10, Train Loss: 0.2347 (value: 0.0014, weighted value: 0.0680, policy: 0.1667, weighted policy: 0.1667), Train Mean Max: 0.9334
Epoch 3/10, Train Loss: 0.2217 (value: 0.0013, weighted value: 0.0626, policy: 0.1591, weighted policy: 0.1591), Train Mean Max: 0.9342
Epoch 4/10, Train Loss: 0.2123 (value: 0.0011, weighted value: 0.0562, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9345
Epoch 5/10, Train Loss: 0.2074 (value: 0.0011, weighted value: 0.0529, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9349
Epoch 6/10, Train Loss: 0.2042 (value: 0.0010, weighted value: 0.0507, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9351
Epoch 7/10, Train Loss: 0.2007 (value: 0.0010, weighted value: 0.0487, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9355
Epoch 8/10, Train Loss: 0.1966 (value: 0.0009, weighted value: 0.0456, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9355
Epoch 9/10, Train Loss: 0.1954 (value: 0.0009, weighted value: 0.0446, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9358
Epoch 10/10, Train Loss: 0.1934 (value: 0.0009, weighted value: 0.0432, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9361
..training done in 76.59 seconds
..evaluation done in 20.17 seconds
Old network+MCTS average reward: 0.7503, min: -0.0463, max: 1.5556, stdev: 0.2409
New network+MCTS average reward: 0.7507, min: -0.0463, max: 1.6019, stdev: 0.2416
Old bare network average reward: 0.7186, min: -0.1019, max: 1.5556, stdev: 0.2467
New bare network average reward: 0.7199, min: -0.1019, max: 1.6019, stdev: 0.2446
External policy "random" average reward: 0.2674, min: -0.3796, max: 1.1389, stdev: 0.2289
External policy "individual greedy" average reward: 0.5540, min: -0.1667, max: 1.4815, stdev: 0.2315
External policy "total greedy" average reward: 0.6554, min: -0.1019, max: 1.4907, stdev: 0.2246
New network won 62 and tied 177 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 528 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.97 seconds
Training examples lengths: [64738, 64926, 64746, 64722, 64589, 64577, 65028, 65031, 64904, 64884]
Total value: 489330.51
Training on 648145 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2111 (value: 0.0011, weighted value: 0.0526, policy: 0.1585, weighted policy: 0.1585), Train Mean Max: 0.9349
Epoch 2/10, Train Loss: 0.2027 (value: 0.0009, weighted value: 0.0474, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9355
Epoch 3/10, Train Loss: 0.1990 (value: 0.0009, weighted value: 0.0463, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9359
Epoch 4/10, Train Loss: 0.1940 (value: 0.0009, weighted value: 0.0429, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9361
Epoch 5/10, Train Loss: 0.1911 (value: 0.0008, weighted value: 0.0415, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9365
Epoch 6/10, Train Loss: 0.1892 (value: 0.0008, weighted value: 0.0400, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9365
Epoch 7/10, Train Loss: 0.1879 (value: 0.0008, weighted value: 0.0392, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9368
Epoch 8/10, Train Loss: 0.1866 (value: 0.0008, weighted value: 0.0381, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9370
Epoch 9/10, Train Loss: 0.1851 (value: 0.0007, weighted value: 0.0373, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9371
Epoch 10/10, Train Loss: 0.1838 (value: 0.0007, weighted value: 0.0361, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9374
..training done in 74.25 seconds
..evaluation done in 21.00 seconds
Old network+MCTS average reward: 0.7403, min: 0.1481, max: 1.6481, stdev: 0.2627
New network+MCTS average reward: 0.7436, min: 0.1481, max: 1.6481, stdev: 0.2622
Old bare network average reward: 0.7084, min: 0.0741, max: 1.6389, stdev: 0.2641
New bare network average reward: 0.7065, min: 0.0648, max: 1.6667, stdev: 0.2684
External policy "random" average reward: 0.2563, min: -0.3519, max: 0.9722, stdev: 0.2373
External policy "individual greedy" average reward: 0.5297, min: -0.0463, max: 1.5833, stdev: 0.2463
External policy "total greedy" average reward: 0.6399, min: 0.0185, max: 1.6481, stdev: 0.2413
New network won 70 and tied 176 out of 300 games (52.67% wins where ties are half wins)
Keeping the new network

Training iteration 529 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.57 seconds
Training examples lengths: [64926, 64746, 64722, 64589, 64577, 65028, 65031, 64904, 64884, 64870]
Total value: 489890.11
Training on 648277 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2038 (value: 0.0010, weighted value: 0.0475, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9358
Epoch 2/10, Train Loss: 0.1960 (value: 0.0009, weighted value: 0.0429, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9366
Epoch 3/10, Train Loss: 0.1913 (value: 0.0008, weighted value: 0.0408, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9368
Epoch 4/10, Train Loss: 0.1872 (value: 0.0008, weighted value: 0.0383, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9372
Epoch 5/10, Train Loss: 0.1869 (value: 0.0008, weighted value: 0.0385, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9372
Epoch 6/10, Train Loss: 0.1839 (value: 0.0007, weighted value: 0.0361, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9375
Epoch 7/10, Train Loss: 0.1835 (value: 0.0007, weighted value: 0.0362, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9376
Epoch 8/10, Train Loss: 0.1807 (value: 0.0007, weighted value: 0.0342, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1809 (value: 0.0007, weighted value: 0.0346, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1793 (value: 0.0006, weighted value: 0.0325, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9380
..training done in 73.57 seconds
..evaluation done in 19.97 seconds
Old network+MCTS average reward: 0.7682, min: 0.1481, max: 1.5648, stdev: 0.2543
New network+MCTS average reward: 0.7658, min: 0.1481, max: 1.5648, stdev: 0.2520
Old bare network average reward: 0.7339, min: 0.1111, max: 1.5648, stdev: 0.2580
New bare network average reward: 0.7384, min: 0.1111, max: 1.5648, stdev: 0.2552
External policy "random" average reward: 0.2837, min: -0.2870, max: 0.9630, stdev: 0.2319
External policy "individual greedy" average reward: 0.5618, min: -0.0741, max: 1.3426, stdev: 0.2566
External policy "total greedy" average reward: 0.6820, min: -0.0833, max: 1.3519, stdev: 0.2350
New network won 60 and tied 177 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 530 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.21 seconds
Training examples lengths: [64746, 64722, 64589, 64577, 65028, 65031, 64904, 64884, 64870, 64699]
Total value: 490369.49
Training on 648050 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2223 (value: 0.0012, weighted value: 0.0578, policy: 0.1645, weighted policy: 0.1645), Train Mean Max: 0.9342
Epoch 2/10, Train Loss: 0.2104 (value: 0.0010, weighted value: 0.0523, policy: 0.1581, weighted policy: 0.1581), Train Mean Max: 0.9353
Epoch 3/10, Train Loss: 0.2009 (value: 0.0009, weighted value: 0.0474, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9360
Epoch 4/10, Train Loss: 0.1952 (value: 0.0009, weighted value: 0.0443, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9364
Epoch 5/10, Train Loss: 0.1944 (value: 0.0009, weighted value: 0.0449, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9364
Epoch 6/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0403, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9367
Epoch 7/10, Train Loss: 0.1883 (value: 0.0008, weighted value: 0.0398, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9369
Epoch 8/10, Train Loss: 0.1869 (value: 0.0008, weighted value: 0.0385, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9371
Epoch 9/10, Train Loss: 0.1853 (value: 0.0007, weighted value: 0.0374, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9373
Epoch 10/10, Train Loss: 0.1833 (value: 0.0007, weighted value: 0.0358, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9376
..training done in 60.10 seconds
..evaluation done in 18.92 seconds
Old network+MCTS average reward: 0.7366, min: 0.1667, max: 1.2963, stdev: 0.2277
New network+MCTS average reward: 0.7391, min: 0.1667, max: 1.3426, stdev: 0.2259
Old bare network average reward: 0.7060, min: 0.1481, max: 1.2963, stdev: 0.2263
New bare network average reward: 0.7052, min: 0.1389, max: 1.2963, stdev: 0.2262
External policy "random" average reward: 0.2673, min: -0.3519, max: 0.8981, stdev: 0.2253
External policy "individual greedy" average reward: 0.5407, min: -0.0741, max: 1.1759, stdev: 0.2224
External policy "total greedy" average reward: 0.6581, min: 0.1204, max: 1.2593, stdev: 0.2200
New network won 74 and tied 171 out of 300 games (53.17% wins where ties are half wins)
Keeping the new network

Training iteration 531 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.26 seconds
Training examples lengths: [64722, 64589, 64577, 65028, 65031, 64904, 64884, 64870, 64699, 64612]
Total value: 490686.16
Training on 647916 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2040 (value: 0.0010, weighted value: 0.0478, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9363
Epoch 2/10, Train Loss: 0.1957 (value: 0.0009, weighted value: 0.0431, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9369
Epoch 3/10, Train Loss: 0.1908 (value: 0.0008, weighted value: 0.0404, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9373
Epoch 4/10, Train Loss: 0.1880 (value: 0.0008, weighted value: 0.0389, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9375
Epoch 5/10, Train Loss: 0.1848 (value: 0.0007, weighted value: 0.0375, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9378
Epoch 6/10, Train Loss: 0.1831 (value: 0.0007, weighted value: 0.0361, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9379
Epoch 7/10, Train Loss: 0.1822 (value: 0.0007, weighted value: 0.0351, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9380
Epoch 8/10, Train Loss: 0.1816 (value: 0.0007, weighted value: 0.0349, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9382
Epoch 9/10, Train Loss: 0.1793 (value: 0.0007, weighted value: 0.0330, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9383
Epoch 10/10, Train Loss: 0.1792 (value: 0.0007, weighted value: 0.0330, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9386
..training done in 71.19 seconds
..evaluation done in 20.83 seconds
Old network+MCTS average reward: 0.7609, min: 0.2130, max: 1.5278, stdev: 0.2312
New network+MCTS average reward: 0.7615, min: 0.2037, max: 1.5278, stdev: 0.2319
Old bare network average reward: 0.7281, min: 0.1019, max: 1.4630, stdev: 0.2363
New bare network average reward: 0.7259, min: 0.1389, max: 1.5093, stdev: 0.2316
External policy "random" average reward: 0.2729, min: -0.4167, max: 1.1019, stdev: 0.2256
External policy "individual greedy" average reward: 0.5485, min: -0.0833, max: 1.4259, stdev: 0.2298
External policy "total greedy" average reward: 0.6584, min: -0.0463, max: 1.5093, stdev: 0.2164
New network won 67 and tied 162 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 532 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.90 seconds
Training examples lengths: [64589, 64577, 65028, 65031, 64904, 64884, 64870, 64699, 64612, 64633]
Total value: 490714.39
Training on 647827 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2220 (value: 0.0012, weighted value: 0.0589, policy: 0.1631, weighted policy: 0.1631), Train Mean Max: 0.9350
Epoch 2/10, Train Loss: 0.2080 (value: 0.0010, weighted value: 0.0514, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9360
Epoch 3/10, Train Loss: 0.2016 (value: 0.0010, weighted value: 0.0479, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9363
Epoch 4/10, Train Loss: 0.1957 (value: 0.0009, weighted value: 0.0446, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9368
Epoch 5/10, Train Loss: 0.1932 (value: 0.0009, weighted value: 0.0431, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1894 (value: 0.0008, weighted value: 0.0407, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9370
Epoch 7/10, Train Loss: 0.1873 (value: 0.0008, weighted value: 0.0391, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1869 (value: 0.0008, weighted value: 0.0389, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1850 (value: 0.0007, weighted value: 0.0372, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1832 (value: 0.0007, weighted value: 0.0355, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9379
..training done in 65.35 seconds
..evaluation done in 18.24 seconds
Old network+MCTS average reward: 0.7487, min: 0.2037, max: 1.4444, stdev: 0.2191
New network+MCTS average reward: 0.7476, min: 0.2593, max: 1.5093, stdev: 0.2166
Old bare network average reward: 0.7175, min: 0.1667, max: 1.3889, stdev: 0.2200
New bare network average reward: 0.7167, min: 0.0833, max: 1.5093, stdev: 0.2198
External policy "random" average reward: 0.2700, min: -0.2593, max: 1.0370, stdev: 0.2099
External policy "individual greedy" average reward: 0.5519, min: -0.0463, max: 1.2593, stdev: 0.2108
External policy "total greedy" average reward: 0.6550, min: 0.0278, max: 1.3426, stdev: 0.2071
New network won 66 and tied 159 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 533 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.12 seconds
Training examples lengths: [64577, 65028, 65031, 64904, 64884, 64870, 64699, 64612, 64633, 64870]
Total value: 490312.56
Training on 648108 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2414 (value: 0.0014, weighted value: 0.0696, policy: 0.1717, weighted policy: 0.1717), Train Mean Max: 0.9335
Epoch 2/10, Train Loss: 0.2198 (value: 0.0012, weighted value: 0.0589, policy: 0.1608, weighted policy: 0.1608), Train Mean Max: 0.9350
Epoch 3/10, Train Loss: 0.2102 (value: 0.0011, weighted value: 0.0541, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9356
Epoch 4/10, Train Loss: 0.2042 (value: 0.0010, weighted value: 0.0507, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9359
Epoch 5/10, Train Loss: 0.2005 (value: 0.0010, weighted value: 0.0483, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9360
Epoch 6/10, Train Loss: 0.1949 (value: 0.0009, weighted value: 0.0444, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9364
Epoch 7/10, Train Loss: 0.1935 (value: 0.0009, weighted value: 0.0437, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9366
Epoch 8/10, Train Loss: 0.1916 (value: 0.0009, weighted value: 0.0432, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9368
Epoch 9/10, Train Loss: 0.1870 (value: 0.0008, weighted value: 0.0388, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9370
Epoch 10/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0394, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9371
..training done in 59.47 seconds
..evaluation done in 20.74 seconds
Old network+MCTS average reward: 0.7381, min: 0.1944, max: 1.4537, stdev: 0.2259
New network+MCTS average reward: 0.7395, min: 0.1944, max: 1.4537, stdev: 0.2270
Old bare network average reward: 0.7083, min: 0.1111, max: 1.4537, stdev: 0.2287
New bare network average reward: 0.7046, min: 0.0833, max: 1.4537, stdev: 0.2300
External policy "random" average reward: 0.2420, min: -0.2500, max: 1.0648, stdev: 0.2104
External policy "individual greedy" average reward: 0.5335, min: -0.0093, max: 1.3148, stdev: 0.2239
External policy "total greedy" average reward: 0.6418, min: 0.1296, max: 1.3981, stdev: 0.2125
New network won 63 and tied 176 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 534 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.82 seconds
Training examples lengths: [65028, 65031, 64904, 64884, 64870, 64699, 64612, 64633, 64870, 64893]
Total value: 490503.82
Training on 648424 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2069 (value: 0.0010, weighted value: 0.0497, policy: 0.1572, weighted policy: 0.1572), Train Mean Max: 0.9357
Epoch 2/10, Train Loss: 0.1985 (value: 0.0009, weighted value: 0.0448, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9363
Epoch 3/10, Train Loss: 0.1944 (value: 0.0009, weighted value: 0.0434, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9366
Epoch 4/10, Train Loss: 0.1901 (value: 0.0008, weighted value: 0.0404, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9369
Epoch 5/10, Train Loss: 0.1886 (value: 0.0008, weighted value: 0.0395, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9370
Epoch 6/10, Train Loss: 0.1869 (value: 0.0008, weighted value: 0.0379, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9372
Epoch 7/10, Train Loss: 0.1859 (value: 0.0008, weighted value: 0.0375, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9374
Epoch 8/10, Train Loss: 0.1836 (value: 0.0007, weighted value: 0.0352, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1831 (value: 0.0007, weighted value: 0.0351, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9378
Epoch 10/10, Train Loss: 0.1820 (value: 0.0007, weighted value: 0.0341, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9379
..training done in 66.54 seconds
..evaluation done in 20.87 seconds
Old network+MCTS average reward: 0.7435, min: 0.1389, max: 1.6389, stdev: 0.2414
New network+MCTS average reward: 0.7451, min: 0.1389, max: 1.6389, stdev: 0.2406
Old bare network average reward: 0.7110, min: 0.1204, max: 1.6389, stdev: 0.2442
New bare network average reward: 0.7095, min: 0.1204, max: 1.6389, stdev: 0.2420
External policy "random" average reward: 0.2602, min: -0.2778, max: 0.9815, stdev: 0.2251
External policy "individual greedy" average reward: 0.5455, min: 0.0000, max: 1.4907, stdev: 0.2332
External policy "total greedy" average reward: 0.6534, min: 0.0833, max: 1.4444, stdev: 0.2280
New network won 75 and tied 157 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 535 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 60.36 seconds
Training examples lengths: [65031, 64904, 64884, 64870, 64699, 64612, 64633, 64870, 64893, 64829]
Total value: 490681.83
Training on 648225 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2038 (value: 0.0009, weighted value: 0.0466, policy: 0.1572, weighted policy: 0.1572), Train Mean Max: 0.9364
Epoch 2/10, Train Loss: 0.1942 (value: 0.0008, weighted value: 0.0415, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9370
Epoch 3/10, Train Loss: 0.1897 (value: 0.0008, weighted value: 0.0396, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9374
Epoch 4/10, Train Loss: 0.1851 (value: 0.0007, weighted value: 0.0366, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1847 (value: 0.0007, weighted value: 0.0369, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1842 (value: 0.0007, weighted value: 0.0359, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9378
Epoch 7/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0339, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9381
Epoch 8/10, Train Loss: 0.1799 (value: 0.0007, weighted value: 0.0329, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9382
Epoch 9/10, Train Loss: 0.1793 (value: 0.0007, weighted value: 0.0327, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9383
Epoch 10/10, Train Loss: 0.1790 (value: 0.0007, weighted value: 0.0325, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9384
..training done in 73.38 seconds
..evaluation done in 19.26 seconds
Old network+MCTS average reward: 0.7361, min: 0.1296, max: 1.4630, stdev: 0.2422
New network+MCTS average reward: 0.7355, min: 0.1296, max: 1.4630, stdev: 0.2394
Old bare network average reward: 0.7041, min: 0.0556, max: 1.4630, stdev: 0.2423
New bare network average reward: 0.7044, min: 0.1019, max: 1.4074, stdev: 0.2426
External policy "random" average reward: 0.2662, min: -0.3981, max: 1.0000, stdev: 0.2378
External policy "individual greedy" average reward: 0.5321, min: -0.0370, max: 1.3148, stdev: 0.2324
External policy "total greedy" average reward: 0.6418, min: 0.1111, max: 1.2963, stdev: 0.2227
New network won 71 and tied 161 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 536 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.01 seconds
Training examples lengths: [64904, 64884, 64870, 64699, 64612, 64633, 64870, 64893, 64829, 64929]
Total value: 490201.11
Training on 648123 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1980 (value: 0.0009, weighted value: 0.0431, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9370
Epoch 2/10, Train Loss: 0.1910 (value: 0.0008, weighted value: 0.0395, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9376
Epoch 3/10, Train Loss: 0.1870 (value: 0.0008, weighted value: 0.0380, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9378
Epoch 4/10, Train Loss: 0.1837 (value: 0.0007, weighted value: 0.0357, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9381
Epoch 5/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0339, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9383
Epoch 6/10, Train Loss: 0.1819 (value: 0.0007, weighted value: 0.0351, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9383
Epoch 7/10, Train Loss: 0.1787 (value: 0.0006, weighted value: 0.0324, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9384
Epoch 8/10, Train Loss: 0.1780 (value: 0.0006, weighted value: 0.0316, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9385
Epoch 9/10, Train Loss: 0.1765 (value: 0.0006, weighted value: 0.0310, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9389
Epoch 10/10, Train Loss: 0.1762 (value: 0.0006, weighted value: 0.0307, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9388
..training done in 72.31 seconds
..evaluation done in 19.58 seconds
Old network+MCTS average reward: 0.7432, min: 0.1111, max: 1.4444, stdev: 0.2366
New network+MCTS average reward: 0.7456, min: 0.0093, max: 1.4167, stdev: 0.2359
Old bare network average reward: 0.7157, min: 0.0648, max: 1.3889, stdev: 0.2380
New bare network average reward: 0.7199, min: 0.1019, max: 1.4167, stdev: 0.2365
External policy "random" average reward: 0.2465, min: -0.4722, max: 1.0556, stdev: 0.2400
External policy "individual greedy" average reward: 0.5303, min: -0.1759, max: 1.1574, stdev: 0.2390
External policy "total greedy" average reward: 0.6492, min: -0.0185, max: 1.2870, stdev: 0.2305
New network won 67 and tied 173 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 537 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.60 seconds
Training examples lengths: [64884, 64870, 64699, 64612, 64633, 64870, 64893, 64829, 64929, 64642]
Total value: 490093.71
Training on 647861 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1980 (value: 0.0008, weighted value: 0.0424, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9371
Epoch 2/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0390, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9378
Epoch 3/10, Train Loss: 0.1854 (value: 0.0007, weighted value: 0.0362, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9381
Epoch 4/10, Train Loss: 0.1817 (value: 0.0007, weighted value: 0.0345, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9383
Epoch 5/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0340, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9383
Epoch 6/10, Train Loss: 0.1798 (value: 0.0007, weighted value: 0.0330, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9385
Epoch 7/10, Train Loss: 0.1774 (value: 0.0006, weighted value: 0.0315, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9388
Epoch 8/10, Train Loss: 0.1779 (value: 0.0007, weighted value: 0.0325, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9389
Epoch 9/10, Train Loss: 0.1755 (value: 0.0006, weighted value: 0.0299, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9389
Epoch 10/10, Train Loss: 0.1754 (value: 0.0006, weighted value: 0.0299, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9389
..training done in 76.12 seconds
..evaluation done in 19.97 seconds
Old network+MCTS average reward: 0.7498, min: 0.1204, max: 1.5093, stdev: 0.2197
New network+MCTS average reward: 0.7551, min: 0.1296, max: 1.6944, stdev: 0.2225
Old bare network average reward: 0.7244, min: 0.1111, max: 1.5093, stdev: 0.2263
New bare network average reward: 0.7211, min: 0.1296, max: 1.5093, stdev: 0.2229
External policy "random" average reward: 0.2798, min: -0.3056, max: 1.3519, stdev: 0.2183
External policy "individual greedy" average reward: 0.5389, min: -0.1389, max: 1.4074, stdev: 0.2257
External policy "total greedy" average reward: 0.6452, min: 0.0370, max: 1.5000, stdev: 0.2225
New network won 79 and tied 171 out of 300 games (54.83% wins where ties are half wins)
Keeping the new network

Training iteration 538 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.05 seconds
Training examples lengths: [64870, 64699, 64612, 64633, 64870, 64893, 64829, 64929, 64642, 64655]
Total value: 490102.03
Training on 647632 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1976 (value: 0.0008, weighted value: 0.0411, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9371
Epoch 2/10, Train Loss: 0.1908 (value: 0.0008, weighted value: 0.0388, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9378
Epoch 3/10, Train Loss: 0.1855 (value: 0.0007, weighted value: 0.0361, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9382
Epoch 4/10, Train Loss: 0.1827 (value: 0.0007, weighted value: 0.0345, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9383
Epoch 5/10, Train Loss: 0.1805 (value: 0.0007, weighted value: 0.0335, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9384
Epoch 6/10, Train Loss: 0.1788 (value: 0.0007, weighted value: 0.0326, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9385
Epoch 7/10, Train Loss: 0.1780 (value: 0.0006, weighted value: 0.0318, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9386
Epoch 8/10, Train Loss: 0.1766 (value: 0.0006, weighted value: 0.0303, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9386
Epoch 9/10, Train Loss: 0.1762 (value: 0.0006, weighted value: 0.0302, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9388
Epoch 10/10, Train Loss: 0.1754 (value: 0.0006, weighted value: 0.0298, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9389
..training done in 65.14 seconds
..evaluation done in 19.30 seconds
Old network+MCTS average reward: 0.7446, min: 0.0741, max: 1.4722, stdev: 0.2496
New network+MCTS average reward: 0.7429, min: 0.1111, max: 1.4722, stdev: 0.2506
Old bare network average reward: 0.7117, min: 0.0648, max: 1.4722, stdev: 0.2519
New bare network average reward: 0.7100, min: 0.0648, max: 1.4722, stdev: 0.2491
External policy "random" average reward: 0.2589, min: -0.3611, max: 0.9352, stdev: 0.2399
External policy "individual greedy" average reward: 0.5425, min: -0.0278, max: 1.3796, stdev: 0.2440
External policy "total greedy" average reward: 0.6503, min: 0.1296, max: 1.4074, stdev: 0.2295
New network won 63 and tied 170 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 539 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.00 seconds
Training examples lengths: [64699, 64612, 64633, 64870, 64893, 64829, 64929, 64642, 64655, 64843]
Total value: 489970.56
Training on 647605 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2181 (value: 0.0011, weighted value: 0.0535, policy: 0.1647, weighted policy: 0.1647), Train Mean Max: 0.9356
Epoch 2/10, Train Loss: 0.2041 (value: 0.0009, weighted value: 0.0463, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9367
Epoch 3/10, Train Loss: 0.1955 (value: 0.0009, weighted value: 0.0429, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9370
Epoch 4/10, Train Loss: 0.1910 (value: 0.0008, weighted value: 0.0408, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9373
Epoch 5/10, Train Loss: 0.1898 (value: 0.0008, weighted value: 0.0396, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9373
Epoch 6/10, Train Loss: 0.1866 (value: 0.0008, weighted value: 0.0377, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9376
Epoch 7/10, Train Loss: 0.1837 (value: 0.0007, weighted value: 0.0355, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9377
Epoch 8/10, Train Loss: 0.1828 (value: 0.0007, weighted value: 0.0352, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1815 (value: 0.0007, weighted value: 0.0343, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1808 (value: 0.0007, weighted value: 0.0330, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9380
..training done in 71.25 seconds
..evaluation done in 20.28 seconds
Old network+MCTS average reward: 0.7665, min: 0.0370, max: 1.6111, stdev: 0.2343
New network+MCTS average reward: 0.7648, min: 0.0370, max: 1.6111, stdev: 0.2354
Old bare network average reward: 0.7296, min: 0.0370, max: 1.6111, stdev: 0.2375
New bare network average reward: 0.7352, min: 0.0370, max: 1.6111, stdev: 0.2419
External policy "random" average reward: 0.2566, min: -0.2593, max: 1.0000, stdev: 0.2160
External policy "individual greedy" average reward: 0.5614, min: -0.0556, max: 1.4630, stdev: 0.2228
External policy "total greedy" average reward: 0.6737, min: 0.0278, max: 1.5278, stdev: 0.2198
New network won 51 and tied 183 out of 300 games (47.50% wins where ties are half wins)
Reverting to the old network

Training iteration 540 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.81 seconds
Training examples lengths: [64612, 64633, 64870, 64893, 64829, 64929, 64642, 64655, 64843, 65028]
Total value: 490211.38
Training on 647934 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2372 (value: 0.0013, weighted value: 0.0648, policy: 0.1724, weighted policy: 0.1724), Train Mean Max: 0.9342
Epoch 2/10, Train Loss: 0.2161 (value: 0.0011, weighted value: 0.0547, policy: 0.1614, weighted policy: 0.1614), Train Mean Max: 0.9356
Epoch 3/10, Train Loss: 0.2061 (value: 0.0010, weighted value: 0.0499, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9361
Epoch 4/10, Train Loss: 0.2008 (value: 0.0010, weighted value: 0.0479, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9363
Epoch 5/10, Train Loss: 0.1957 (value: 0.0009, weighted value: 0.0440, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9366
Epoch 6/10, Train Loss: 0.1923 (value: 0.0008, weighted value: 0.0414, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9367
Epoch 7/10, Train Loss: 0.1907 (value: 0.0008, weighted value: 0.0406, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9368
Epoch 8/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0387, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9370
Epoch 9/10, Train Loss: 0.1870 (value: 0.0008, weighted value: 0.0379, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9372
Epoch 10/10, Train Loss: 0.1848 (value: 0.0007, weighted value: 0.0369, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9374
..training done in 67.40 seconds
..evaluation done in 19.37 seconds
Old network+MCTS average reward: 0.7477, min: 0.1296, max: 1.5370, stdev: 0.2324
New network+MCTS average reward: 0.7468, min: 0.1111, max: 1.5648, stdev: 0.2323
Old bare network average reward: 0.7140, min: 0.1111, max: 1.5370, stdev: 0.2349
New bare network average reward: 0.7173, min: 0.1111, max: 1.5370, stdev: 0.2389
External policy "random" average reward: 0.2560, min: -0.4167, max: 1.0278, stdev: 0.2142
External policy "individual greedy" average reward: 0.5439, min: -0.0093, max: 1.3241, stdev: 0.2219
External policy "total greedy" average reward: 0.6638, min: 0.1759, max: 1.4167, stdev: 0.2097
New network won 61 and tied 164 out of 300 games (47.67% wins where ties are half wins)
Reverting to the old network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_540

Training iteration 541 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.25 seconds
Training examples lengths: [64633, 64870, 64893, 64829, 64929, 64642, 64655, 64843, 65028, 64736]
Total value: 490527.19
Training on 648058 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2560 (value: 0.0015, weighted value: 0.0772, policy: 0.1788, weighted policy: 0.1788), Train Mean Max: 0.9330
Epoch 2/10, Train Loss: 0.2296 (value: 0.0013, weighted value: 0.0630, policy: 0.1666, weighted policy: 0.1666), Train Mean Max: 0.9347
Epoch 3/10, Train Loss: 0.2175 (value: 0.0012, weighted value: 0.0587, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9353
Epoch 4/10, Train Loss: 0.2078 (value: 0.0010, weighted value: 0.0522, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9354
Epoch 5/10, Train Loss: 0.2037 (value: 0.0010, weighted value: 0.0500, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9357
Epoch 6/10, Train Loss: 0.1990 (value: 0.0009, weighted value: 0.0472, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9359
Epoch 7/10, Train Loss: 0.1966 (value: 0.0009, weighted value: 0.0451, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9362
Epoch 8/10, Train Loss: 0.1943 (value: 0.0009, weighted value: 0.0435, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9363
Epoch 9/10, Train Loss: 0.1916 (value: 0.0008, weighted value: 0.0418, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9365
Epoch 10/10, Train Loss: 0.1905 (value: 0.0008, weighted value: 0.0400, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9366
..training done in 73.18 seconds
..evaluation done in 18.77 seconds
Old network+MCTS average reward: 0.7679, min: 0.1296, max: 1.4815, stdev: 0.2548
New network+MCTS average reward: 0.7710, min: 0.1296, max: 1.4815, stdev: 0.2554
Old bare network average reward: 0.7404, min: 0.1204, max: 1.4722, stdev: 0.2606
New bare network average reward: 0.7370, min: 0.1204, max: 1.4815, stdev: 0.2613
External policy "random" average reward: 0.2662, min: -0.3148, max: 0.9259, stdev: 0.2372
External policy "individual greedy" average reward: 0.5725, min: 0.0648, max: 1.2407, stdev: 0.2355
External policy "total greedy" average reward: 0.6711, min: 0.1296, max: 1.3519, stdev: 0.2281
New network won 83 and tied 160 out of 300 games (54.33% wins where ties are half wins)
Keeping the new network

Training iteration 542 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.80 seconds
Training examples lengths: [64870, 64893, 64829, 64929, 64642, 64655, 64843, 65028, 64736, 64702]
Total value: 490706.42
Training on 648127 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2086 (value: 0.0010, weighted value: 0.0507, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9354
Epoch 2/10, Train Loss: 0.2009 (value: 0.0009, weighted value: 0.0454, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9360
Epoch 3/10, Train Loss: 0.1960 (value: 0.0009, weighted value: 0.0439, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9364
Epoch 4/10, Train Loss: 0.1922 (value: 0.0008, weighted value: 0.0418, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9365
Epoch 5/10, Train Loss: 0.1899 (value: 0.0008, weighted value: 0.0407, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9368
Epoch 6/10, Train Loss: 0.1878 (value: 0.0008, weighted value: 0.0383, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9369
Epoch 7/10, Train Loss: 0.1855 (value: 0.0007, weighted value: 0.0371, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1852 (value: 0.0007, weighted value: 0.0371, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9373
Epoch 9/10, Train Loss: 0.1837 (value: 0.0007, weighted value: 0.0356, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9374
Epoch 10/10, Train Loss: 0.1819 (value: 0.0007, weighted value: 0.0337, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9376
..training done in 69.78 seconds
..evaluation done in 20.10 seconds
Old network+MCTS average reward: 0.7489, min: 0.2222, max: 1.4352, stdev: 0.2264
New network+MCTS average reward: 0.7506, min: 0.1944, max: 1.4352, stdev: 0.2261
Old bare network average reward: 0.7208, min: 0.1667, max: 1.4167, stdev: 0.2357
New bare network average reward: 0.7202, min: 0.1944, max: 1.4167, stdev: 0.2304
External policy "random" average reward: 0.2677, min: -0.3519, max: 0.8611, stdev: 0.2156
External policy "individual greedy" average reward: 0.5419, min: 0.0833, max: 1.2407, stdev: 0.2166
External policy "total greedy" average reward: 0.6582, min: 0.1111, max: 1.2685, stdev: 0.2147
New network won 76 and tied 155 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 543 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.53 seconds
Training examples lengths: [64893, 64829, 64929, 64642, 64655, 64843, 65028, 64736, 64702, 64627]
Total value: 491009.71
Training on 647884 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2044 (value: 0.0009, weighted value: 0.0469, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9361
Epoch 2/10, Train Loss: 0.1953 (value: 0.0008, weighted value: 0.0412, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9367
Epoch 3/10, Train Loss: 0.1907 (value: 0.0008, weighted value: 0.0403, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9371
Epoch 4/10, Train Loss: 0.1877 (value: 0.0008, weighted value: 0.0380, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9371
Epoch 5/10, Train Loss: 0.1842 (value: 0.0007, weighted value: 0.0356, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9374
Epoch 6/10, Train Loss: 0.1832 (value: 0.0007, weighted value: 0.0356, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9376
Epoch 7/10, Train Loss: 0.1830 (value: 0.0007, weighted value: 0.0351, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9375
Epoch 8/10, Train Loss: 0.1796 (value: 0.0007, weighted value: 0.0326, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9379
Epoch 9/10, Train Loss: 0.1805 (value: 0.0007, weighted value: 0.0331, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1801 (value: 0.0007, weighted value: 0.0330, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9380
..training done in 68.05 seconds
..evaluation done in 19.47 seconds
Old network+MCTS average reward: 0.7442, min: 0.1759, max: 1.5741, stdev: 0.2393
New network+MCTS average reward: 0.7451, min: 0.1204, max: 1.5463, stdev: 0.2408
Old bare network average reward: 0.7121, min: 0.1481, max: 1.5000, stdev: 0.2389
New bare network average reward: 0.7116, min: 0.1204, max: 1.4907, stdev: 0.2385
External policy "random" average reward: 0.2555, min: -0.3704, max: 1.0185, stdev: 0.2254
External policy "individual greedy" average reward: 0.5356, min: 0.0093, max: 1.2685, stdev: 0.2255
External policy "total greedy" average reward: 0.6458, min: 0.0556, max: 1.2963, stdev: 0.2141
New network won 76 and tied 161 out of 300 games (52.17% wins where ties are half wins)
Keeping the new network

Training iteration 544 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.62 seconds
Training examples lengths: [64829, 64929, 64642, 64655, 64843, 65028, 64736, 64702, 64627, 64601]
Total value: 490809.21
Training on 647592 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1984 (value: 0.0009, weighted value: 0.0426, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9367
Epoch 2/10, Train Loss: 0.1928 (value: 0.0008, weighted value: 0.0404, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9374
Epoch 3/10, Train Loss: 0.1866 (value: 0.0008, weighted value: 0.0376, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9379
Epoch 4/10, Train Loss: 0.1832 (value: 0.0007, weighted value: 0.0356, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9380
Epoch 5/10, Train Loss: 0.1830 (value: 0.0007, weighted value: 0.0361, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9380
Epoch 6/10, Train Loss: 0.1805 (value: 0.0007, weighted value: 0.0337, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9382
Epoch 7/10, Train Loss: 0.1795 (value: 0.0007, weighted value: 0.0326, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9383
Epoch 8/10, Train Loss: 0.1780 (value: 0.0006, weighted value: 0.0316, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1785 (value: 0.0006, weighted value: 0.0319, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9386
Epoch 10/10, Train Loss: 0.1766 (value: 0.0006, weighted value: 0.0306, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9387
..training done in 68.99 seconds
..evaluation done in 28.32 seconds
Old network+MCTS average reward: 0.7651, min: 0.1481, max: 1.6667, stdev: 0.2581
New network+MCTS average reward: 0.7652, min: 0.1481, max: 1.6667, stdev: 0.2575
Old bare network average reward: 0.7276, min: -0.0185, max: 1.6667, stdev: 0.2632
New bare network average reward: 0.7307, min: 0.1481, max: 1.6667, stdev: 0.2635
External policy "random" average reward: 0.2755, min: -0.3241, max: 1.1111, stdev: 0.2400
External policy "individual greedy" average reward: 0.5456, min: 0.0000, max: 1.6019, stdev: 0.2478
External policy "total greedy" average reward: 0.6624, min: -0.0556, max: 1.6852, stdev: 0.2386
New network won 53 and tied 187 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 545 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.06 seconds
Training examples lengths: [64929, 64642, 64655, 64843, 65028, 64736, 64702, 64627, 64601, 64577]
Total value: 490264.39
Training on 647340 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2196 (value: 0.0011, weighted value: 0.0554, policy: 0.1641, weighted policy: 0.1641), Train Mean Max: 0.9352
Epoch 2/10, Train Loss: 0.2059 (value: 0.0010, weighted value: 0.0484, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9363
Epoch 3/10, Train Loss: 0.1978 (value: 0.0009, weighted value: 0.0453, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9367
Epoch 4/10, Train Loss: 0.1936 (value: 0.0008, weighted value: 0.0424, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9369
Epoch 5/10, Train Loss: 0.1913 (value: 0.0008, weighted value: 0.0411, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9371
Epoch 6/10, Train Loss: 0.1877 (value: 0.0008, weighted value: 0.0387, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9374
Epoch 7/10, Train Loss: 0.1864 (value: 0.0008, weighted value: 0.0377, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9376
Epoch 8/10, Train Loss: 0.1837 (value: 0.0007, weighted value: 0.0359, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1838 (value: 0.0007, weighted value: 0.0361, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1803 (value: 0.0007, weighted value: 0.0338, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9380
..training done in 69.49 seconds
..evaluation done in 19.16 seconds
Old network+MCTS average reward: 0.7658, min: 0.0093, max: 1.5648, stdev: 0.2392
New network+MCTS average reward: 0.7684, min: 0.0278, max: 1.5926, stdev: 0.2397
Old bare network average reward: 0.7357, min: 0.0741, max: 1.5648, stdev: 0.2432
New bare network average reward: 0.7372, min: -0.0278, max: 1.5741, stdev: 0.2461
External policy "random" average reward: 0.2881, min: -0.3241, max: 0.9722, stdev: 0.2206
External policy "individual greedy" average reward: 0.5581, min: 0.0000, max: 1.2407, stdev: 0.2230
External policy "total greedy" average reward: 0.6758, min: -0.0370, max: 1.5648, stdev: 0.2335
New network won 70 and tied 173 out of 300 games (52.17% wins where ties are half wins)
Keeping the new network

Training iteration 546 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.80 seconds
Training examples lengths: [64642, 64655, 64843, 65028, 64736, 64702, 64627, 64601, 64577, 64729]
Total value: 490212.47
Training on 647140 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2025 (value: 0.0009, weighted value: 0.0454, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9363
Epoch 2/10, Train Loss: 0.1934 (value: 0.0008, weighted value: 0.0413, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9371
Epoch 3/10, Train Loss: 0.1895 (value: 0.0008, weighted value: 0.0396, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9374
Epoch 4/10, Train Loss: 0.1862 (value: 0.0008, weighted value: 0.0379, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1836 (value: 0.0007, weighted value: 0.0355, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9377
Epoch 6/10, Train Loss: 0.1826 (value: 0.0007, weighted value: 0.0347, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9379
Epoch 7/10, Train Loss: 0.1812 (value: 0.0007, weighted value: 0.0337, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9380
Epoch 8/10, Train Loss: 0.1806 (value: 0.0007, weighted value: 0.0339, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9382
Epoch 9/10, Train Loss: 0.1796 (value: 0.0007, weighted value: 0.0332, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9383
Epoch 10/10, Train Loss: 0.1771 (value: 0.0006, weighted value: 0.0307, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9385
..training done in 74.37 seconds
..evaluation done in 21.19 seconds
Old network+MCTS average reward: 0.7306, min: 0.1389, max: 1.4907, stdev: 0.2292
New network+MCTS average reward: 0.7285, min: 0.1389, max: 1.4630, stdev: 0.2298
Old bare network average reward: 0.6903, min: 0.1296, max: 1.3426, stdev: 0.2297
New bare network average reward: 0.6945, min: 0.0648, max: 1.3426, stdev: 0.2323
External policy "random" average reward: 0.2364, min: -0.3148, max: 0.8333, stdev: 0.2144
External policy "individual greedy" average reward: 0.5228, min: -0.0185, max: 1.0833, stdev: 0.2062
External policy "total greedy" average reward: 0.6302, min: 0.1019, max: 1.4167, stdev: 0.2165
New network won 45 and tied 180 out of 300 games (45.00% wins where ties are half wins)
Reverting to the old network

Training iteration 547 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.24 seconds
Training examples lengths: [64655, 64843, 65028, 64736, 64702, 64627, 64601, 64577, 64729, 64976]
Total value: 490341.06
Training on 647474 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2235 (value: 0.0011, weighted value: 0.0573, policy: 0.1662, weighted policy: 0.1662), Train Mean Max: 0.9349
Epoch 2/10, Train Loss: 0.2083 (value: 0.0010, weighted value: 0.0499, policy: 0.1584, weighted policy: 0.1584), Train Mean Max: 0.9360
Epoch 3/10, Train Loss: 0.2004 (value: 0.0009, weighted value: 0.0470, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9364
Epoch 4/10, Train Loss: 0.1941 (value: 0.0009, weighted value: 0.0429, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9367
Epoch 5/10, Train Loss: 0.1915 (value: 0.0008, weighted value: 0.0417, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1898 (value: 0.0008, weighted value: 0.0408, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9370
Epoch 7/10, Train Loss: 0.1863 (value: 0.0008, weighted value: 0.0380, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9372
Epoch 8/10, Train Loss: 0.1855 (value: 0.0007, weighted value: 0.0371, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9373
Epoch 9/10, Train Loss: 0.1846 (value: 0.0007, weighted value: 0.0363, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9375
Epoch 10/10, Train Loss: 0.1824 (value: 0.0007, weighted value: 0.0350, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9377
..training done in 67.86 seconds
..evaluation done in 20.30 seconds
Old network+MCTS average reward: 0.7439, min: 0.1111, max: 1.3333, stdev: 0.2345
New network+MCTS average reward: 0.7408, min: 0.1111, max: 1.3333, stdev: 0.2343
Old bare network average reward: 0.7105, min: 0.1111, max: 1.2870, stdev: 0.2359
New bare network average reward: 0.7099, min: 0.0648, max: 1.2870, stdev: 0.2389
External policy "random" average reward: 0.2541, min: -0.3426, max: 0.8241, stdev: 0.2286
External policy "individual greedy" average reward: 0.5455, min: -0.0370, max: 1.1204, stdev: 0.2163
External policy "total greedy" average reward: 0.6474, min: 0.0648, max: 1.3148, stdev: 0.2195
New network won 58 and tied 173 out of 300 games (48.17% wins where ties are half wins)
Reverting to the old network

Training iteration 548 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.19 seconds
Training examples lengths: [64843, 65028, 64736, 64702, 64627, 64601, 64577, 64729, 64976, 64955]
Total value: 491058.85
Training on 647774 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2395 (value: 0.0013, weighted value: 0.0670, policy: 0.1726, weighted policy: 0.1726), Train Mean Max: 0.9337
Epoch 2/10, Train Loss: 0.2211 (value: 0.0012, weighted value: 0.0579, policy: 0.1632, weighted policy: 0.1632), Train Mean Max: 0.9350
Epoch 3/10, Train Loss: 0.2086 (value: 0.0010, weighted value: 0.0524, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9357
Epoch 4/10, Train Loss: 0.2024 (value: 0.0010, weighted value: 0.0493, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9360
Epoch 5/10, Train Loss: 0.1990 (value: 0.0009, weighted value: 0.0467, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9360
Epoch 6/10, Train Loss: 0.1956 (value: 0.0009, weighted value: 0.0449, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9362
Epoch 7/10, Train Loss: 0.1908 (value: 0.0008, weighted value: 0.0414, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9366
Epoch 8/10, Train Loss: 0.1899 (value: 0.0008, weighted value: 0.0410, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9367
Epoch 9/10, Train Loss: 0.1883 (value: 0.0008, weighted value: 0.0389, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9369
Epoch 10/10, Train Loss: 0.1863 (value: 0.0008, weighted value: 0.0376, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9372
..training done in 67.37 seconds
..evaluation done in 18.88 seconds
Old network+MCTS average reward: 0.7653, min: 0.1481, max: 1.4722, stdev: 0.2353
New network+MCTS average reward: 0.7670, min: 0.1481, max: 1.5833, stdev: 0.2337
Old bare network average reward: 0.7387, min: 0.1481, max: 1.4815, stdev: 0.2356
New bare network average reward: 0.7388, min: 0.1481, max: 1.4815, stdev: 0.2348
External policy "random" average reward: 0.2752, min: -0.2870, max: 1.0093, stdev: 0.2319
External policy "individual greedy" average reward: 0.5523, min: -0.0556, max: 1.2315, stdev: 0.2336
External policy "total greedy" average reward: 0.6688, min: 0.1296, max: 1.4167, stdev: 0.2303
New network won 73 and tied 164 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 549 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 62.26 seconds
Training examples lengths: [65028, 64736, 64702, 64627, 64601, 64577, 64729, 64976, 64955, 64479]
Total value: 490785.19
Training on 647410 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2061 (value: 0.0010, weighted value: 0.0494, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9359
Epoch 2/10, Train Loss: 0.1979 (value: 0.0009, weighted value: 0.0435, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9363
Epoch 3/10, Train Loss: 0.1928 (value: 0.0008, weighted value: 0.0423, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9371
Epoch 4/10, Train Loss: 0.1892 (value: 0.0008, weighted value: 0.0400, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9373
Epoch 5/10, Train Loss: 0.1875 (value: 0.0008, weighted value: 0.0389, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9374
Epoch 6/10, Train Loss: 0.1850 (value: 0.0008, weighted value: 0.0375, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9377
Epoch 7/10, Train Loss: 0.1843 (value: 0.0007, weighted value: 0.0369, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9377
Epoch 8/10, Train Loss: 0.1827 (value: 0.0007, weighted value: 0.0346, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1823 (value: 0.0007, weighted value: 0.0354, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9381
Epoch 10/10, Train Loss: 0.1797 (value: 0.0007, weighted value: 0.0333, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9382
..training done in 75.46 seconds
..evaluation done in 21.08 seconds
Old network+MCTS average reward: 0.7333, min: 0.0926, max: 1.4537, stdev: 0.2396
New network+MCTS average reward: 0.7335, min: 0.1296, max: 1.4537, stdev: 0.2398
Old bare network average reward: 0.6947, min: 0.0370, max: 1.4537, stdev: 0.2467
New bare network average reward: 0.6960, min: 0.0370, max: 1.4537, stdev: 0.2439
External policy "random" average reward: 0.2588, min: -0.3056, max: 0.8889, stdev: 0.2145
External policy "individual greedy" average reward: 0.5293, min: 0.0093, max: 1.2130, stdev: 0.2137
External policy "total greedy" average reward: 0.6373, min: 0.0556, max: 1.3148, stdev: 0.2216
New network won 72 and tied 158 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 550 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.60 seconds
Training examples lengths: [64736, 64702, 64627, 64601, 64577, 64729, 64976, 64955, 64479, 64985]
Total value: 490526.32
Training on 647367 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2010 (value: 0.0009, weighted value: 0.0452, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9368
Epoch 2/10, Train Loss: 0.1924 (value: 0.0008, weighted value: 0.0403, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9373
Epoch 3/10, Train Loss: 0.1875 (value: 0.0008, weighted value: 0.0382, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9376
Epoch 4/10, Train Loss: 0.1861 (value: 0.0008, weighted value: 0.0379, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9378
Epoch 5/10, Train Loss: 0.1827 (value: 0.0007, weighted value: 0.0358, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9382
Epoch 6/10, Train Loss: 0.1822 (value: 0.0007, weighted value: 0.0345, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9381
Epoch 7/10, Train Loss: 0.1802 (value: 0.0007, weighted value: 0.0335, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9384
Epoch 8/10, Train Loss: 0.1790 (value: 0.0007, weighted value: 0.0331, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9385
Epoch 9/10, Train Loss: 0.1775 (value: 0.0006, weighted value: 0.0320, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9386
Epoch 10/10, Train Loss: 0.1784 (value: 0.0006, weighted value: 0.0319, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9387
..training done in 66.66 seconds
..evaluation done in 19.93 seconds
Old network+MCTS average reward: 0.7404, min: 0.1852, max: 1.8889, stdev: 0.2385
New network+MCTS average reward: 0.7420, min: 0.1852, max: 1.8704, stdev: 0.2383
Old bare network average reward: 0.7119, min: 0.1852, max: 1.8889, stdev: 0.2425
New bare network average reward: 0.7098, min: 0.1852, max: 1.7963, stdev: 0.2393
External policy "random" average reward: 0.2696, min: -0.5093, max: 1.0926, stdev: 0.2350
External policy "individual greedy" average reward: 0.5463, min: -0.0556, max: 1.2870, stdev: 0.2329
External policy "total greedy" average reward: 0.6518, min: 0.0833, max: 1.4074, stdev: 0.2220
New network won 78 and tied 157 out of 300 games (52.17% wins where ties are half wins)
Keeping the new network

Training iteration 551 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 60.38 seconds
Training examples lengths: [64702, 64627, 64601, 64577, 64729, 64976, 64955, 64479, 64985, 64614]
Total value: 490478.50
Training on 647245 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2010 (value: 0.0009, weighted value: 0.0437, policy: 0.1573, weighted policy: 0.1573), Train Mean Max: 0.9369
Epoch 2/10, Train Loss: 0.1931 (value: 0.0008, weighted value: 0.0399, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9376
Epoch 3/10, Train Loss: 0.1881 (value: 0.0008, weighted value: 0.0382, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9378
Epoch 4/10, Train Loss: 0.1868 (value: 0.0007, weighted value: 0.0364, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9378
Epoch 5/10, Train Loss: 0.1838 (value: 0.0007, weighted value: 0.0355, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9382
Epoch 6/10, Train Loss: 0.1802 (value: 0.0007, weighted value: 0.0335, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9385
Epoch 7/10, Train Loss: 0.1812 (value: 0.0007, weighted value: 0.0333, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9384
Epoch 8/10, Train Loss: 0.1803 (value: 0.0006, weighted value: 0.0318, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1777 (value: 0.0006, weighted value: 0.0314, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9388
Epoch 10/10, Train Loss: 0.1778 (value: 0.0006, weighted value: 0.0309, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9388
..training done in 74.52 seconds
..evaluation done in 20.08 seconds
Old network+MCTS average reward: 0.7563, min: 0.0185, max: 1.8333, stdev: 0.2457
New network+MCTS average reward: 0.7581, min: 0.0185, max: 1.8333, stdev: 0.2448
Old bare network average reward: 0.7297, min: -0.0370, max: 1.7685, stdev: 0.2494
New bare network average reward: 0.7263, min: -0.0463, max: 1.7685, stdev: 0.2471
External policy "random" average reward: 0.2619, min: -0.3796, max: 1.0370, stdev: 0.2413
External policy "individual greedy" average reward: 0.5420, min: -0.0093, max: 1.3704, stdev: 0.2367
External policy "total greedy" average reward: 0.6555, min: 0.0741, max: 1.5185, stdev: 0.2267
New network won 71 and tied 164 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 552 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.89 seconds
Training examples lengths: [64627, 64601, 64577, 64729, 64976, 64955, 64479, 64985, 64614, 64822]
Total value: 490278.76
Training on 647365 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1990 (value: 0.0009, weighted value: 0.0427, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9374
Epoch 2/10, Train Loss: 0.1923 (value: 0.0008, weighted value: 0.0396, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9376
Epoch 3/10, Train Loss: 0.1869 (value: 0.0007, weighted value: 0.0363, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9378
Epoch 4/10, Train Loss: 0.1836 (value: 0.0007, weighted value: 0.0353, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9382
Epoch 5/10, Train Loss: 0.1812 (value: 0.0007, weighted value: 0.0334, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9382
Epoch 6/10, Train Loss: 0.1794 (value: 0.0006, weighted value: 0.0324, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9383
Epoch 7/10, Train Loss: 0.1788 (value: 0.0006, weighted value: 0.0320, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9386
Epoch 8/10, Train Loss: 0.1792 (value: 0.0007, weighted value: 0.0329, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9386
Epoch 9/10, Train Loss: 0.1755 (value: 0.0006, weighted value: 0.0296, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9388
Epoch 10/10, Train Loss: 0.1771 (value: 0.0006, weighted value: 0.0303, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9386
..training done in 65.89 seconds
..evaluation done in 19.21 seconds
Old network+MCTS average reward: 0.7649, min: 0.2500, max: 1.3426, stdev: 0.2147
New network+MCTS average reward: 0.7664, min: 0.2222, max: 1.3426, stdev: 0.2154
Old bare network average reward: 0.7324, min: 0.2222, max: 1.3426, stdev: 0.2152
New bare network average reward: 0.7352, min: 0.2222, max: 1.3241, stdev: 0.2130
External policy "random" average reward: 0.2733, min: -0.4537, max: 0.8704, stdev: 0.2292
External policy "individual greedy" average reward: 0.5544, min: 0.0463, max: 1.1481, stdev: 0.2186
External policy "total greedy" average reward: 0.6667, min: 0.1574, max: 1.1759, stdev: 0.2046
New network won 68 and tied 171 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 553 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.60 seconds
Training examples lengths: [64601, 64577, 64729, 64976, 64955, 64479, 64985, 64614, 64822, 64634]
Total value: 490824.51
Training on 647372 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1963 (value: 0.0008, weighted value: 0.0414, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9373
Epoch 2/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0376, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9379
Epoch 3/10, Train Loss: 0.1837 (value: 0.0007, weighted value: 0.0359, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9383
Epoch 4/10, Train Loss: 0.1811 (value: 0.0007, weighted value: 0.0340, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9385
Epoch 5/10, Train Loss: 0.1800 (value: 0.0007, weighted value: 0.0329, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9385
Epoch 6/10, Train Loss: 0.1794 (value: 0.0007, weighted value: 0.0330, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9387
Epoch 7/10, Train Loss: 0.1764 (value: 0.0006, weighted value: 0.0307, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9389
Epoch 8/10, Train Loss: 0.1764 (value: 0.0006, weighted value: 0.0306, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9389
Epoch 9/10, Train Loss: 0.1767 (value: 0.0006, weighted value: 0.0306, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9391
Epoch 10/10, Train Loss: 0.1746 (value: 0.0006, weighted value: 0.0293, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9392
..training done in 63.71 seconds
..evaluation done in 18.54 seconds
Old network+MCTS average reward: 0.7547, min: -0.0741, max: 1.5093, stdev: 0.2336
New network+MCTS average reward: 0.7534, min: 0.0278, max: 1.5093, stdev: 0.2330
Old bare network average reward: 0.7217, min: -0.0278, max: 1.5185, stdev: 0.2398
New bare network average reward: 0.7200, min: -0.0278, max: 1.4074, stdev: 0.2367
External policy "random" average reward: 0.2644, min: -0.4537, max: 1.0185, stdev: 0.2159
External policy "individual greedy" average reward: 0.5466, min: -0.1296, max: 1.2593, stdev: 0.2202
External policy "total greedy" average reward: 0.6555, min: 0.0093, max: 1.4722, stdev: 0.2178
New network won 61 and tied 157 out of 300 games (46.50% wins where ties are half wins)
Reverting to the old network

Training iteration 554 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.70 seconds
Training examples lengths: [64577, 64729, 64976, 64955, 64479, 64985, 64614, 64822, 64634, 64746]
Total value: 490891.06
Training on 647517 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2147 (value: 0.0010, weighted value: 0.0523, policy: 0.1624, weighted policy: 0.1624), Train Mean Max: 0.9357
Epoch 2/10, Train Loss: 0.2038 (value: 0.0009, weighted value: 0.0469, policy: 0.1569, weighted policy: 0.1569), Train Mean Max: 0.9367
Epoch 3/10, Train Loss: 0.1937 (value: 0.0008, weighted value: 0.0417, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9373
Epoch 4/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0404, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9375
Epoch 5/10, Train Loss: 0.1874 (value: 0.0008, weighted value: 0.0385, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9377
Epoch 6/10, Train Loss: 0.1842 (value: 0.0007, weighted value: 0.0368, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9380
Epoch 7/10, Train Loss: 0.1821 (value: 0.0007, weighted value: 0.0352, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9381
Epoch 8/10, Train Loss: 0.1816 (value: 0.0007, weighted value: 0.0352, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9382
Epoch 9/10, Train Loss: 0.1799 (value: 0.0007, weighted value: 0.0341, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1770 (value: 0.0006, weighted value: 0.0315, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9385
..training done in 70.66 seconds
..evaluation done in 22.96 seconds
Old network+MCTS average reward: 0.7600, min: 0.0556, max: 1.5833, stdev: 0.2477
New network+MCTS average reward: 0.7616, min: 0.0741, max: 1.5833, stdev: 0.2470
Old bare network average reward: 0.7277, min: 0.0463, max: 1.5833, stdev: 0.2527
New bare network average reward: 0.7252, min: 0.0463, max: 1.5648, stdev: 0.2531
External policy "random" average reward: 0.2745, min: -0.3426, max: 1.0648, stdev: 0.2414
External policy "individual greedy" average reward: 0.5573, min: -0.1296, max: 1.2315, stdev: 0.2366
External policy "total greedy" average reward: 0.6753, min: -0.0463, max: 1.4630, stdev: 0.2372
New network won 64 and tied 178 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 555 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.80 seconds
Training examples lengths: [64729, 64976, 64955, 64479, 64985, 64614, 64822, 64634, 64746, 64837]
Total value: 491206.18
Training on 647777 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2003 (value: 0.0009, weighted value: 0.0450, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9369
Epoch 2/10, Train Loss: 0.1912 (value: 0.0008, weighted value: 0.0397, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9377
Epoch 3/10, Train Loss: 0.1862 (value: 0.0008, weighted value: 0.0380, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9381
Epoch 4/10, Train Loss: 0.1838 (value: 0.0007, weighted value: 0.0369, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9383
Epoch 5/10, Train Loss: 0.1801 (value: 0.0007, weighted value: 0.0336, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9384
Epoch 6/10, Train Loss: 0.1811 (value: 0.0007, weighted value: 0.0340, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9384
Epoch 7/10, Train Loss: 0.1786 (value: 0.0007, weighted value: 0.0326, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9387
Epoch 8/10, Train Loss: 0.1771 (value: 0.0006, weighted value: 0.0322, policy: 0.1449, weighted policy: 0.1449), Train Mean Max: 0.9390
Epoch 9/10, Train Loss: 0.1775 (value: 0.0006, weighted value: 0.0318, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9389
Epoch 10/10, Train Loss: 0.1756 (value: 0.0006, weighted value: 0.0304, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9391
..training done in 68.63 seconds
..evaluation done in 19.16 seconds
Old network+MCTS average reward: 0.7415, min: 0.0741, max: 1.4537, stdev: 0.2329
New network+MCTS average reward: 0.7415, min: 0.1019, max: 1.3981, stdev: 0.2339
Old bare network average reward: 0.7081, min: 0.0556, max: 1.3981, stdev: 0.2377
New bare network average reward: 0.7098, min: 0.0556, max: 1.5278, stdev: 0.2433
External policy "random" average reward: 0.2614, min: -0.2593, max: 0.8704, stdev: 0.2150
External policy "individual greedy" average reward: 0.5445, min: -0.0463, max: 1.3148, stdev: 0.2314
External policy "total greedy" average reward: 0.6498, min: 0.1111, max: 1.3148, stdev: 0.2206
New network won 69 and tied 164 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 556 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.78 seconds
Training examples lengths: [64976, 64955, 64479, 64985, 64614, 64822, 64634, 64746, 64837, 64603]
Total value: 491539.82
Training on 647651 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1954 (value: 0.0008, weighted value: 0.0418, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9376
Epoch 2/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0389, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9380
Epoch 3/10, Train Loss: 0.1852 (value: 0.0007, weighted value: 0.0361, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9383
Epoch 4/10, Train Loss: 0.1819 (value: 0.0007, weighted value: 0.0351, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9387
Epoch 5/10, Train Loss: 0.1783 (value: 0.0007, weighted value: 0.0330, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9389
Epoch 6/10, Train Loss: 0.1784 (value: 0.0007, weighted value: 0.0331, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9388
Epoch 7/10, Train Loss: 0.1767 (value: 0.0006, weighted value: 0.0314, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9389
Epoch 8/10, Train Loss: 0.1751 (value: 0.0006, weighted value: 0.0303, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9391
Epoch 9/10, Train Loss: 0.1754 (value: 0.0006, weighted value: 0.0308, policy: 0.1447, weighted policy: 0.1447), Train Mean Max: 0.9392
Epoch 10/10, Train Loss: 0.1747 (value: 0.0006, weighted value: 0.0298, policy: 0.1449, weighted policy: 0.1449), Train Mean Max: 0.9393
..training done in 78.34 seconds
..evaluation done in 27.56 seconds
Old network+MCTS average reward: 0.7121, min: 0.1019, max: 1.3981, stdev: 0.2267
New network+MCTS average reward: 0.7117, min: 0.0278, max: 1.4352, stdev: 0.2302
Old bare network average reward: 0.6805, min: 0.0556, max: 1.3981, stdev: 0.2286
New bare network average reward: 0.6778, min: -0.0185, max: 1.3981, stdev: 0.2324
External policy "random" average reward: 0.2538, min: -0.3148, max: 0.9352, stdev: 0.2167
External policy "individual greedy" average reward: 0.5127, min: -0.1667, max: 1.1481, stdev: 0.2184
External policy "total greedy" average reward: 0.6321, min: 0.0741, max: 1.2315, stdev: 0.2133
New network won 71 and tied 157 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 557 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 66.88 seconds
Training examples lengths: [64955, 64479, 64985, 64614, 64822, 64634, 64746, 64837, 64603, 65121]
Total value: 491412.93
Training on 647796 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2167 (value: 0.0011, weighted value: 0.0528, policy: 0.1639, weighted policy: 0.1639), Train Mean Max: 0.9357
Epoch 2/10, Train Loss: 0.2035 (value: 0.0009, weighted value: 0.0468, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9368
Epoch 3/10, Train Loss: 0.1945 (value: 0.0009, weighted value: 0.0433, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9373
Epoch 4/10, Train Loss: 0.1893 (value: 0.0008, weighted value: 0.0406, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1868 (value: 0.0008, weighted value: 0.0386, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1852 (value: 0.0008, weighted value: 0.0377, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9379
Epoch 7/10, Train Loss: 0.1835 (value: 0.0007, weighted value: 0.0361, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9380
Epoch 8/10, Train Loss: 0.1817 (value: 0.0007, weighted value: 0.0353, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9382
Epoch 9/10, Train Loss: 0.1795 (value: 0.0007, weighted value: 0.0336, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1780 (value: 0.0006, weighted value: 0.0321, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9385
..training done in 72.76 seconds
..evaluation done in 27.67 seconds
Old network+MCTS average reward: 0.7475, min: 0.1481, max: 1.5648, stdev: 0.2413
New network+MCTS average reward: 0.7469, min: 0.1944, max: 1.5648, stdev: 0.2419
Old bare network average reward: 0.7140, min: 0.1204, max: 1.5648, stdev: 0.2465
New bare network average reward: 0.7152, min: 0.1481, max: 1.5648, stdev: 0.2475
External policy "random" average reward: 0.2550, min: -0.2870, max: 0.9907, stdev: 0.2321
External policy "individual greedy" average reward: 0.5376, min: -0.1019, max: 1.3241, stdev: 0.2335
External policy "total greedy" average reward: 0.6492, min: 0.1019, max: 1.3519, stdev: 0.2295
New network won 60 and tied 177 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 558 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 61.86 seconds
Training examples lengths: [64479, 64985, 64614, 64822, 64634, 64746, 64837, 64603, 65121, 64844]
Total value: 491241.44
Training on 647685 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2351 (value: 0.0013, weighted value: 0.0657, policy: 0.1694, weighted policy: 0.1694), Train Mean Max: 0.9343
Epoch 2/10, Train Loss: 0.2153 (value: 0.0011, weighted value: 0.0550, policy: 0.1602, weighted policy: 0.1602), Train Mean Max: 0.9355
Epoch 3/10, Train Loss: 0.2061 (value: 0.0010, weighted value: 0.0508, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9361
Epoch 4/10, Train Loss: 0.1986 (value: 0.0009, weighted value: 0.0467, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9365
Epoch 5/10, Train Loss: 0.1942 (value: 0.0009, weighted value: 0.0440, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9367
Epoch 6/10, Train Loss: 0.1930 (value: 0.0009, weighted value: 0.0434, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9369
Epoch 7/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0399, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9371
Epoch 8/10, Train Loss: 0.1873 (value: 0.0008, weighted value: 0.0394, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1856 (value: 0.0008, weighted value: 0.0378, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9374
Epoch 10/10, Train Loss: 0.1835 (value: 0.0007, weighted value: 0.0360, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9378
..training done in 77.45 seconds
..evaluation done in 20.91 seconds
Old network+MCTS average reward: 0.7462, min: 0.2407, max: 1.3056, stdev: 0.2244
New network+MCTS average reward: 0.7459, min: 0.2407, max: 1.3241, stdev: 0.2299
Old bare network average reward: 0.7131, min: 0.1574, max: 1.2963, stdev: 0.2319
New bare network average reward: 0.7151, min: 0.1852, max: 1.2963, stdev: 0.2278
External policy "random" average reward: 0.2465, min: -0.3241, max: 0.8611, stdev: 0.2205
External policy "individual greedy" average reward: 0.5348, min: -0.0185, max: 1.1389, stdev: 0.2173
External policy "total greedy" average reward: 0.6441, min: 0.0556, max: 1.1852, stdev: 0.2146
New network won 62 and tied 162 out of 300 games (47.67% wins where ties are half wins)
Reverting to the old network

Training iteration 559 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.39 seconds
Training examples lengths: [64985, 64614, 64822, 64634, 64746, 64837, 64603, 65121, 64844, 65080]
Total value: 492206.43
Training on 648286 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2526 (value: 0.0015, weighted value: 0.0762, policy: 0.1765, weighted policy: 0.1765), Train Mean Max: 0.9329
Epoch 2/10, Train Loss: 0.2269 (value: 0.0013, weighted value: 0.0626, policy: 0.1643, weighted policy: 0.1643), Train Mean Max: 0.9345
Epoch 3/10, Train Loss: 0.2136 (value: 0.0011, weighted value: 0.0567, policy: 0.1569, weighted policy: 0.1569), Train Mean Max: 0.9352
Epoch 4/10, Train Loss: 0.2063 (value: 0.0011, weighted value: 0.0530, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9357
Epoch 5/10, Train Loss: 0.2010 (value: 0.0010, weighted value: 0.0488, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9358
Epoch 6/10, Train Loss: 0.1973 (value: 0.0009, weighted value: 0.0469, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9361
Epoch 7/10, Train Loss: 0.1935 (value: 0.0009, weighted value: 0.0436, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9363
Epoch 8/10, Train Loss: 0.1914 (value: 0.0009, weighted value: 0.0428, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9367
Epoch 9/10, Train Loss: 0.1901 (value: 0.0008, weighted value: 0.0411, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9367
Epoch 10/10, Train Loss: 0.1869 (value: 0.0008, weighted value: 0.0390, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9371
..training done in 82.36 seconds
..evaluation done in 21.70 seconds
Old network+MCTS average reward: 0.7106, min: 0.0370, max: 1.3704, stdev: 0.2255
New network+MCTS average reward: 0.7109, min: 0.0370, max: 1.3333, stdev: 0.2299
Old bare network average reward: 0.6768, min: 0.0093, max: 1.3519, stdev: 0.2318
New bare network average reward: 0.6777, min: -0.0093, max: 1.3519, stdev: 0.2313
External policy "random" average reward: 0.2351, min: -0.4907, max: 0.9907, stdev: 0.2305
External policy "individual greedy" average reward: 0.5078, min: -0.0741, max: 1.2315, stdev: 0.2238
External policy "total greedy" average reward: 0.6274, min: 0.0370, max: 1.2870, stdev: 0.2173
New network won 63 and tied 173 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 560 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.61 seconds
Training examples lengths: [64614, 64822, 64634, 64746, 64837, 64603, 65121, 64844, 65080, 64786]
Total value: 492499.08
Training on 648087 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2710 (value: 0.0018, weighted value: 0.0882, policy: 0.1828, weighted policy: 0.1828), Train Mean Max: 0.9316
Epoch 2/10, Train Loss: 0.2390 (value: 0.0014, weighted value: 0.0723, policy: 0.1668, weighted policy: 0.1668), Train Mean Max: 0.9334
Epoch 3/10, Train Loss: 0.2238 (value: 0.0013, weighted value: 0.0629, policy: 0.1609, weighted policy: 0.1609), Train Mean Max: 0.9342
Epoch 4/10, Train Loss: 0.2137 (value: 0.0012, weighted value: 0.0578, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9345
Epoch 5/10, Train Loss: 0.2072 (value: 0.0011, weighted value: 0.0542, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9349
Epoch 6/10, Train Loss: 0.2028 (value: 0.0010, weighted value: 0.0506, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9352
Epoch 7/10, Train Loss: 0.1998 (value: 0.0010, weighted value: 0.0477, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9354
Epoch 8/10, Train Loss: 0.1968 (value: 0.0009, weighted value: 0.0463, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9358
Epoch 9/10, Train Loss: 0.1932 (value: 0.0009, weighted value: 0.0436, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9361
Epoch 10/10, Train Loss: 0.1911 (value: 0.0008, weighted value: 0.0418, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9363
..training done in 79.05 seconds
..evaluation done in 20.01 seconds
Old network+MCTS average reward: 0.7407, min: 0.0463, max: 1.4537, stdev: 0.2318
New network+MCTS average reward: 0.7406, min: 0.0463, max: 1.5741, stdev: 0.2383
Old bare network average reward: 0.7100, min: 0.0370, max: 1.4537, stdev: 0.2401
New bare network average reward: 0.7107, min: 0.0370, max: 1.6019, stdev: 0.2402
External policy "random" average reward: 0.2606, min: -0.4074, max: 1.1574, stdev: 0.2349
External policy "individual greedy" average reward: 0.5250, min: -0.0370, max: 1.4352, stdev: 0.2338
External policy "total greedy" average reward: 0.6431, min: 0.0278, max: 1.5556, stdev: 0.2177
New network won 66 and tied 159 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_560

Training iteration 561 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 64.79 seconds
Training examples lengths: [64822, 64634, 64746, 64837, 64603, 65121, 64844, 65080, 64786, 64769]
Total value: 492390.56
Training on 648242 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2832 (value: 0.0019, weighted value: 0.0965, policy: 0.1867, weighted policy: 0.1867), Train Mean Max: 0.9305
Epoch 2/10, Train Loss: 0.2502 (value: 0.0016, weighted value: 0.0791, policy: 0.1711, weighted policy: 0.1711), Train Mean Max: 0.9324
Epoch 3/10, Train Loss: 0.2316 (value: 0.0014, weighted value: 0.0689, policy: 0.1627, weighted policy: 0.1627), Train Mean Max: 0.9333
Epoch 4/10, Train Loss: 0.2212 (value: 0.0013, weighted value: 0.0626, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9337
Epoch 5/10, Train Loss: 0.2144 (value: 0.0012, weighted value: 0.0587, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9340
Epoch 6/10, Train Loss: 0.2083 (value: 0.0011, weighted value: 0.0547, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9345
Epoch 7/10, Train Loss: 0.2039 (value: 0.0010, weighted value: 0.0516, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9347
Epoch 8/10, Train Loss: 0.2005 (value: 0.0010, weighted value: 0.0489, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9351
Epoch 9/10, Train Loss: 0.1980 (value: 0.0009, weighted value: 0.0470, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9353
Epoch 10/10, Train Loss: 0.1949 (value: 0.0009, weighted value: 0.0441, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9358
..training done in 78.73 seconds
..evaluation done in 20.45 seconds
Old network+MCTS average reward: 0.7386, min: 0.1481, max: 1.4907, stdev: 0.2339
New network+MCTS average reward: 0.7419, min: 0.1574, max: 1.4907, stdev: 0.2349
Old bare network average reward: 0.7113, min: 0.1574, max: 1.4074, stdev: 0.2348
New bare network average reward: 0.7136, min: 0.1574, max: 1.4074, stdev: 0.2354
External policy "random" average reward: 0.2604, min: -0.3333, max: 0.9074, stdev: 0.2205
External policy "individual greedy" average reward: 0.5297, min: -0.0648, max: 1.0741, stdev: 0.2191
External policy "total greedy" average reward: 0.6434, min: 0.1389, max: 1.2963, stdev: 0.2118
New network won 66 and tied 181 out of 300 games (52.17% wins where ties are half wins)
Keeping the new network

Training iteration 562 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.26 seconds
Training examples lengths: [64634, 64746, 64837, 64603, 65121, 64844, 65080, 64786, 64769, 64774]
Total value: 492877.31
Training on 648194 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2120 (value: 0.0011, weighted value: 0.0544, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9347
Epoch 2/10, Train Loss: 0.2019 (value: 0.0010, weighted value: 0.0483, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9354
Epoch 3/10, Train Loss: 0.1976 (value: 0.0009, weighted value: 0.0463, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9359
Epoch 4/10, Train Loss: 0.1953 (value: 0.0009, weighted value: 0.0450, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9360
Epoch 5/10, Train Loss: 0.1909 (value: 0.0008, weighted value: 0.0419, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9364
Epoch 6/10, Train Loss: 0.1886 (value: 0.0008, weighted value: 0.0407, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9366
Epoch 7/10, Train Loss: 0.1870 (value: 0.0008, weighted value: 0.0388, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9369
Epoch 8/10, Train Loss: 0.1863 (value: 0.0008, weighted value: 0.0389, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9372
Epoch 9/10, Train Loss: 0.1847 (value: 0.0008, weighted value: 0.0376, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9373
Epoch 10/10, Train Loss: 0.1826 (value: 0.0007, weighted value: 0.0360, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9377
..training done in 77.70 seconds
..evaluation done in 19.53 seconds
Old network+MCTS average reward: 0.7373, min: 0.1944, max: 1.5185, stdev: 0.2256
New network+MCTS average reward: 0.7343, min: 0.2222, max: 1.5185, stdev: 0.2282
Old bare network average reward: 0.7016, min: 0.1944, max: 1.5093, stdev: 0.2314
New bare network average reward: 0.7007, min: 0.1574, max: 1.5093, stdev: 0.2309
External policy "random" average reward: 0.2477, min: -0.3519, max: 1.0370, stdev: 0.2322
External policy "individual greedy" average reward: 0.5274, min: -0.0463, max: 1.1852, stdev: 0.2202
External policy "total greedy" average reward: 0.6369, min: 0.0093, max: 1.3981, stdev: 0.2114
New network won 58 and tied 169 out of 300 games (47.50% wins where ties are half wins)
Reverting to the old network

Training iteration 563 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.22 seconds
Training examples lengths: [64746, 64837, 64603, 65121, 64844, 65080, 64786, 64769, 64774, 64702]
Total value: 492707.19
Training on 648262 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2267 (value: 0.0013, weighted value: 0.0630, policy: 0.1637, weighted policy: 0.1637), Train Mean Max: 0.9334
Epoch 2/10, Train Loss: 0.2158 (value: 0.0012, weighted value: 0.0577, policy: 0.1581, weighted policy: 0.1581), Train Mean Max: 0.9343
Epoch 3/10, Train Loss: 0.2053 (value: 0.0010, weighted value: 0.0518, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9349
Epoch 4/10, Train Loss: 0.2017 (value: 0.0010, weighted value: 0.0490, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9353
Epoch 5/10, Train Loss: 0.1990 (value: 0.0009, weighted value: 0.0470, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9355
Epoch 6/10, Train Loss: 0.1940 (value: 0.0009, weighted value: 0.0439, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9360
Epoch 7/10, Train Loss: 0.1928 (value: 0.0009, weighted value: 0.0431, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9361
Epoch 8/10, Train Loss: 0.1903 (value: 0.0008, weighted value: 0.0413, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9365
Epoch 9/10, Train Loss: 0.1886 (value: 0.0008, weighted value: 0.0400, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9368
Epoch 10/10, Train Loss: 0.1857 (value: 0.0008, weighted value: 0.0378, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9369
..training done in 82.41 seconds
..evaluation done in 20.71 seconds
Old network+MCTS average reward: 0.7315, min: 0.2037, max: 1.4074, stdev: 0.2078
New network+MCTS average reward: 0.7317, min: 0.2685, max: 1.3981, stdev: 0.2093
Old bare network average reward: 0.7002, min: 0.2037, max: 1.4074, stdev: 0.2123
New bare network average reward: 0.7021, min: 0.2500, max: 1.3796, stdev: 0.2121
External policy "random" average reward: 0.2672, min: -0.2963, max: 0.8704, stdev: 0.2142
External policy "individual greedy" average reward: 0.5199, min: 0.0648, max: 1.1944, stdev: 0.2085
External policy "total greedy" average reward: 0.6396, min: 0.1852, max: 1.3426, stdev: 0.1997
New network won 79 and tied 142 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 564 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.24 seconds
Training examples lengths: [64837, 64603, 65121, 64844, 65080, 64786, 64769, 64774, 64702, 64880]
Total value: 493206.67
Training on 648396 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2053 (value: 0.0010, weighted value: 0.0493, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9354
Epoch 2/10, Train Loss: 0.1977 (value: 0.0009, weighted value: 0.0446, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9362
Epoch 3/10, Train Loss: 0.1918 (value: 0.0008, weighted value: 0.0420, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9366
Epoch 4/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0404, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9368
Epoch 5/10, Train Loss: 0.1871 (value: 0.0008, weighted value: 0.0393, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9370
Epoch 6/10, Train Loss: 0.1859 (value: 0.0008, weighted value: 0.0379, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9372
Epoch 7/10, Train Loss: 0.1838 (value: 0.0007, weighted value: 0.0358, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9375
Epoch 8/10, Train Loss: 0.1825 (value: 0.0007, weighted value: 0.0354, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9376
Epoch 9/10, Train Loss: 0.1822 (value: 0.0007, weighted value: 0.0358, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9378
Epoch 10/10, Train Loss: 0.1797 (value: 0.0007, weighted value: 0.0333, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9378
..training done in 68.26 seconds
..evaluation done in 19.24 seconds
Old network+MCTS average reward: 0.7527, min: 0.0278, max: 1.4907, stdev: 0.2358
New network+MCTS average reward: 0.7525, min: 0.0741, max: 1.4907, stdev: 0.2343
Old bare network average reward: 0.7188, min: 0.0278, max: 1.4907, stdev: 0.2375
New bare network average reward: 0.7225, min: 0.0741, max: 1.4907, stdev: 0.2370
External policy "random" average reward: 0.2725, min: -0.3611, max: 0.8056, stdev: 0.2238
External policy "individual greedy" average reward: 0.5521, min: 0.0278, max: 1.3148, stdev: 0.2300
External policy "total greedy" average reward: 0.6625, min: 0.0185, max: 1.4537, stdev: 0.2253
New network won 66 and tied 173 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 565 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.49 seconds
Training examples lengths: [64603, 65121, 64844, 65080, 64786, 64769, 64774, 64702, 64880, 64903]
Total value: 493437.39
Training on 648462 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1997 (value: 0.0009, weighted value: 0.0440, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9365
Epoch 2/10, Train Loss: 0.1934 (value: 0.0008, weighted value: 0.0418, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9373
Epoch 3/10, Train Loss: 0.1875 (value: 0.0008, weighted value: 0.0390, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9377
Epoch 4/10, Train Loss: 0.1852 (value: 0.0008, weighted value: 0.0377, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9378
Epoch 5/10, Train Loss: 0.1831 (value: 0.0007, weighted value: 0.0363, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9379
Epoch 6/10, Train Loss: 0.1816 (value: 0.0007, weighted value: 0.0346, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9381
Epoch 7/10, Train Loss: 0.1796 (value: 0.0007, weighted value: 0.0337, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9383
Epoch 8/10, Train Loss: 0.1781 (value: 0.0007, weighted value: 0.0327, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1792 (value: 0.0007, weighted value: 0.0330, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1768 (value: 0.0006, weighted value: 0.0309, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9387
..training done in 75.54 seconds
..evaluation done in 20.11 seconds
Old network+MCTS average reward: 0.7485, min: 0.1389, max: 1.6667, stdev: 0.2353
New network+MCTS average reward: 0.7519, min: 0.1389, max: 1.6667, stdev: 0.2322
Old bare network average reward: 0.7197, min: 0.1389, max: 1.6667, stdev: 0.2355
New bare network average reward: 0.7190, min: 0.1389, max: 1.6667, stdev: 0.2335
External policy "random" average reward: 0.2672, min: -0.3333, max: 0.9907, stdev: 0.2336
External policy "individual greedy" average reward: 0.5375, min: -0.0833, max: 1.4259, stdev: 0.2296
External policy "total greedy" average reward: 0.6539, min: 0.1667, max: 1.6019, stdev: 0.2162
New network won 68 and tied 177 out of 300 games (52.17% wins where ties are half wins)
Keeping the new network

Training iteration 566 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.28 seconds
Training examples lengths: [65121, 64844, 65080, 64786, 64769, 64774, 64702, 64880, 64903, 64907]
Total value: 493349.09
Training on 648766 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1982 (value: 0.0009, weighted value: 0.0445, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9376
Epoch 2/10, Train Loss: 0.1885 (value: 0.0008, weighted value: 0.0382, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9381
Epoch 3/10, Train Loss: 0.1850 (value: 0.0007, weighted value: 0.0374, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9384
Epoch 4/10, Train Loss: 0.1812 (value: 0.0007, weighted value: 0.0350, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9387
Epoch 5/10, Train Loss: 0.1800 (value: 0.0007, weighted value: 0.0343, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9387
Epoch 6/10, Train Loss: 0.1789 (value: 0.0007, weighted value: 0.0331, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9389
Epoch 7/10, Train Loss: 0.1774 (value: 0.0006, weighted value: 0.0320, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9390
Epoch 8/10, Train Loss: 0.1777 (value: 0.0007, weighted value: 0.0329, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9391
Epoch 9/10, Train Loss: 0.1754 (value: 0.0006, weighted value: 0.0309, policy: 0.1445, weighted policy: 0.1445), Train Mean Max: 0.9393
Epoch 10/10, Train Loss: 0.1739 (value: 0.0006, weighted value: 0.0291, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9393
..training done in 75.19 seconds
..evaluation done in 20.60 seconds
Old network+MCTS average reward: 0.7659, min: 0.1852, max: 1.3704, stdev: 0.2227
New network+MCTS average reward: 0.7665, min: 0.1852, max: 1.3426, stdev: 0.2221
Old bare network average reward: 0.7269, min: 0.1852, max: 1.3704, stdev: 0.2268
New bare network average reward: 0.7344, min: 0.1852, max: 1.4630, stdev: 0.2258
External policy "random" average reward: 0.2850, min: -0.3426, max: 0.9352, stdev: 0.2302
External policy "individual greedy" average reward: 0.5490, min: -0.1389, max: 1.1481, stdev: 0.2307
External policy "total greedy" average reward: 0.6627, min: 0.1759, max: 1.2315, stdev: 0.2208
New network won 62 and tied 177 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 567 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.23 seconds
Training examples lengths: [64844, 65080, 64786, 64769, 64774, 64702, 64880, 64903, 64907, 64974]
Total value: 493949.95
Training on 648619 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1963 (value: 0.0009, weighted value: 0.0428, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9381
Epoch 2/10, Train Loss: 0.1891 (value: 0.0008, weighted value: 0.0388, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9386
Epoch 3/10, Train Loss: 0.1831 (value: 0.0007, weighted value: 0.0358, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9389
Epoch 4/10, Train Loss: 0.1802 (value: 0.0007, weighted value: 0.0342, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9391
Epoch 5/10, Train Loss: 0.1784 (value: 0.0007, weighted value: 0.0338, policy: 0.1446, weighted policy: 0.1446), Train Mean Max: 0.9393
Epoch 6/10, Train Loss: 0.1774 (value: 0.0006, weighted value: 0.0323, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9395
Epoch 7/10, Train Loss: 0.1763 (value: 0.0006, weighted value: 0.0319, policy: 0.1443, weighted policy: 0.1443), Train Mean Max: 0.9394
Epoch 8/10, Train Loss: 0.1741 (value: 0.0006, weighted value: 0.0302, policy: 0.1439, weighted policy: 0.1439), Train Mean Max: 0.9396
Epoch 9/10, Train Loss: 0.1736 (value: 0.0006, weighted value: 0.0302, policy: 0.1434, weighted policy: 0.1434), Train Mean Max: 0.9397
Epoch 10/10, Train Loss: 0.1744 (value: 0.0006, weighted value: 0.0302, policy: 0.1442, weighted policy: 0.1442), Train Mean Max: 0.9396
..training done in 74.25 seconds
..evaluation done in 20.08 seconds
Old network+MCTS average reward: 0.7587, min: 0.1111, max: 1.4630, stdev: 0.2381
New network+MCTS average reward: 0.7591, min: 0.1111, max: 1.4352, stdev: 0.2374
Old bare network average reward: 0.7240, min: 0.1019, max: 1.3981, stdev: 0.2391
New bare network average reward: 0.7283, min: 0.1204, max: 1.3981, stdev: 0.2393
External policy "random" average reward: 0.2719, min: -0.3796, max: 0.9722, stdev: 0.2350
External policy "individual greedy" average reward: 0.5457, min: -0.0185, max: 1.3148, stdev: 0.2278
External policy "total greedy" average reward: 0.6608, min: 0.0648, max: 1.5093, stdev: 0.2231
New network won 62 and tied 181 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 568 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.41 seconds
Training examples lengths: [65080, 64786, 64769, 64774, 64702, 64880, 64903, 64907, 64974, 64732]
Total value: 494207.39
Training on 648507 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1953 (value: 0.0008, weighted value: 0.0409, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9380
Epoch 2/10, Train Loss: 0.1878 (value: 0.0008, weighted value: 0.0379, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9387
Epoch 3/10, Train Loss: 0.1817 (value: 0.0007, weighted value: 0.0346, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9392
Epoch 4/10, Train Loss: 0.1799 (value: 0.0007, weighted value: 0.0339, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9392
Epoch 5/10, Train Loss: 0.1775 (value: 0.0007, weighted value: 0.0327, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9394
Epoch 6/10, Train Loss: 0.1765 (value: 0.0006, weighted value: 0.0320, policy: 0.1445, weighted policy: 0.1445), Train Mean Max: 0.9394
Epoch 7/10, Train Loss: 0.1754 (value: 0.0006, weighted value: 0.0306, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9394
Epoch 8/10, Train Loss: 0.1741 (value: 0.0006, weighted value: 0.0300, policy: 0.1441, weighted policy: 0.1441), Train Mean Max: 0.9397
Epoch 9/10, Train Loss: 0.1744 (value: 0.0006, weighted value: 0.0299, policy: 0.1445, weighted policy: 0.1445), Train Mean Max: 0.9397
Epoch 10/10, Train Loss: 0.1725 (value: 0.0006, weighted value: 0.0287, policy: 0.1438, weighted policy: 0.1438), Train Mean Max: 0.9399
..training done in 70.80 seconds
..evaluation done in 20.10 seconds
Old network+MCTS average reward: 0.7393, min: 0.2407, max: 1.6481, stdev: 0.2201
New network+MCTS average reward: 0.7402, min: 0.2407, max: 1.6481, stdev: 0.2184
Old bare network average reward: 0.7096, min: 0.2037, max: 1.6481, stdev: 0.2209
New bare network average reward: 0.7076, min: 0.1852, max: 1.6481, stdev: 0.2174
External policy "random" average reward: 0.2617, min: -0.3611, max: 0.9537, stdev: 0.2095
External policy "individual greedy" average reward: 0.5445, min: 0.0185, max: 1.2685, stdev: 0.2127
External policy "total greedy" average reward: 0.6510, min: 0.1852, max: 1.3333, stdev: 0.2051
New network won 62 and tied 172 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 569 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.87 seconds
Training examples lengths: [64786, 64769, 64774, 64702, 64880, 64903, 64907, 64974, 64732, 64955]
Total value: 494276.16
Training on 648382 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2160 (value: 0.0011, weighted value: 0.0543, policy: 0.1617, weighted policy: 0.1617), Train Mean Max: 0.9365
Epoch 2/10, Train Loss: 0.2012 (value: 0.0009, weighted value: 0.0461, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9375
Epoch 3/10, Train Loss: 0.1936 (value: 0.0009, weighted value: 0.0430, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9380
Epoch 4/10, Train Loss: 0.1876 (value: 0.0008, weighted value: 0.0398, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9383
Epoch 5/10, Train Loss: 0.1862 (value: 0.0008, weighted value: 0.0393, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9383
Epoch 6/10, Train Loss: 0.1838 (value: 0.0007, weighted value: 0.0373, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9384
Epoch 7/10, Train Loss: 0.1817 (value: 0.0007, weighted value: 0.0361, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9387
Epoch 8/10, Train Loss: 0.1794 (value: 0.0007, weighted value: 0.0338, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9387
Epoch 9/10, Train Loss: 0.1794 (value: 0.0007, weighted value: 0.0336, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9389
Epoch 10/10, Train Loss: 0.1785 (value: 0.0006, weighted value: 0.0325, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9390
..training done in 88.02 seconds
..evaluation done in 21.31 seconds
Old network+MCTS average reward: 0.7676, min: 0.0556, max: 1.5370, stdev: 0.2622
New network+MCTS average reward: 0.7684, min: 0.0648, max: 1.5370, stdev: 0.2611
Old bare network average reward: 0.7364, min: 0.0093, max: 1.5093, stdev: 0.2662
New bare network average reward: 0.7357, min: 0.0185, max: 1.5278, stdev: 0.2666
External policy "random" average reward: 0.2854, min: -0.3241, max: 1.0278, stdev: 0.2416
External policy "individual greedy" average reward: 0.5733, min: -0.0926, max: 1.2593, stdev: 0.2491
External policy "total greedy" average reward: 0.6770, min: -0.0556, max: 1.4167, stdev: 0.2486
New network won 64 and tied 171 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 570 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 60.35 seconds
Training examples lengths: [64769, 64774, 64702, 64880, 64903, 64907, 64974, 64732, 64955, 64717]
Total value: 494139.57
Training on 648313 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2357 (value: 0.0013, weighted value: 0.0653, policy: 0.1704, weighted policy: 0.1704), Train Mean Max: 0.9351
Epoch 2/10, Train Loss: 0.2155 (value: 0.0011, weighted value: 0.0554, policy: 0.1601, weighted policy: 0.1601), Train Mean Max: 0.9364
Epoch 3/10, Train Loss: 0.2025 (value: 0.0010, weighted value: 0.0492, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9369
Epoch 4/10, Train Loss: 0.1972 (value: 0.0009, weighted value: 0.0460, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9370
Epoch 5/10, Train Loss: 0.1934 (value: 0.0009, weighted value: 0.0439, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9373
Epoch 6/10, Train Loss: 0.1920 (value: 0.0009, weighted value: 0.0432, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9375
Epoch 7/10, Train Loss: 0.1874 (value: 0.0008, weighted value: 0.0396, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9377
Epoch 8/10, Train Loss: 0.1856 (value: 0.0008, weighted value: 0.0379, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1846 (value: 0.0007, weighted value: 0.0373, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9380
Epoch 10/10, Train Loss: 0.1826 (value: 0.0007, weighted value: 0.0360, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9381
..training done in 80.93 seconds
..evaluation done in 19.92 seconds
Old network+MCTS average reward: 0.7336, min: 0.0833, max: 1.4537, stdev: 0.2407
New network+MCTS average reward: 0.7363, min: 0.0833, max: 1.4907, stdev: 0.2428
Old bare network average reward: 0.7091, min: 0.0463, max: 1.4907, stdev: 0.2409
New bare network average reward: 0.7066, min: 0.0648, max: 1.4907, stdev: 0.2457
External policy "random" average reward: 0.2578, min: -0.4074, max: 0.8056, stdev: 0.2227
External policy "individual greedy" average reward: 0.5393, min: -0.0556, max: 1.1852, stdev: 0.2252
External policy "total greedy" average reward: 0.6530, min: 0.0926, max: 1.1944, stdev: 0.2205
New network won 81 and tied 161 out of 300 games (53.83% wins where ties are half wins)
Keeping the new network

Training iteration 571 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.55 seconds
Training examples lengths: [64774, 64702, 64880, 64903, 64907, 64974, 64732, 64955, 64717, 64841]
Total value: 494268.19
Training on 648385 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2015 (value: 0.0009, weighted value: 0.0463, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9368
Epoch 2/10, Train Loss: 0.1945 (value: 0.0008, weighted value: 0.0420, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9372
Epoch 3/10, Train Loss: 0.1904 (value: 0.0008, weighted value: 0.0407, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9376
Epoch 4/10, Train Loss: 0.1863 (value: 0.0008, weighted value: 0.0380, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9380
Epoch 5/10, Train Loss: 0.1836 (value: 0.0007, weighted value: 0.0369, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9381
Epoch 6/10, Train Loss: 0.1824 (value: 0.0007, weighted value: 0.0352, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9382
Epoch 7/10, Train Loss: 0.1815 (value: 0.0007, weighted value: 0.0348, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9383
Epoch 8/10, Train Loss: 0.1799 (value: 0.0007, weighted value: 0.0333, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1795 (value: 0.0007, weighted value: 0.0331, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9385
Epoch 10/10, Train Loss: 0.1767 (value: 0.0006, weighted value: 0.0316, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9388
..training done in 80.93 seconds
..evaluation done in 21.22 seconds
Old network+MCTS average reward: 0.7367, min: 0.1389, max: 1.5648, stdev: 0.2331
New network+MCTS average reward: 0.7366, min: 0.1389, max: 1.5463, stdev: 0.2296
Old bare network average reward: 0.7045, min: 0.0556, max: 1.4259, stdev: 0.2339
New bare network average reward: 0.7015, min: 0.1389, max: 1.4259, stdev: 0.2306
External policy "random" average reward: 0.2518, min: -0.3519, max: 1.1667, stdev: 0.2353
External policy "individual greedy" average reward: 0.5280, min: -0.0556, max: 1.4907, stdev: 0.2360
External policy "total greedy" average reward: 0.6479, min: 0.1481, max: 1.6111, stdev: 0.2226
New network won 69 and tied 168 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 572 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 60.45 seconds
Training examples lengths: [64702, 64880, 64903, 64907, 64974, 64732, 64955, 64717, 64841, 64662]
Total value: 493772.03
Training on 648273 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1977 (value: 0.0009, weighted value: 0.0433, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9372
Epoch 2/10, Train Loss: 0.1903 (value: 0.0008, weighted value: 0.0395, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9378
Epoch 3/10, Train Loss: 0.1853 (value: 0.0007, weighted value: 0.0369, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9381
Epoch 4/10, Train Loss: 0.1841 (value: 0.0007, weighted value: 0.0368, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9382
Epoch 5/10, Train Loss: 0.1804 (value: 0.0007, weighted value: 0.0341, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9385
Epoch 6/10, Train Loss: 0.1797 (value: 0.0007, weighted value: 0.0330, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9384
Epoch 7/10, Train Loss: 0.1794 (value: 0.0007, weighted value: 0.0330, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9386
Epoch 8/10, Train Loss: 0.1779 (value: 0.0006, weighted value: 0.0322, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9388
Epoch 9/10, Train Loss: 0.1770 (value: 0.0006, weighted value: 0.0313, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9390
Epoch 10/10, Train Loss: 0.1755 (value: 0.0006, weighted value: 0.0307, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9390
..training done in 78.44 seconds
..evaluation done in 20.33 seconds
Old network+MCTS average reward: 0.7475, min: 0.1852, max: 1.6296, stdev: 0.2362
New network+MCTS average reward: 0.7482, min: 0.1852, max: 1.5556, stdev: 0.2372
Old bare network average reward: 0.7136, min: 0.0926, max: 1.5370, stdev: 0.2427
New bare network average reward: 0.7162, min: 0.1296, max: 1.5556, stdev: 0.2406
External policy "random" average reward: 0.2557, min: -0.3056, max: 1.0556, stdev: 0.2364
External policy "individual greedy" average reward: 0.5466, min: 0.0000, max: 1.4167, stdev: 0.2313
External policy "total greedy" average reward: 0.6559, min: 0.1296, max: 1.4537, stdev: 0.2239
New network won 57 and tied 192 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 573 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.38 seconds
Training examples lengths: [64880, 64903, 64907, 64974, 64732, 64955, 64717, 64841, 64662, 64759]
Total value: 494163.92
Training on 648330 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1971 (value: 0.0008, weighted value: 0.0424, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9372
Epoch 2/10, Train Loss: 0.1887 (value: 0.0008, weighted value: 0.0384, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9381
Epoch 3/10, Train Loss: 0.1844 (value: 0.0007, weighted value: 0.0358, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9384
Epoch 4/10, Train Loss: 0.1809 (value: 0.0007, weighted value: 0.0343, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9384
Epoch 5/10, Train Loss: 0.1786 (value: 0.0007, weighted value: 0.0333, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9386
Epoch 6/10, Train Loss: 0.1793 (value: 0.0007, weighted value: 0.0330, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9385
Epoch 7/10, Train Loss: 0.1764 (value: 0.0006, weighted value: 0.0314, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9389
Epoch 8/10, Train Loss: 0.1760 (value: 0.0006, weighted value: 0.0307, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9388
Epoch 9/10, Train Loss: 0.1761 (value: 0.0006, weighted value: 0.0306, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9390
Epoch 10/10, Train Loss: 0.1749 (value: 0.0006, weighted value: 0.0294, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9390
..training done in 75.77 seconds
..evaluation done in 20.18 seconds
Old network+MCTS average reward: 0.7237, min: 0.0556, max: 1.4167, stdev: 0.2245
New network+MCTS average reward: 0.7234, min: 0.0556, max: 1.4167, stdev: 0.2271
Old bare network average reward: 0.6872, min: 0.0556, max: 1.3981, stdev: 0.2311
New bare network average reward: 0.6875, min: 0.0556, max: 1.3981, stdev: 0.2302
External policy "random" average reward: 0.2340, min: -0.3241, max: 0.8333, stdev: 0.2180
External policy "individual greedy" average reward: 0.5231, min: -0.0926, max: 1.2500, stdev: 0.2167
External policy "total greedy" average reward: 0.6310, min: 0.0278, max: 1.3704, stdev: 0.2145
New network won 59 and tied 175 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 574 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 61.55 seconds
Training examples lengths: [64903, 64907, 64974, 64732, 64955, 64717, 64841, 64662, 64759, 64692]
Total value: 494304.44
Training on 648142 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2159 (value: 0.0011, weighted value: 0.0542, policy: 0.1617, weighted policy: 0.1617), Train Mean Max: 0.9358
Epoch 2/10, Train Loss: 0.2025 (value: 0.0009, weighted value: 0.0458, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9368
Epoch 3/10, Train Loss: 0.1943 (value: 0.0009, weighted value: 0.0434, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9373
Epoch 4/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0410, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1874 (value: 0.0008, weighted value: 0.0393, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9377
Epoch 6/10, Train Loss: 0.1843 (value: 0.0007, weighted value: 0.0372, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9379
Epoch 7/10, Train Loss: 0.1836 (value: 0.0007, weighted value: 0.0365, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1817 (value: 0.0007, weighted value: 0.0351, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1808 (value: 0.0007, weighted value: 0.0337, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9383
Epoch 10/10, Train Loss: 0.1795 (value: 0.0007, weighted value: 0.0332, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9385
..training done in 76.54 seconds
..evaluation done in 20.38 seconds
Old network+MCTS average reward: 0.7336, min: 0.0370, max: 1.3611, stdev: 0.2263
New network+MCTS average reward: 0.7355, min: 0.0648, max: 1.3981, stdev: 0.2254
Old bare network average reward: 0.7027, min: 0.0093, max: 1.3704, stdev: 0.2309
New bare network average reward: 0.7024, min: 0.0093, max: 1.3704, stdev: 0.2297
External policy "random" average reward: 0.2437, min: -0.2870, max: 0.8796, stdev: 0.2103
External policy "individual greedy" average reward: 0.5248, min: -0.0463, max: 1.1389, stdev: 0.2250
External policy "total greedy" average reward: 0.6423, min: 0.1204, max: 1.2500, stdev: 0.2156
New network won 67 and tied 174 out of 300 games (51.33% wins where ties are half wins)
Keeping the new network

Training iteration 575 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.37 seconds
Training examples lengths: [64907, 64974, 64732, 64955, 64717, 64841, 64662, 64759, 64692, 64789]
Total value: 494006.50
Training on 648028 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1997 (value: 0.0009, weighted value: 0.0450, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9369
Epoch 2/10, Train Loss: 0.1913 (value: 0.0008, weighted value: 0.0393, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9374
Epoch 3/10, Train Loss: 0.1877 (value: 0.0008, weighted value: 0.0381, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9378
Epoch 4/10, Train Loss: 0.1845 (value: 0.0007, weighted value: 0.0373, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9380
Epoch 5/10, Train Loss: 0.1815 (value: 0.0007, weighted value: 0.0348, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9382
Epoch 6/10, Train Loss: 0.1809 (value: 0.0007, weighted value: 0.0340, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9382
Epoch 7/10, Train Loss: 0.1800 (value: 0.0007, weighted value: 0.0335, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9382
Epoch 8/10, Train Loss: 0.1785 (value: 0.0006, weighted value: 0.0325, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9385
Epoch 9/10, Train Loss: 0.1764 (value: 0.0006, weighted value: 0.0313, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9387
Epoch 10/10, Train Loss: 0.1774 (value: 0.0006, weighted value: 0.0310, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9385
..training done in 60.67 seconds
..evaluation done in 19.68 seconds
Old network+MCTS average reward: 0.7573, min: 0.1759, max: 1.4074, stdev: 0.2282
New network+MCTS average reward: 0.7552, min: 0.1389, max: 1.4074, stdev: 0.2278
Old bare network average reward: 0.7232, min: 0.1296, max: 1.4074, stdev: 0.2325
New bare network average reward: 0.7196, min: 0.0463, max: 1.3796, stdev: 0.2365
External policy "random" average reward: 0.2557, min: -0.3148, max: 0.8611, stdev: 0.2298
External policy "individual greedy" average reward: 0.5356, min: -0.1019, max: 1.2500, stdev: 0.2272
External policy "total greedy" average reward: 0.6522, min: 0.1481, max: 1.3148, stdev: 0.2196
New network won 62 and tied 169 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 576 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.79 seconds
Training examples lengths: [64974, 64732, 64955, 64717, 64841, 64662, 64759, 64692, 64789, 65028]
Total value: 494297.05
Training on 648149 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2173 (value: 0.0011, weighted value: 0.0554, policy: 0.1619, weighted policy: 0.1619), Train Mean Max: 0.9355
Epoch 2/10, Train Loss: 0.2053 (value: 0.0010, weighted value: 0.0486, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9362
Epoch 3/10, Train Loss: 0.1969 (value: 0.0009, weighted value: 0.0449, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9368
Epoch 4/10, Train Loss: 0.1920 (value: 0.0008, weighted value: 0.0424, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9369
Epoch 5/10, Train Loss: 0.1894 (value: 0.0008, weighted value: 0.0403, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9371
Epoch 6/10, Train Loss: 0.1877 (value: 0.0008, weighted value: 0.0394, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9372
Epoch 7/10, Train Loss: 0.1858 (value: 0.0007, weighted value: 0.0370, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9374
Epoch 8/10, Train Loss: 0.1834 (value: 0.0007, weighted value: 0.0355, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9377
Epoch 9/10, Train Loss: 0.1827 (value: 0.0007, weighted value: 0.0356, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9378
Epoch 10/10, Train Loss: 0.1817 (value: 0.0007, weighted value: 0.0344, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9379
..training done in 63.76 seconds
..evaluation done in 20.35 seconds
Old network+MCTS average reward: 0.7455, min: 0.1296, max: 1.6389, stdev: 0.2434
New network+MCTS average reward: 0.7451, min: 0.1481, max: 1.6389, stdev: 0.2469
Old bare network average reward: 0.7152, min: 0.1296, max: 1.6389, stdev: 0.2448
New bare network average reward: 0.7152, min: 0.1296, max: 1.6389, stdev: 0.2474
External policy "random" average reward: 0.2684, min: -0.4722, max: 0.9537, stdev: 0.2331
External policy "individual greedy" average reward: 0.5384, min: 0.0093, max: 1.3241, stdev: 0.2319
External policy "total greedy" average reward: 0.6490, min: 0.1204, max: 1.4074, stdev: 0.2293
New network won 66 and tied 171 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 577 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.03 seconds
Training examples lengths: [64732, 64955, 64717, 64841, 64662, 64759, 64692, 64789, 65028, 64665]
Total value: 493936.95
Training on 647840 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2010 (value: 0.0009, weighted value: 0.0449, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9365
Epoch 2/10, Train Loss: 0.1937 (value: 0.0008, weighted value: 0.0403, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9372
Epoch 3/10, Train Loss: 0.1877 (value: 0.0008, weighted value: 0.0384, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9374
Epoch 4/10, Train Loss: 0.1863 (value: 0.0008, weighted value: 0.0376, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9377
Epoch 5/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0352, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9377
Epoch 6/10, Train Loss: 0.1823 (value: 0.0007, weighted value: 0.0349, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9378
Epoch 7/10, Train Loss: 0.1808 (value: 0.0007, weighted value: 0.0341, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9380
Epoch 8/10, Train Loss: 0.1793 (value: 0.0007, weighted value: 0.0325, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9382
Epoch 9/10, Train Loss: 0.1784 (value: 0.0006, weighted value: 0.0316, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9382
Epoch 10/10, Train Loss: 0.1784 (value: 0.0006, weighted value: 0.0311, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9383
..training done in 73.42 seconds
..evaluation done in 20.13 seconds
Old network+MCTS average reward: 0.7490, min: 0.0185, max: 1.4259, stdev: 0.2416
New network+MCTS average reward: 0.7506, min: 0.1296, max: 1.4259, stdev: 0.2395
Old bare network average reward: 0.7175, min: 0.0000, max: 1.4259, stdev: 0.2479
New bare network average reward: 0.7171, min: 0.0185, max: 1.3796, stdev: 0.2458
External policy "random" average reward: 0.2583, min: -0.2963, max: 0.8241, stdev: 0.2415
External policy "individual greedy" average reward: 0.5360, min: -0.1667, max: 1.2778, stdev: 0.2300
External policy "total greedy" average reward: 0.6500, min: 0.0278, max: 1.2500, stdev: 0.2310
New network won 59 and tied 176 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 578 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.38 seconds
Training examples lengths: [64955, 64717, 64841, 64662, 64759, 64692, 64789, 65028, 64665, 64717]
Total value: 493916.12
Training on 647825 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2195 (value: 0.0011, weighted value: 0.0547, policy: 0.1649, weighted policy: 0.1649), Train Mean Max: 0.9350
Epoch 2/10, Train Loss: 0.2051 (value: 0.0010, weighted value: 0.0485, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9362
Epoch 3/10, Train Loss: 0.2003 (value: 0.0009, weighted value: 0.0470, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9364
Epoch 4/10, Train Loss: 0.1935 (value: 0.0009, weighted value: 0.0426, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9368
Epoch 5/10, Train Loss: 0.1894 (value: 0.0008, weighted value: 0.0399, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9370
Epoch 6/10, Train Loss: 0.1879 (value: 0.0008, weighted value: 0.0388, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9372
Epoch 7/10, Train Loss: 0.1870 (value: 0.0008, weighted value: 0.0383, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1842 (value: 0.0007, weighted value: 0.0360, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1834 (value: 0.0007, weighted value: 0.0356, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9375
Epoch 10/10, Train Loss: 0.1818 (value: 0.0007, weighted value: 0.0342, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9377
..training done in 82.75 seconds
..evaluation done in 20.59 seconds
Old network+MCTS average reward: 0.7327, min: 0.1019, max: 1.3519, stdev: 0.2346
New network+MCTS average reward: 0.7370, min: 0.1204, max: 1.3704, stdev: 0.2337
Old bare network average reward: 0.7051, min: 0.1019, max: 1.3426, stdev: 0.2364
New bare network average reward: 0.7102, min: 0.1019, max: 1.3704, stdev: 0.2354
External policy "random" average reward: 0.2525, min: -0.2963, max: 1.0185, stdev: 0.2298
External policy "individual greedy" average reward: 0.5366, min: -0.1296, max: 1.1667, stdev: 0.2411
External policy "total greedy" average reward: 0.6514, min: 0.0556, max: 1.3333, stdev: 0.2346
New network won 79 and tied 166 out of 300 games (54.00% wins where ties are half wins)
Keeping the new network

Training iteration 579 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.21 seconds
Training examples lengths: [64717, 64841, 64662, 64759, 64692, 64789, 65028, 64665, 64717, 65090]
Total value: 494129.30
Training on 647960 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2018 (value: 0.0009, weighted value: 0.0452, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9364
Epoch 2/10, Train Loss: 0.1947 (value: 0.0008, weighted value: 0.0412, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9369
Epoch 3/10, Train Loss: 0.1894 (value: 0.0008, weighted value: 0.0392, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9373
Epoch 4/10, Train Loss: 0.1868 (value: 0.0008, weighted value: 0.0380, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9375
Epoch 5/10, Train Loss: 0.1833 (value: 0.0007, weighted value: 0.0351, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1816 (value: 0.0007, weighted value: 0.0341, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9377
Epoch 7/10, Train Loss: 0.1818 (value: 0.0007, weighted value: 0.0346, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1802 (value: 0.0006, weighted value: 0.0323, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9380
Epoch 9/10, Train Loss: 0.1801 (value: 0.0007, weighted value: 0.0331, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9382
Epoch 10/10, Train Loss: 0.1775 (value: 0.0006, weighted value: 0.0311, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9383
..training done in 73.42 seconds
..evaluation done in 20.54 seconds
Old network+MCTS average reward: 0.7419, min: 0.0463, max: 1.6667, stdev: 0.2380
New network+MCTS average reward: 0.7437, min: 0.0833, max: 1.6667, stdev: 0.2374
Old bare network average reward: 0.7083, min: 0.0463, max: 1.6667, stdev: 0.2426
New bare network average reward: 0.7101, min: 0.0833, max: 1.6204, stdev: 0.2401
External policy "random" average reward: 0.2623, min: -0.4722, max: 1.0463, stdev: 0.2282
External policy "individual greedy" average reward: 0.5404, min: -0.0648, max: 1.3519, stdev: 0.2189
External policy "total greedy" average reward: 0.6611, min: 0.0556, max: 1.5370, stdev: 0.2183
New network won 66 and tied 172 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 580 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.33 seconds
Training examples lengths: [64841, 64662, 64759, 64692, 64789, 65028, 64665, 64717, 65090, 64641]
Total value: 494207.94
Training on 647884 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1984 (value: 0.0008, weighted value: 0.0424, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9368
Epoch 2/10, Train Loss: 0.1905 (value: 0.0008, weighted value: 0.0383, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9375
Epoch 3/10, Train Loss: 0.1876 (value: 0.0008, weighted value: 0.0385, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9378
Epoch 4/10, Train Loss: 0.1826 (value: 0.0007, weighted value: 0.0351, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9380
Epoch 5/10, Train Loss: 0.1815 (value: 0.0007, weighted value: 0.0342, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9380
Epoch 6/10, Train Loss: 0.1792 (value: 0.0007, weighted value: 0.0328, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9383
Epoch 7/10, Train Loss: 0.1786 (value: 0.0006, weighted value: 0.0323, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9383
Epoch 8/10, Train Loss: 0.1774 (value: 0.0006, weighted value: 0.0317, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1771 (value: 0.0006, weighted value: 0.0307, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9385
Epoch 10/10, Train Loss: 0.1762 (value: 0.0006, weighted value: 0.0295, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9387
..training done in 83.08 seconds
..evaluation done in 21.17 seconds
Old network+MCTS average reward: 0.7490, min: 0.2222, max: 1.8241, stdev: 0.2325
New network+MCTS average reward: 0.7503, min: 0.2222, max: 1.8241, stdev: 0.2317
Old bare network average reward: 0.7162, min: 0.1944, max: 1.7500, stdev: 0.2307
New bare network average reward: 0.7190, min: 0.1944, max: 1.7870, stdev: 0.2338
External policy "random" average reward: 0.2528, min: -0.2593, max: 1.1852, stdev: 0.2230
External policy "individual greedy" average reward: 0.5253, min: -0.0648, max: 1.6296, stdev: 0.2220
External policy "total greedy" average reward: 0.6532, min: 0.1759, max: 1.6296, stdev: 0.2143
New network won 60 and tied 184 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_580

Training iteration 581 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 64.53 seconds
Training examples lengths: [64662, 64759, 64692, 64789, 65028, 64665, 64717, 65090, 64641, 64989]
Total value: 494487.44
Training on 648032 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1960 (value: 0.0008, weighted value: 0.0417, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9372
Epoch 2/10, Train Loss: 0.1883 (value: 0.0007, weighted value: 0.0373, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9378
Epoch 3/10, Train Loss: 0.1847 (value: 0.0007, weighted value: 0.0355, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9381
Epoch 4/10, Train Loss: 0.1822 (value: 0.0007, weighted value: 0.0344, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9384
Epoch 5/10, Train Loss: 0.1787 (value: 0.0007, weighted value: 0.0328, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9386
Epoch 6/10, Train Loss: 0.1790 (value: 0.0006, weighted value: 0.0320, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9385
Epoch 7/10, Train Loss: 0.1775 (value: 0.0006, weighted value: 0.0315, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9387
Epoch 8/10, Train Loss: 0.1767 (value: 0.0006, weighted value: 0.0307, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9387
Epoch 9/10, Train Loss: 0.1751 (value: 0.0006, weighted value: 0.0298, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9390
Epoch 10/10, Train Loss: 0.1740 (value: 0.0006, weighted value: 0.0289, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9390
..training done in 75.23 seconds
..evaluation done in 21.47 seconds
Old network+MCTS average reward: 0.7449, min: 0.1574, max: 1.4074, stdev: 0.2250
New network+MCTS average reward: 0.7464, min: 0.1296, max: 1.4907, stdev: 0.2251
Old bare network average reward: 0.7130, min: 0.1296, max: 1.4074, stdev: 0.2270
New bare network average reward: 0.7137, min: 0.1296, max: 1.4815, stdev: 0.2306
External policy "random" average reward: 0.2735, min: -0.2778, max: 0.8796, stdev: 0.2144
External policy "individual greedy" average reward: 0.5425, min: 0.0093, max: 1.2130, stdev: 0.2091
External policy "total greedy" average reward: 0.6562, min: 0.1389, max: 1.4352, stdev: 0.2102
New network won 66 and tied 180 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network

Training iteration 582 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.92 seconds
Training examples lengths: [64759, 64692, 64789, 65028, 64665, 64717, 65090, 64641, 64989, 64996]
Total value: 495464.18
Training on 648366 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1972 (value: 0.0008, weighted value: 0.0418, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9374
Epoch 2/10, Train Loss: 0.1876 (value: 0.0007, weighted value: 0.0367, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9381
Epoch 3/10, Train Loss: 0.1830 (value: 0.0007, weighted value: 0.0351, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9384
Epoch 4/10, Train Loss: 0.1805 (value: 0.0007, weighted value: 0.0333, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9384
Epoch 5/10, Train Loss: 0.1792 (value: 0.0007, weighted value: 0.0326, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9387
Epoch 6/10, Train Loss: 0.1778 (value: 0.0006, weighted value: 0.0315, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9387
Epoch 7/10, Train Loss: 0.1758 (value: 0.0006, weighted value: 0.0302, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9389
Epoch 8/10, Train Loss: 0.1748 (value: 0.0006, weighted value: 0.0297, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9390
Epoch 9/10, Train Loss: 0.1750 (value: 0.0006, weighted value: 0.0292, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9390
Epoch 10/10, Train Loss: 0.1749 (value: 0.0006, weighted value: 0.0291, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9392
..training done in 77.73 seconds
..evaluation done in 21.41 seconds
Old network+MCTS average reward: 0.7240, min: 0.1667, max: 1.7593, stdev: 0.2442
New network+MCTS average reward: 0.7244, min: 0.1481, max: 1.7593, stdev: 0.2438
Old bare network average reward: 0.6915, min: 0.1204, max: 1.7593, stdev: 0.2504
New bare network average reward: 0.6953, min: 0.1204, max: 1.7593, stdev: 0.2492
External policy "random" average reward: 0.2469, min: -0.2963, max: 1.1389, stdev: 0.2366
External policy "individual greedy" average reward: 0.5249, min: -0.0648, max: 1.3704, stdev: 0.2382
External policy "total greedy" average reward: 0.6311, min: 0.0741, max: 1.4444, stdev: 0.2280
New network won 58 and tied 177 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 583 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 66.08 seconds
Training examples lengths: [64692, 64789, 65028, 64665, 64717, 65090, 64641, 64989, 64996, 64868]
Total value: 495583.74
Training on 648475 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2160 (value: 0.0011, weighted value: 0.0526, policy: 0.1634, weighted policy: 0.1634), Train Mean Max: 0.9359
Epoch 2/10, Train Loss: 0.2013 (value: 0.0009, weighted value: 0.0449, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9370
Epoch 3/10, Train Loss: 0.1948 (value: 0.0009, weighted value: 0.0430, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9376
Epoch 4/10, Train Loss: 0.1888 (value: 0.0008, weighted value: 0.0392, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1863 (value: 0.0008, weighted value: 0.0376, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9379
Epoch 6/10, Train Loss: 0.1844 (value: 0.0007, weighted value: 0.0365, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9380
Epoch 7/10, Train Loss: 0.1819 (value: 0.0007, weighted value: 0.0350, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9382
Epoch 8/10, Train Loss: 0.1803 (value: 0.0007, weighted value: 0.0338, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1792 (value: 0.0006, weighted value: 0.0325, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9383
Epoch 10/10, Train Loss: 0.1780 (value: 0.0006, weighted value: 0.0321, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9386
..training done in 84.41 seconds
..evaluation done in 21.46 seconds
Old network+MCTS average reward: 0.7103, min: 0.0926, max: 1.3611, stdev: 0.2167
New network+MCTS average reward: 0.7141, min: 0.0185, max: 1.4630, stdev: 0.2175
Old bare network average reward: 0.6821, min: 0.0463, max: 1.3241, stdev: 0.2172
New bare network average reward: 0.6780, min: -0.0093, max: 1.3611, stdev: 0.2213
External policy "random" average reward: 0.2369, min: -0.3704, max: 0.9907, stdev: 0.2232
External policy "individual greedy" average reward: 0.5165, min: -0.0556, max: 1.4537, stdev: 0.2138
External policy "total greedy" average reward: 0.6233, min: -0.1574, max: 1.2315, stdev: 0.2038
New network won 78 and tied 165 out of 300 games (53.50% wins where ties are half wins)
Keeping the new network

Training iteration 584 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.80 seconds
Training examples lengths: [64789, 65028, 64665, 64717, 65090, 64641, 64989, 64996, 64868, 64820]
Total value: 495831.52
Training on 648603 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1965 (value: 0.0008, weighted value: 0.0419, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9371
Epoch 2/10, Train Loss: 0.1921 (value: 0.0008, weighted value: 0.0406, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9376
Epoch 3/10, Train Loss: 0.1847 (value: 0.0007, weighted value: 0.0357, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9380
Epoch 4/10, Train Loss: 0.1822 (value: 0.0007, weighted value: 0.0347, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9382
Epoch 5/10, Train Loss: 0.1814 (value: 0.0007, weighted value: 0.0345, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9383
Epoch 6/10, Train Loss: 0.1791 (value: 0.0006, weighted value: 0.0323, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9384
Epoch 7/10, Train Loss: 0.1782 (value: 0.0006, weighted value: 0.0320, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9386
Epoch 8/10, Train Loss: 0.1777 (value: 0.0006, weighted value: 0.0317, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9387
Epoch 9/10, Train Loss: 0.1753 (value: 0.0006, weighted value: 0.0298, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9389
Epoch 10/10, Train Loss: 0.1752 (value: 0.0006, weighted value: 0.0299, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9390
..training done in 76.41 seconds
..evaluation done in 22.69 seconds
Old network+MCTS average reward: 0.7621, min: -0.0278, max: 1.4907, stdev: 0.2442
New network+MCTS average reward: 0.7592, min: -0.0833, max: 1.5000, stdev: 0.2492
Old bare network average reward: 0.7321, min: -0.0833, max: 1.4352, stdev: 0.2468
New bare network average reward: 0.7294, min: -0.0833, max: 1.4907, stdev: 0.2466
External policy "random" average reward: 0.2619, min: -0.4815, max: 0.8519, stdev: 0.2297
External policy "individual greedy" average reward: 0.5512, min: -0.0278, max: 1.2037, stdev: 0.2331
External policy "total greedy" average reward: 0.6624, min: 0.0556, max: 1.2870, stdev: 0.2292
New network won 62 and tied 161 out of 300 games (47.50% wins where ties are half wins)
Reverting to the old network

Training iteration 585 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 60.57 seconds
Training examples lengths: [65028, 64665, 64717, 65090, 64641, 64989, 64996, 64868, 64820, 64553]
Total value: 495648.44
Training on 648367 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2194 (value: 0.0011, weighted value: 0.0567, policy: 0.1627, weighted policy: 0.1627), Train Mean Max: 0.9355
Epoch 2/10, Train Loss: 0.2016 (value: 0.0009, weighted value: 0.0459, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9366
Epoch 3/10, Train Loss: 0.1937 (value: 0.0008, weighted value: 0.0423, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9371
Epoch 4/10, Train Loss: 0.1910 (value: 0.0008, weighted value: 0.0416, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9374
Epoch 5/10, Train Loss: 0.1873 (value: 0.0008, weighted value: 0.0390, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9375
Epoch 6/10, Train Loss: 0.1854 (value: 0.0008, weighted value: 0.0377, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9377
Epoch 7/10, Train Loss: 0.1833 (value: 0.0007, weighted value: 0.0358, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9377
Epoch 8/10, Train Loss: 0.1811 (value: 0.0007, weighted value: 0.0345, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1816 (value: 0.0007, weighted value: 0.0347, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9382
Epoch 10/10, Train Loss: 0.1789 (value: 0.0007, weighted value: 0.0330, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9384
..training done in 81.60 seconds
..evaluation done in 20.77 seconds
Old network+MCTS average reward: 0.7679, min: 0.0926, max: 1.3611, stdev: 0.2140
New network+MCTS average reward: 0.7663, min: 0.0556, max: 1.3241, stdev: 0.2157
Old bare network average reward: 0.7405, min: 0.1111, max: 1.3056, stdev: 0.2183
New bare network average reward: 0.7377, min: 0.0463, max: 1.4537, stdev: 0.2212
External policy "random" average reward: 0.2665, min: -0.2963, max: 0.8241, stdev: 0.2248
External policy "individual greedy" average reward: 0.5493, min: 0.0093, max: 1.0741, stdev: 0.2074
External policy "total greedy" average reward: 0.6561, min: 0.0833, max: 1.2870, stdev: 0.2069
New network won 61 and tied 164 out of 300 games (47.67% wins where ties are half wins)
Reverting to the old network

Training iteration 586 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.67 seconds
Training examples lengths: [64665, 64717, 65090, 64641, 64989, 64996, 64868, 64820, 64553, 64420]
Total value: 495059.75
Training on 647759 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2353 (value: 0.0013, weighted value: 0.0660, policy: 0.1693, weighted policy: 0.1693), Train Mean Max: 0.9343
Epoch 2/10, Train Loss: 0.2166 (value: 0.0011, weighted value: 0.0564, policy: 0.1602, weighted policy: 0.1602), Train Mean Max: 0.9355
Epoch 3/10, Train Loss: 0.2046 (value: 0.0010, weighted value: 0.0502, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9361
Epoch 4/10, Train Loss: 0.1997 (value: 0.0010, weighted value: 0.0478, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9364
Epoch 5/10, Train Loss: 0.1936 (value: 0.0009, weighted value: 0.0434, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9366
Epoch 6/10, Train Loss: 0.1914 (value: 0.0008, weighted value: 0.0419, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9370
Epoch 7/10, Train Loss: 0.1893 (value: 0.0008, weighted value: 0.0406, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9371
Epoch 8/10, Train Loss: 0.1871 (value: 0.0008, weighted value: 0.0393, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9373
Epoch 9/10, Train Loss: 0.1846 (value: 0.0007, weighted value: 0.0372, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9376
Epoch 10/10, Train Loss: 0.1840 (value: 0.0007, weighted value: 0.0365, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9375
..training done in 89.67 seconds
..evaluation done in 20.39 seconds
Old network+MCTS average reward: 0.7731, min: 0.1019, max: 1.4907, stdev: 0.2350
New network+MCTS average reward: 0.7735, min: -0.0093, max: 1.4907, stdev: 0.2368
Old bare network average reward: 0.7376, min: 0.1019, max: 1.4167, stdev: 0.2369
New bare network average reward: 0.7376, min: 0.0556, max: 1.4907, stdev: 0.2371
External policy "random" average reward: 0.2823, min: -0.2500, max: 0.9907, stdev: 0.2400
External policy "individual greedy" average reward: 0.5534, min: 0.0833, max: 1.3148, stdev: 0.2271
External policy "total greedy" average reward: 0.6693, min: 0.0741, max: 1.3241, stdev: 0.2293
New network won 66 and tied 174 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 587 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.92 seconds
Training examples lengths: [64717, 65090, 64641, 64989, 64996, 64868, 64820, 64553, 64420, 64627]
Total value: 494973.32
Training on 647721 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2026 (value: 0.0009, weighted value: 0.0459, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9362
Epoch 2/10, Train Loss: 0.1949 (value: 0.0008, weighted value: 0.0425, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9369
Epoch 3/10, Train Loss: 0.1904 (value: 0.0008, weighted value: 0.0403, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9372
Epoch 4/10, Train Loss: 0.1867 (value: 0.0008, weighted value: 0.0385, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1843 (value: 0.0007, weighted value: 0.0369, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9377
Epoch 6/10, Train Loss: 0.1825 (value: 0.0007, weighted value: 0.0354, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9378
Epoch 7/10, Train Loss: 0.1819 (value: 0.0007, weighted value: 0.0349, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9380
Epoch 8/10, Train Loss: 0.1802 (value: 0.0007, weighted value: 0.0336, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9382
Epoch 9/10, Train Loss: 0.1783 (value: 0.0006, weighted value: 0.0322, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9385
Epoch 10/10, Train Loss: 0.1779 (value: 0.0006, weighted value: 0.0324, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9385
..training done in 75.15 seconds
..evaluation done in 22.19 seconds
Old network+MCTS average reward: 0.7656, min: 0.1111, max: 1.5000, stdev: 0.2147
New network+MCTS average reward: 0.7619, min: 0.1111, max: 1.5000, stdev: 0.2161
Old bare network average reward: 0.7357, min: 0.1111, max: 1.4722, stdev: 0.2184
New bare network average reward: 0.7336, min: 0.1111, max: 1.4722, stdev: 0.2165
External policy "random" average reward: 0.2632, min: -0.3333, max: 0.9722, stdev: 0.2299
External policy "individual greedy" average reward: 0.5559, min: 0.0000, max: 1.1481, stdev: 0.2251
External policy "total greedy" average reward: 0.6641, min: 0.0741, max: 1.2130, stdev: 0.2101
New network won 56 and tied 177 out of 300 games (48.17% wins where ties are half wins)
Reverting to the old network

Training iteration 588 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 61.16 seconds
Training examples lengths: [65090, 64641, 64989, 64996, 64868, 64820, 64553, 64420, 64627, 65046]
Total value: 495063.13
Training on 648050 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2205 (value: 0.0011, weighted value: 0.0567, policy: 0.1638, weighted policy: 0.1638), Train Mean Max: 0.9348
Epoch 2/10, Train Loss: 0.2078 (value: 0.0010, weighted value: 0.0507, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9358
Epoch 3/10, Train Loss: 0.1995 (value: 0.0009, weighted value: 0.0470, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9364
Epoch 4/10, Train Loss: 0.1952 (value: 0.0009, weighted value: 0.0442, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9365
Epoch 5/10, Train Loss: 0.1903 (value: 0.0008, weighted value: 0.0416, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1883 (value: 0.0008, weighted value: 0.0396, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9370
Epoch 7/10, Train Loss: 0.1871 (value: 0.0008, weighted value: 0.0390, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9372
Epoch 8/10, Train Loss: 0.1856 (value: 0.0008, weighted value: 0.0376, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1834 (value: 0.0007, weighted value: 0.0356, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9375
Epoch 10/10, Train Loss: 0.1827 (value: 0.0007, weighted value: 0.0351, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9376
..training done in 79.07 seconds
..evaluation done in 20.46 seconds
Old network+MCTS average reward: 0.7668, min: 0.1944, max: 1.3333, stdev: 0.2407
New network+MCTS average reward: 0.7683, min: 0.1852, max: 1.3333, stdev: 0.2396
Old bare network average reward: 0.7381, min: 0.1111, max: 1.3241, stdev: 0.2419
New bare network average reward: 0.7359, min: 0.1481, max: 1.3333, stdev: 0.2422
External policy "random" average reward: 0.2389, min: -0.4259, max: 1.1296, stdev: 0.2249
External policy "individual greedy" average reward: 0.5423, min: -0.0833, max: 1.2593, stdev: 0.2382
External policy "total greedy" average reward: 0.6548, min: 0.0648, max: 1.3796, stdev: 0.2298
New network won 72 and tied 167 out of 300 games (51.83% wins where ties are half wins)
Keeping the new network

Training iteration 589 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.85 seconds
Training examples lengths: [64641, 64989, 64996, 64868, 64820, 64553, 64420, 64627, 65046, 64761]
Total value: 494481.92
Training on 647721 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2020 (value: 0.0009, weighted value: 0.0460, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9362
Epoch 2/10, Train Loss: 0.1950 (value: 0.0008, weighted value: 0.0421, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9368
Epoch 3/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0386, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9373
Epoch 4/10, Train Loss: 0.1878 (value: 0.0008, weighted value: 0.0389, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9374
Epoch 5/10, Train Loss: 0.1840 (value: 0.0007, weighted value: 0.0358, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0353, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9377
Epoch 7/10, Train Loss: 0.1819 (value: 0.0007, weighted value: 0.0345, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9380
Epoch 8/10, Train Loss: 0.1791 (value: 0.0007, weighted value: 0.0329, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1797 (value: 0.0007, weighted value: 0.0332, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9382
Epoch 10/10, Train Loss: 0.1780 (value: 0.0006, weighted value: 0.0315, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9382
..training done in 79.94 seconds
..evaluation done in 20.99 seconds
Old network+MCTS average reward: 0.7538, min: 0.1667, max: 1.4167, stdev: 0.2395
New network+MCTS average reward: 0.7504, min: 0.1481, max: 1.4444, stdev: 0.2392
Old bare network average reward: 0.7213, min: 0.1296, max: 1.4259, stdev: 0.2439
New bare network average reward: 0.7216, min: 0.1296, max: 1.4259, stdev: 0.2460
External policy "random" average reward: 0.2673, min: -0.3796, max: 0.8981, stdev: 0.2194
External policy "individual greedy" average reward: 0.5476, min: -0.1019, max: 1.2315, stdev: 0.2249
External policy "total greedy" average reward: 0.6590, min: -0.0556, max: 1.4167, stdev: 0.2248
New network won 58 and tied 166 out of 300 games (47.00% wins where ties are half wins)
Reverting to the old network

Training iteration 590 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.82 seconds
Training examples lengths: [64989, 64996, 64868, 64820, 64553, 64420, 64627, 65046, 64761, 64897]
Total value: 494678.99
Training on 647977 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2204 (value: 0.0011, weighted value: 0.0568, policy: 0.1636, weighted policy: 0.1636), Train Mean Max: 0.9348
Epoch 2/10, Train Loss: 0.2065 (value: 0.0010, weighted value: 0.0495, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9358
Epoch 3/10, Train Loss: 0.1995 (value: 0.0009, weighted value: 0.0463, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9363
Epoch 4/10, Train Loss: 0.1947 (value: 0.0009, weighted value: 0.0437, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9366
Epoch 5/10, Train Loss: 0.1913 (value: 0.0008, weighted value: 0.0412, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9368
Epoch 6/10, Train Loss: 0.1886 (value: 0.0008, weighted value: 0.0400, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9370
Epoch 7/10, Train Loss: 0.1868 (value: 0.0008, weighted value: 0.0387, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9371
Epoch 8/10, Train Loss: 0.1843 (value: 0.0007, weighted value: 0.0363, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9373
Epoch 9/10, Train Loss: 0.1833 (value: 0.0007, weighted value: 0.0354, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9374
Epoch 10/10, Train Loss: 0.1831 (value: 0.0007, weighted value: 0.0358, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9377
..training done in 79.42 seconds
..evaluation done in 20.73 seconds
Old network+MCTS average reward: 0.7455, min: 0.2037, max: 1.4444, stdev: 0.2375
New network+MCTS average reward: 0.7482, min: 0.2037, max: 1.4444, stdev: 0.2363
Old bare network average reward: 0.7127, min: 0.1389, max: 1.4444, stdev: 0.2438
New bare network average reward: 0.7156, min: 0.1389, max: 1.4444, stdev: 0.2371
External policy "random" average reward: 0.2666, min: -0.3333, max: 0.8241, stdev: 0.2198
External policy "individual greedy" average reward: 0.5424, min: 0.0648, max: 1.2407, stdev: 0.2272
External policy "total greedy" average reward: 0.6551, min: 0.0741, max: 1.4074, stdev: 0.2320
New network won 77 and tied 162 out of 300 games (52.67% wins where ties are half wins)
Keeping the new network

Training iteration 591 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.97 seconds
Training examples lengths: [64996, 64868, 64820, 64553, 64420, 64627, 65046, 64761, 64897, 64637]
Total value: 494202.18
Training on 647625 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2010 (value: 0.0009, weighted value: 0.0451, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9362
Epoch 2/10, Train Loss: 0.1938 (value: 0.0008, weighted value: 0.0408, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9369
Epoch 3/10, Train Loss: 0.1891 (value: 0.0008, weighted value: 0.0393, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9372
Epoch 4/10, Train Loss: 0.1862 (value: 0.0007, weighted value: 0.0373, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9374
Epoch 5/10, Train Loss: 0.1842 (value: 0.0007, weighted value: 0.0361, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1818 (value: 0.0007, weighted value: 0.0348, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9379
Epoch 7/10, Train Loss: 0.1812 (value: 0.0007, weighted value: 0.0345, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1791 (value: 0.0007, weighted value: 0.0327, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1789 (value: 0.0006, weighted value: 0.0322, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9381
Epoch 10/10, Train Loss: 0.1777 (value: 0.0006, weighted value: 0.0312, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9383
..training done in 75.36 seconds
..evaluation done in 21.26 seconds
Old network+MCTS average reward: 0.7352, min: 0.0370, max: 1.5833, stdev: 0.2257
New network+MCTS average reward: 0.7360, min: 0.0463, max: 1.5833, stdev: 0.2227
Old bare network average reward: 0.7066, min: -0.0463, max: 1.5833, stdev: 0.2243
New bare network average reward: 0.7051, min: -0.0648, max: 1.5648, stdev: 0.2278
External policy "random" average reward: 0.2651, min: -0.3148, max: 0.8148, stdev: 0.2102
External policy "individual greedy" average reward: 0.5329, min: -0.1759, max: 1.2593, stdev: 0.2166
External policy "total greedy" average reward: 0.6490, min: -0.0648, max: 1.4722, stdev: 0.2173
New network won 69 and tied 172 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 592 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 61.77 seconds
Training examples lengths: [64868, 64820, 64553, 64420, 64627, 65046, 64761, 64897, 64637, 64905]
Total value: 493863.08
Training on 647534 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1986 (value: 0.0008, weighted value: 0.0419, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9367
Epoch 2/10, Train Loss: 0.1910 (value: 0.0008, weighted value: 0.0389, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9377
Epoch 3/10, Train Loss: 0.1854 (value: 0.0007, weighted value: 0.0365, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9379
Epoch 4/10, Train Loss: 0.1822 (value: 0.0007, weighted value: 0.0349, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9381
Epoch 5/10, Train Loss: 0.1810 (value: 0.0007, weighted value: 0.0340, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9381
Epoch 6/10, Train Loss: 0.1790 (value: 0.0006, weighted value: 0.0325, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9382
Epoch 7/10, Train Loss: 0.1783 (value: 0.0006, weighted value: 0.0319, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9383
Epoch 8/10, Train Loss: 0.1768 (value: 0.0006, weighted value: 0.0308, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9386
Epoch 9/10, Train Loss: 0.1764 (value: 0.0006, weighted value: 0.0302, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9386
Epoch 10/10, Train Loss: 0.1758 (value: 0.0006, weighted value: 0.0300, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9388
..training done in 78.91 seconds
..evaluation done in 29.36 seconds
Old network+MCTS average reward: 0.7664, min: 0.1667, max: 1.3796, stdev: 0.2521
New network+MCTS average reward: 0.7665, min: 0.1667, max: 1.3796, stdev: 0.2496
Old bare network average reward: 0.7318, min: 0.1667, max: 1.3796, stdev: 0.2598
New bare network average reward: 0.7353, min: 0.1667, max: 1.3796, stdev: 0.2584
External policy "random" average reward: 0.2601, min: -0.3148, max: 0.9444, stdev: 0.2228
External policy "individual greedy" average reward: 0.5569, min: 0.0370, max: 1.3426, stdev: 0.2296
External policy "total greedy" average reward: 0.6614, min: 0.1667, max: 1.2685, stdev: 0.2248
New network won 58 and tied 188 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 593 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.46 seconds
Training examples lengths: [64820, 64553, 64420, 64627, 65046, 64761, 64897, 64637, 64905, 64878]
Total value: 493765.48
Training on 647544 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1960 (value: 0.0008, weighted value: 0.0412, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9372
Epoch 2/10, Train Loss: 0.1894 (value: 0.0008, weighted value: 0.0377, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9379
Epoch 3/10, Train Loss: 0.1839 (value: 0.0007, weighted value: 0.0347, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9381
Epoch 4/10, Train Loss: 0.1809 (value: 0.0007, weighted value: 0.0340, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9383
Epoch 5/10, Train Loss: 0.1800 (value: 0.0007, weighted value: 0.0333, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9383
Epoch 6/10, Train Loss: 0.1773 (value: 0.0006, weighted value: 0.0314, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9386
Epoch 7/10, Train Loss: 0.1762 (value: 0.0006, weighted value: 0.0309, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9387
Epoch 8/10, Train Loss: 0.1754 (value: 0.0006, weighted value: 0.0302, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9388
Epoch 9/10, Train Loss: 0.1748 (value: 0.0006, weighted value: 0.0289, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9388
Epoch 10/10, Train Loss: 0.1747 (value: 0.0006, weighted value: 0.0289, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9389
..training done in 79.79 seconds
..evaluation done in 19.76 seconds
Old network+MCTS average reward: 0.7487, min: 0.0741, max: 1.4815, stdev: 0.2354
New network+MCTS average reward: 0.7473, min: 0.1852, max: 1.4815, stdev: 0.2355
Old bare network average reward: 0.7186, min: 0.0741, max: 1.4815, stdev: 0.2387
New bare network average reward: 0.7193, min: 0.0000, max: 1.4259, stdev: 0.2387
External policy "random" average reward: 0.2696, min: -0.4352, max: 1.0370, stdev: 0.2318
External policy "individual greedy" average reward: 0.5360, min: -0.0093, max: 1.2870, stdev: 0.2333
External policy "total greedy" average reward: 0.6512, min: 0.0833, max: 1.3519, stdev: 0.2235
New network won 62 and tied 174 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 594 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.14 seconds
Training examples lengths: [64553, 64420, 64627, 65046, 64761, 64897, 64637, 64905, 64878, 64904]
Total value: 493335.84
Training on 647628 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2149 (value: 0.0010, weighted value: 0.0525, policy: 0.1624, weighted policy: 0.1624), Train Mean Max: 0.9358
Epoch 2/10, Train Loss: 0.2018 (value: 0.0009, weighted value: 0.0459, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9368
Epoch 3/10, Train Loss: 0.1936 (value: 0.0008, weighted value: 0.0418, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9371
Epoch 4/10, Train Loss: 0.1892 (value: 0.0008, weighted value: 0.0397, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9375
Epoch 5/10, Train Loss: 0.1863 (value: 0.0008, weighted value: 0.0382, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1837 (value: 0.0007, weighted value: 0.0363, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9378
Epoch 7/10, Train Loss: 0.1828 (value: 0.0007, weighted value: 0.0358, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9380
Epoch 8/10, Train Loss: 0.1806 (value: 0.0007, weighted value: 0.0336, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9382
Epoch 9/10, Train Loss: 0.1800 (value: 0.0007, weighted value: 0.0331, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9383
Epoch 10/10, Train Loss: 0.1781 (value: 0.0006, weighted value: 0.0320, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9385
..training done in 76.45 seconds
..evaluation done in 20.95 seconds
Old network+MCTS average reward: 0.7590, min: 0.0370, max: 1.5000, stdev: 0.2310
New network+MCTS average reward: 0.7583, min: 0.0370, max: 1.5000, stdev: 0.2338
Old bare network average reward: 0.7325, min: 0.0185, max: 1.5000, stdev: 0.2312
New bare network average reward: 0.7327, min: 0.0370, max: 1.5000, stdev: 0.2332
External policy "random" average reward: 0.2723, min: -0.3148, max: 0.9815, stdev: 0.2222
External policy "individual greedy" average reward: 0.5364, min: -0.2407, max: 1.1667, stdev: 0.2288
External policy "total greedy" average reward: 0.6650, min: 0.0370, max: 1.2407, stdev: 0.2199
New network won 57 and tied 181 out of 300 games (49.17% wins where ties are half wins)
Reverting to the old network

Training iteration 595 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.23 seconds
Training examples lengths: [64420, 64627, 65046, 64761, 64897, 64637, 64905, 64878, 64904, 64657]
Total value: 492988.95
Training on 647732 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2332 (value: 0.0013, weighted value: 0.0628, policy: 0.1704, weighted policy: 0.1704), Train Mean Max: 0.9344
Epoch 2/10, Train Loss: 0.2132 (value: 0.0011, weighted value: 0.0532, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9358
Epoch 3/10, Train Loss: 0.2018 (value: 0.0010, weighted value: 0.0477, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9363
Epoch 4/10, Train Loss: 0.1969 (value: 0.0009, weighted value: 0.0458, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9366
Epoch 5/10, Train Loss: 0.1926 (value: 0.0009, weighted value: 0.0431, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9368
Epoch 6/10, Train Loss: 0.1903 (value: 0.0008, weighted value: 0.0409, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9369
Epoch 7/10, Train Loss: 0.1866 (value: 0.0008, weighted value: 0.0385, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1860 (value: 0.0008, weighted value: 0.0377, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1840 (value: 0.0007, weighted value: 0.0367, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1818 (value: 0.0007, weighted value: 0.0342, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9378
..training done in 71.68 seconds
..evaluation done in 22.53 seconds
Old network+MCTS average reward: 0.7599, min: 0.2037, max: 1.4259, stdev: 0.2265
New network+MCTS average reward: 0.7624, min: 0.2593, max: 1.4259, stdev: 0.2274
Old bare network average reward: 0.7273, min: 0.1852, max: 1.4167, stdev: 0.2239
New bare network average reward: 0.7296, min: 0.1852, max: 1.4259, stdev: 0.2301
External policy "random" average reward: 0.2710, min: -0.2222, max: 0.9722, stdev: 0.2168
External policy "individual greedy" average reward: 0.5465, min: -0.1019, max: 1.2407, stdev: 0.2138
External policy "total greedy" average reward: 0.6624, min: 0.1667, max: 1.3796, stdev: 0.2075
New network won 82 and tied 159 out of 300 games (53.83% wins where ties are half wins)
Keeping the new network

Training iteration 596 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 60.36 seconds
Training examples lengths: [64627, 65046, 64761, 64897, 64637, 64905, 64878, 64904, 64657, 64902]
Total value: 494095.82
Training on 648214 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2009 (value: 0.0009, weighted value: 0.0450, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9364
Epoch 2/10, Train Loss: 0.1939 (value: 0.0008, weighted value: 0.0417, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9371
Epoch 3/10, Train Loss: 0.1895 (value: 0.0008, weighted value: 0.0397, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9373
Epoch 4/10, Train Loss: 0.1848 (value: 0.0007, weighted value: 0.0368, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1836 (value: 0.0007, weighted value: 0.0359, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9378
Epoch 6/10, Train Loss: 0.1816 (value: 0.0007, weighted value: 0.0350, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9380
Epoch 7/10, Train Loss: 0.1812 (value: 0.0007, weighted value: 0.0345, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9380
Epoch 8/10, Train Loss: 0.1791 (value: 0.0006, weighted value: 0.0324, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1782 (value: 0.0006, weighted value: 0.0321, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9382
Epoch 10/10, Train Loss: 0.1768 (value: 0.0006, weighted value: 0.0312, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9385
..training done in 69.53 seconds
..evaluation done in 20.85 seconds
Old network+MCTS average reward: 0.7473, min: 0.1389, max: 1.7963, stdev: 0.2449
New network+MCTS average reward: 0.7465, min: 0.2130, max: 1.7963, stdev: 0.2454
Old bare network average reward: 0.7102, min: 0.1389, max: 1.6759, stdev: 0.2482
New bare network average reward: 0.7137, min: 0.1204, max: 1.6667, stdev: 0.2464
External policy "random" average reward: 0.2615, min: -0.3148, max: 0.9259, stdev: 0.2251
External policy "individual greedy" average reward: 0.5396, min: -0.0833, max: 1.3519, stdev: 0.2329
External policy "total greedy" average reward: 0.6440, min: 0.1389, max: 1.4722, stdev: 0.2242
New network won 67 and tied 170 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 597 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.49 seconds
Training examples lengths: [65046, 64761, 64897, 64637, 64905, 64878, 64904, 64657, 64902, 64870]
Total value: 494960.22
Training on 648457 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1964 (value: 0.0008, weighted value: 0.0417, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9370
Epoch 2/10, Train Loss: 0.1893 (value: 0.0008, weighted value: 0.0384, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9377
Epoch 3/10, Train Loss: 0.1850 (value: 0.0007, weighted value: 0.0369, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9381
Epoch 4/10, Train Loss: 0.1817 (value: 0.0007, weighted value: 0.0344, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9383
Epoch 5/10, Train Loss: 0.1801 (value: 0.0007, weighted value: 0.0338, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9384
Epoch 6/10, Train Loss: 0.1779 (value: 0.0007, weighted value: 0.0325, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9385
Epoch 7/10, Train Loss: 0.1771 (value: 0.0006, weighted value: 0.0319, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9388
Epoch 8/10, Train Loss: 0.1767 (value: 0.0006, weighted value: 0.0310, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9388
Epoch 9/10, Train Loss: 0.1758 (value: 0.0006, weighted value: 0.0298, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9389
Epoch 10/10, Train Loss: 0.1744 (value: 0.0006, weighted value: 0.0295, policy: 0.1449, weighted policy: 0.1449), Train Mean Max: 0.9391
..training done in 70.62 seconds
..evaluation done in 20.98 seconds
Old network+MCTS average reward: 0.7831, min: 0.3148, max: 1.6296, stdev: 0.2336
New network+MCTS average reward: 0.7815, min: 0.2963, max: 1.5648, stdev: 0.2331
Old bare network average reward: 0.7498, min: 0.2593, max: 1.6019, stdev: 0.2371
New bare network average reward: 0.7472, min: 0.2407, max: 1.6019, stdev: 0.2382
External policy "random" average reward: 0.2757, min: -0.3148, max: 0.9815, stdev: 0.2124
External policy "individual greedy" average reward: 0.5701, min: -0.1296, max: 1.4259, stdev: 0.2316
External policy "total greedy" average reward: 0.6825, min: 0.0093, max: 1.5926, stdev: 0.2286
New network won 57 and tied 180 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 598 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.16 seconds
Training examples lengths: [64761, 64897, 64637, 64905, 64878, 64904, 64657, 64902, 64870, 64644]
Total value: 494359.87
Training on 648055 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2158 (value: 0.0011, weighted value: 0.0539, policy: 0.1620, weighted policy: 0.1620), Train Mean Max: 0.9357
Epoch 2/10, Train Loss: 0.2038 (value: 0.0010, weighted value: 0.0475, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9366
Epoch 3/10, Train Loss: 0.1946 (value: 0.0009, weighted value: 0.0431, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9372
Epoch 4/10, Train Loss: 0.1897 (value: 0.0008, weighted value: 0.0402, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9374
Epoch 5/10, Train Loss: 0.1877 (value: 0.0008, weighted value: 0.0398, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1855 (value: 0.0008, weighted value: 0.0377, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9377
Epoch 7/10, Train Loss: 0.1825 (value: 0.0007, weighted value: 0.0359, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1815 (value: 0.0007, weighted value: 0.0354, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1799 (value: 0.0007, weighted value: 0.0334, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9383
Epoch 10/10, Train Loss: 0.1785 (value: 0.0006, weighted value: 0.0324, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9385
..training done in 70.89 seconds
..evaluation done in 20.10 seconds
Old network+MCTS average reward: 0.7493, min: 0.1389, max: 1.3426, stdev: 0.2269
New network+MCTS average reward: 0.7498, min: 0.1389, max: 1.3426, stdev: 0.2298
Old bare network average reward: 0.7170, min: 0.1019, max: 1.2963, stdev: 0.2305
New bare network average reward: 0.7178, min: 0.1204, max: 1.3333, stdev: 0.2254
External policy "random" average reward: 0.2534, min: -0.3426, max: 0.8704, stdev: 0.2227
External policy "individual greedy" average reward: 0.5294, min: -0.0278, max: 1.1852, stdev: 0.2220
External policy "total greedy" average reward: 0.6439, min: 0.1204, max: 1.2685, stdev: 0.2172
New network won 69 and tied 164 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 599 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 60.12 seconds
Training examples lengths: [64897, 64637, 64905, 64878, 64904, 64657, 64902, 64870, 64644, 65008]
Total value: 494864.75
Training on 648302 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1981 (value: 0.0009, weighted value: 0.0437, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9370
Epoch 2/10, Train Loss: 0.1912 (value: 0.0008, weighted value: 0.0395, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9376
Epoch 3/10, Train Loss: 0.1867 (value: 0.0008, weighted value: 0.0380, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9380
Epoch 4/10, Train Loss: 0.1826 (value: 0.0007, weighted value: 0.0355, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9382
Epoch 5/10, Train Loss: 0.1809 (value: 0.0007, weighted value: 0.0348, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9384
Epoch 6/10, Train Loss: 0.1789 (value: 0.0007, weighted value: 0.0334, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9387
Epoch 7/10, Train Loss: 0.1786 (value: 0.0007, weighted value: 0.0328, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9386
Epoch 8/10, Train Loss: 0.1770 (value: 0.0006, weighted value: 0.0318, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9389
Epoch 9/10, Train Loss: 0.1752 (value: 0.0006, weighted value: 0.0305, policy: 0.1447, weighted policy: 0.1447), Train Mean Max: 0.9391
Epoch 10/10, Train Loss: 0.1765 (value: 0.0006, weighted value: 0.0310, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9390
..training done in 75.25 seconds
..evaluation done in 19.91 seconds
Old network+MCTS average reward: 0.7458, min: 0.0463, max: 1.5278, stdev: 0.2381
New network+MCTS average reward: 0.7449, min: 0.0463, max: 1.6204, stdev: 0.2391
Old bare network average reward: 0.7165, min: 0.0185, max: 1.5278, stdev: 0.2374
New bare network average reward: 0.7175, min: -0.0370, max: 1.5000, stdev: 0.2373
External policy "random" average reward: 0.2601, min: -0.3333, max: 0.8333, stdev: 0.2139
External policy "individual greedy" average reward: 0.5466, min: 0.0463, max: 1.3241, stdev: 0.2177
External policy "total greedy" average reward: 0.6550, min: 0.0648, max: 1.4907, stdev: 0.2232
New network won 61 and tied 183 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 600 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.36 seconds
Training examples lengths: [64637, 64905, 64878, 64904, 64657, 64902, 64870, 64644, 65008, 64853]
Total value: 494974.61
Training on 648258 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1965 (value: 0.0008, weighted value: 0.0411, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9375
Epoch 2/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0393, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9383
Epoch 3/10, Train Loss: 0.1828 (value: 0.0007, weighted value: 0.0352, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9386
Epoch 4/10, Train Loss: 0.1806 (value: 0.0007, weighted value: 0.0344, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9387
Epoch 5/10, Train Loss: 0.1784 (value: 0.0007, weighted value: 0.0330, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9388
Epoch 6/10, Train Loss: 0.1774 (value: 0.0006, weighted value: 0.0322, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9389
Epoch 7/10, Train Loss: 0.1761 (value: 0.0006, weighted value: 0.0310, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9391
Epoch 8/10, Train Loss: 0.1765 (value: 0.0006, weighted value: 0.0315, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9391
Epoch 9/10, Train Loss: 0.1748 (value: 0.0006, weighted value: 0.0301, policy: 0.1446, weighted policy: 0.1446), Train Mean Max: 0.9394
Epoch 10/10, Train Loss: 0.1729 (value: 0.0006, weighted value: 0.0282, policy: 0.1447, weighted policy: 0.1447), Train Mean Max: 0.9394
..training done in 74.41 seconds
..evaluation done in 19.89 seconds
Old network+MCTS average reward: 0.7775, min: 0.1111, max: 1.5185, stdev: 0.2296
New network+MCTS average reward: 0.7826, min: 0.1759, max: 1.5185, stdev: 0.2266
Old bare network average reward: 0.7512, min: 0.0741, max: 1.5185, stdev: 0.2321
New bare network average reward: 0.7511, min: 0.0741, max: 1.5185, stdev: 0.2324
External policy "random" average reward: 0.2939, min: -0.4167, max: 0.9259, stdev: 0.2212
External policy "individual greedy" average reward: 0.5752, min: -0.0556, max: 1.3796, stdev: 0.2187
External policy "total greedy" average reward: 0.6767, min: 0.2037, max: 1.4444, stdev: 0.2150
New network won 76 and tied 176 out of 300 games (54.67% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_600

Training iteration 601 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.48 seconds
Training examples lengths: [64905, 64878, 64904, 64657, 64902, 64870, 64644, 65008, 64853, 64804]
Total value: 495139.47
Training on 648425 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1951 (value: 0.0008, weighted value: 0.0409, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9377
Epoch 2/10, Train Loss: 0.1886 (value: 0.0007, weighted value: 0.0374, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9383
Epoch 3/10, Train Loss: 0.1822 (value: 0.0007, weighted value: 0.0342, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9387
Epoch 4/10, Train Loss: 0.1800 (value: 0.0007, weighted value: 0.0342, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9389
Epoch 5/10, Train Loss: 0.1780 (value: 0.0006, weighted value: 0.0325, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9389
Epoch 6/10, Train Loss: 0.1764 (value: 0.0006, weighted value: 0.0315, policy: 0.1449, weighted policy: 0.1449), Train Mean Max: 0.9389
Epoch 7/10, Train Loss: 0.1749 (value: 0.0006, weighted value: 0.0296, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9390
Epoch 8/10, Train Loss: 0.1760 (value: 0.0006, weighted value: 0.0308, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9392
Epoch 9/10, Train Loss: 0.1736 (value: 0.0006, weighted value: 0.0292, policy: 0.1444, weighted policy: 0.1444), Train Mean Max: 0.9394
Epoch 10/10, Train Loss: 0.1721 (value: 0.0006, weighted value: 0.0283, policy: 0.1438, weighted policy: 0.1438), Train Mean Max: 0.9394
..training done in 75.79 seconds
..evaluation done in 20.43 seconds
Old network+MCTS average reward: 0.7769, min: 0.2778, max: 1.4815, stdev: 0.2194
New network+MCTS average reward: 0.7760, min: 0.2593, max: 1.4352, stdev: 0.2193
Old bare network average reward: 0.7533, min: 0.2778, max: 1.4352, stdev: 0.2200
New bare network average reward: 0.7443, min: 0.0556, max: 1.4352, stdev: 0.2275
External policy "random" average reward: 0.2696, min: -0.3148, max: 0.9815, stdev: 0.2233
External policy "individual greedy" average reward: 0.5478, min: -0.0926, max: 1.2870, stdev: 0.2261
External policy "total greedy" average reward: 0.6777, min: 0.1389, max: 1.2778, stdev: 0.2099
New network won 45 and tied 190 out of 300 games (46.67% wins where ties are half wins)
Reverting to the old network

Training iteration 602 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 60.96 seconds
Training examples lengths: [64878, 64904, 64657, 64902, 64870, 64644, 65008, 64853, 64804, 64925]
Total value: 495420.80
Training on 648445 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2139 (value: 0.0011, weighted value: 0.0526, policy: 0.1613, weighted policy: 0.1613), Train Mean Max: 0.9361
Epoch 2/10, Train Loss: 0.1998 (value: 0.0009, weighted value: 0.0448, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9373
Epoch 3/10, Train Loss: 0.1944 (value: 0.0009, weighted value: 0.0431, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9375
Epoch 4/10, Train Loss: 0.1872 (value: 0.0008, weighted value: 0.0389, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9379
Epoch 5/10, Train Loss: 0.1843 (value: 0.0007, weighted value: 0.0371, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9381
Epoch 6/10, Train Loss: 0.1841 (value: 0.0007, weighted value: 0.0363, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9380
Epoch 7/10, Train Loss: 0.1817 (value: 0.0007, weighted value: 0.0347, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9383
Epoch 8/10, Train Loss: 0.1802 (value: 0.0007, weighted value: 0.0337, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9385
Epoch 9/10, Train Loss: 0.1782 (value: 0.0007, weighted value: 0.0326, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9386
Epoch 10/10, Train Loss: 0.1773 (value: 0.0006, weighted value: 0.0318, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9388
..training done in 75.40 seconds
..evaluation done in 18.92 seconds
Old network+MCTS average reward: 0.7569, min: -0.0370, max: 1.6944, stdev: 0.2436
New network+MCTS average reward: 0.7582, min: -0.0463, max: 1.6944, stdev: 0.2424
Old bare network average reward: 0.7306, min: -0.0278, max: 1.6944, stdev: 0.2478
New bare network average reward: 0.7287, min: -0.0556, max: 1.6944, stdev: 0.2479
External policy "random" average reward: 0.2734, min: -0.4630, max: 1.1019, stdev: 0.2259
External policy "individual greedy" average reward: 0.5605, min: -0.0463, max: 1.1296, stdev: 0.2273
External policy "total greedy" average reward: 0.6704, min: 0.0278, max: 1.4815, stdev: 0.2341
New network won 66 and tied 169 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 603 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.72 seconds
Training examples lengths: [64904, 64657, 64902, 64870, 64644, 65008, 64853, 64804, 64925, 64445]
Total value: 494811.91
Training on 648012 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1969 (value: 0.0009, weighted value: 0.0432, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9374
Epoch 2/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0389, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9379
Epoch 3/10, Train Loss: 0.1849 (value: 0.0007, weighted value: 0.0363, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9382
Epoch 4/10, Train Loss: 0.1826 (value: 0.0007, weighted value: 0.0353, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9384
Epoch 5/10, Train Loss: 0.1802 (value: 0.0007, weighted value: 0.0339, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9386
Epoch 6/10, Train Loss: 0.1788 (value: 0.0007, weighted value: 0.0326, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9387
Epoch 7/10, Train Loss: 0.1778 (value: 0.0006, weighted value: 0.0323, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9388
Epoch 8/10, Train Loss: 0.1756 (value: 0.0006, weighted value: 0.0306, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9389
Epoch 9/10, Train Loss: 0.1757 (value: 0.0006, weighted value: 0.0304, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9390
Epoch 10/10, Train Loss: 0.1749 (value: 0.0006, weighted value: 0.0299, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9392
..training done in 66.29 seconds
..evaluation done in 20.53 seconds
Old network+MCTS average reward: 0.7396, min: 0.1667, max: 1.5926, stdev: 0.2432
New network+MCTS average reward: 0.7405, min: 0.1574, max: 1.5648, stdev: 0.2432
Old bare network average reward: 0.7112, min: 0.1204, max: 1.6019, stdev: 0.2471
New bare network average reward: 0.7103, min: 0.1204, max: 1.5833, stdev: 0.2469
External policy "random" average reward: 0.2544, min: -0.3519, max: 1.0648, stdev: 0.2280
External policy "individual greedy" average reward: 0.5338, min: -0.0370, max: 1.2407, stdev: 0.2312
External policy "total greedy" average reward: 0.6471, min: 0.0556, max: 1.4722, stdev: 0.2291
New network won 62 and tied 173 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 604 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.46 seconds
Training examples lengths: [64657, 64902, 64870, 64644, 65008, 64853, 64804, 64925, 64445, 65048]
Total value: 495070.03
Training on 648156 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2174 (value: 0.0011, weighted value: 0.0548, policy: 0.1627, weighted policy: 0.1627), Train Mean Max: 0.9356
Epoch 2/10, Train Loss: 0.2027 (value: 0.0009, weighted value: 0.0465, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9368
Epoch 3/10, Train Loss: 0.1944 (value: 0.0009, weighted value: 0.0429, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9372
Epoch 4/10, Train Loss: 0.1906 (value: 0.0008, weighted value: 0.0413, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9374
Epoch 5/10, Train Loss: 0.1869 (value: 0.0008, weighted value: 0.0385, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9377
Epoch 6/10, Train Loss: 0.1855 (value: 0.0008, weighted value: 0.0377, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9378
Epoch 7/10, Train Loss: 0.1832 (value: 0.0007, weighted value: 0.0360, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9380
Epoch 8/10, Train Loss: 0.1815 (value: 0.0007, weighted value: 0.0349, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9382
Epoch 9/10, Train Loss: 0.1804 (value: 0.0007, weighted value: 0.0339, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9383
Epoch 10/10, Train Loss: 0.1783 (value: 0.0006, weighted value: 0.0323, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9385
..training done in 72.69 seconds
..evaluation done in 19.92 seconds
Old network+MCTS average reward: 0.7531, min: 0.0463, max: 1.6481, stdev: 0.2426
New network+MCTS average reward: 0.7505, min: 0.1852, max: 1.6481, stdev: 0.2414
Old bare network average reward: 0.7213, min: 0.1204, max: 1.6111, stdev: 0.2442
New bare network average reward: 0.7189, min: 0.1204, max: 1.6481, stdev: 0.2452
External policy "random" average reward: 0.2390, min: -0.4352, max: 0.8611, stdev: 0.2356
External policy "individual greedy" average reward: 0.5365, min: -0.0833, max: 1.4537, stdev: 0.2329
External policy "total greedy" average reward: 0.6583, min: -0.0741, max: 1.4907, stdev: 0.2316
New network won 77 and tied 154 out of 300 games (51.33% wins where ties are half wins)
Keeping the new network

Training iteration 605 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.70 seconds
Training examples lengths: [64902, 64870, 64644, 65008, 64853, 64804, 64925, 64445, 65048, 64465]
Total value: 495567.09
Training on 647964 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1976 (value: 0.0009, weighted value: 0.0434, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9369
Epoch 2/10, Train Loss: 0.1928 (value: 0.0008, weighted value: 0.0406, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9375
Epoch 3/10, Train Loss: 0.1859 (value: 0.0007, weighted value: 0.0365, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9378
Epoch 4/10, Train Loss: 0.1842 (value: 0.0007, weighted value: 0.0364, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9381
Epoch 5/10, Train Loss: 0.1806 (value: 0.0007, weighted value: 0.0339, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9382
Epoch 6/10, Train Loss: 0.1800 (value: 0.0007, weighted value: 0.0340, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9383
Epoch 7/10, Train Loss: 0.1782 (value: 0.0006, weighted value: 0.0319, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9385
Epoch 8/10, Train Loss: 0.1780 (value: 0.0006, weighted value: 0.0322, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9385
Epoch 9/10, Train Loss: 0.1775 (value: 0.0006, weighted value: 0.0312, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9387
Epoch 10/10, Train Loss: 0.1750 (value: 0.0006, weighted value: 0.0295, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9389
..training done in 74.54 seconds
..evaluation done in 19.74 seconds
Old network+MCTS average reward: 0.7566, min: 0.1759, max: 1.6019, stdev: 0.2366
New network+MCTS average reward: 0.7552, min: 0.1759, max: 1.5648, stdev: 0.2374
Old bare network average reward: 0.7229, min: 0.1019, max: 1.5370, stdev: 0.2401
New bare network average reward: 0.7237, min: 0.1759, max: 1.5370, stdev: 0.2394
External policy "random" average reward: 0.2385, min: -0.3889, max: 1.0093, stdev: 0.2264
External policy "individual greedy" average reward: 0.5377, min: 0.0556, max: 1.2870, stdev: 0.2146
External policy "total greedy" average reward: 0.6454, min: 0.1389, max: 1.4907, stdev: 0.2224
New network won 63 and tied 175 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 606 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.09 seconds
Training examples lengths: [64870, 64644, 65008, 64853, 64804, 64925, 64445, 65048, 64465, 64823]
Total value: 494916.09
Training on 647885 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1965 (value: 0.0008, weighted value: 0.0423, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9373
Epoch 2/10, Train Loss: 0.1882 (value: 0.0008, weighted value: 0.0377, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9380
Epoch 3/10, Train Loss: 0.1840 (value: 0.0007, weighted value: 0.0358, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9383
Epoch 4/10, Train Loss: 0.1806 (value: 0.0007, weighted value: 0.0334, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9384
Epoch 5/10, Train Loss: 0.1796 (value: 0.0007, weighted value: 0.0333, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9385
Epoch 6/10, Train Loss: 0.1786 (value: 0.0007, weighted value: 0.0327, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9386
Epoch 7/10, Train Loss: 0.1767 (value: 0.0006, weighted value: 0.0314, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9388
Epoch 8/10, Train Loss: 0.1758 (value: 0.0006, weighted value: 0.0297, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9388
Epoch 9/10, Train Loss: 0.1749 (value: 0.0006, weighted value: 0.0295, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9391
Epoch 10/10, Train Loss: 0.1743 (value: 0.0006, weighted value: 0.0292, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9392
..training done in 64.14 seconds
..evaluation done in 19.34 seconds
Old network+MCTS average reward: 0.7474, min: 0.1481, max: 1.6296, stdev: 0.2341
New network+MCTS average reward: 0.7438, min: 0.1667, max: 1.7222, stdev: 0.2347
Old bare network average reward: 0.7160, min: 0.1481, max: 1.6852, stdev: 0.2371
New bare network average reward: 0.7163, min: 0.1667, max: 1.6852, stdev: 0.2339
External policy "random" average reward: 0.2604, min: -0.3241, max: 1.1481, stdev: 0.2302
External policy "individual greedy" average reward: 0.5284, min: 0.0556, max: 1.3333, stdev: 0.2221
External policy "total greedy" average reward: 0.6385, min: 0.1852, max: 1.4815, stdev: 0.2139
New network won 53 and tied 171 out of 300 games (46.17% wins where ties are half wins)
Reverting to the old network

Training iteration 607 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.16 seconds
Training examples lengths: [64644, 65008, 64853, 64804, 64925, 64445, 65048, 64465, 64823, 64619]
Total value: 494343.56
Training on 647634 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2172 (value: 0.0011, weighted value: 0.0536, policy: 0.1636, weighted policy: 0.1636), Train Mean Max: 0.9355
Epoch 2/10, Train Loss: 0.2035 (value: 0.0009, weighted value: 0.0467, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9367
Epoch 3/10, Train Loss: 0.1946 (value: 0.0008, weighted value: 0.0424, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9372
Epoch 4/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0403, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9374
Epoch 5/10, Train Loss: 0.1865 (value: 0.0008, weighted value: 0.0383, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9374
Epoch 6/10, Train Loss: 0.1833 (value: 0.0007, weighted value: 0.0361, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9378
Epoch 7/10, Train Loss: 0.1823 (value: 0.0007, weighted value: 0.0351, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1823 (value: 0.0007, weighted value: 0.0353, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9382
Epoch 9/10, Train Loss: 0.1799 (value: 0.0007, weighted value: 0.0330, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9382
Epoch 10/10, Train Loss: 0.1790 (value: 0.0007, weighted value: 0.0330, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9384
..training done in 64.88 seconds
..evaluation done in 19.51 seconds
Old network+MCTS average reward: 0.7546, min: 0.1111, max: 1.7222, stdev: 0.2586
New network+MCTS average reward: 0.7566, min: 0.1111, max: 1.6944, stdev: 0.2599
Old bare network average reward: 0.7282, min: 0.0463, max: 1.6944, stdev: 0.2678
New bare network average reward: 0.7280, min: 0.0463, max: 1.6944, stdev: 0.2668
External policy "random" average reward: 0.2627, min: -0.3519, max: 0.9815, stdev: 0.2409
External policy "individual greedy" average reward: 0.5544, min: -0.1944, max: 1.4815, stdev: 0.2558
External policy "total greedy" average reward: 0.6615, min: 0.1019, max: 1.5556, stdev: 0.2488
New network won 62 and tied 173 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 608 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.51 seconds
Training examples lengths: [65008, 64853, 64804, 64925, 64445, 65048, 64465, 64823, 64619, 64578]
Total value: 494312.72
Training on 647568 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2341 (value: 0.0013, weighted value: 0.0635, policy: 0.1706, weighted policy: 0.1706), Train Mean Max: 0.9341
Epoch 2/10, Train Loss: 0.2142 (value: 0.0011, weighted value: 0.0535, policy: 0.1607, weighted policy: 0.1607), Train Mean Max: 0.9355
Epoch 3/10, Train Loss: 0.2037 (value: 0.0010, weighted value: 0.0484, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9361
Epoch 4/10, Train Loss: 0.1969 (value: 0.0009, weighted value: 0.0454, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9366
Epoch 5/10, Train Loss: 0.1937 (value: 0.0009, weighted value: 0.0436, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9367
Epoch 6/10, Train Loss: 0.1899 (value: 0.0008, weighted value: 0.0406, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9370
Epoch 7/10, Train Loss: 0.1882 (value: 0.0008, weighted value: 0.0393, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9371
Epoch 8/10, Train Loss: 0.1853 (value: 0.0007, weighted value: 0.0373, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1852 (value: 0.0008, weighted value: 0.0378, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9374
Epoch 10/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0340, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9379
..training done in 69.53 seconds
..evaluation done in 20.19 seconds
Old network+MCTS average reward: 0.7502, min: 0.1389, max: 1.3426, stdev: 0.2315
New network+MCTS average reward: 0.7508, min: 0.1296, max: 1.4259, stdev: 0.2337
Old bare network average reward: 0.7219, min: 0.0926, max: 1.3241, stdev: 0.2351
New bare network average reward: 0.7230, min: 0.1019, max: 1.4259, stdev: 0.2382
External policy "random" average reward: 0.2699, min: -0.3611, max: 1.0370, stdev: 0.2304
External policy "individual greedy" average reward: 0.5265, min: 0.0093, max: 1.2222, stdev: 0.2219
External policy "total greedy" average reward: 0.6478, min: 0.0648, max: 1.2778, stdev: 0.2188
New network won 63 and tied 167 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 609 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.27 seconds
Training examples lengths: [64853, 64804, 64925, 64445, 65048, 64465, 64823, 64619, 64578, 64701]
Total value: 493796.23
Training on 647261 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2490 (value: 0.0015, weighted value: 0.0737, policy: 0.1754, weighted policy: 0.1754), Train Mean Max: 0.9330
Epoch 2/10, Train Loss: 0.2272 (value: 0.0012, weighted value: 0.0609, policy: 0.1663, weighted policy: 0.1663), Train Mean Max: 0.9344
Epoch 3/10, Train Loss: 0.2151 (value: 0.0011, weighted value: 0.0557, policy: 0.1594, weighted policy: 0.1594), Train Mean Max: 0.9351
Epoch 4/10, Train Loss: 0.2060 (value: 0.0010, weighted value: 0.0507, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9355
Epoch 5/10, Train Loss: 0.2001 (value: 0.0010, weighted value: 0.0478, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9360
Epoch 6/10, Train Loss: 0.1972 (value: 0.0009, weighted value: 0.0455, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9361
Epoch 7/10, Train Loss: 0.1938 (value: 0.0009, weighted value: 0.0427, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9363
Epoch 8/10, Train Loss: 0.1898 (value: 0.0008, weighted value: 0.0413, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9367
Epoch 9/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0398, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9368
Epoch 10/10, Train Loss: 0.1873 (value: 0.0008, weighted value: 0.0377, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9368
..training done in 64.67 seconds
..evaluation done in 19.11 seconds
Old network+MCTS average reward: 0.7624, min: -0.0741, max: 1.4630, stdev: 0.2272
New network+MCTS average reward: 0.7652, min: 0.0463, max: 1.4630, stdev: 0.2232
Old bare network average reward: 0.7357, min: -0.0556, max: 1.4259, stdev: 0.2287
New bare network average reward: 0.7363, min: -0.0556, max: 1.3796, stdev: 0.2300
External policy "random" average reward: 0.2768, min: -0.3241, max: 0.8889, stdev: 0.2100
External policy "individual greedy" average reward: 0.5470, min: 0.0000, max: 1.3519, stdev: 0.2228
External policy "total greedy" average reward: 0.6565, min: 0.0926, max: 1.3519, stdev: 0.2129
New network won 76 and tied 161 out of 300 games (52.17% wins where ties are half wins)
Keeping the new network

Training iteration 610 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.08 seconds
Training examples lengths: [64804, 64925, 64445, 65048, 64465, 64823, 64619, 64578, 64701, 65013]
Total value: 494651.92
Training on 647421 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2074 (value: 0.0010, weighted value: 0.0485, policy: 0.1589, weighted policy: 0.1589), Train Mean Max: 0.9356
Epoch 2/10, Train Loss: 0.1981 (value: 0.0009, weighted value: 0.0438, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9365
Epoch 3/10, Train Loss: 0.1924 (value: 0.0008, weighted value: 0.0410, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9367
Epoch 4/10, Train Loss: 0.1899 (value: 0.0008, weighted value: 0.0397, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9370
Epoch 5/10, Train Loss: 0.1873 (value: 0.0008, weighted value: 0.0385, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9372
Epoch 6/10, Train Loss: 0.1847 (value: 0.0007, weighted value: 0.0369, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9374
Epoch 7/10, Train Loss: 0.1826 (value: 0.0007, weighted value: 0.0346, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9375
Epoch 8/10, Train Loss: 0.1830 (value: 0.0007, weighted value: 0.0352, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9377
Epoch 9/10, Train Loss: 0.1803 (value: 0.0007, weighted value: 0.0332, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9378
Epoch 10/10, Train Loss: 0.1793 (value: 0.0006, weighted value: 0.0320, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9380
..training done in 61.42 seconds
..evaluation done in 19.04 seconds
Old network+MCTS average reward: 0.7698, min: 0.1944, max: 1.5278, stdev: 0.2415
New network+MCTS average reward: 0.7691, min: 0.2222, max: 1.5278, stdev: 0.2416
Old bare network average reward: 0.7378, min: 0.1389, max: 1.5370, stdev: 0.2492
New bare network average reward: 0.7382, min: 0.1574, max: 1.5278, stdev: 0.2467
External policy "random" average reward: 0.2762, min: -0.3981, max: 0.9722, stdev: 0.2181
External policy "individual greedy" average reward: 0.5506, min: -0.0370, max: 1.2407, stdev: 0.2221
External policy "total greedy" average reward: 0.6623, min: 0.1389, max: 1.4444, stdev: 0.2186
New network won 72 and tied 152 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 611 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.56 seconds
Training examples lengths: [64925, 64445, 65048, 64465, 64823, 64619, 64578, 64701, 65013, 64873]
Total value: 495338.25
Training on 647490 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2247 (value: 0.0012, weighted value: 0.0578, policy: 0.1669, weighted policy: 0.1669), Train Mean Max: 0.9344
Epoch 2/10, Train Loss: 0.2091 (value: 0.0010, weighted value: 0.0504, policy: 0.1587, weighted policy: 0.1587), Train Mean Max: 0.9356
Epoch 3/10, Train Loss: 0.2009 (value: 0.0009, weighted value: 0.0475, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9360
Epoch 4/10, Train Loss: 0.1961 (value: 0.0009, weighted value: 0.0445, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9361
Epoch 5/10, Train Loss: 0.1926 (value: 0.0008, weighted value: 0.0422, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9365
Epoch 6/10, Train Loss: 0.1897 (value: 0.0008, weighted value: 0.0404, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9367
Epoch 7/10, Train Loss: 0.1880 (value: 0.0008, weighted value: 0.0389, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9369
Epoch 8/10, Train Loss: 0.1869 (value: 0.0008, weighted value: 0.0380, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9372
Epoch 9/10, Train Loss: 0.1834 (value: 0.0007, weighted value: 0.0361, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9375
Epoch 10/10, Train Loss: 0.1827 (value: 0.0007, weighted value: 0.0355, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9376
..training done in 62.52 seconds
..evaluation done in 18.77 seconds
Old network+MCTS average reward: 0.7522, min: 0.2130, max: 1.4722, stdev: 0.2293
New network+MCTS average reward: 0.7547, min: 0.2222, max: 1.4722, stdev: 0.2279
Old bare network average reward: 0.7246, min: 0.1481, max: 1.3519, stdev: 0.2299
New bare network average reward: 0.7231, min: 0.1944, max: 1.3519, stdev: 0.2286
External policy "random" average reward: 0.2504, min: -0.3056, max: 0.8148, stdev: 0.2215
External policy "individual greedy" average reward: 0.5317, min: -0.0926, max: 1.2037, stdev: 0.2237
External policy "total greedy" average reward: 0.6481, min: 0.1574, max: 1.2593, stdev: 0.2138
New network won 67 and tied 172 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 612 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.80 seconds
Training examples lengths: [64445, 65048, 64465, 64823, 64619, 64578, 64701, 65013, 64873, 64763]
Total value: 495245.96
Training on 647328 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2011 (value: 0.0009, weighted value: 0.0457, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9361
Epoch 2/10, Train Loss: 0.1948 (value: 0.0008, weighted value: 0.0414, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9367
Epoch 3/10, Train Loss: 0.1898 (value: 0.0008, weighted value: 0.0394, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9371
Epoch 4/10, Train Loss: 0.1870 (value: 0.0008, weighted value: 0.0380, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9374
Epoch 5/10, Train Loss: 0.1833 (value: 0.0007, weighted value: 0.0357, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9378
Epoch 6/10, Train Loss: 0.1828 (value: 0.0007, weighted value: 0.0358, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9377
Epoch 7/10, Train Loss: 0.1816 (value: 0.0007, weighted value: 0.0337, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9378
Epoch 8/10, Train Loss: 0.1800 (value: 0.0007, weighted value: 0.0333, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1784 (value: 0.0006, weighted value: 0.0323, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9383
Epoch 10/10, Train Loss: 0.1777 (value: 0.0006, weighted value: 0.0308, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9383
..training done in 62.67 seconds
..evaluation done in 18.79 seconds
Old network+MCTS average reward: 0.7437, min: 0.1574, max: 1.3148, stdev: 0.2323
New network+MCTS average reward: 0.7432, min: 0.1574, max: 1.3333, stdev: 0.2332
Old bare network average reward: 0.7123, min: 0.1204, max: 1.3056, stdev: 0.2334
New bare network average reward: 0.7130, min: 0.0926, max: 1.2963, stdev: 0.2360
External policy "random" average reward: 0.2613, min: -0.2130, max: 0.9907, stdev: 0.2106
External policy "individual greedy" average reward: 0.5300, min: 0.0556, max: 1.2407, stdev: 0.2117
External policy "total greedy" average reward: 0.6427, min: 0.1296, max: 1.2315, stdev: 0.2162
New network won 73 and tied 158 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 613 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 60.66 seconds
Training examples lengths: [65048, 64465, 64823, 64619, 64578, 64701, 65013, 64873, 64763, 64733]
Total value: 495582.97
Training on 647616 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1997 (value: 0.0009, weighted value: 0.0426, policy: 0.1571, weighted policy: 0.1571), Train Mean Max: 0.9367
Epoch 2/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0384, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9375
Epoch 3/10, Train Loss: 0.1857 (value: 0.0007, weighted value: 0.0362, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9377
Epoch 4/10, Train Loss: 0.1826 (value: 0.0007, weighted value: 0.0349, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9380
Epoch 5/10, Train Loss: 0.1806 (value: 0.0007, weighted value: 0.0336, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9380
Epoch 6/10, Train Loss: 0.1809 (value: 0.0007, weighted value: 0.0334, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9381
Epoch 7/10, Train Loss: 0.1775 (value: 0.0006, weighted value: 0.0311, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9384
Epoch 8/10, Train Loss: 0.1774 (value: 0.0006, weighted value: 0.0307, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1763 (value: 0.0006, weighted value: 0.0306, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9387
Epoch 10/10, Train Loss: 0.1748 (value: 0.0006, weighted value: 0.0291, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9387
..training done in 68.40 seconds
..evaluation done in 19.43 seconds
Old network+MCTS average reward: 0.7616, min: 0.1481, max: 1.6204, stdev: 0.2479
New network+MCTS average reward: 0.7623, min: 0.1759, max: 1.6111, stdev: 0.2506
Old bare network average reward: 0.7272, min: 0.1759, max: 1.5833, stdev: 0.2512
New bare network average reward: 0.7285, min: 0.1759, max: 1.5833, stdev: 0.2492
External policy "random" average reward: 0.2649, min: -0.3056, max: 0.8981, stdev: 0.2348
External policy "individual greedy" average reward: 0.5504, min: -0.0370, max: 1.4167, stdev: 0.2335
External policy "total greedy" average reward: 0.6666, min: 0.1944, max: 1.3796, stdev: 0.2282
New network won 65 and tied 161 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 614 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.26 seconds
Training examples lengths: [64465, 64823, 64619, 64578, 64701, 65013, 64873, 64763, 64733, 64729]
Total value: 495640.69
Training on 647297 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2202 (value: 0.0011, weighted value: 0.0546, policy: 0.1656, weighted policy: 0.1656), Train Mean Max: 0.9351
Epoch 2/10, Train Loss: 0.2057 (value: 0.0009, weighted value: 0.0469, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9362
Epoch 3/10, Train Loss: 0.1965 (value: 0.0009, weighted value: 0.0428, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9367
Epoch 4/10, Train Loss: 0.1927 (value: 0.0008, weighted value: 0.0423, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9369
Epoch 5/10, Train Loss: 0.1868 (value: 0.0008, weighted value: 0.0376, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9372
Epoch 6/10, Train Loss: 0.1873 (value: 0.0007, weighted value: 0.0372, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9372
Epoch 7/10, Train Loss: 0.1854 (value: 0.0007, weighted value: 0.0366, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9376
Epoch 8/10, Train Loss: 0.1832 (value: 0.0007, weighted value: 0.0347, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9377
Epoch 9/10, Train Loss: 0.1817 (value: 0.0007, weighted value: 0.0343, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9378
Epoch 10/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0337, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9380
..training done in 62.97 seconds
..evaluation done in 19.64 seconds
Old network+MCTS average reward: 0.7387, min: 0.1574, max: 1.5185, stdev: 0.2314
New network+MCTS average reward: 0.7379, min: 0.1574, max: 1.5185, stdev: 0.2331
Old bare network average reward: 0.7048, min: 0.1111, max: 1.5000, stdev: 0.2372
New bare network average reward: 0.7040, min: 0.0926, max: 1.5185, stdev: 0.2369
External policy "random" average reward: 0.2486, min: -0.3333, max: 1.3519, stdev: 0.2346
External policy "individual greedy" average reward: 0.5273, min: 0.0000, max: 1.5278, stdev: 0.2323
External policy "total greedy" average reward: 0.6264, min: 0.0185, max: 1.6019, stdev: 0.2185
New network won 63 and tied 171 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 615 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.43 seconds
Training examples lengths: [64823, 64619, 64578, 64701, 65013, 64873, 64763, 64733, 64729, 64926]
Total value: 496242.99
Training on 647758 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2346 (value: 0.0013, weighted value: 0.0635, policy: 0.1711, weighted policy: 0.1711), Train Mean Max: 0.9340
Epoch 2/10, Train Loss: 0.2158 (value: 0.0011, weighted value: 0.0545, policy: 0.1613, weighted policy: 0.1613), Train Mean Max: 0.9352
Epoch 3/10, Train Loss: 0.2053 (value: 0.0010, weighted value: 0.0492, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9357
Epoch 4/10, Train Loss: 0.1998 (value: 0.0009, weighted value: 0.0465, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9360
Epoch 5/10, Train Loss: 0.1936 (value: 0.0009, weighted value: 0.0427, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9365
Epoch 6/10, Train Loss: 0.1903 (value: 0.0008, weighted value: 0.0407, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9367
Epoch 7/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0402, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9367
Epoch 8/10, Train Loss: 0.1871 (value: 0.0008, weighted value: 0.0378, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9370
Epoch 9/10, Train Loss: 0.1848 (value: 0.0007, weighted value: 0.0367, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9372
Epoch 10/10, Train Loss: 0.1841 (value: 0.0007, weighted value: 0.0362, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9373
..training done in 61.01 seconds
..evaluation done in 19.23 seconds
Old network+MCTS average reward: 0.7518, min: 0.1389, max: 1.7222, stdev: 0.2374
New network+MCTS average reward: 0.7511, min: 0.1389, max: 1.6389, stdev: 0.2332
Old bare network average reward: 0.7229, min: 0.1389, max: 1.5741, stdev: 0.2334
New bare network average reward: 0.7226, min: 0.1389, max: 1.6296, stdev: 0.2345
External policy "random" average reward: 0.2500, min: -0.2963, max: 0.8426, stdev: 0.2187
External policy "individual greedy" average reward: 0.5345, min: 0.0370, max: 1.3056, stdev: 0.2281
External policy "total greedy" average reward: 0.6426, min: 0.1481, max: 1.4259, stdev: 0.2226
New network won 71 and tied 165 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 616 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.32 seconds
Training examples lengths: [64619, 64578, 64701, 65013, 64873, 64763, 64733, 64729, 64926, 64517]
Total value: 496094.33
Training on 647452 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2015 (value: 0.0009, weighted value: 0.0449, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9361
Epoch 2/10, Train Loss: 0.1944 (value: 0.0008, weighted value: 0.0413, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9368
Epoch 3/10, Train Loss: 0.1908 (value: 0.0008, weighted value: 0.0399, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9370
Epoch 4/10, Train Loss: 0.1874 (value: 0.0007, weighted value: 0.0375, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9374
Epoch 5/10, Train Loss: 0.1853 (value: 0.0007, weighted value: 0.0361, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9374
Epoch 6/10, Train Loss: 0.1824 (value: 0.0007, weighted value: 0.0353, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9378
Epoch 7/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0339, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9378
Epoch 8/10, Train Loss: 0.1805 (value: 0.0007, weighted value: 0.0334, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9380
Epoch 9/10, Train Loss: 0.1791 (value: 0.0006, weighted value: 0.0318, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9380
Epoch 10/10, Train Loss: 0.1785 (value: 0.0006, weighted value: 0.0316, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9382
..training done in 65.19 seconds
..evaluation done in 19.01 seconds
Old network+MCTS average reward: 0.7693, min: 0.1944, max: 1.6111, stdev: 0.2501
New network+MCTS average reward: 0.7708, min: 0.1759, max: 1.6111, stdev: 0.2499
Old bare network average reward: 0.7418, min: 0.1204, max: 1.6296, stdev: 0.2501
New bare network average reward: 0.7381, min: 0.1204, max: 1.6574, stdev: 0.2565
External policy "random" average reward: 0.2667, min: -0.3056, max: 1.3241, stdev: 0.2427
External policy "individual greedy" average reward: 0.5644, min: -0.0648, max: 1.5741, stdev: 0.2346
External policy "total greedy" average reward: 0.6731, min: 0.1111, max: 1.4444, stdev: 0.2298
New network won 64 and tied 185 out of 300 games (52.17% wins where ties are half wins)
Keeping the new network

Training iteration 617 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 92.10 seconds
Training examples lengths: [64578, 64701, 65013, 64873, 64763, 64733, 64729, 64926, 64517, 64707]
Total value: 495973.11
Training on 647540 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1979 (value: 0.0009, weighted value: 0.0427, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9369
Epoch 2/10, Train Loss: 0.1902 (value: 0.0008, weighted value: 0.0377, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9373
Epoch 3/10, Train Loss: 0.1869 (value: 0.0007, weighted value: 0.0373, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9377
Epoch 4/10, Train Loss: 0.1819 (value: 0.0007, weighted value: 0.0336, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9380
Epoch 5/10, Train Loss: 0.1807 (value: 0.0007, weighted value: 0.0331, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9382
Epoch 6/10, Train Loss: 0.1805 (value: 0.0007, weighted value: 0.0332, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9381
Epoch 7/10, Train Loss: 0.1781 (value: 0.0006, weighted value: 0.0313, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9384
Epoch 8/10, Train Loss: 0.1779 (value: 0.0006, weighted value: 0.0309, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9385
Epoch 9/10, Train Loss: 0.1756 (value: 0.0006, weighted value: 0.0300, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9388
Epoch 10/10, Train Loss: 0.1748 (value: 0.0006, weighted value: 0.0291, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9388
..training done in 61.80 seconds
..evaluation done in 18.67 seconds
Old network+MCTS average reward: 0.7473, min: 0.2500, max: 1.4167, stdev: 0.2164
New network+MCTS average reward: 0.7496, min: 0.2500, max: 1.4537, stdev: 0.2191
Old bare network average reward: 0.7154, min: 0.1296, max: 1.3796, stdev: 0.2230
New bare network average reward: 0.7173, min: 0.1944, max: 1.3796, stdev: 0.2223
External policy "random" average reward: 0.2560, min: -0.4074, max: 0.8611, stdev: 0.2118
External policy "individual greedy" average reward: 0.5451, min: -0.1019, max: 1.1019, stdev: 0.2279
External policy "total greedy" average reward: 0.6553, min: 0.0556, max: 1.2130, stdev: 0.2152
New network won 63 and tied 176 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 618 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.26 seconds
Training examples lengths: [64701, 65013, 64873, 64763, 64733, 64729, 64926, 64517, 64707, 64773]
Total value: 497127.26
Training on 647735 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1955 (value: 0.0008, weighted value: 0.0411, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9372
Epoch 2/10, Train Loss: 0.1879 (value: 0.0007, weighted value: 0.0370, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9380
Epoch 3/10, Train Loss: 0.1835 (value: 0.0007, weighted value: 0.0353, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9383
Epoch 4/10, Train Loss: 0.1795 (value: 0.0007, weighted value: 0.0326, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9385
Epoch 5/10, Train Loss: 0.1788 (value: 0.0006, weighted value: 0.0325, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9384
Epoch 6/10, Train Loss: 0.1779 (value: 0.0006, weighted value: 0.0317, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9387
Epoch 7/10, Train Loss: 0.1774 (value: 0.0006, weighted value: 0.0308, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9388
Epoch 8/10, Train Loss: 0.1755 (value: 0.0006, weighted value: 0.0298, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9390
Epoch 9/10, Train Loss: 0.1742 (value: 0.0006, weighted value: 0.0292, policy: 0.1449, weighted policy: 0.1449), Train Mean Max: 0.9392
Epoch 10/10, Train Loss: 0.1744 (value: 0.0006, weighted value: 0.0285, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9391
..training done in 61.84 seconds
..evaluation done in 18.43 seconds
Old network+MCTS average reward: 0.7568, min: 0.1389, max: 1.4815, stdev: 0.2247
New network+MCTS average reward: 0.7593, min: 0.1667, max: 1.4815, stdev: 0.2267
Old bare network average reward: 0.7257, min: 0.1204, max: 1.3796, stdev: 0.2287
New bare network average reward: 0.7245, min: 0.0926, max: 1.4815, stdev: 0.2288
External policy "random" average reward: 0.2435, min: -0.2778, max: 0.8148, stdev: 0.2138
External policy "individual greedy" average reward: 0.5316, min: -0.1574, max: 1.0833, stdev: 0.2092
External policy "total greedy" average reward: 0.6446, min: 0.0093, max: 1.3056, stdev: 0.2108
New network won 73 and tied 175 out of 300 games (53.50% wins where ties are half wins)
Keeping the new network

Training iteration 619 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.85 seconds
Training examples lengths: [65013, 64873, 64763, 64733, 64729, 64926, 64517, 64707, 64773, 64783]
Total value: 497682.68
Training on 647817 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1941 (value: 0.0008, weighted value: 0.0395, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9376
Epoch 2/10, Train Loss: 0.1881 (value: 0.0007, weighted value: 0.0371, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9382
Epoch 3/10, Train Loss: 0.1817 (value: 0.0007, weighted value: 0.0337, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9386
Epoch 4/10, Train Loss: 0.1783 (value: 0.0006, weighted value: 0.0320, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9387
Epoch 5/10, Train Loss: 0.1780 (value: 0.0006, weighted value: 0.0322, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9387
Epoch 6/10, Train Loss: 0.1757 (value: 0.0006, weighted value: 0.0304, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9388
Epoch 7/10, Train Loss: 0.1753 (value: 0.0006, weighted value: 0.0298, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9389
Epoch 8/10, Train Loss: 0.1749 (value: 0.0006, weighted value: 0.0293, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9390
Epoch 9/10, Train Loss: 0.1732 (value: 0.0006, weighted value: 0.0284, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9392
Epoch 10/10, Train Loss: 0.1722 (value: 0.0006, weighted value: 0.0276, policy: 0.1446, weighted policy: 0.1446), Train Mean Max: 0.9394
..training done in 61.17 seconds
..evaluation done in 18.83 seconds
Old network+MCTS average reward: 0.7911, min: 0.2037, max: 1.7315, stdev: 0.2651
New network+MCTS average reward: 0.7923, min: 0.2037, max: 1.6852, stdev: 0.2649
Old bare network average reward: 0.7567, min: 0.1481, max: 1.6759, stdev: 0.2649
New bare network average reward: 0.7596, min: 0.2037, max: 1.6574, stdev: 0.2652
External policy "random" average reward: 0.2800, min: -0.2315, max: 1.0648, stdev: 0.2436
External policy "individual greedy" average reward: 0.5661, min: -0.0556, max: 1.4259, stdev: 0.2442
External policy "total greedy" average reward: 0.6875, min: 0.1019, max: 1.4907, stdev: 0.2467
New network won 67 and tied 178 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network

Training iteration 620 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.70 seconds
Training examples lengths: [64873, 64763, 64733, 64729, 64926, 64517, 64707, 64773, 64783, 64693]
Total value: 496788.32
Training on 647497 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1952 (value: 0.0008, weighted value: 0.0398, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9373
Epoch 2/10, Train Loss: 0.1878 (value: 0.0007, weighted value: 0.0354, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9381
Epoch 3/10, Train Loss: 0.1828 (value: 0.0007, weighted value: 0.0335, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9384
Epoch 4/10, Train Loss: 0.1796 (value: 0.0006, weighted value: 0.0323, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9386
Epoch 5/10, Train Loss: 0.1786 (value: 0.0006, weighted value: 0.0318, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9386
Epoch 6/10, Train Loss: 0.1769 (value: 0.0006, weighted value: 0.0304, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9387
Epoch 7/10, Train Loss: 0.1755 (value: 0.0006, weighted value: 0.0296, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9389
Epoch 8/10, Train Loss: 0.1733 (value: 0.0006, weighted value: 0.0281, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9390
Epoch 9/10, Train Loss: 0.1744 (value: 0.0006, weighted value: 0.0286, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9391
Epoch 10/10, Train Loss: 0.1734 (value: 0.0006, weighted value: 0.0280, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9390
..training done in 60.39 seconds
..evaluation done in 19.12 seconds
Old network+MCTS average reward: 0.7745, min: 0.0926, max: 1.4444, stdev: 0.2343
New network+MCTS average reward: 0.7762, min: 0.0926, max: 1.4630, stdev: 0.2354
Old bare network average reward: 0.7461, min: 0.0093, max: 1.4444, stdev: 0.2349
New bare network average reward: 0.7466, min: 0.0926, max: 1.3981, stdev: 0.2334
External policy "random" average reward: 0.2590, min: -0.2963, max: 0.9352, stdev: 0.2175
External policy "individual greedy" average reward: 0.5400, min: 0.0000, max: 1.3426, stdev: 0.2251
External policy "total greedy" average reward: 0.6703, min: 0.1296, max: 1.3889, stdev: 0.2231
New network won 80 and tied 161 out of 300 games (53.50% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_620

Training iteration 621 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.37 seconds
Training examples lengths: [64763, 64733, 64729, 64926, 64517, 64707, 64773, 64783, 64693, 65055]
Total value: 496533.37
Training on 647679 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1949 (value: 0.0008, weighted value: 0.0391, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9372
Epoch 2/10, Train Loss: 0.1868 (value: 0.0007, weighted value: 0.0357, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9380
Epoch 3/10, Train Loss: 0.1820 (value: 0.0007, weighted value: 0.0336, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9382
Epoch 4/10, Train Loss: 0.1794 (value: 0.0006, weighted value: 0.0322, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9385
Epoch 5/10, Train Loss: 0.1777 (value: 0.0006, weighted value: 0.0309, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9383
Epoch 6/10, Train Loss: 0.1764 (value: 0.0006, weighted value: 0.0297, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9385
Epoch 7/10, Train Loss: 0.1758 (value: 0.0006, weighted value: 0.0298, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9386
Epoch 8/10, Train Loss: 0.1741 (value: 0.0006, weighted value: 0.0282, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9389
Epoch 9/10, Train Loss: 0.1743 (value: 0.0006, weighted value: 0.0280, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9388
Epoch 10/10, Train Loss: 0.1729 (value: 0.0005, weighted value: 0.0274, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9390
..training done in 63.89 seconds
..evaluation done in 20.37 seconds
Old network+MCTS average reward: 0.7687, min: 0.1574, max: 1.4167, stdev: 0.2319
New network+MCTS average reward: 0.7693, min: 0.1759, max: 1.3889, stdev: 0.2307
Old bare network average reward: 0.7373, min: 0.1019, max: 1.3889, stdev: 0.2374
New bare network average reward: 0.7382, min: 0.1019, max: 1.3796, stdev: 0.2358
External policy "random" average reward: 0.2657, min: -0.3148, max: 0.9167, stdev: 0.2138
External policy "individual greedy" average reward: 0.5511, min: -0.0278, max: 1.2963, stdev: 0.2209
External policy "total greedy" average reward: 0.6758, min: 0.1204, max: 1.3148, stdev: 0.2110
New network won 66 and tied 165 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 622 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.46 seconds
Training examples lengths: [64733, 64729, 64926, 64517, 64707, 64773, 64783, 64693, 65055, 64693]
Total value: 496181.98
Training on 647609 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2147 (value: 0.0010, weighted value: 0.0506, policy: 0.1641, weighted policy: 0.1641), Train Mean Max: 0.9357
Epoch 2/10, Train Loss: 0.2016 (value: 0.0009, weighted value: 0.0446, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9368
Epoch 3/10, Train Loss: 0.1922 (value: 0.0008, weighted value: 0.0397, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9373
Epoch 4/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0386, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9374
Epoch 5/10, Train Loss: 0.1850 (value: 0.0007, weighted value: 0.0364, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1825 (value: 0.0007, weighted value: 0.0346, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9377
Epoch 7/10, Train Loss: 0.1817 (value: 0.0007, weighted value: 0.0340, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9378
Epoch 8/10, Train Loss: 0.1798 (value: 0.0006, weighted value: 0.0321, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9380
Epoch 9/10, Train Loss: 0.1791 (value: 0.0006, weighted value: 0.0321, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9382
Epoch 10/10, Train Loss: 0.1778 (value: 0.0006, weighted value: 0.0312, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9383
..training done in 69.90 seconds
..evaluation done in 20.69 seconds
Old network+MCTS average reward: 0.7551, min: -0.0556, max: 1.6019, stdev: 0.2524
New network+MCTS average reward: 0.7574, min: 0.0648, max: 1.6019, stdev: 0.2519
Old bare network average reward: 0.7234, min: -0.0556, max: 1.6204, stdev: 0.2535
New bare network average reward: 0.7230, min: -0.0556, max: 1.5556, stdev: 0.2538
External policy "random" average reward: 0.2592, min: -0.3148, max: 1.0463, stdev: 0.2214
External policy "individual greedy" average reward: 0.5310, min: -0.0741, max: 1.3056, stdev: 0.2385
External policy "total greedy" average reward: 0.6427, min: 0.1111, max: 1.3704, stdev: 0.2355
New network won 68 and tied 162 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 623 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.15 seconds
Training examples lengths: [64729, 64926, 64517, 64707, 64773, 64783, 64693, 65055, 64693, 64898]
Total value: 497127.16
Training on 647774 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2321 (value: 0.0012, weighted value: 0.0617, policy: 0.1704, weighted policy: 0.1704), Train Mean Max: 0.9346
Epoch 2/10, Train Loss: 0.2135 (value: 0.0011, weighted value: 0.0527, policy: 0.1608, weighted policy: 0.1608), Train Mean Max: 0.9358
Epoch 3/10, Train Loss: 0.2013 (value: 0.0009, weighted value: 0.0470, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9363
Epoch 4/10, Train Loss: 0.1949 (value: 0.0009, weighted value: 0.0429, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9367
Epoch 5/10, Train Loss: 0.1928 (value: 0.0009, weighted value: 0.0428, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1891 (value: 0.0008, weighted value: 0.0403, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9369
Epoch 7/10, Train Loss: 0.1861 (value: 0.0007, weighted value: 0.0372, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9372
Epoch 8/10, Train Loss: 0.1839 (value: 0.0007, weighted value: 0.0363, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1817 (value: 0.0007, weighted value: 0.0346, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9376
Epoch 10/10, Train Loss: 0.1815 (value: 0.0007, weighted value: 0.0337, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9378
..training done in 65.84 seconds
..evaluation done in 18.69 seconds
Old network+MCTS average reward: 0.7640, min: 0.1852, max: 1.5278, stdev: 0.2286
New network+MCTS average reward: 0.7669, min: 0.2315, max: 1.5278, stdev: 0.2273
Old bare network average reward: 0.7319, min: 0.1574, max: 1.4907, stdev: 0.2320
New bare network average reward: 0.7313, min: 0.1852, max: 1.4907, stdev: 0.2306
External policy "random" average reward: 0.2509, min: -0.3241, max: 1.3796, stdev: 0.2241
External policy "individual greedy" average reward: 0.5396, min: 0.0463, max: 1.4815, stdev: 0.2209
External policy "total greedy" average reward: 0.6692, min: 0.2315, max: 1.5648, stdev: 0.2161
New network won 62 and tied 175 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 624 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.02 seconds
Training examples lengths: [64926, 64517, 64707, 64773, 64783, 64693, 65055, 64693, 64898, 64879]
Total value: 497106.27
Training on 647924 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2504 (value: 0.0015, weighted value: 0.0731, policy: 0.1773, weighted policy: 0.1773), Train Mean Max: 0.9333
Epoch 2/10, Train Loss: 0.2224 (value: 0.0012, weighted value: 0.0593, policy: 0.1631, weighted policy: 0.1631), Train Mean Max: 0.9349
Epoch 3/10, Train Loss: 0.2106 (value: 0.0011, weighted value: 0.0535, policy: 0.1570, weighted policy: 0.1570), Train Mean Max: 0.9355
Epoch 4/10, Train Loss: 0.2036 (value: 0.0010, weighted value: 0.0493, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9357
Epoch 5/10, Train Loss: 0.1990 (value: 0.0009, weighted value: 0.0472, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9361
Epoch 6/10, Train Loss: 0.1942 (value: 0.0009, weighted value: 0.0440, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9363
Epoch 7/10, Train Loss: 0.1915 (value: 0.0008, weighted value: 0.0417, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9365
Epoch 8/10, Train Loss: 0.1889 (value: 0.0008, weighted value: 0.0398, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9367
Epoch 9/10, Train Loss: 0.1869 (value: 0.0008, weighted value: 0.0383, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9370
Epoch 10/10, Train Loss: 0.1847 (value: 0.0007, weighted value: 0.0369, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9373
..training done in 66.16 seconds
..evaluation done in 19.59 seconds
Old network+MCTS average reward: 0.7439, min: 0.1667, max: 1.5000, stdev: 0.2248
New network+MCTS average reward: 0.7458, min: 0.1204, max: 1.5000, stdev: 0.2269
Old bare network average reward: 0.7121, min: 0.0926, max: 1.4722, stdev: 0.2280
New bare network average reward: 0.7144, min: -0.0185, max: 1.4722, stdev: 0.2294
External policy "random" average reward: 0.2601, min: -0.3611, max: 0.9167, stdev: 0.2236
External policy "individual greedy" average reward: 0.5228, min: -0.0926, max: 1.1481, stdev: 0.2291
External policy "total greedy" average reward: 0.6387, min: 0.0278, max: 1.2407, stdev: 0.2158
New network won 71 and tied 173 out of 300 games (52.50% wins where ties are half wins)
Keeping the new network

Training iteration 625 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.12 seconds
Training examples lengths: [64517, 64707, 64773, 64783, 64693, 65055, 64693, 64898, 64879, 64788]
Total value: 496752.29
Training on 647786 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2037 (value: 0.0010, weighted value: 0.0476, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9358
Epoch 2/10, Train Loss: 0.1957 (value: 0.0008, weighted value: 0.0425, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9365
Epoch 3/10, Train Loss: 0.1906 (value: 0.0008, weighted value: 0.0402, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9368
Epoch 4/10, Train Loss: 0.1870 (value: 0.0008, weighted value: 0.0378, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9369
Epoch 5/10, Train Loss: 0.1850 (value: 0.0007, weighted value: 0.0368, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9373
Epoch 6/10, Train Loss: 0.1844 (value: 0.0007, weighted value: 0.0361, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9374
Epoch 7/10, Train Loss: 0.1810 (value: 0.0007, weighted value: 0.0335, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9376
Epoch 8/10, Train Loss: 0.1818 (value: 0.0007, weighted value: 0.0341, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9377
Epoch 9/10, Train Loss: 0.1789 (value: 0.0006, weighted value: 0.0324, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9380
Epoch 10/10, Train Loss: 0.1780 (value: 0.0006, weighted value: 0.0320, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9382
..training done in 62.30 seconds
..evaluation done in 18.32 seconds
Old network+MCTS average reward: 0.7427, min: 0.2130, max: 1.4444, stdev: 0.2323
New network+MCTS average reward: 0.7429, min: 0.1852, max: 1.4444, stdev: 0.2357
Old bare network average reward: 0.7097, min: 0.1204, max: 1.3889, stdev: 0.2345
New bare network average reward: 0.7145, min: 0.1481, max: 1.3981, stdev: 0.2347
External policy "random" average reward: 0.2592, min: -0.2778, max: 0.9352, stdev: 0.2210
External policy "individual greedy" average reward: 0.5285, min: -0.1481, max: 1.3333, stdev: 0.2399
External policy "total greedy" average reward: 0.6448, min: 0.0648, max: 1.3241, stdev: 0.2167
New network won 72 and tied 172 out of 300 games (52.67% wins where ties are half wins)
Keeping the new network

Training iteration 626 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 65.71 seconds
Training examples lengths: [64707, 64773, 64783, 64693, 65055, 64693, 64898, 64879, 64788, 64956]
Total value: 497077.64
Training on 648225 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1986 (value: 0.0009, weighted value: 0.0435, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9366
Epoch 2/10, Train Loss: 0.1897 (value: 0.0008, weighted value: 0.0380, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9372
Epoch 3/10, Train Loss: 0.1859 (value: 0.0007, weighted value: 0.0365, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9375
Epoch 4/10, Train Loss: 0.1839 (value: 0.0007, weighted value: 0.0360, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9377
Epoch 5/10, Train Loss: 0.1811 (value: 0.0007, weighted value: 0.0340, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9379
Epoch 6/10, Train Loss: 0.1792 (value: 0.0006, weighted value: 0.0322, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9380
Epoch 7/10, Train Loss: 0.1796 (value: 0.0007, weighted value: 0.0327, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9382
Epoch 8/10, Train Loss: 0.1768 (value: 0.0006, weighted value: 0.0309, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1760 (value: 0.0006, weighted value: 0.0302, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1764 (value: 0.0006, weighted value: 0.0303, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9385
..training done in 71.82 seconds
..evaluation done in 20.85 seconds
Old network+MCTS average reward: 0.7514, min: 0.1574, max: 1.6204, stdev: 0.2487
New network+MCTS average reward: 0.7524, min: 0.1852, max: 1.6204, stdev: 0.2485
Old bare network average reward: 0.7236, min: 0.0833, max: 1.5463, stdev: 0.2508
New bare network average reward: 0.7257, min: 0.0833, max: 1.6204, stdev: 0.2529
External policy "random" average reward: 0.2716, min: -0.3519, max: 0.9074, stdev: 0.2337
External policy "individual greedy" average reward: 0.5496, min: 0.0093, max: 1.4259, stdev: 0.2445
External policy "total greedy" average reward: 0.6511, min: 0.1296, max: 1.5000, stdev: 0.2419
New network won 60 and tied 180 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 627 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.25 seconds
Training examples lengths: [64773, 64783, 64693, 65055, 64693, 64898, 64879, 64788, 64956, 65198]
Total value: 497994.05
Training on 648716 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1947 (value: 0.0008, weighted value: 0.0408, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9371
Epoch 2/10, Train Loss: 0.1870 (value: 0.0007, weighted value: 0.0368, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9379
Epoch 3/10, Train Loss: 0.1837 (value: 0.0007, weighted value: 0.0360, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9381
Epoch 4/10, Train Loss: 0.1800 (value: 0.0007, weighted value: 0.0328, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9382
Epoch 5/10, Train Loss: 0.1803 (value: 0.0007, weighted value: 0.0336, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9385
Epoch 6/10, Train Loss: 0.1761 (value: 0.0006, weighted value: 0.0308, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9386
Epoch 7/10, Train Loss: 0.1760 (value: 0.0006, weighted value: 0.0310, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9388
Epoch 8/10, Train Loss: 0.1753 (value: 0.0006, weighted value: 0.0299, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9388
Epoch 9/10, Train Loss: 0.1740 (value: 0.0006, weighted value: 0.0288, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9390
Epoch 10/10, Train Loss: 0.1735 (value: 0.0006, weighted value: 0.0293, policy: 0.1442, weighted policy: 0.1442), Train Mean Max: 0.9392
..training done in 60.89 seconds
..evaluation done in 18.52 seconds
Old network+MCTS average reward: 0.7334, min: 0.1204, max: 1.4074, stdev: 0.2339
New network+MCTS average reward: 0.7320, min: 0.1019, max: 1.3611, stdev: 0.2307
Old bare network average reward: 0.7029, min: 0.1296, max: 1.3796, stdev: 0.2346
New bare network average reward: 0.7020, min: 0.1204, max: 1.3611, stdev: 0.2326
External policy "random" average reward: 0.2347, min: -0.3704, max: 1.2222, stdev: 0.2309
External policy "individual greedy" average reward: 0.5306, min: -0.2037, max: 1.1852, stdev: 0.2293
External policy "total greedy" average reward: 0.6435, min: 0.2130, max: 1.3241, stdev: 0.2190
New network won 58 and tied 181 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 628 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.52 seconds
Training examples lengths: [64783, 64693, 65055, 64693, 64898, 64879, 64788, 64956, 65198, 64871]
Total value: 497535.50
Training on 648814 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2140 (value: 0.0010, weighted value: 0.0520, policy: 0.1620, weighted policy: 0.1620), Train Mean Max: 0.9356
Epoch 2/10, Train Loss: 0.2001 (value: 0.0009, weighted value: 0.0441, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9365
Epoch 3/10, Train Loss: 0.1924 (value: 0.0008, weighted value: 0.0416, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9371
Epoch 4/10, Train Loss: 0.1877 (value: 0.0008, weighted value: 0.0389, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9373
Epoch 5/10, Train Loss: 0.1855 (value: 0.0008, weighted value: 0.0380, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1838 (value: 0.0007, weighted value: 0.0369, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9376
Epoch 7/10, Train Loss: 0.1808 (value: 0.0007, weighted value: 0.0344, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1796 (value: 0.0007, weighted value: 0.0335, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1789 (value: 0.0006, weighted value: 0.0323, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9382
Epoch 10/10, Train Loss: 0.1778 (value: 0.0006, weighted value: 0.0318, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9384
..training done in 62.39 seconds
..evaluation done in 18.41 seconds
Old network+MCTS average reward: 0.7649, min: 0.2222, max: 1.6389, stdev: 0.2278
New network+MCTS average reward: 0.7614, min: 0.2222, max: 1.6389, stdev: 0.2303
Old bare network average reward: 0.7308, min: 0.2222, max: 1.6019, stdev: 0.2324
New bare network average reward: 0.7335, min: 0.1759, max: 1.6019, stdev: 0.2316
External policy "random" average reward: 0.2629, min: -0.2963, max: 1.0741, stdev: 0.2295
External policy "individual greedy" average reward: 0.5455, min: 0.0370, max: 1.3426, stdev: 0.2207
External policy "total greedy" average reward: 0.6632, min: 0.1852, max: 1.3426, stdev: 0.2105
New network won 60 and tied 173 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 629 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 60.31 seconds
Training examples lengths: [64693, 65055, 64693, 64898, 64879, 64788, 64956, 65198, 64871, 64829]
Total value: 497625.17
Training on 648860 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2313 (value: 0.0012, weighted value: 0.0619, policy: 0.1694, weighted policy: 0.1694), Train Mean Max: 0.9341
Epoch 2/10, Train Loss: 0.2120 (value: 0.0011, weighted value: 0.0532, policy: 0.1588, weighted policy: 0.1588), Train Mean Max: 0.9355
Epoch 3/10, Train Loss: 0.2000 (value: 0.0009, weighted value: 0.0474, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9362
Epoch 4/10, Train Loss: 0.1949 (value: 0.0009, weighted value: 0.0446, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9365
Epoch 5/10, Train Loss: 0.1916 (value: 0.0008, weighted value: 0.0422, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9368
Epoch 6/10, Train Loss: 0.1889 (value: 0.0008, weighted value: 0.0404, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1863 (value: 0.0008, weighted value: 0.0386, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9372
Epoch 8/10, Train Loss: 0.1845 (value: 0.0007, weighted value: 0.0369, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1827 (value: 0.0007, weighted value: 0.0359, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9378
Epoch 10/10, Train Loss: 0.1810 (value: 0.0007, weighted value: 0.0346, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9380
..training done in 60.33 seconds
..evaluation done in 18.86 seconds
Old network+MCTS average reward: 0.7809, min: 0.1759, max: 1.4722, stdev: 0.2407
New network+MCTS average reward: 0.7809, min: 0.1574, max: 1.4907, stdev: 0.2414
Old bare network average reward: 0.7541, min: 0.1204, max: 1.4722, stdev: 0.2429
New bare network average reward: 0.7511, min: 0.1204, max: 1.4722, stdev: 0.2406
External policy "random" average reward: 0.2863, min: -0.2315, max: 1.1111, stdev: 0.2253
External policy "individual greedy" average reward: 0.5665, min: 0.0370, max: 1.2963, stdev: 0.2375
External policy "total greedy" average reward: 0.6784, min: 0.1204, max: 1.2778, stdev: 0.2216
New network won 65 and tied 170 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 630 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.97 seconds
Training examples lengths: [65055, 64693, 64898, 64879, 64788, 64956, 65198, 64871, 64829, 64912]
Total value: 497877.85
Training on 649079 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1993 (value: 0.0009, weighted value: 0.0446, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9366
Epoch 2/10, Train Loss: 0.1921 (value: 0.0008, weighted value: 0.0414, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9374
Epoch 3/10, Train Loss: 0.1870 (value: 0.0008, weighted value: 0.0385, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9377
Epoch 4/10, Train Loss: 0.1848 (value: 0.0007, weighted value: 0.0374, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9378
Epoch 5/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0356, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9382
Epoch 6/10, Train Loss: 0.1786 (value: 0.0007, weighted value: 0.0333, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9384
Epoch 7/10, Train Loss: 0.1796 (value: 0.0007, weighted value: 0.0339, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9384
Epoch 8/10, Train Loss: 0.1777 (value: 0.0006, weighted value: 0.0322, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9387
Epoch 9/10, Train Loss: 0.1766 (value: 0.0006, weighted value: 0.0312, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9388
Epoch 10/10, Train Loss: 0.1759 (value: 0.0006, weighted value: 0.0310, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9390
..training done in 59.39 seconds
..evaluation done in 18.30 seconds
Old network+MCTS average reward: 0.7364, min: 0.2130, max: 1.4352, stdev: 0.2270
New network+MCTS average reward: 0.7371, min: 0.2130, max: 1.4352, stdev: 0.2249
Old bare network average reward: 0.7046, min: 0.0648, max: 1.4352, stdev: 0.2302
New bare network average reward: 0.7073, min: 0.2130, max: 1.4352, stdev: 0.2284
External policy "random" average reward: 0.2330, min: -0.2685, max: 0.8981, stdev: 0.2126
External policy "individual greedy" average reward: 0.5215, min: 0.0278, max: 1.2500, stdev: 0.2096
External policy "total greedy" average reward: 0.6321, min: 0.1111, max: 1.3056, stdev: 0.2083
New network won 59 and tied 176 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 631 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.29 seconds
Training examples lengths: [64693, 64898, 64879, 64788, 64956, 65198, 64871, 64829, 64912, 65098]
Total value: 498396.44
Training on 649122 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2164 (value: 0.0011, weighted value: 0.0554, policy: 0.1611, weighted policy: 0.1611), Train Mean Max: 0.9353
Epoch 2/10, Train Loss: 0.2038 (value: 0.0010, weighted value: 0.0485, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9363
Epoch 3/10, Train Loss: 0.1957 (value: 0.0009, weighted value: 0.0446, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9368
Epoch 4/10, Train Loss: 0.1914 (value: 0.0009, weighted value: 0.0426, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9373
Epoch 5/10, Train Loss: 0.1875 (value: 0.0008, weighted value: 0.0397, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9375
Epoch 6/10, Train Loss: 0.1861 (value: 0.0008, weighted value: 0.0389, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9376
Epoch 7/10, Train Loss: 0.1835 (value: 0.0007, weighted value: 0.0371, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1823 (value: 0.0007, weighted value: 0.0357, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1802 (value: 0.0007, weighted value: 0.0342, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9383
Epoch 10/10, Train Loss: 0.1791 (value: 0.0007, weighted value: 0.0337, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9385
..training done in 58.58 seconds
..evaluation done in 18.94 seconds
Old network+MCTS average reward: 0.7588, min: 0.2037, max: 1.7315, stdev: 0.2436
New network+MCTS average reward: 0.7577, min: 0.1574, max: 1.7315, stdev: 0.2430
Old bare network average reward: 0.7277, min: 0.0463, max: 1.7315, stdev: 0.2481
New bare network average reward: 0.7278, min: 0.1389, max: 1.7315, stdev: 0.2484
External policy "random" average reward: 0.2707, min: -0.2778, max: 1.1574, stdev: 0.2287
External policy "individual greedy" average reward: 0.5431, min: 0.0000, max: 1.6852, stdev: 0.2353
External policy "total greedy" average reward: 0.6578, min: 0.0741, max: 1.7407, stdev: 0.2268
New network won 59 and tied 174 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 632 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.92 seconds
Training examples lengths: [64898, 64879, 64788, 64956, 65198, 64871, 64829, 64912, 65098, 65102]
Total value: 500098.14
Training on 649531 examples
Training with 318 batches of size 2048
Epoch 1/10, Train Loss: 0.2333 (value: 0.0013, weighted value: 0.0660, policy: 0.1673, weighted policy: 0.1673), Train Mean Max: 0.9341
Epoch 2/10, Train Loss: 0.2145 (value: 0.0011, weighted value: 0.0555, policy: 0.1591, weighted policy: 0.1591), Train Mean Max: 0.9355
Epoch 3/10, Train Loss: 0.2058 (value: 0.0010, weighted value: 0.0512, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9358
Epoch 4/10, Train Loss: 0.1991 (value: 0.0009, weighted value: 0.0475, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9363
Epoch 5/10, Train Loss: 0.1944 (value: 0.0009, weighted value: 0.0448, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9368
Epoch 6/10, Train Loss: 0.1915 (value: 0.0009, weighted value: 0.0429, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1874 (value: 0.0008, weighted value: 0.0399, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9372
Epoch 8/10, Train Loss: 0.1868 (value: 0.0008, weighted value: 0.0397, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1842 (value: 0.0008, weighted value: 0.0375, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1820 (value: 0.0007, weighted value: 0.0358, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9380
..training done in 66.33 seconds
..evaluation done in 21.07 seconds
Old network+MCTS average reward: 0.7514, min: 0.2130, max: 1.7778, stdev: 0.2364
New network+MCTS average reward: 0.7520, min: 0.2037, max: 1.7778, stdev: 0.2359
Old bare network average reward: 0.7189, min: 0.1019, max: 1.7778, stdev: 0.2431
New bare network average reward: 0.7188, min: 0.1019, max: 1.7778, stdev: 0.2405
External policy "random" average reward: 0.2648, min: -0.3704, max: 1.2963, stdev: 0.2318
External policy "individual greedy" average reward: 0.5427, min: -0.1204, max: 1.6574, stdev: 0.2414
External policy "total greedy" average reward: 0.6438, min: -0.0741, max: 1.5556, stdev: 0.2263
New network won 80 and tied 155 out of 300 games (52.50% wins where ties are half wins)
Keeping the new network

Training iteration 633 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.56 seconds
Training examples lengths: [64879, 64788, 64956, 65198, 64871, 64829, 64912, 65098, 65102, 64735]
Total value: 499449.18
Training on 649368 examples
Training with 318 batches of size 2048
Epoch 1/10, Train Loss: 0.2027 (value: 0.0009, weighted value: 0.0468, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9364
Epoch 2/10, Train Loss: 0.1946 (value: 0.0008, weighted value: 0.0418, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9371
Epoch 3/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0399, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9375
Epoch 4/10, Train Loss: 0.1862 (value: 0.0008, weighted value: 0.0379, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9377
Epoch 5/10, Train Loss: 0.1840 (value: 0.0007, weighted value: 0.0369, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9381
Epoch 6/10, Train Loss: 0.1823 (value: 0.0007, weighted value: 0.0344, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9381
Epoch 7/10, Train Loss: 0.1812 (value: 0.0007, weighted value: 0.0348, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9384
Epoch 8/10, Train Loss: 0.1804 (value: 0.0007, weighted value: 0.0332, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9383
Epoch 9/10, Train Loss: 0.1784 (value: 0.0007, weighted value: 0.0325, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9385
Epoch 10/10, Train Loss: 0.1778 (value: 0.0006, weighted value: 0.0320, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9386
..training done in 62.96 seconds
..evaluation done in 18.75 seconds
Old network+MCTS average reward: 0.7613, min: 0.1481, max: 1.3981, stdev: 0.2303
New network+MCTS average reward: 0.7603, min: 0.1481, max: 1.3981, stdev: 0.2302
Old bare network average reward: 0.7333, min: 0.1389, max: 1.3981, stdev: 0.2345
New bare network average reward: 0.7291, min: 0.1481, max: 1.3981, stdev: 0.2327
External policy "random" average reward: 0.2565, min: -0.4259, max: 1.0093, stdev: 0.2237
External policy "individual greedy" average reward: 0.5368, min: -0.0648, max: 1.2500, stdev: 0.2257
External policy "total greedy" average reward: 0.6555, min: 0.1204, max: 1.3426, stdev: 0.2171
New network won 56 and tied 175 out of 300 games (47.83% wins where ties are half wins)
Reverting to the old network

Training iteration 634 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.77 seconds
Training examples lengths: [64788, 64956, 65198, 64871, 64829, 64912, 65098, 65102, 64735, 64943]
Total value: 499831.35
Training on 649432 examples
Training with 318 batches of size 2048
Epoch 1/10, Train Loss: 0.2214 (value: 0.0011, weighted value: 0.0570, policy: 0.1644, weighted policy: 0.1644), Train Mean Max: 0.9348
Epoch 2/10, Train Loss: 0.2070 (value: 0.0010, weighted value: 0.0493, policy: 0.1578, weighted policy: 0.1578), Train Mean Max: 0.9359
Epoch 3/10, Train Loss: 0.1983 (value: 0.0009, weighted value: 0.0456, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9364
Epoch 4/10, Train Loss: 0.1954 (value: 0.0009, weighted value: 0.0447, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9366
Epoch 5/10, Train Loss: 0.1914 (value: 0.0008, weighted value: 0.0416, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1876 (value: 0.0008, weighted value: 0.0383, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9372
Epoch 7/10, Train Loss: 0.1864 (value: 0.0008, weighted value: 0.0381, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1851 (value: 0.0007, weighted value: 0.0375, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9376
Epoch 9/10, Train Loss: 0.1828 (value: 0.0007, weighted value: 0.0349, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9378
Epoch 10/10, Train Loss: 0.1818 (value: 0.0007, weighted value: 0.0347, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9379
..training done in 64.05 seconds
..evaluation done in 18.75 seconds
Old network+MCTS average reward: 0.7267, min: 0.1481, max: 1.4815, stdev: 0.2297
New network+MCTS average reward: 0.7270, min: 0.0741, max: 1.4722, stdev: 0.2319
Old bare network average reward: 0.6972, min: 0.0741, max: 1.4722, stdev: 0.2363
New bare network average reward: 0.6993, min: 0.0741, max: 1.4722, stdev: 0.2375
External policy "random" average reward: 0.2440, min: -0.2870, max: 0.9352, stdev: 0.2071
External policy "individual greedy" average reward: 0.5177, min: -0.1481, max: 1.2130, stdev: 0.2291
External policy "total greedy" average reward: 0.6224, min: 0.0463, max: 1.2870, stdev: 0.2203
New network won 60 and tied 181 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 635 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.85 seconds
Training examples lengths: [64956, 65198, 64871, 64829, 64912, 65098, 65102, 64735, 64943, 64556]
Total value: 500015.72
Training on 649200 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1998 (value: 0.0009, weighted value: 0.0455, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9367
Epoch 2/10, Train Loss: 0.1916 (value: 0.0008, weighted value: 0.0411, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9373
Epoch 3/10, Train Loss: 0.1863 (value: 0.0008, weighted value: 0.0385, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9377
Epoch 4/10, Train Loss: 0.1849 (value: 0.0008, weighted value: 0.0375, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9378
Epoch 5/10, Train Loss: 0.1821 (value: 0.0007, weighted value: 0.0354, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9381
Epoch 6/10, Train Loss: 0.1809 (value: 0.0007, weighted value: 0.0351, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9383
Epoch 7/10, Train Loss: 0.1788 (value: 0.0007, weighted value: 0.0329, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9384
Epoch 8/10, Train Loss: 0.1791 (value: 0.0007, weighted value: 0.0330, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9385
Epoch 9/10, Train Loss: 0.1772 (value: 0.0006, weighted value: 0.0317, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9387
Epoch 10/10, Train Loss: 0.1754 (value: 0.0006, weighted value: 0.0308, policy: 0.1445, weighted policy: 0.1445), Train Mean Max: 0.9389
..training done in 60.35 seconds
..evaluation done in 18.89 seconds
Old network+MCTS average reward: 0.7646, min: 0.1667, max: 1.4352, stdev: 0.2431
New network+MCTS average reward: 0.7715, min: 0.1481, max: 1.4815, stdev: 0.2442
Old bare network average reward: 0.7387, min: 0.1481, max: 1.3889, stdev: 0.2411
New bare network average reward: 0.7398, min: 0.1481, max: 1.4444, stdev: 0.2405
External policy "random" average reward: 0.2791, min: -0.2593, max: 0.9167, stdev: 0.2290
External policy "individual greedy" average reward: 0.5516, min: -0.0185, max: 1.3889, stdev: 0.2356
External policy "total greedy" average reward: 0.6682, min: 0.0463, max: 1.4907, stdev: 0.2254
New network won 72 and tied 174 out of 300 games (53.00% wins where ties are half wins)
Keeping the new network

Training iteration 636 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.10 seconds
Training examples lengths: [65198, 64871, 64829, 64912, 65098, 65102, 64735, 64943, 64556, 64708]
Total value: 500216.86
Training on 648952 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1946 (value: 0.0008, weighted value: 0.0415, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9375
Epoch 2/10, Train Loss: 0.1870 (value: 0.0007, weighted value: 0.0375, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9381
Epoch 3/10, Train Loss: 0.1839 (value: 0.0007, weighted value: 0.0366, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9385
Epoch 4/10, Train Loss: 0.1790 (value: 0.0007, weighted value: 0.0332, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9388
Epoch 5/10, Train Loss: 0.1785 (value: 0.0007, weighted value: 0.0334, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9389
Epoch 6/10, Train Loss: 0.1765 (value: 0.0006, weighted value: 0.0323, policy: 0.1442, weighted policy: 0.1442), Train Mean Max: 0.9391
Epoch 7/10, Train Loss: 0.1768 (value: 0.0006, weighted value: 0.0321, policy: 0.1447, weighted policy: 0.1447), Train Mean Max: 0.9391
Epoch 8/10, Train Loss: 0.1740 (value: 0.0006, weighted value: 0.0295, policy: 0.1445, weighted policy: 0.1445), Train Mean Max: 0.9392
Epoch 9/10, Train Loss: 0.1732 (value: 0.0006, weighted value: 0.0293, policy: 0.1439, weighted policy: 0.1439), Train Mean Max: 0.9394
Epoch 10/10, Train Loss: 0.1732 (value: 0.0006, weighted value: 0.0296, policy: 0.1436, weighted policy: 0.1436), Train Mean Max: 0.9395
..training done in 67.13 seconds
..evaluation done in 20.40 seconds
Old network+MCTS average reward: 0.7602, min: 0.1944, max: 1.4537, stdev: 0.2371
New network+MCTS average reward: 0.7614, min: 0.2315, max: 1.4537, stdev: 0.2381
Old bare network average reward: 0.7310, min: 0.1574, max: 1.3796, stdev: 0.2376
New bare network average reward: 0.7315, min: 0.1574, max: 1.3704, stdev: 0.2417
External policy "random" average reward: 0.2702, min: -0.3241, max: 0.9167, stdev: 0.2371
External policy "individual greedy" average reward: 0.5481, min: 0.0000, max: 1.4074, stdev: 0.2386
External policy "total greedy" average reward: 0.6539, min: 0.1759, max: 1.3519, stdev: 0.2224
New network won 65 and tied 169 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 637 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.53 seconds
Training examples lengths: [64871, 64829, 64912, 65098, 65102, 64735, 64943, 64556, 64708, 64871]
Total value: 499353.03
Training on 648625 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2139 (value: 0.0011, weighted value: 0.0529, policy: 0.1611, weighted policy: 0.1611), Train Mean Max: 0.9359
Epoch 2/10, Train Loss: 0.2013 (value: 0.0009, weighted value: 0.0456, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9369
Epoch 3/10, Train Loss: 0.1932 (value: 0.0008, weighted value: 0.0422, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9373
Epoch 4/10, Train Loss: 0.1871 (value: 0.0008, weighted value: 0.0389, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9378
Epoch 5/10, Train Loss: 0.1853 (value: 0.0008, weighted value: 0.0385, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9379
Epoch 6/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0364, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9379
Epoch 7/10, Train Loss: 0.1810 (value: 0.0007, weighted value: 0.0350, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9381
Epoch 8/10, Train Loss: 0.1805 (value: 0.0007, weighted value: 0.0345, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9382
Epoch 9/10, Train Loss: 0.1779 (value: 0.0006, weighted value: 0.0323, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9385
Epoch 10/10, Train Loss: 0.1784 (value: 0.0007, weighted value: 0.0327, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9385
..training done in 60.03 seconds
..evaluation done in 19.06 seconds
Old network+MCTS average reward: 0.7692, min: 0.2037, max: 1.4907, stdev: 0.2293
New network+MCTS average reward: 0.7696, min: 0.1944, max: 1.4907, stdev: 0.2301
Old bare network average reward: 0.7409, min: 0.2037, max: 1.4722, stdev: 0.2319
New bare network average reward: 0.7404, min: 0.1574, max: 1.4722, stdev: 0.2315
External policy "random" average reward: 0.2800, min: -0.3148, max: 1.0648, stdev: 0.2217
External policy "individual greedy" average reward: 0.5533, min: -0.1204, max: 1.1481, stdev: 0.2151
External policy "total greedy" average reward: 0.6661, min: 0.1574, max: 1.3241, stdev: 0.2214
New network won 63 and tied 181 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 638 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.87 seconds
Training examples lengths: [64829, 64912, 65098, 65102, 64735, 64943, 64556, 64708, 64871, 64794]
Total value: 499663.68
Training on 648548 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1968 (value: 0.0008, weighted value: 0.0419, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9370
Epoch 2/10, Train Loss: 0.1893 (value: 0.0008, weighted value: 0.0387, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9379
Epoch 3/10, Train Loss: 0.1848 (value: 0.0007, weighted value: 0.0365, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9380
Epoch 4/10, Train Loss: 0.1818 (value: 0.0007, weighted value: 0.0347, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9382
Epoch 5/10, Train Loss: 0.1801 (value: 0.0007, weighted value: 0.0334, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9383
Epoch 6/10, Train Loss: 0.1786 (value: 0.0007, weighted value: 0.0327, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9385
Epoch 7/10, Train Loss: 0.1774 (value: 0.0006, weighted value: 0.0319, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9387
Epoch 8/10, Train Loss: 0.1758 (value: 0.0006, weighted value: 0.0307, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9387
Epoch 9/10, Train Loss: 0.1753 (value: 0.0006, weighted value: 0.0300, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9388
Epoch 10/10, Train Loss: 0.1751 (value: 0.0006, weighted value: 0.0298, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9390
..training done in 66.83 seconds
..evaluation done in 19.84 seconds
Old network+MCTS average reward: 0.7570, min: 0.1204, max: 1.3796, stdev: 0.2328
New network+MCTS average reward: 0.7578, min: 0.1019, max: 1.4537, stdev: 0.2332
Old bare network average reward: 0.7258, min: 0.1204, max: 1.3611, stdev: 0.2383
New bare network average reward: 0.7251, min: 0.1019, max: 1.3333, stdev: 0.2405
External policy "random" average reward: 0.2587, min: -0.1944, max: 1.1574, stdev: 0.2207
External policy "individual greedy" average reward: 0.5453, min: -0.0463, max: 1.1667, stdev: 0.2241
External policy "total greedy" average reward: 0.6553, min: 0.0463, max: 1.4167, stdev: 0.2165
New network won 64 and tied 175 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 639 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.78 seconds
Training examples lengths: [64912, 65098, 65102, 64735, 64943, 64556, 64708, 64871, 64794, 64919]
Total value: 499674.42
Training on 648638 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1947 (value: 0.0008, weighted value: 0.0405, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9373
Epoch 2/10, Train Loss: 0.1873 (value: 0.0007, weighted value: 0.0358, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9380
Epoch 3/10, Train Loss: 0.1835 (value: 0.0007, weighted value: 0.0358, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9382
Epoch 4/10, Train Loss: 0.1808 (value: 0.0007, weighted value: 0.0343, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9385
Epoch 5/10, Train Loss: 0.1773 (value: 0.0006, weighted value: 0.0316, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9386
Epoch 6/10, Train Loss: 0.1757 (value: 0.0006, weighted value: 0.0308, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9388
Epoch 7/10, Train Loss: 0.1761 (value: 0.0006, weighted value: 0.0309, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9387
Epoch 8/10, Train Loss: 0.1761 (value: 0.0006, weighted value: 0.0303, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9388
Epoch 9/10, Train Loss: 0.1746 (value: 0.0006, weighted value: 0.0288, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9390
Epoch 10/10, Train Loss: 0.1736 (value: 0.0006, weighted value: 0.0288, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9392
..training done in 66.80 seconds
..evaluation done in 18.65 seconds
Old network+MCTS average reward: 0.7529, min: 0.1019, max: 1.6667, stdev: 0.2369
New network+MCTS average reward: 0.7545, min: 0.1019, max: 1.6852, stdev: 0.2357
Old bare network average reward: 0.7203, min: -0.0278, max: 1.6204, stdev: 0.2419
New bare network average reward: 0.7238, min: 0.1019, max: 1.6852, stdev: 0.2472
External policy "random" average reward: 0.2577, min: -0.3519, max: 1.0463, stdev: 0.2217
External policy "individual greedy" average reward: 0.5434, min: -0.0278, max: 1.4444, stdev: 0.2309
External policy "total greedy" average reward: 0.6559, min: 0.0741, max: 1.4630, stdev: 0.2178
New network won 65 and tied 177 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 640 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.07 seconds
Training examples lengths: [65098, 65102, 64735, 64943, 64556, 64708, 64871, 64794, 64919, 64768]
Total value: 499829.68
Training on 648494 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1941 (value: 0.0008, weighted value: 0.0401, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9375
Epoch 2/10, Train Loss: 0.1858 (value: 0.0007, weighted value: 0.0358, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9384
Epoch 3/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0336, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9386
Epoch 4/10, Train Loss: 0.1800 (value: 0.0007, weighted value: 0.0329, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9386
Epoch 5/10, Train Loss: 0.1775 (value: 0.0006, weighted value: 0.0311, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9388
Epoch 6/10, Train Loss: 0.1764 (value: 0.0006, weighted value: 0.0313, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9390
Epoch 7/10, Train Loss: 0.1744 (value: 0.0006, weighted value: 0.0296, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9389
Epoch 8/10, Train Loss: 0.1736 (value: 0.0006, weighted value: 0.0289, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9391
Epoch 9/10, Train Loss: 0.1733 (value: 0.0006, weighted value: 0.0288, policy: 0.1445, weighted policy: 0.1445), Train Mean Max: 0.9393
Epoch 10/10, Train Loss: 0.1727 (value: 0.0005, weighted value: 0.0275, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9392
..training done in 59.55 seconds
..evaluation done in 19.27 seconds
Old network+MCTS average reward: 0.7289, min: 0.0093, max: 1.4537, stdev: 0.2192
New network+MCTS average reward: 0.7298, min: 0.0556, max: 1.4537, stdev: 0.2182
Old bare network average reward: 0.7025, min: 0.0093, max: 1.4537, stdev: 0.2235
New bare network average reward: 0.7042, min: 0.0648, max: 1.4537, stdev: 0.2203
External policy "random" average reward: 0.2443, min: -0.4074, max: 0.8056, stdev: 0.2030
External policy "individual greedy" average reward: 0.5226, min: -0.0278, max: 1.3704, stdev: 0.2062
External policy "total greedy" average reward: 0.6440, min: 0.0185, max: 1.3241, stdev: 0.2123
New network won 68 and tied 174 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_640

Training iteration 641 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 71.54 seconds
Training examples lengths: [65102, 64735, 64943, 64556, 64708, 64871, 64794, 64919, 64768, 64919]
Total value: 499990.13
Training on 648315 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1931 (value: 0.0008, weighted value: 0.0388, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9377
Epoch 2/10, Train Loss: 0.1863 (value: 0.0007, weighted value: 0.0357, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9383
Epoch 3/10, Train Loss: 0.1800 (value: 0.0006, weighted value: 0.0325, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9386
Epoch 4/10, Train Loss: 0.1786 (value: 0.0007, weighted value: 0.0327, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9389
Epoch 5/10, Train Loss: 0.1760 (value: 0.0006, weighted value: 0.0303, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9388
Epoch 6/10, Train Loss: 0.1755 (value: 0.0006, weighted value: 0.0302, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9390
Epoch 7/10, Train Loss: 0.1738 (value: 0.0006, weighted value: 0.0288, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9392
Epoch 8/10, Train Loss: 0.1739 (value: 0.0006, weighted value: 0.0289, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9392
Epoch 9/10, Train Loss: 0.1731 (value: 0.0006, weighted value: 0.0277, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9393
Epoch 10/10, Train Loss: 0.1717 (value: 0.0005, weighted value: 0.0273, policy: 0.1444, weighted policy: 0.1444), Train Mean Max: 0.9394
..training done in 67.26 seconds
..evaluation done in 18.87 seconds
Old network+MCTS average reward: 0.7713, min: 0.1111, max: 1.4352, stdev: 0.2475
New network+MCTS average reward: 0.7722, min: 0.1296, max: 1.4907, stdev: 0.2490
Old bare network average reward: 0.7471, min: 0.1111, max: 1.4259, stdev: 0.2511
New bare network average reward: 0.7484, min: 0.1111, max: 1.4259, stdev: 0.2525
External policy "random" average reward: 0.2817, min: -0.2593, max: 1.0833, stdev: 0.2544
External policy "individual greedy" average reward: 0.5538, min: -0.0093, max: 1.2870, stdev: 0.2526
External policy "total greedy" average reward: 0.6621, min: 0.1667, max: 1.3889, stdev: 0.2397
New network won 51 and tied 194 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 642 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 60.03 seconds
Training examples lengths: [64735, 64943, 64556, 64708, 64871, 64794, 64919, 64768, 64919, 64672]
Total value: 498574.46
Training on 647885 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2134 (value: 0.0010, weighted value: 0.0510, policy: 0.1624, weighted policy: 0.1624), Train Mean Max: 0.9360
Epoch 2/10, Train Loss: 0.1988 (value: 0.0009, weighted value: 0.0427, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9370
Epoch 3/10, Train Loss: 0.1911 (value: 0.0008, weighted value: 0.0401, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9376
Epoch 4/10, Train Loss: 0.1866 (value: 0.0008, weighted value: 0.0377, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9378
Epoch 5/10, Train Loss: 0.1837 (value: 0.0007, weighted value: 0.0363, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9380
Epoch 6/10, Train Loss: 0.1811 (value: 0.0007, weighted value: 0.0345, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9380
Epoch 7/10, Train Loss: 0.1788 (value: 0.0006, weighted value: 0.0324, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9381
Epoch 8/10, Train Loss: 0.1783 (value: 0.0006, weighted value: 0.0324, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1784 (value: 0.0006, weighted value: 0.0317, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1761 (value: 0.0006, weighted value: 0.0309, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9387
..training done in 59.90 seconds
..evaluation done in 19.49 seconds
Old network+MCTS average reward: 0.7646, min: 0.0926, max: 1.4815, stdev: 0.2251
New network+MCTS average reward: 0.7645, min: 0.0556, max: 1.4815, stdev: 0.2269
Old bare network average reward: 0.7259, min: 0.0000, max: 1.4352, stdev: 0.2313
New bare network average reward: 0.7291, min: 0.0556, max: 1.4259, stdev: 0.2294
External policy "random" average reward: 0.2560, min: -0.3148, max: 0.9722, stdev: 0.2123
External policy "individual greedy" average reward: 0.5484, min: -0.0370, max: 1.1574, stdev: 0.2097
External policy "total greedy" average reward: 0.6571, min: -0.0185, max: 1.2685, stdev: 0.2172
New network won 63 and tied 171 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 643 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.59 seconds
Training examples lengths: [64943, 64556, 64708, 64871, 64794, 64919, 64768, 64919, 64672, 64773]
Total value: 498967.91
Training on 647923 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2308 (value: 0.0012, weighted value: 0.0613, policy: 0.1695, weighted policy: 0.1695), Train Mean Max: 0.9348
Epoch 2/10, Train Loss: 0.2096 (value: 0.0010, weighted value: 0.0507, policy: 0.1589, weighted policy: 0.1589), Train Mean Max: 0.9361
Epoch 3/10, Train Loss: 0.2011 (value: 0.0010, weighted value: 0.0477, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9367
Epoch 4/10, Train Loss: 0.1940 (value: 0.0009, weighted value: 0.0439, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9371
Epoch 5/10, Train Loss: 0.1894 (value: 0.0008, weighted value: 0.0405, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9372
Epoch 6/10, Train Loss: 0.1871 (value: 0.0008, weighted value: 0.0389, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9374
Epoch 7/10, Train Loss: 0.1858 (value: 0.0008, weighted value: 0.0379, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9376
Epoch 8/10, Train Loss: 0.1836 (value: 0.0007, weighted value: 0.0365, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1812 (value: 0.0007, weighted value: 0.0347, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9380
Epoch 10/10, Train Loss: 0.1804 (value: 0.0007, weighted value: 0.0339, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9383
..training done in 59.48 seconds
..evaluation done in 19.03 seconds
Old network+MCTS average reward: 0.7819, min: 0.0370, max: 1.5093, stdev: 0.2326
New network+MCTS average reward: 0.7807, min: 0.0370, max: 1.3611, stdev: 0.2331
Old bare network average reward: 0.7495, min: 0.0185, max: 1.3519, stdev: 0.2307
New bare network average reward: 0.7518, min: 0.0370, max: 1.3519, stdev: 0.2314
External policy "random" average reward: 0.2722, min: -0.4259, max: 0.8796, stdev: 0.2239
External policy "individual greedy" average reward: 0.5446, min: -0.0556, max: 1.0926, stdev: 0.2215
External policy "total greedy" average reward: 0.6550, min: 0.0556, max: 1.2963, stdev: 0.2169
New network won 70 and tied 167 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 644 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.03 seconds
Training examples lengths: [64556, 64708, 64871, 64794, 64919, 64768, 64919, 64672, 64773, 64943]
Total value: 499144.18
Training on 647923 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1977 (value: 0.0009, weighted value: 0.0435, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9369
Epoch 2/10, Train Loss: 0.1906 (value: 0.0008, weighted value: 0.0395, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9377
Epoch 3/10, Train Loss: 0.1866 (value: 0.0008, weighted value: 0.0383, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9379
Epoch 4/10, Train Loss: 0.1831 (value: 0.0007, weighted value: 0.0355, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9381
Epoch 5/10, Train Loss: 0.1810 (value: 0.0007, weighted value: 0.0347, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9385
Epoch 6/10, Train Loss: 0.1794 (value: 0.0007, weighted value: 0.0338, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9385
Epoch 7/10, Train Loss: 0.1774 (value: 0.0007, weighted value: 0.0326, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9387
Epoch 8/10, Train Loss: 0.1768 (value: 0.0006, weighted value: 0.0318, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9388
Epoch 9/10, Train Loss: 0.1758 (value: 0.0006, weighted value: 0.0307, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9388
Epoch 10/10, Train Loss: 0.1763 (value: 0.0006, weighted value: 0.0317, policy: 0.1446, weighted policy: 0.1446), Train Mean Max: 0.9391
..training done in 64.70 seconds
..evaluation done in 18.52 seconds
Old network+MCTS average reward: 0.7601, min: 0.0000, max: 1.4074, stdev: 0.2365
New network+MCTS average reward: 0.7618, min: -0.0185, max: 1.4074, stdev: 0.2362
Old bare network average reward: 0.7284, min: -0.0463, max: 1.3611, stdev: 0.2389
New bare network average reward: 0.7328, min: -0.0463, max: 1.4074, stdev: 0.2367
External policy "random" average reward: 0.2715, min: -0.5556, max: 1.0741, stdev: 0.2370
External policy "individual greedy" average reward: 0.5479, min: -0.1852, max: 1.2222, stdev: 0.2352
External policy "total greedy" average reward: 0.6653, min: 0.0093, max: 1.3426, stdev: 0.2257
New network won 64 and tied 180 out of 300 games (51.33% wins where ties are half wins)
Keeping the new network

Training iteration 645 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.46 seconds
Training examples lengths: [64708, 64871, 64794, 64919, 64768, 64919, 64672, 64773, 64943, 64628]
Total value: 499151.49
Training on 647995 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1936 (value: 0.0008, weighted value: 0.0402, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9375
Epoch 2/10, Train Loss: 0.1874 (value: 0.0007, weighted value: 0.0371, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9380
Epoch 3/10, Train Loss: 0.1837 (value: 0.0007, weighted value: 0.0361, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9385
Epoch 4/10, Train Loss: 0.1802 (value: 0.0007, weighted value: 0.0337, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9385
Epoch 5/10, Train Loss: 0.1793 (value: 0.0007, weighted value: 0.0334, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9387
Epoch 6/10, Train Loss: 0.1755 (value: 0.0006, weighted value: 0.0305, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9390
Epoch 7/10, Train Loss: 0.1765 (value: 0.0006, weighted value: 0.0315, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9390
Epoch 8/10, Train Loss: 0.1750 (value: 0.0006, weighted value: 0.0300, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9391
Epoch 9/10, Train Loss: 0.1735 (value: 0.0006, weighted value: 0.0291, policy: 0.1444, weighted policy: 0.1444), Train Mean Max: 0.9393
Epoch 10/10, Train Loss: 0.1734 (value: 0.0006, weighted value: 0.0288, policy: 0.1446, weighted policy: 0.1446), Train Mean Max: 0.9393
..training done in 74.25 seconds
..evaluation done in 19.36 seconds
Old network+MCTS average reward: 0.7619, min: 0.2315, max: 1.5370, stdev: 0.2430
New network+MCTS average reward: 0.7673, min: 0.2685, max: 1.5278, stdev: 0.2420
Old bare network average reward: 0.7352, min: 0.1944, max: 1.5278, stdev: 0.2471
New bare network average reward: 0.7380, min: 0.2315, max: 1.4907, stdev: 0.2400
External policy "random" average reward: 0.2635, min: -0.2685, max: 0.8796, stdev: 0.2131
External policy "individual greedy" average reward: 0.5511, min: 0.0741, max: 1.2778, stdev: 0.2247
External policy "total greedy" average reward: 0.6647, min: 0.0370, max: 1.2407, stdev: 0.2269
New network won 71 and tied 180 out of 300 games (53.67% wins where ties are half wins)
Keeping the new network

Training iteration 646 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.42 seconds
Training examples lengths: [64871, 64794, 64919, 64768, 64919, 64672, 64773, 64943, 64628, 64894]
Total value: 499076.75
Training on 648181 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1931 (value: 0.0008, weighted value: 0.0389, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9378
Epoch 2/10, Train Loss: 0.1870 (value: 0.0007, weighted value: 0.0365, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9385
Epoch 3/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0334, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9389
Epoch 4/10, Train Loss: 0.1786 (value: 0.0007, weighted value: 0.0326, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9391
Epoch 5/10, Train Loss: 0.1764 (value: 0.0006, weighted value: 0.0311, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9390
Epoch 6/10, Train Loss: 0.1752 (value: 0.0006, weighted value: 0.0306, policy: 0.1446, weighted policy: 0.1446), Train Mean Max: 0.9392
Epoch 7/10, Train Loss: 0.1748 (value: 0.0006, weighted value: 0.0308, policy: 0.1440, weighted policy: 0.1440), Train Mean Max: 0.9393
Epoch 8/10, Train Loss: 0.1727 (value: 0.0006, weighted value: 0.0285, policy: 0.1442, weighted policy: 0.1442), Train Mean Max: 0.9393
Epoch 9/10, Train Loss: 0.1734 (value: 0.0006, weighted value: 0.0292, policy: 0.1442, weighted policy: 0.1442), Train Mean Max: 0.9395
Epoch 10/10, Train Loss: 0.1707 (value: 0.0005, weighted value: 0.0267, policy: 0.1440, weighted policy: 0.1440), Train Mean Max: 0.9395
..training done in 62.54 seconds
..evaluation done in 18.18 seconds
Old network+MCTS average reward: 0.7454, min: 0.1759, max: 1.5463, stdev: 0.2333
New network+MCTS average reward: 0.7458, min: 0.1852, max: 1.5648, stdev: 0.2349
Old bare network average reward: 0.7134, min: 0.1852, max: 1.5463, stdev: 0.2363
New bare network average reward: 0.7154, min: 0.1481, max: 1.5185, stdev: 0.2348
External policy "random" average reward: 0.2574, min: -0.3148, max: 0.8889, stdev: 0.2234
External policy "individual greedy" average reward: 0.5363, min: 0.0093, max: 1.2315, stdev: 0.2303
External policy "total greedy" average reward: 0.6471, min: 0.0741, max: 1.4259, stdev: 0.2230
New network won 80 and tied 148 out of 300 games (51.33% wins where ties are half wins)
Keeping the new network

Training iteration 647 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 62.20 seconds
Training examples lengths: [64794, 64919, 64768, 64919, 64672, 64773, 64943, 64628, 64894, 64722]
Total value: 498851.32
Training on 648032 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1916 (value: 0.0008, weighted value: 0.0387, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9380
Epoch 2/10, Train Loss: 0.1847 (value: 0.0007, weighted value: 0.0350, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9387
Epoch 3/10, Train Loss: 0.1799 (value: 0.0007, weighted value: 0.0327, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9390
Epoch 4/10, Train Loss: 0.1778 (value: 0.0006, weighted value: 0.0317, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9390
Epoch 5/10, Train Loss: 0.1764 (value: 0.0006, weighted value: 0.0312, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9393
Epoch 6/10, Train Loss: 0.1740 (value: 0.0006, weighted value: 0.0299, policy: 0.1441, weighted policy: 0.1441), Train Mean Max: 0.9393
Epoch 7/10, Train Loss: 0.1737 (value: 0.0006, weighted value: 0.0291, policy: 0.1446, weighted policy: 0.1446), Train Mean Max: 0.9394
Epoch 8/10, Train Loss: 0.1723 (value: 0.0006, weighted value: 0.0280, policy: 0.1443, weighted policy: 0.1443), Train Mean Max: 0.9396
Epoch 9/10, Train Loss: 0.1718 (value: 0.0006, weighted value: 0.0278, policy: 0.1440, weighted policy: 0.1440), Train Mean Max: 0.9397
Epoch 10/10, Train Loss: 0.1711 (value: 0.0005, weighted value: 0.0270, policy: 0.1441, weighted policy: 0.1441), Train Mean Max: 0.9397
..training done in 73.06 seconds
..evaluation done in 23.30 seconds
Old network+MCTS average reward: 0.7431, min: -0.0185, max: 1.6296, stdev: 0.2648
New network+MCTS average reward: 0.7440, min: 0.0000, max: 1.6019, stdev: 0.2608
Old bare network average reward: 0.7162, min: -0.0185, max: 1.6296, stdev: 0.2668
New bare network average reward: 0.7168, min: -0.0185, max: 1.6019, stdev: 0.2642
External policy "random" average reward: 0.2537, min: -0.3981, max: 1.1296, stdev: 0.2378
External policy "individual greedy" average reward: 0.5324, min: 0.0370, max: 1.4259, stdev: 0.2418
External policy "total greedy" average reward: 0.6432, min: -0.0093, max: 1.6204, stdev: 0.2446
New network won 68 and tied 169 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 648 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.24 seconds
Training examples lengths: [64919, 64768, 64919, 64672, 64773, 64943, 64628, 64894, 64722, 64856]
Total value: 499057.66
Training on 648094 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1903 (value: 0.0008, weighted value: 0.0383, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9382
Epoch 2/10, Train Loss: 0.1849 (value: 0.0007, weighted value: 0.0355, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9388
Epoch 3/10, Train Loss: 0.1796 (value: 0.0007, weighted value: 0.0326, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9392
Epoch 4/10, Train Loss: 0.1768 (value: 0.0006, weighted value: 0.0317, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9393
Epoch 5/10, Train Loss: 0.1735 (value: 0.0006, weighted value: 0.0296, policy: 0.1439, weighted policy: 0.1439), Train Mean Max: 0.9395
Epoch 6/10, Train Loss: 0.1737 (value: 0.0006, weighted value: 0.0297, policy: 0.1441, weighted policy: 0.1441), Train Mean Max: 0.9395
Epoch 7/10, Train Loss: 0.1722 (value: 0.0006, weighted value: 0.0284, policy: 0.1437, weighted policy: 0.1437), Train Mean Max: 0.9396
Epoch 8/10, Train Loss: 0.1726 (value: 0.0006, weighted value: 0.0286, policy: 0.1439, weighted policy: 0.1439), Train Mean Max: 0.9396
Epoch 9/10, Train Loss: 0.1715 (value: 0.0006, weighted value: 0.0278, policy: 0.1437, weighted policy: 0.1437), Train Mean Max: 0.9399
Epoch 10/10, Train Loss: 0.1695 (value: 0.0005, weighted value: 0.0259, policy: 0.1436, weighted policy: 0.1436), Train Mean Max: 0.9400
..training done in 64.78 seconds
..evaluation done in 18.19 seconds
Old network+MCTS average reward: 0.7799, min: -0.0370, max: 1.5741, stdev: 0.2487
New network+MCTS average reward: 0.7808, min: -0.0185, max: 1.5556, stdev: 0.2509
Old bare network average reward: 0.7473, min: -0.0556, max: 1.5556, stdev: 0.2545
New bare network average reward: 0.7481, min: -0.0556, max: 1.5556, stdev: 0.2558
External policy "random" average reward: 0.2702, min: -0.4167, max: 1.1574, stdev: 0.2433
External policy "individual greedy" average reward: 0.5530, min: -0.0093, max: 1.4537, stdev: 0.2420
External policy "total greedy" average reward: 0.6714, min: 0.0463, max: 1.4815, stdev: 0.2460
New network won 61 and tied 181 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 649 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.83 seconds
Training examples lengths: [64768, 64919, 64672, 64773, 64943, 64628, 64894, 64722, 64856, 64931]
Total value: 498666.72
Training on 648106 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1916 (value: 0.0008, weighted value: 0.0389, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9381
Epoch 2/10, Train Loss: 0.1826 (value: 0.0007, weighted value: 0.0335, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9390
Epoch 3/10, Train Loss: 0.1804 (value: 0.0007, weighted value: 0.0339, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9391
Epoch 4/10, Train Loss: 0.1750 (value: 0.0006, weighted value: 0.0300, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9393
Epoch 5/10, Train Loss: 0.1755 (value: 0.0006, weighted value: 0.0308, policy: 0.1447, weighted policy: 0.1447), Train Mean Max: 0.9394
Epoch 6/10, Train Loss: 0.1729 (value: 0.0006, weighted value: 0.0289, policy: 0.1440, weighted policy: 0.1440), Train Mean Max: 0.9396
Epoch 7/10, Train Loss: 0.1721 (value: 0.0006, weighted value: 0.0283, policy: 0.1438, weighted policy: 0.1438), Train Mean Max: 0.9396
Epoch 8/10, Train Loss: 0.1722 (value: 0.0006, weighted value: 0.0282, policy: 0.1440, weighted policy: 0.1440), Train Mean Max: 0.9397
Epoch 9/10, Train Loss: 0.1706 (value: 0.0005, weighted value: 0.0274, policy: 0.1431, weighted policy: 0.1431), Train Mean Max: 0.9399
Epoch 10/10, Train Loss: 0.1698 (value: 0.0005, weighted value: 0.0261, policy: 0.1437, weighted policy: 0.1437), Train Mean Max: 0.9399
..training done in 60.31 seconds
..evaluation done in 18.20 seconds
Old network+MCTS average reward: 0.7652, min: -0.0741, max: 1.5185, stdev: 0.2321
New network+MCTS average reward: 0.7676, min: -0.0741, max: 1.5185, stdev: 0.2311
Old bare network average reward: 0.7391, min: -0.0556, max: 1.5185, stdev: 0.2331
New bare network average reward: 0.7380, min: -0.0556, max: 1.5185, stdev: 0.2324
External policy "random" average reward: 0.2672, min: -0.4815, max: 0.9167, stdev: 0.2175
External policy "individual greedy" average reward: 0.5541, min: -0.1296, max: 1.3426, stdev: 0.2256
External policy "total greedy" average reward: 0.6618, min: 0.0833, max: 1.4907, stdev: 0.2183
New network won 67 and tied 175 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 650 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.19 seconds
Training examples lengths: [64919, 64672, 64773, 64943, 64628, 64894, 64722, 64856, 64931, 64411]
Total value: 497457.10
Training on 647749 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1900 (value: 0.0007, weighted value: 0.0373, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9383
Epoch 2/10, Train Loss: 0.1843 (value: 0.0007, weighted value: 0.0346, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9389
Epoch 3/10, Train Loss: 0.1781 (value: 0.0006, weighted value: 0.0318, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9392
Epoch 4/10, Train Loss: 0.1768 (value: 0.0006, weighted value: 0.0309, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9392
Epoch 5/10, Train Loss: 0.1743 (value: 0.0006, weighted value: 0.0296, policy: 0.1447, weighted policy: 0.1447), Train Mean Max: 0.9394
Epoch 6/10, Train Loss: 0.1722 (value: 0.0006, weighted value: 0.0286, policy: 0.1436, weighted policy: 0.1436), Train Mean Max: 0.9396
Epoch 7/10, Train Loss: 0.1728 (value: 0.0006, weighted value: 0.0289, policy: 0.1439, weighted policy: 0.1439), Train Mean Max: 0.9395
Epoch 8/10, Train Loss: 0.1712 (value: 0.0005, weighted value: 0.0273, policy: 0.1439, weighted policy: 0.1439), Train Mean Max: 0.9396
Epoch 9/10, Train Loss: 0.1710 (value: 0.0005, weighted value: 0.0274, policy: 0.1436, weighted policy: 0.1436), Train Mean Max: 0.9397
Epoch 10/10, Train Loss: 0.1693 (value: 0.0005, weighted value: 0.0257, policy: 0.1437, weighted policy: 0.1437), Train Mean Max: 0.9399
..training done in 59.83 seconds
..evaluation done in 18.17 seconds
Old network+MCTS average reward: 0.7698, min: 0.2685, max: 1.4722, stdev: 0.2165
New network+MCTS average reward: 0.7715, min: 0.2870, max: 1.5556, stdev: 0.2151
Old bare network average reward: 0.7480, min: 0.2315, max: 1.4722, stdev: 0.2155
New bare network average reward: 0.7429, min: 0.2685, max: 1.4722, stdev: 0.2174
External policy "random" average reward: 0.2855, min: -0.2870, max: 1.0093, stdev: 0.2215
External policy "individual greedy" average reward: 0.5540, min: 0.0463, max: 1.2778, stdev: 0.2178
External policy "total greedy" average reward: 0.6677, min: 0.2407, max: 1.2963, stdev: 0.2025
New network won 64 and tied 185 out of 300 games (52.17% wins where ties are half wins)
Keeping the new network

Training iteration 651 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.68 seconds
Training examples lengths: [64672, 64773, 64943, 64628, 64894, 64722, 64856, 64931, 64411, 64887]
Total value: 496610.12
Training on 647717 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1920 (value: 0.0008, weighted value: 0.0380, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9380
Epoch 2/10, Train Loss: 0.1836 (value: 0.0007, weighted value: 0.0333, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9386
Epoch 3/10, Train Loss: 0.1806 (value: 0.0007, weighted value: 0.0330, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9390
Epoch 4/10, Train Loss: 0.1767 (value: 0.0006, weighted value: 0.0309, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9391
Epoch 5/10, Train Loss: 0.1743 (value: 0.0006, weighted value: 0.0290, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9392
Epoch 6/10, Train Loss: 0.1730 (value: 0.0006, weighted value: 0.0285, policy: 0.1446, weighted policy: 0.1446), Train Mean Max: 0.9393
Epoch 7/10, Train Loss: 0.1732 (value: 0.0006, weighted value: 0.0279, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9393
Epoch 8/10, Train Loss: 0.1741 (value: 0.0006, weighted value: 0.0289, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9395
Epoch 9/10, Train Loss: 0.1699 (value: 0.0005, weighted value: 0.0260, policy: 0.1439, weighted policy: 0.1439), Train Mean Max: 0.9396
Epoch 10/10, Train Loss: 0.1701 (value: 0.0005, weighted value: 0.0263, policy: 0.1438, weighted policy: 0.1438), Train Mean Max: 0.9396
..training done in 60.67 seconds
..evaluation done in 18.00 seconds
Old network+MCTS average reward: 0.7582, min: 0.2315, max: 1.4722, stdev: 0.2177
New network+MCTS average reward: 0.7574, min: 0.2222, max: 1.4537, stdev: 0.2153
Old bare network average reward: 0.7295, min: 0.2037, max: 1.4537, stdev: 0.2215
New bare network average reward: 0.7249, min: 0.1944, max: 1.4537, stdev: 0.2198
External policy "random" average reward: 0.2523, min: -0.2037, max: 0.8519, stdev: 0.1998
External policy "individual greedy" average reward: 0.5354, min: 0.0463, max: 1.1852, stdev: 0.2049
External policy "total greedy" average reward: 0.6509, min: 0.1204, max: 1.2963, stdev: 0.1958
New network won 47 and tied 191 out of 300 games (47.50% wins where ties are half wins)
Reverting to the old network

Training iteration 652 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.65 seconds
Training examples lengths: [64773, 64943, 64628, 64894, 64722, 64856, 64931, 64411, 64887, 64784]
Total value: 497117.50
Training on 647829 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2118 (value: 0.0010, weighted value: 0.0492, policy: 0.1627, weighted policy: 0.1627), Train Mean Max: 0.9364
Epoch 2/10, Train Loss: 0.1966 (value: 0.0008, weighted value: 0.0412, policy: 0.1554, weighted policy: 0.1554), Train Mean Max: 0.9375
Epoch 3/10, Train Loss: 0.1901 (value: 0.0008, weighted value: 0.0394, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9378
Epoch 4/10, Train Loss: 0.1849 (value: 0.0007, weighted value: 0.0362, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9381
Epoch 5/10, Train Loss: 0.1826 (value: 0.0007, weighted value: 0.0350, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9382
Epoch 6/10, Train Loss: 0.1795 (value: 0.0007, weighted value: 0.0330, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9384
Epoch 7/10, Train Loss: 0.1789 (value: 0.0006, weighted value: 0.0323, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9384
Epoch 8/10, Train Loss: 0.1765 (value: 0.0006, weighted value: 0.0305, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9386
Epoch 9/10, Train Loss: 0.1764 (value: 0.0006, weighted value: 0.0305, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9388
Epoch 10/10, Train Loss: 0.1752 (value: 0.0006, weighted value: 0.0296, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9389
..training done in 59.88 seconds
..evaluation done in 23.20 seconds
Old network+MCTS average reward: 0.7274, min: 0.0741, max: 1.4259, stdev: 0.2375
New network+MCTS average reward: 0.7283, min: 0.1111, max: 1.4537, stdev: 0.2400
Old bare network average reward: 0.6972, min: 0.0093, max: 1.4259, stdev: 0.2414
New bare network average reward: 0.6966, min: 0.0370, max: 1.3796, stdev: 0.2403
External policy "random" average reward: 0.2373, min: -0.3426, max: 0.9630, stdev: 0.2108
External policy "individual greedy" average reward: 0.5298, min: -0.0833, max: 1.2870, stdev: 0.2294
External policy "total greedy" average reward: 0.6369, min: 0.0556, max: 1.3148, stdev: 0.2180
New network won 68 and tied 171 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 653 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.92 seconds
Training examples lengths: [64943, 64628, 64894, 64722, 64856, 64931, 64411, 64887, 64784, 65040]
Total value: 497842.72
Training on 648096 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1957 (value: 0.0008, weighted value: 0.0405, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9371
Epoch 2/10, Train Loss: 0.1880 (value: 0.0007, weighted value: 0.0364, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9378
Epoch 3/10, Train Loss: 0.1824 (value: 0.0007, weighted value: 0.0336, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9380
Epoch 4/10, Train Loss: 0.1808 (value: 0.0007, weighted value: 0.0335, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9382
Epoch 5/10, Train Loss: 0.1774 (value: 0.0006, weighted value: 0.0309, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9385
Epoch 6/10, Train Loss: 0.1782 (value: 0.0006, weighted value: 0.0318, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9384
Epoch 7/10, Train Loss: 0.1749 (value: 0.0006, weighted value: 0.0292, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9386
Epoch 8/10, Train Loss: 0.1760 (value: 0.0006, weighted value: 0.0302, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9387
Epoch 9/10, Train Loss: 0.1725 (value: 0.0005, weighted value: 0.0273, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9388
Epoch 10/10, Train Loss: 0.1732 (value: 0.0006, weighted value: 0.0279, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9389
..training done in 67.90 seconds
..evaluation done in 18.79 seconds
Old network+MCTS average reward: 0.7659, min: 0.1019, max: 1.4722, stdev: 0.2370
New network+MCTS average reward: 0.7645, min: 0.1389, max: 1.4722, stdev: 0.2371
Old bare network average reward: 0.7339, min: 0.1019, max: 1.4259, stdev: 0.2381
New bare network average reward: 0.7340, min: 0.0926, max: 1.4722, stdev: 0.2415
External policy "random" average reward: 0.2569, min: -0.2500, max: 1.0278, stdev: 0.2212
External policy "individual greedy" average reward: 0.5442, min: -0.0278, max: 1.3519, stdev: 0.2347
External policy "total greedy" average reward: 0.6634, min: 0.0926, max: 1.3889, stdev: 0.2328
New network won 58 and tied 171 out of 300 games (47.83% wins where ties are half wins)
Reverting to the old network

Training iteration 654 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.78 seconds
Training examples lengths: [64628, 64894, 64722, 64856, 64931, 64411, 64887, 64784, 65040, 64753]
Total value: 497093.97
Training on 647906 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2143 (value: 0.0010, weighted value: 0.0514, policy: 0.1630, weighted policy: 0.1630), Train Mean Max: 0.9353
Epoch 2/10, Train Loss: 0.2000 (value: 0.0009, weighted value: 0.0437, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9363
Epoch 3/10, Train Loss: 0.1926 (value: 0.0008, weighted value: 0.0406, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9368
Epoch 4/10, Train Loss: 0.1882 (value: 0.0008, weighted value: 0.0386, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9371
Epoch 5/10, Train Loss: 0.1856 (value: 0.0007, weighted value: 0.0364, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9373
Epoch 6/10, Train Loss: 0.1833 (value: 0.0007, weighted value: 0.0347, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9374
Epoch 7/10, Train Loss: 0.1811 (value: 0.0007, weighted value: 0.0335, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9377
Epoch 8/10, Train Loss: 0.1801 (value: 0.0007, weighted value: 0.0328, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1799 (value: 0.0006, weighted value: 0.0323, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1766 (value: 0.0006, weighted value: 0.0297, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9381
..training done in 58.67 seconds
..evaluation done in 18.99 seconds
Old network+MCTS average reward: 0.7674, min: 0.2130, max: 1.4630, stdev: 0.2325
New network+MCTS average reward: 0.7664, min: 0.2315, max: 1.4630, stdev: 0.2277
Old bare network average reward: 0.7380, min: 0.1389, max: 1.4630, stdev: 0.2343
New bare network average reward: 0.7385, min: 0.1389, max: 1.3889, stdev: 0.2305
External policy "random" average reward: 0.2712, min: -0.3796, max: 0.9074, stdev: 0.2314
External policy "individual greedy" average reward: 0.5410, min: -0.0185, max: 1.4167, stdev: 0.2354
External policy "total greedy" average reward: 0.6614, min: 0.0185, max: 1.3241, stdev: 0.2172
New network won 56 and tied 184 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 655 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.89 seconds
Training examples lengths: [64894, 64722, 64856, 64931, 64411, 64887, 64784, 65040, 64753, 64687]
Total value: 497144.73
Training on 647965 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2320 (value: 0.0012, weighted value: 0.0615, policy: 0.1704, weighted policy: 0.1704), Train Mean Max: 0.9338
Epoch 2/10, Train Loss: 0.2122 (value: 0.0010, weighted value: 0.0518, policy: 0.1605, weighted policy: 0.1605), Train Mean Max: 0.9353
Epoch 3/10, Train Loss: 0.2041 (value: 0.0010, weighted value: 0.0482, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9357
Epoch 4/10, Train Loss: 0.1953 (value: 0.0009, weighted value: 0.0430, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9361
Epoch 5/10, Train Loss: 0.1929 (value: 0.0008, weighted value: 0.0417, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9363
Epoch 6/10, Train Loss: 0.1892 (value: 0.0008, weighted value: 0.0394, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9366
Epoch 7/10, Train Loss: 0.1863 (value: 0.0007, weighted value: 0.0370, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9367
Epoch 8/10, Train Loss: 0.1840 (value: 0.0007, weighted value: 0.0354, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9369
Epoch 9/10, Train Loss: 0.1834 (value: 0.0007, weighted value: 0.0353, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9370
Epoch 10/10, Train Loss: 0.1815 (value: 0.0007, weighted value: 0.0339, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9374
..training done in 61.02 seconds
..evaluation done in 17.58 seconds
Old network+MCTS average reward: 0.7590, min: 0.2130, max: 1.5741, stdev: 0.2196
New network+MCTS average reward: 0.7580, min: 0.2130, max: 1.5741, stdev: 0.2188
Old bare network average reward: 0.7292, min: 0.1574, max: 1.5741, stdev: 0.2262
New bare network average reward: 0.7278, min: 0.1574, max: 1.5278, stdev: 0.2195
External policy "random" average reward: 0.2685, min: -0.2593, max: 0.9074, stdev: 0.2221
External policy "individual greedy" average reward: 0.5360, min: -0.0185, max: 1.3981, stdev: 0.2108
External policy "total greedy" average reward: 0.6490, min: 0.1667, max: 1.4352, stdev: 0.2107
New network won 60 and tied 180 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 656 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.90 seconds
Training examples lengths: [64722, 64856, 64931, 64411, 64887, 64784, 65040, 64753, 64687, 64704]
Total value: 497989.07
Training on 647775 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1988 (value: 0.0009, weighted value: 0.0428, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9359
Epoch 2/10, Train Loss: 0.1921 (value: 0.0008, weighted value: 0.0393, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9366
Epoch 3/10, Train Loss: 0.1877 (value: 0.0007, weighted value: 0.0372, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9370
Epoch 4/10, Train Loss: 0.1847 (value: 0.0007, weighted value: 0.0358, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9372
Epoch 5/10, Train Loss: 0.1817 (value: 0.0007, weighted value: 0.0340, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9375
Epoch 6/10, Train Loss: 0.1802 (value: 0.0007, weighted value: 0.0334, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9376
Epoch 7/10, Train Loss: 0.1790 (value: 0.0006, weighted value: 0.0314, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9375
Epoch 8/10, Train Loss: 0.1785 (value: 0.0006, weighted value: 0.0317, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9379
Epoch 9/10, Train Loss: 0.1778 (value: 0.0006, weighted value: 0.0303, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9380
Epoch 10/10, Train Loss: 0.1768 (value: 0.0006, weighted value: 0.0304, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9382
..training done in 59.96 seconds
..evaluation done in 17.59 seconds
Old network+MCTS average reward: 0.7660, min: 0.1574, max: 1.4259, stdev: 0.2322
New network+MCTS average reward: 0.7668, min: 0.1574, max: 1.4259, stdev: 0.2300
Old bare network average reward: 0.7381, min: 0.1574, max: 1.4259, stdev: 0.2361
New bare network average reward: 0.7341, min: 0.1574, max: 1.4259, stdev: 0.2359
External policy "random" average reward: 0.2621, min: -0.3426, max: 0.8704, stdev: 0.2283
External policy "individual greedy" average reward: 0.5571, min: 0.0463, max: 1.2963, stdev: 0.2206
External policy "total greedy" average reward: 0.6650, min: 0.0648, max: 1.3611, stdev: 0.2154
New network won 54 and tied 180 out of 300 games (48.00% wins where ties are half wins)
Reverting to the old network

Training iteration 657 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.83 seconds
Training examples lengths: [64856, 64931, 64411, 64887, 64784, 65040, 64753, 64687, 64704, 64632]
Total value: 498506.25
Training on 647685 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2169 (value: 0.0011, weighted value: 0.0542, policy: 0.1628, weighted policy: 0.1628), Train Mean Max: 0.9347
Epoch 2/10, Train Loss: 0.2022 (value: 0.0009, weighted value: 0.0460, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9358
Epoch 3/10, Train Loss: 0.1952 (value: 0.0009, weighted value: 0.0429, policy: 0.1523, weighted policy: 0.1523), Train Mean Max: 0.9362
Epoch 4/10, Train Loss: 0.1911 (value: 0.0008, weighted value: 0.0403, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9364
Epoch 5/10, Train Loss: 0.1891 (value: 0.0008, weighted value: 0.0394, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9367
Epoch 6/10, Train Loss: 0.1851 (value: 0.0007, weighted value: 0.0368, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9369
Epoch 7/10, Train Loss: 0.1839 (value: 0.0007, weighted value: 0.0361, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9372
Epoch 8/10, Train Loss: 0.1817 (value: 0.0007, weighted value: 0.0338, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9372
Epoch 9/10, Train Loss: 0.1817 (value: 0.0007, weighted value: 0.0339, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9374
Epoch 10/10, Train Loss: 0.1796 (value: 0.0006, weighted value: 0.0320, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9376
..training done in 60.63 seconds
..evaluation done in 18.29 seconds
Old network+MCTS average reward: 0.7372, min: 0.1852, max: 1.7037, stdev: 0.2397
New network+MCTS average reward: 0.7380, min: 0.1481, max: 1.7037, stdev: 0.2436
Old bare network average reward: 0.7104, min: 0.1481, max: 1.7037, stdev: 0.2410
New bare network average reward: 0.7105, min: 0.1481, max: 1.7037, stdev: 0.2413
External policy "random" average reward: 0.2418, min: -0.3241, max: 0.9444, stdev: 0.2282
External policy "individual greedy" average reward: 0.5227, min: -0.0833, max: 1.4907, stdev: 0.2331
External policy "total greedy" average reward: 0.6386, min: 0.1296, max: 1.4352, stdev: 0.2208
New network won 73 and tied 173 out of 300 games (53.17% wins where ties are half wins)
Keeping the new network

Training iteration 658 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.10 seconds
Training examples lengths: [64931, 64411, 64887, 64784, 65040, 64753, 64687, 64704, 64632, 64652]
Total value: 498088.26
Training on 647481 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1985 (value: 0.0008, weighted value: 0.0417, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9361
Epoch 2/10, Train Loss: 0.1924 (value: 0.0008, weighted value: 0.0380, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9367
Epoch 3/10, Train Loss: 0.1868 (value: 0.0007, weighted value: 0.0369, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9373
Epoch 4/10, Train Loss: 0.1833 (value: 0.0007, weighted value: 0.0348, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9373
Epoch 5/10, Train Loss: 0.1808 (value: 0.0007, weighted value: 0.0329, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9375
Epoch 6/10, Train Loss: 0.1805 (value: 0.0007, weighted value: 0.0330, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9376
Epoch 7/10, Train Loss: 0.1783 (value: 0.0006, weighted value: 0.0311, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9377
Epoch 8/10, Train Loss: 0.1773 (value: 0.0006, weighted value: 0.0303, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9380
Epoch 9/10, Train Loss: 0.1764 (value: 0.0006, weighted value: 0.0296, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9380
Epoch 10/10, Train Loss: 0.1763 (value: 0.0006, weighted value: 0.0295, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9382
..training done in 59.75 seconds
..evaluation done in 18.12 seconds
Old network+MCTS average reward: 0.7703, min: 0.1389, max: 1.6019, stdev: 0.2374
New network+MCTS average reward: 0.7672, min: 0.1389, max: 1.6019, stdev: 0.2357
Old bare network average reward: 0.7354, min: 0.1296, max: 1.6019, stdev: 0.2346
New bare network average reward: 0.7357, min: 0.1389, max: 1.5185, stdev: 0.2337
External policy "random" average reward: 0.2685, min: -0.3426, max: 1.0833, stdev: 0.2283
External policy "individual greedy" average reward: 0.5539, min: 0.0093, max: 1.5370, stdev: 0.2313
External policy "total greedy" average reward: 0.6646, min: -0.0093, max: 1.5463, stdev: 0.2275
New network won 55 and tied 176 out of 300 games (47.67% wins where ties are half wins)
Reverting to the old network

Training iteration 659 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.84 seconds
Training examples lengths: [64411, 64887, 64784, 65040, 64753, 64687, 64704, 64632, 64652, 64602]
Total value: 498126.60
Training on 647152 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2169 (value: 0.0010, weighted value: 0.0523, policy: 0.1645, weighted policy: 0.1645), Train Mean Max: 0.9346
Epoch 2/10, Train Loss: 0.2040 (value: 0.0009, weighted value: 0.0463, policy: 0.1577, weighted policy: 0.1577), Train Mean Max: 0.9358
Epoch 3/10, Train Loss: 0.1956 (value: 0.0008, weighted value: 0.0425, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9362
Epoch 4/10, Train Loss: 0.1913 (value: 0.0008, weighted value: 0.0405, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9364
Epoch 5/10, Train Loss: 0.1875 (value: 0.0008, weighted value: 0.0377, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9365
Epoch 6/10, Train Loss: 0.1848 (value: 0.0007, weighted value: 0.0361, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9369
Epoch 7/10, Train Loss: 0.1840 (value: 0.0007, weighted value: 0.0351, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9369
Epoch 8/10, Train Loss: 0.1822 (value: 0.0007, weighted value: 0.0338, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9372
Epoch 9/10, Train Loss: 0.1807 (value: 0.0007, weighted value: 0.0329, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9374
Epoch 10/10, Train Loss: 0.1786 (value: 0.0006, weighted value: 0.0313, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9376
..training done in 57.61 seconds
..evaluation done in 17.75 seconds
Old network+MCTS average reward: 0.7544, min: 0.0926, max: 1.4722, stdev: 0.2328
New network+MCTS average reward: 0.7549, min: 0.1111, max: 1.4722, stdev: 0.2316
Old bare network average reward: 0.7230, min: 0.0000, max: 1.4074, stdev: 0.2314
New bare network average reward: 0.7214, min: -0.0093, max: 1.4074, stdev: 0.2340
External policy "random" average reward: 0.2547, min: -0.3333, max: 0.9259, stdev: 0.2293
External policy "individual greedy" average reward: 0.5296, min: -0.0833, max: 1.3889, stdev: 0.2172
External policy "total greedy" average reward: 0.6453, min: 0.0000, max: 1.4259, stdev: 0.2159
New network won 65 and tied 174 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 660 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.64 seconds
Training examples lengths: [64887, 64784, 65040, 64753, 64687, 64704, 64632, 64652, 64602, 65185]
Total value: 500265.51
Training on 647926 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1988 (value: 0.0009, weighted value: 0.0431, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9363
Epoch 2/10, Train Loss: 0.1910 (value: 0.0008, weighted value: 0.0384, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9369
Epoch 3/10, Train Loss: 0.1860 (value: 0.0007, weighted value: 0.0365, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9372
Epoch 4/10, Train Loss: 0.1832 (value: 0.0007, weighted value: 0.0347, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9374
Epoch 5/10, Train Loss: 0.1827 (value: 0.0007, weighted value: 0.0340, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1790 (value: 0.0006, weighted value: 0.0315, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9377
Epoch 7/10, Train Loss: 0.1784 (value: 0.0006, weighted value: 0.0314, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1773 (value: 0.0006, weighted value: 0.0307, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9380
Epoch 9/10, Train Loss: 0.1771 (value: 0.0006, weighted value: 0.0299, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9380
Epoch 10/10, Train Loss: 0.1756 (value: 0.0006, weighted value: 0.0289, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9383
..training done in 58.68 seconds
..evaluation done in 18.40 seconds
Old network+MCTS average reward: 0.7534, min: 0.2130, max: 1.4444, stdev: 0.2102
New network+MCTS average reward: 0.7535, min: 0.2130, max: 1.4815, stdev: 0.2103
Old bare network average reward: 0.7217, min: 0.2130, max: 1.4444, stdev: 0.2117
New bare network average reward: 0.7249, min: 0.1852, max: 1.3333, stdev: 0.2095
External policy "random" average reward: 0.2485, min: -0.3426, max: 1.1111, stdev: 0.2229
External policy "individual greedy" average reward: 0.5397, min: 0.0648, max: 1.2685, stdev: 0.2025
External policy "total greedy" average reward: 0.6526, min: 0.1759, max: 1.3426, stdev: 0.2043
New network won 75 and tied 170 out of 300 games (53.33% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_660

Training iteration 661 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.39 seconds
Training examples lengths: [64784, 65040, 64753, 64687, 64704, 64632, 64652, 64602, 65185, 65053]
Total value: 501134.99
Training on 648092 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1954 (value: 0.0008, weighted value: 0.0398, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9368
Epoch 2/10, Train Loss: 0.1877 (value: 0.0007, weighted value: 0.0362, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9376
Epoch 3/10, Train Loss: 0.1831 (value: 0.0007, weighted value: 0.0343, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9378
Epoch 4/10, Train Loss: 0.1797 (value: 0.0006, weighted value: 0.0322, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9380
Epoch 5/10, Train Loss: 0.1789 (value: 0.0006, weighted value: 0.0321, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9381
Epoch 6/10, Train Loss: 0.1766 (value: 0.0006, weighted value: 0.0302, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9383
Epoch 7/10, Train Loss: 0.1755 (value: 0.0006, weighted value: 0.0295, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9384
Epoch 8/10, Train Loss: 0.1751 (value: 0.0006, weighted value: 0.0289, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9385
Epoch 9/10, Train Loss: 0.1742 (value: 0.0006, weighted value: 0.0285, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9386
Epoch 10/10, Train Loss: 0.1724 (value: 0.0005, weighted value: 0.0274, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9388
..training done in 65.19 seconds
..evaluation done in 18.80 seconds
Old network+MCTS average reward: 0.7931, min: 0.1667, max: 1.5185, stdev: 0.2440
New network+MCTS average reward: 0.7899, min: 0.1759, max: 1.5648, stdev: 0.2453
Old bare network average reward: 0.7618, min: 0.1389, max: 1.4907, stdev: 0.2502
New bare network average reward: 0.7587, min: 0.1389, max: 1.4630, stdev: 0.2516
External policy "random" average reward: 0.2770, min: -0.3333, max: 0.9815, stdev: 0.2233
External policy "individual greedy" average reward: 0.5703, min: 0.0556, max: 1.2315, stdev: 0.2310
External policy "total greedy" average reward: 0.6752, min: 0.1574, max: 1.4444, stdev: 0.2343
New network won 52 and tied 181 out of 300 games (47.50% wins where ties are half wins)
Reverting to the old network

Training iteration 662 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.03 seconds
Training examples lengths: [65040, 64753, 64687, 64704, 64632, 64652, 64602, 65185, 65053, 64768]
Total value: 501321.56
Training on 648076 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2146 (value: 0.0010, weighted value: 0.0514, policy: 0.1632, weighted policy: 0.1632), Train Mean Max: 0.9355
Epoch 2/10, Train Loss: 0.2001 (value: 0.0009, weighted value: 0.0437, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9366
Epoch 3/10, Train Loss: 0.1926 (value: 0.0008, weighted value: 0.0411, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9370
Epoch 4/10, Train Loss: 0.1874 (value: 0.0008, weighted value: 0.0379, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9372
Epoch 5/10, Train Loss: 0.1860 (value: 0.0007, weighted value: 0.0370, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9374
Epoch 6/10, Train Loss: 0.1821 (value: 0.0007, weighted value: 0.0343, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9376
Epoch 7/10, Train Loss: 0.1807 (value: 0.0007, weighted value: 0.0334, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9377
Epoch 8/10, Train Loss: 0.1801 (value: 0.0007, weighted value: 0.0327, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1778 (value: 0.0006, weighted value: 0.0312, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9382
Epoch 10/10, Train Loss: 0.1771 (value: 0.0006, weighted value: 0.0308, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9383
..training done in 59.98 seconds
..evaluation done in 17.95 seconds
Old network+MCTS average reward: 0.7276, min: 0.1667, max: 1.5093, stdev: 0.2281
New network+MCTS average reward: 0.7310, min: 0.2407, max: 1.5093, stdev: 0.2250
Old bare network average reward: 0.7007, min: 0.0556, max: 1.4815, stdev: 0.2338
New bare network average reward: 0.7008, min: 0.1204, max: 1.4815, stdev: 0.2307
External policy "random" average reward: 0.2398, min: -0.2870, max: 0.8889, stdev: 0.2118
External policy "individual greedy" average reward: 0.5194, min: 0.0648, max: 1.3426, stdev: 0.2094
External policy "total greedy" average reward: 0.6218, min: 0.0093, max: 1.4907, stdev: 0.2143
New network won 71 and tied 172 out of 300 games (52.33% wins where ties are half wins)
Keeping the new network

Training iteration 663 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.14 seconds
Training examples lengths: [64753, 64687, 64704, 64632, 64652, 64602, 65185, 65053, 64768, 64685]
Total value: 500505.98
Training on 647721 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1951 (value: 0.0008, weighted value: 0.0402, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9368
Epoch 2/10, Train Loss: 0.1889 (value: 0.0007, weighted value: 0.0369, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9374
Epoch 3/10, Train Loss: 0.1824 (value: 0.0007, weighted value: 0.0342, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9378
Epoch 4/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0338, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9379
Epoch 5/10, Train Loss: 0.1775 (value: 0.0006, weighted value: 0.0310, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9382
Epoch 6/10, Train Loss: 0.1773 (value: 0.0006, weighted value: 0.0310, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9383
Epoch 7/10, Train Loss: 0.1770 (value: 0.0006, weighted value: 0.0304, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9383
Epoch 8/10, Train Loss: 0.1752 (value: 0.0006, weighted value: 0.0294, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9386
Epoch 9/10, Train Loss: 0.1747 (value: 0.0006, weighted value: 0.0287, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9387
Epoch 10/10, Train Loss: 0.1726 (value: 0.0005, weighted value: 0.0272, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9388
..training done in 63.97 seconds
..evaluation done in 18.42 seconds
Old network+MCTS average reward: 0.7354, min: 0.1759, max: 1.4444, stdev: 0.2186
New network+MCTS average reward: 0.7357, min: 0.1759, max: 1.4444, stdev: 0.2167
Old bare network average reward: 0.7010, min: 0.1204, max: 1.4444, stdev: 0.2227
New bare network average reward: 0.7040, min: 0.1759, max: 1.4444, stdev: 0.2188
External policy "random" average reward: 0.2338, min: -0.5093, max: 0.8981, stdev: 0.2219
External policy "individual greedy" average reward: 0.5163, min: -0.0741, max: 1.0926, stdev: 0.2309
External policy "total greedy" average reward: 0.6305, min: 0.0556, max: 1.2407, stdev: 0.2234
New network won 57 and tied 178 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 664 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.86 seconds
Training examples lengths: [64687, 64704, 64632, 64652, 64602, 65185, 65053, 64768, 64685, 64929]
Total value: 501293.72
Training on 647897 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2136 (value: 0.0010, weighted value: 0.0519, policy: 0.1617, weighted policy: 0.1617), Train Mean Max: 0.9355
Epoch 2/10, Train Loss: 0.1996 (value: 0.0009, weighted value: 0.0436, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9365
Epoch 3/10, Train Loss: 0.1927 (value: 0.0008, weighted value: 0.0417, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9370
Epoch 4/10, Train Loss: 0.1873 (value: 0.0008, weighted value: 0.0392, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9373
Epoch 5/10, Train Loss: 0.1841 (value: 0.0007, weighted value: 0.0360, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9375
Epoch 6/10, Train Loss: 0.1816 (value: 0.0007, weighted value: 0.0346, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9377
Epoch 7/10, Train Loss: 0.1814 (value: 0.0007, weighted value: 0.0342, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9378
Epoch 8/10, Train Loss: 0.1798 (value: 0.0007, weighted value: 0.0330, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1780 (value: 0.0006, weighted value: 0.0313, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9383
Epoch 10/10, Train Loss: 0.1768 (value: 0.0006, weighted value: 0.0311, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9384
..training done in 63.66 seconds
..evaluation done in 18.33 seconds
Old network+MCTS average reward: 0.7575, min: 0.0741, max: 1.7315, stdev: 0.2389
New network+MCTS average reward: 0.7598, min: 0.0556, max: 1.7315, stdev: 0.2398
Old bare network average reward: 0.7322, min: 0.0556, max: 1.7222, stdev: 0.2429
New bare network average reward: 0.7306, min: 0.0556, max: 1.7315, stdev: 0.2435
External policy "random" average reward: 0.2663, min: -0.5093, max: 1.0278, stdev: 0.2287
External policy "individual greedy" average reward: 0.5458, min: -0.0556, max: 1.5093, stdev: 0.2260
External policy "total greedy" average reward: 0.6586, min: 0.0648, max: 1.6296, stdev: 0.2255
New network won 64 and tied 178 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 665 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.85 seconds
Training examples lengths: [64704, 64632, 64652, 64602, 65185, 65053, 64768, 64685, 64929, 64846]
Total value: 501156.67
Training on 648056 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1953 (value: 0.0008, weighted value: 0.0407, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9369
Epoch 2/10, Train Loss: 0.1888 (value: 0.0007, weighted value: 0.0373, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9374
Epoch 3/10, Train Loss: 0.1832 (value: 0.0007, weighted value: 0.0346, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9378
Epoch 4/10, Train Loss: 0.1809 (value: 0.0007, weighted value: 0.0339, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9380
Epoch 5/10, Train Loss: 0.1797 (value: 0.0006, weighted value: 0.0324, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9380
Epoch 6/10, Train Loss: 0.1772 (value: 0.0006, weighted value: 0.0302, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9382
Epoch 7/10, Train Loss: 0.1767 (value: 0.0006, weighted value: 0.0311, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9385
Epoch 8/10, Train Loss: 0.1756 (value: 0.0006, weighted value: 0.0292, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9385
Epoch 9/10, Train Loss: 0.1745 (value: 0.0006, weighted value: 0.0290, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9388
Epoch 10/10, Train Loss: 0.1731 (value: 0.0006, weighted value: 0.0275, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9388
..training done in 66.33 seconds
..evaluation done in 17.41 seconds
Old network+MCTS average reward: 0.7471, min: 0.0833, max: 1.5926, stdev: 0.2429
New network+MCTS average reward: 0.7468, min: 0.0833, max: 1.5926, stdev: 0.2425
Old bare network average reward: 0.7194, min: 0.0185, max: 1.4630, stdev: 0.2427
New bare network average reward: 0.7181, min: 0.0648, max: 1.4630, stdev: 0.2456
External policy "random" average reward: 0.2625, min: -0.4074, max: 0.9352, stdev: 0.2292
External policy "individual greedy" average reward: 0.5367, min: 0.0093, max: 1.2500, stdev: 0.2357
External policy "total greedy" average reward: 0.6442, min: 0.1389, max: 1.3056, stdev: 0.2272
New network won 59 and tied 187 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 666 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.27 seconds
Training examples lengths: [64632, 64652, 64602, 65185, 65053, 64768, 64685, 64929, 64846, 64961]
Total value: 500867.69
Training on 648313 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1958 (value: 0.0008, weighted value: 0.0404, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9372
Epoch 2/10, Train Loss: 0.1855 (value: 0.0007, weighted value: 0.0344, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9378
Epoch 3/10, Train Loss: 0.1821 (value: 0.0007, weighted value: 0.0338, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9383
Epoch 4/10, Train Loss: 0.1780 (value: 0.0006, weighted value: 0.0317, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9383
Epoch 5/10, Train Loss: 0.1780 (value: 0.0006, weighted value: 0.0319, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9384
Epoch 6/10, Train Loss: 0.1757 (value: 0.0006, weighted value: 0.0296, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9385
Epoch 7/10, Train Loss: 0.1744 (value: 0.0006, weighted value: 0.0289, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9388
Epoch 8/10, Train Loss: 0.1738 (value: 0.0006, weighted value: 0.0282, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9387
Epoch 9/10, Train Loss: 0.1726 (value: 0.0006, weighted value: 0.0276, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9390
Epoch 10/10, Train Loss: 0.1731 (value: 0.0006, weighted value: 0.0281, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9390
..training done in 59.84 seconds
..evaluation done in 18.25 seconds
Old network+MCTS average reward: 0.7755, min: -0.0093, max: 1.6481, stdev: 0.2569
New network+MCTS average reward: 0.7784, min: 0.0926, max: 1.6204, stdev: 0.2574
Old bare network average reward: 0.7451, min: -0.0093, max: 1.6204, stdev: 0.2590
New bare network average reward: 0.7463, min: -0.0093, max: 1.6574, stdev: 0.2612
External policy "random" average reward: 0.2847, min: -0.3981, max: 0.9444, stdev: 0.2313
External policy "individual greedy" average reward: 0.5578, min: -0.2500, max: 1.4352, stdev: 0.2455
External policy "total greedy" average reward: 0.6654, min: 0.0000, max: 1.5093, stdev: 0.2430
New network won 77 and tied 163 out of 300 games (52.83% wins where ties are half wins)
Keeping the new network

Training iteration 667 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.95 seconds
Training examples lengths: [64652, 64602, 65185, 65053, 64768, 64685, 64929, 64846, 64961, 64744]
Total value: 500969.14
Training on 648425 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1933 (value: 0.0008, weighted value: 0.0391, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9374
Epoch 2/10, Train Loss: 0.1851 (value: 0.0007, weighted value: 0.0341, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9381
Epoch 3/10, Train Loss: 0.1818 (value: 0.0007, weighted value: 0.0336, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9384
Epoch 4/10, Train Loss: 0.1785 (value: 0.0006, weighted value: 0.0314, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9385
Epoch 5/10, Train Loss: 0.1762 (value: 0.0006, weighted value: 0.0303, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9387
Epoch 6/10, Train Loss: 0.1748 (value: 0.0006, weighted value: 0.0297, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9388
Epoch 7/10, Train Loss: 0.1742 (value: 0.0006, weighted value: 0.0290, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9388
Epoch 8/10, Train Loss: 0.1725 (value: 0.0005, weighted value: 0.0273, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9390
Epoch 9/10, Train Loss: 0.1731 (value: 0.0006, weighted value: 0.0283, policy: 0.1449, weighted policy: 0.1449), Train Mean Max: 0.9389
Epoch 10/10, Train Loss: 0.1721 (value: 0.0005, weighted value: 0.0268, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9392
..training done in 59.56 seconds
..evaluation done in 18.16 seconds
Old network+MCTS average reward: 0.7686, min: 0.0926, max: 1.4167, stdev: 0.2306
New network+MCTS average reward: 0.7682, min: 0.1481, max: 1.4167, stdev: 0.2284
Old bare network average reward: 0.7401, min: 0.0185, max: 1.4167, stdev: 0.2321
New bare network average reward: 0.7410, min: 0.0185, max: 1.4167, stdev: 0.2302
External policy "random" average reward: 0.2863, min: -0.3889, max: 0.9537, stdev: 0.2292
External policy "individual greedy" average reward: 0.5566, min: 0.0278, max: 1.3056, stdev: 0.2286
External policy "total greedy" average reward: 0.6660, min: 0.1204, max: 1.3611, stdev: 0.2222
New network won 63 and tied 174 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 668 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.37 seconds
Training examples lengths: [64602, 65185, 65053, 64768, 64685, 64929, 64846, 64961, 64744, 64699]
Total value: 500727.31
Training on 648472 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1926 (value: 0.0008, weighted value: 0.0375, policy: 0.1550, weighted policy: 0.1550), Train Mean Max: 0.9376
Epoch 2/10, Train Loss: 0.1843 (value: 0.0007, weighted value: 0.0344, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9382
Epoch 3/10, Train Loss: 0.1793 (value: 0.0006, weighted value: 0.0318, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9387
Epoch 4/10, Train Loss: 0.1781 (value: 0.0006, weighted value: 0.0314, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9387
Epoch 5/10, Train Loss: 0.1758 (value: 0.0006, weighted value: 0.0298, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9387
Epoch 6/10, Train Loss: 0.1744 (value: 0.0006, weighted value: 0.0286, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9388
Epoch 7/10, Train Loss: 0.1737 (value: 0.0006, weighted value: 0.0284, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9390
Epoch 8/10, Train Loss: 0.1719 (value: 0.0005, weighted value: 0.0268, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9391
Epoch 9/10, Train Loss: 0.1719 (value: 0.0005, weighted value: 0.0272, policy: 0.1447, weighted policy: 0.1447), Train Mean Max: 0.9391
Epoch 10/10, Train Loss: 0.1711 (value: 0.0005, weighted value: 0.0262, policy: 0.1449, weighted policy: 0.1449), Train Mean Max: 0.9393
..training done in 59.79 seconds
..evaluation done in 19.09 seconds
Old network+MCTS average reward: 0.7909, min: 0.2407, max: 1.4259, stdev: 0.2249
New network+MCTS average reward: 0.7870, min: 0.2500, max: 1.4259, stdev: 0.2249
Old bare network average reward: 0.7617, min: 0.2222, max: 1.4074, stdev: 0.2261
New bare network average reward: 0.7610, min: 0.2222, max: 1.4259, stdev: 0.2251
External policy "random" average reward: 0.2790, min: -0.2407, max: 0.9630, stdev: 0.2137
External policy "individual greedy" average reward: 0.5724, min: 0.0278, max: 1.3241, stdev: 0.2196
External policy "total greedy" average reward: 0.6747, min: 0.1481, max: 1.2685, stdev: 0.2107
New network won 49 and tied 177 out of 300 games (45.83% wins where ties are half wins)
Reverting to the old network

Training iteration 669 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.28 seconds
Training examples lengths: [65185, 65053, 64768, 64685, 64929, 64846, 64961, 64744, 64699, 64940]
Total value: 501059.38
Training on 648810 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2127 (value: 0.0010, weighted value: 0.0497, policy: 0.1630, weighted policy: 0.1630), Train Mean Max: 0.9360
Epoch 2/10, Train Loss: 0.1985 (value: 0.0009, weighted value: 0.0428, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9371
Epoch 3/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0390, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9375
Epoch 4/10, Train Loss: 0.1861 (value: 0.0007, weighted value: 0.0367, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1826 (value: 0.0007, weighted value: 0.0345, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9378
Epoch 6/10, Train Loss: 0.1808 (value: 0.0007, weighted value: 0.0339, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9380
Epoch 7/10, Train Loss: 0.1795 (value: 0.0006, weighted value: 0.0325, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9380
Epoch 8/10, Train Loss: 0.1778 (value: 0.0006, weighted value: 0.0312, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9382
Epoch 9/10, Train Loss: 0.1772 (value: 0.0006, weighted value: 0.0304, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9382
Epoch 10/10, Train Loss: 0.1758 (value: 0.0006, weighted value: 0.0297, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9385
..training done in 64.22 seconds
..evaluation done in 18.42 seconds
Old network+MCTS average reward: 0.7665, min: 0.1574, max: 1.4907, stdev: 0.2327
New network+MCTS average reward: 0.7674, min: 0.0556, max: 1.4907, stdev: 0.2320
Old bare network average reward: 0.7366, min: 0.0556, max: 1.3981, stdev: 0.2357
New bare network average reward: 0.7366, min: 0.0556, max: 1.3704, stdev: 0.2343
External policy "random" average reward: 0.2655, min: -0.4259, max: 0.9444, stdev: 0.2268
External policy "individual greedy" average reward: 0.5490, min: -0.0833, max: 1.1759, stdev: 0.2272
External policy "total greedy" average reward: 0.6571, min: 0.0370, max: 1.2037, stdev: 0.2190
New network won 58 and tied 191 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 670 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 69.78 seconds
Training examples lengths: [65053, 64768, 64685, 64929, 64846, 64961, 64744, 64699, 64940, 64898]
Total value: 500073.06
Training on 648523 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1952 (value: 0.0008, weighted value: 0.0399, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9369
Epoch 2/10, Train Loss: 0.1883 (value: 0.0007, weighted value: 0.0368, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9376
Epoch 3/10, Train Loss: 0.1830 (value: 0.0007, weighted value: 0.0347, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9381
Epoch 4/10, Train Loss: 0.1796 (value: 0.0007, weighted value: 0.0326, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9380
Epoch 5/10, Train Loss: 0.1789 (value: 0.0006, weighted value: 0.0322, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9382
Epoch 6/10, Train Loss: 0.1765 (value: 0.0006, weighted value: 0.0299, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9383
Epoch 7/10, Train Loss: 0.1756 (value: 0.0006, weighted value: 0.0299, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9386
Epoch 8/10, Train Loss: 0.1755 (value: 0.0006, weighted value: 0.0294, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9386
Epoch 9/10, Train Loss: 0.1733 (value: 0.0006, weighted value: 0.0281, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9388
Epoch 10/10, Train Loss: 0.1725 (value: 0.0005, weighted value: 0.0273, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9390
..training done in 68.80 seconds
..evaluation done in 18.83 seconds
Old network+MCTS average reward: 0.7538, min: 0.1852, max: 1.5370, stdev: 0.2263
New network+MCTS average reward: 0.7536, min: 0.1852, max: 1.5370, stdev: 0.2273
Old bare network average reward: 0.7238, min: 0.1667, max: 1.5370, stdev: 0.2318
New bare network average reward: 0.7201, min: 0.1667, max: 1.4722, stdev: 0.2336
External policy "random" average reward: 0.2475, min: -0.3056, max: 0.9815, stdev: 0.2072
External policy "individual greedy" average reward: 0.5241, min: -0.0741, max: 1.2685, stdev: 0.2195
External policy "total greedy" average reward: 0.6351, min: 0.1111, max: 1.4722, stdev: 0.2130
New network won 64 and tied 181 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 671 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.92 seconds
Training examples lengths: [64768, 64685, 64929, 64846, 64961, 64744, 64699, 64940, 64898, 64846]
Total value: 499634.68
Training on 648316 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1943 (value: 0.0008, weighted value: 0.0396, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9372
Epoch 2/10, Train Loss: 0.1858 (value: 0.0007, weighted value: 0.0347, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9381
Epoch 3/10, Train Loss: 0.1806 (value: 0.0007, weighted value: 0.0326, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9384
Epoch 4/10, Train Loss: 0.1778 (value: 0.0006, weighted value: 0.0313, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9385
Epoch 5/10, Train Loss: 0.1756 (value: 0.0006, weighted value: 0.0303, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9387
Epoch 6/10, Train Loss: 0.1744 (value: 0.0006, weighted value: 0.0293, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9388
Epoch 7/10, Train Loss: 0.1744 (value: 0.0006, weighted value: 0.0292, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9388
Epoch 8/10, Train Loss: 0.1730 (value: 0.0006, weighted value: 0.0278, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9389
Epoch 9/10, Train Loss: 0.1725 (value: 0.0006, weighted value: 0.0277, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9391
Epoch 10/10, Train Loss: 0.1714 (value: 0.0005, weighted value: 0.0264, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9392
..training done in 64.14 seconds
..evaluation done in 18.58 seconds
Old network+MCTS average reward: 0.7517, min: 0.1574, max: 1.3148, stdev: 0.2357
New network+MCTS average reward: 0.7479, min: 0.1574, max: 1.3148, stdev: 0.2370
Old bare network average reward: 0.7225, min: 0.1574, max: 1.3148, stdev: 0.2418
New bare network average reward: 0.7217, min: 0.1574, max: 1.3148, stdev: 0.2393
External policy "random" average reward: 0.2606, min: -0.3889, max: 0.8981, stdev: 0.2337
External policy "individual greedy" average reward: 0.5426, min: -0.0648, max: 1.2593, stdev: 0.2341
External policy "total greedy" average reward: 0.6463, min: 0.1574, max: 1.2222, stdev: 0.2245
New network won 53 and tied 174 out of 300 games (46.67% wins where ties are half wins)
Reverting to the old network

Training iteration 672 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.79 seconds
Training examples lengths: [64685, 64929, 64846, 64961, 64744, 64699, 64940, 64898, 64846, 64871]
Total value: 499529.12
Training on 648419 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2130 (value: 0.0010, weighted value: 0.0513, policy: 0.1617, weighted policy: 0.1617), Train Mean Max: 0.9358
Epoch 2/10, Train Loss: 0.1975 (value: 0.0008, weighted value: 0.0419, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9368
Epoch 3/10, Train Loss: 0.1916 (value: 0.0008, weighted value: 0.0407, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9374
Epoch 4/10, Train Loss: 0.1849 (value: 0.0007, weighted value: 0.0365, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9375
Epoch 5/10, Train Loss: 0.1843 (value: 0.0007, weighted value: 0.0369, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9379
Epoch 6/10, Train Loss: 0.1795 (value: 0.0007, weighted value: 0.0328, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9379
Epoch 7/10, Train Loss: 0.1800 (value: 0.0007, weighted value: 0.0335, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9381
Epoch 8/10, Train Loss: 0.1773 (value: 0.0006, weighted value: 0.0309, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9383
Epoch 9/10, Train Loss: 0.1777 (value: 0.0006, weighted value: 0.0315, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1748 (value: 0.0006, weighted value: 0.0294, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9387
..training done in 68.95 seconds
..evaluation done in 19.16 seconds
Old network+MCTS average reward: 0.7619, min: 0.2222, max: 1.4722, stdev: 0.2231
New network+MCTS average reward: 0.7594, min: 0.1944, max: 1.4722, stdev: 0.2251
Old bare network average reward: 0.7331, min: 0.1204, max: 1.4722, stdev: 0.2259
New bare network average reward: 0.7294, min: 0.1204, max: 1.4167, stdev: 0.2248
External policy "random" average reward: 0.2725, min: -0.2778, max: 0.8796, stdev: 0.2348
External policy "individual greedy" average reward: 0.5395, min: 0.0185, max: 1.2037, stdev: 0.2187
External policy "total greedy" average reward: 0.6566, min: 0.1852, max: 1.2222, stdev: 0.2116
New network won 52 and tied 175 out of 300 games (46.50% wins where ties are half wins)
Reverting to the old network

Training iteration 673 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.64 seconds
Training examples lengths: [64929, 64846, 64961, 64744, 64699, 64940, 64898, 64846, 64871, 64883]
Total value: 499834.05
Training on 648617 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2307 (value: 0.0012, weighted value: 0.0621, policy: 0.1686, weighted policy: 0.1686), Train Mean Max: 0.9345
Epoch 2/10, Train Loss: 0.2092 (value: 0.0010, weighted value: 0.0502, policy: 0.1590, weighted policy: 0.1590), Train Mean Max: 0.9359
Epoch 3/10, Train Loss: 0.1990 (value: 0.0009, weighted value: 0.0457, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9365
Epoch 4/10, Train Loss: 0.1934 (value: 0.0009, weighted value: 0.0428, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9367
Epoch 5/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0392, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9370
Epoch 6/10, Train Loss: 0.1865 (value: 0.0008, weighted value: 0.0383, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9372
Epoch 7/10, Train Loss: 0.1840 (value: 0.0007, weighted value: 0.0366, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9375
Epoch 8/10, Train Loss: 0.1825 (value: 0.0007, weighted value: 0.0355, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9377
Epoch 9/10, Train Loss: 0.1805 (value: 0.0007, weighted value: 0.0334, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1793 (value: 0.0007, weighted value: 0.0329, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9381
..training done in 65.11 seconds
..evaluation done in 17.90 seconds
Old network+MCTS average reward: 0.7521, min: 0.1667, max: 1.5185, stdev: 0.2450
New network+MCTS average reward: 0.7558, min: 0.1667, max: 1.5185, stdev: 0.2483
Old bare network average reward: 0.7238, min: 0.1944, max: 1.5000, stdev: 0.2539
New bare network average reward: 0.7231, min: 0.1389, max: 1.5000, stdev: 0.2534
External policy "random" average reward: 0.2581, min: -0.2685, max: 0.8981, stdev: 0.2288
External policy "individual greedy" average reward: 0.5382, min: 0.0370, max: 1.3241, stdev: 0.2414
External policy "total greedy" average reward: 0.6490, min: 0.1389, max: 1.4074, stdev: 0.2380
New network won 62 and tied 174 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 674 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.08 seconds
Training examples lengths: [64846, 64961, 64744, 64699, 64940, 64898, 64846, 64871, 64883, 64796]
Total value: 499712.89
Training on 648484 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2465 (value: 0.0014, weighted value: 0.0708, policy: 0.1757, weighted policy: 0.1757), Train Mean Max: 0.9329
Epoch 2/10, Train Loss: 0.2204 (value: 0.0011, weighted value: 0.0572, policy: 0.1633, weighted policy: 0.1633), Train Mean Max: 0.9348
Epoch 3/10, Train Loss: 0.2073 (value: 0.0010, weighted value: 0.0521, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9354
Epoch 4/10, Train Loss: 0.2005 (value: 0.0010, weighted value: 0.0479, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9358
Epoch 5/10, Train Loss: 0.1945 (value: 0.0009, weighted value: 0.0442, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9362
Epoch 6/10, Train Loss: 0.1911 (value: 0.0008, weighted value: 0.0418, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9364
Epoch 7/10, Train Loss: 0.1894 (value: 0.0008, weighted value: 0.0404, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9365
Epoch 8/10, Train Loss: 0.1858 (value: 0.0008, weighted value: 0.0380, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9370
Epoch 9/10, Train Loss: 0.1846 (value: 0.0007, weighted value: 0.0371, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9371
Epoch 10/10, Train Loss: 0.1825 (value: 0.0007, weighted value: 0.0354, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9373
..training done in 64.60 seconds
..evaluation done in 18.57 seconds
Old network+MCTS average reward: 0.7586, min: -0.0278, max: 1.3611, stdev: 0.2329
New network+MCTS average reward: 0.7584, min: 0.1019, max: 1.3611, stdev: 0.2311
Old bare network average reward: 0.7321, min: 0.0556, max: 1.3611, stdev: 0.2358
New bare network average reward: 0.7321, min: 0.0556, max: 1.3981, stdev: 0.2370
External policy "random" average reward: 0.2700, min: -0.2593, max: 1.1296, stdev: 0.2285
External policy "individual greedy" average reward: 0.5547, min: -0.0370, max: 1.2222, stdev: 0.2281
External policy "total greedy" average reward: 0.6637, min: -0.0093, max: 1.2870, stdev: 0.2268
New network won 66 and tied 168 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 675 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.03 seconds
Training examples lengths: [64961, 64744, 64699, 64940, 64898, 64846, 64871, 64883, 64796, 64717]
Total value: 500099.83
Training on 648355 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2000 (value: 0.0009, weighted value: 0.0448, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9361
Epoch 2/10, Train Loss: 0.1932 (value: 0.0008, weighted value: 0.0417, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9368
Epoch 3/10, Train Loss: 0.1873 (value: 0.0008, weighted value: 0.0379, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9371
Epoch 4/10, Train Loss: 0.1842 (value: 0.0007, weighted value: 0.0364, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9374
Epoch 5/10, Train Loss: 0.1824 (value: 0.0007, weighted value: 0.0355, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9375
Epoch 6/10, Train Loss: 0.1808 (value: 0.0007, weighted value: 0.0341, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9378
Epoch 7/10, Train Loss: 0.1800 (value: 0.0007, weighted value: 0.0337, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9380
Epoch 8/10, Train Loss: 0.1778 (value: 0.0006, weighted value: 0.0314, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1771 (value: 0.0006, weighted value: 0.0316, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1760 (value: 0.0006, weighted value: 0.0302, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9385
..training done in 74.73 seconds
..evaluation done in 25.77 seconds
Old network+MCTS average reward: 0.7527, min: 0.1389, max: 1.6204, stdev: 0.2332
New network+MCTS average reward: 0.7519, min: 0.2130, max: 1.6296, stdev: 0.2311
Old bare network average reward: 0.7233, min: 0.1852, max: 1.6204, stdev: 0.2317
New bare network average reward: 0.7256, min: 0.2037, max: 1.6296, stdev: 0.2338
External policy "random" average reward: 0.2555, min: -0.2778, max: 1.0093, stdev: 0.2159
External policy "individual greedy" average reward: 0.5348, min: 0.0093, max: 1.2500, stdev: 0.2105
External policy "total greedy" average reward: 0.6521, min: 0.1111, max: 1.4352, stdev: 0.2091
New network won 60 and tied 173 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 676 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.75 seconds
Training examples lengths: [64744, 64699, 64940, 64898, 64846, 64871, 64883, 64796, 64717, 64538]
Total value: 499608.71
Training on 647932 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2180 (value: 0.0011, weighted value: 0.0553, policy: 0.1627, weighted policy: 0.1627), Train Mean Max: 0.9346
Epoch 2/10, Train Loss: 0.2044 (value: 0.0010, weighted value: 0.0479, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9357
Epoch 3/10, Train Loss: 0.1992 (value: 0.0009, weighted value: 0.0461, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9361
Epoch 4/10, Train Loss: 0.1917 (value: 0.0008, weighted value: 0.0415, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9365
Epoch 5/10, Train Loss: 0.1888 (value: 0.0008, weighted value: 0.0398, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9368
Epoch 6/10, Train Loss: 0.1855 (value: 0.0007, weighted value: 0.0372, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9370
Epoch 7/10, Train Loss: 0.1853 (value: 0.0008, weighted value: 0.0375, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9372
Epoch 8/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0353, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1816 (value: 0.0007, weighted value: 0.0348, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1788 (value: 0.0006, weighted value: 0.0322, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9378
..training done in 74.51 seconds
..evaluation done in 18.76 seconds
Old network+MCTS average reward: 0.7664, min: 0.0926, max: 1.6852, stdev: 0.2408
New network+MCTS average reward: 0.7681, min: 0.0833, max: 1.6852, stdev: 0.2396
Old bare network average reward: 0.7401, min: 0.0648, max: 1.6852, stdev: 0.2438
New bare network average reward: 0.7398, min: 0.0741, max: 1.6111, stdev: 0.2423
External policy "random" average reward: 0.2672, min: -0.2963, max: 1.1574, stdev: 0.2319
External policy "individual greedy" average reward: 0.5501, min: -0.0463, max: 1.3611, stdev: 0.2300
External policy "total greedy" average reward: 0.6649, min: 0.1296, max: 1.5370, stdev: 0.2245
New network won 67 and tied 175 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 677 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.82 seconds
Training examples lengths: [64699, 64940, 64898, 64846, 64871, 64883, 64796, 64717, 64538, 64483]
Total value: 499523.15
Training on 647671 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1980 (value: 0.0009, weighted value: 0.0431, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9363
Epoch 2/10, Train Loss: 0.1913 (value: 0.0008, weighted value: 0.0400, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9371
Epoch 3/10, Train Loss: 0.1849 (value: 0.0007, weighted value: 0.0364, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9376
Epoch 4/10, Train Loss: 0.1840 (value: 0.0007, weighted value: 0.0362, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1805 (value: 0.0007, weighted value: 0.0336, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9378
Epoch 6/10, Train Loss: 0.1796 (value: 0.0007, weighted value: 0.0335, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9381
Epoch 7/10, Train Loss: 0.1783 (value: 0.0006, weighted value: 0.0319, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9381
Epoch 8/10, Train Loss: 0.1782 (value: 0.0006, weighted value: 0.0317, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9383
Epoch 9/10, Train Loss: 0.1755 (value: 0.0006, weighted value: 0.0300, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9385
Epoch 10/10, Train Loss: 0.1744 (value: 0.0006, weighted value: 0.0292, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9387
..training done in 65.16 seconds
..evaluation done in 19.41 seconds
Old network+MCTS average reward: 0.7778, min: 0.0833, max: 1.6944, stdev: 0.2456
New network+MCTS average reward: 0.7772, min: 0.0000, max: 1.6574, stdev: 0.2467
Old bare network average reward: 0.7502, min: 0.0000, max: 1.6759, stdev: 0.2473
New bare network average reward: 0.7438, min: 0.0000, max: 1.6574, stdev: 0.2462
External policy "random" average reward: 0.2556, min: -0.3796, max: 1.0278, stdev: 0.2368
External policy "individual greedy" average reward: 0.5464, min: -0.1944, max: 1.5000, stdev: 0.2407
External policy "total greedy" average reward: 0.6614, min: 0.0463, max: 1.6111, stdev: 0.2371
New network won 61 and tied 179 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 678 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.48 seconds
Training examples lengths: [64940, 64898, 64846, 64871, 64883, 64796, 64717, 64538, 64483, 65020]
Total value: 499989.30
Training on 647992 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1944 (value: 0.0008, weighted value: 0.0401, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9371
Epoch 2/10, Train Loss: 0.1871 (value: 0.0007, weighted value: 0.0358, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9378
Epoch 3/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0344, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9381
Epoch 4/10, Train Loss: 0.1804 (value: 0.0007, weighted value: 0.0334, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9382
Epoch 5/10, Train Loss: 0.1786 (value: 0.0006, weighted value: 0.0324, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9384
Epoch 6/10, Train Loss: 0.1758 (value: 0.0006, weighted value: 0.0304, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9385
Epoch 7/10, Train Loss: 0.1760 (value: 0.0006, weighted value: 0.0303, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9387
Epoch 8/10, Train Loss: 0.1751 (value: 0.0006, weighted value: 0.0297, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9388
Epoch 9/10, Train Loss: 0.1727 (value: 0.0006, weighted value: 0.0278, policy: 0.1449, weighted policy: 0.1449), Train Mean Max: 0.9390
Epoch 10/10, Train Loss: 0.1730 (value: 0.0006, weighted value: 0.0281, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9390
..training done in 61.63 seconds
..evaluation done in 18.47 seconds
Old network+MCTS average reward: 0.7794, min: 0.1389, max: 1.6759, stdev: 0.2296
New network+MCTS average reward: 0.7756, min: 0.1389, max: 1.6389, stdev: 0.2320
Old bare network average reward: 0.7495, min: 0.1389, max: 1.5463, stdev: 0.2309
New bare network average reward: 0.7471, min: 0.1389, max: 1.5463, stdev: 0.2312
External policy "random" average reward: 0.2729, min: -0.2870, max: 1.2500, stdev: 0.2355
External policy "individual greedy" average reward: 0.5538, min: -0.0093, max: 1.6296, stdev: 0.2372
External policy "total greedy" average reward: 0.6745, min: 0.0648, max: 1.5000, stdev: 0.2291
New network won 50 and tied 180 out of 300 games (46.67% wins where ties are half wins)
Reverting to the old network

Training iteration 679 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.25 seconds
Training examples lengths: [64898, 64846, 64871, 64883, 64796, 64717, 64538, 64483, 65020, 65082]
Total value: 500393.26
Training on 648134 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2129 (value: 0.0010, weighted value: 0.0508, policy: 0.1621, weighted policy: 0.1621), Train Mean Max: 0.9357
Epoch 2/10, Train Loss: 0.1982 (value: 0.0009, weighted value: 0.0435, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9368
Epoch 3/10, Train Loss: 0.1914 (value: 0.0008, weighted value: 0.0401, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9372
Epoch 4/10, Train Loss: 0.1874 (value: 0.0008, weighted value: 0.0383, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9375
Epoch 5/10, Train Loss: 0.1843 (value: 0.0007, weighted value: 0.0368, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9377
Epoch 6/10, Train Loss: 0.1816 (value: 0.0007, weighted value: 0.0347, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9378
Epoch 7/10, Train Loss: 0.1797 (value: 0.0007, weighted value: 0.0333, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9381
Epoch 8/10, Train Loss: 0.1794 (value: 0.0007, weighted value: 0.0332, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1767 (value: 0.0006, weighted value: 0.0312, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9385
Epoch 10/10, Train Loss: 0.1754 (value: 0.0006, weighted value: 0.0303, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9386
..training done in 60.08 seconds
..evaluation done in 19.13 seconds
Old network+MCTS average reward: 0.7909, min: 0.0370, max: 1.5000, stdev: 0.2457
New network+MCTS average reward: 0.7899, min: 0.0463, max: 1.5000, stdev: 0.2437
Old bare network average reward: 0.7616, min: 0.0370, max: 1.5000, stdev: 0.2479
New bare network average reward: 0.7616, min: 0.0370, max: 1.5000, stdev: 0.2455
External policy "random" average reward: 0.2696, min: -0.3889, max: 1.0926, stdev: 0.2432
External policy "individual greedy" average reward: 0.5579, min: -0.0648, max: 1.3056, stdev: 0.2540
External policy "total greedy" average reward: 0.6842, min: -0.0370, max: 1.3981, stdev: 0.2394
New network won 64 and tied 176 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 680 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.89 seconds
Training examples lengths: [64846, 64871, 64883, 64796, 64717, 64538, 64483, 65020, 65082, 64756]
Total value: 500250.73
Training on 647992 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1954 (value: 0.0008, weighted value: 0.0411, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9369
Epoch 2/10, Train Loss: 0.1879 (value: 0.0007, weighted value: 0.0364, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9375
Epoch 3/10, Train Loss: 0.1837 (value: 0.0007, weighted value: 0.0353, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9379
Epoch 4/10, Train Loss: 0.1809 (value: 0.0007, weighted value: 0.0337, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9380
Epoch 5/10, Train Loss: 0.1788 (value: 0.0006, weighted value: 0.0325, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9382
Epoch 6/10, Train Loss: 0.1766 (value: 0.0006, weighted value: 0.0306, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9384
Epoch 7/10, Train Loss: 0.1756 (value: 0.0006, weighted value: 0.0303, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9386
Epoch 8/10, Train Loss: 0.1751 (value: 0.0006, weighted value: 0.0295, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9385
Epoch 9/10, Train Loss: 0.1735 (value: 0.0006, weighted value: 0.0281, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9388
Epoch 10/10, Train Loss: 0.1750 (value: 0.0006, weighted value: 0.0299, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9389
..training done in 60.99 seconds
..evaluation done in 18.19 seconds
Old network+MCTS average reward: 0.7480, min: 0.0926, max: 1.5370, stdev: 0.2404
New network+MCTS average reward: 0.7463, min: 0.0926, max: 1.4352, stdev: 0.2399
Old bare network average reward: 0.7175, min: 0.0833, max: 1.3889, stdev: 0.2377
New bare network average reward: 0.7173, min: 0.0278, max: 1.3889, stdev: 0.2403
External policy "random" average reward: 0.2430, min: -0.3796, max: 0.9167, stdev: 0.2301
External policy "individual greedy" average reward: 0.5273, min: 0.0185, max: 1.1389, stdev: 0.2275
External policy "total greedy" average reward: 0.6420, min: 0.0370, max: 1.3148, stdev: 0.2227
New network won 56 and tied 180 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_680

Training iteration 681 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.79 seconds
Training examples lengths: [64871, 64883, 64796, 64717, 64538, 64483, 65020, 65082, 64756, 64888]
Total value: 500428.32
Training on 648034 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2147 (value: 0.0010, weighted value: 0.0513, policy: 0.1635, weighted policy: 0.1635), Train Mean Max: 0.9352
Epoch 2/10, Train Loss: 0.2005 (value: 0.0009, weighted value: 0.0445, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9365
Epoch 3/10, Train Loss: 0.1919 (value: 0.0008, weighted value: 0.0402, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9370
Epoch 4/10, Train Loss: 0.1875 (value: 0.0008, weighted value: 0.0380, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9372
Epoch 5/10, Train Loss: 0.1848 (value: 0.0007, weighted value: 0.0367, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9373
Epoch 6/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0358, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9374
Epoch 7/10, Train Loss: 0.1807 (value: 0.0007, weighted value: 0.0336, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9376
Epoch 8/10, Train Loss: 0.1792 (value: 0.0006, weighted value: 0.0324, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1791 (value: 0.0006, weighted value: 0.0322, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9380
Epoch 10/10, Train Loss: 0.1770 (value: 0.0006, weighted value: 0.0304, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9382
..training done in 59.67 seconds
..evaluation done in 18.04 seconds
Old network+MCTS average reward: 0.7748, min: 0.1111, max: 1.4907, stdev: 0.2442
New network+MCTS average reward: 0.7736, min: 0.1111, max: 1.4907, stdev: 0.2419
Old bare network average reward: 0.7478, min: 0.0185, max: 1.4167, stdev: 0.2472
New bare network average reward: 0.7491, min: -0.0278, max: 1.4907, stdev: 0.2462
External policy "random" average reward: 0.2670, min: -0.3148, max: 1.0370, stdev: 0.2374
External policy "individual greedy" average reward: 0.5543, min: -0.1204, max: 1.2315, stdev: 0.2405
External policy "total greedy" average reward: 0.6684, min: 0.0556, max: 1.3333, stdev: 0.2335
New network won 63 and tied 175 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 682 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.83 seconds
Training examples lengths: [64883, 64796, 64717, 64538, 64483, 65020, 65082, 64756, 64888, 64780]
Total value: 500338.81
Training on 647943 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1962 (value: 0.0008, weighted value: 0.0409, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9366
Epoch 2/10, Train Loss: 0.1881 (value: 0.0007, weighted value: 0.0362, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9373
Epoch 3/10, Train Loss: 0.1851 (value: 0.0007, weighted value: 0.0356, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9376
Epoch 4/10, Train Loss: 0.1821 (value: 0.0007, weighted value: 0.0340, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9379
Epoch 5/10, Train Loss: 0.1793 (value: 0.0006, weighted value: 0.0318, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9380
Epoch 6/10, Train Loss: 0.1782 (value: 0.0006, weighted value: 0.0316, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9383
Epoch 7/10, Train Loss: 0.1776 (value: 0.0006, weighted value: 0.0308, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9382
Epoch 8/10, Train Loss: 0.1750 (value: 0.0006, weighted value: 0.0289, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1746 (value: 0.0006, weighted value: 0.0287, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9386
Epoch 10/10, Train Loss: 0.1737 (value: 0.0006, weighted value: 0.0276, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9387
..training done in 59.23 seconds
..evaluation done in 18.78 seconds
Old network+MCTS average reward: 0.7748, min: 0.0185, max: 1.6111, stdev: 0.2326
New network+MCTS average reward: 0.7730, min: 0.0463, max: 1.6759, stdev: 0.2327
Old bare network average reward: 0.7426, min: 0.0185, max: 1.5741, stdev: 0.2327
New bare network average reward: 0.7450, min: 0.0463, max: 1.6574, stdev: 0.2348
External policy "random" average reward: 0.2602, min: -0.3056, max: 1.1944, stdev: 0.2420
External policy "individual greedy" average reward: 0.5541, min: 0.0000, max: 1.4167, stdev: 0.2326
External policy "total greedy" average reward: 0.6675, min: 0.1667, max: 1.5648, stdev: 0.2241
New network won 61 and tied 170 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 683 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.62 seconds
Training examples lengths: [64796, 64717, 64538, 64483, 65020, 65082, 64756, 64888, 64780, 64974]
Total value: 500926.10
Training on 648034 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2150 (value: 0.0010, weighted value: 0.0516, policy: 0.1635, weighted policy: 0.1635), Train Mean Max: 0.9350
Epoch 2/10, Train Loss: 0.2023 (value: 0.0009, weighted value: 0.0447, policy: 0.1575, weighted policy: 0.1575), Train Mean Max: 0.9361
Epoch 3/10, Train Loss: 0.1927 (value: 0.0008, weighted value: 0.0409, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9367
Epoch 4/10, Train Loss: 0.1891 (value: 0.0008, weighted value: 0.0394, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9370
Epoch 5/10, Train Loss: 0.1856 (value: 0.0007, weighted value: 0.0367, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9370
Epoch 6/10, Train Loss: 0.1839 (value: 0.0007, weighted value: 0.0352, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9373
Epoch 7/10, Train Loss: 0.1831 (value: 0.0007, weighted value: 0.0344, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1810 (value: 0.0007, weighted value: 0.0335, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9377
Epoch 9/10, Train Loss: 0.1784 (value: 0.0006, weighted value: 0.0311, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1779 (value: 0.0006, weighted value: 0.0306, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9380
..training done in 59.57 seconds
..evaluation done in 18.93 seconds
Old network+MCTS average reward: 0.7800, min: 0.2222, max: 1.4907, stdev: 0.2352
New network+MCTS average reward: 0.7830, min: 0.2222, max: 1.4907, stdev: 0.2341
Old bare network average reward: 0.7554, min: 0.2222, max: 1.4074, stdev: 0.2348
New bare network average reward: 0.7526, min: 0.2222, max: 1.4074, stdev: 0.2336
External policy "random" average reward: 0.2758, min: -0.4722, max: 1.0926, stdev: 0.2453
External policy "individual greedy" average reward: 0.5468, min: -0.0741, max: 1.2963, stdev: 0.2436
External policy "total greedy" average reward: 0.6641, min: 0.0833, max: 1.4259, stdev: 0.2281
New network won 78 and tied 168 out of 300 games (54.00% wins where ties are half wins)
Keeping the new network

Training iteration 684 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.27 seconds
Training examples lengths: [64717, 64538, 64483, 65020, 65082, 64756, 64888, 64780, 64974, 64814]
Total value: 500743.36
Training on 648052 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1963 (value: 0.0008, weighted value: 0.0405, policy: 0.1558, weighted policy: 0.1558), Train Mean Max: 0.9364
Epoch 2/10, Train Loss: 0.1897 (value: 0.0008, weighted value: 0.0376, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9372
Epoch 3/10, Train Loss: 0.1844 (value: 0.0007, weighted value: 0.0354, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9374
Epoch 4/10, Train Loss: 0.1814 (value: 0.0007, weighted value: 0.0330, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1816 (value: 0.0007, weighted value: 0.0339, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9378
Epoch 6/10, Train Loss: 0.1783 (value: 0.0006, weighted value: 0.0310, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9379
Epoch 7/10, Train Loss: 0.1769 (value: 0.0006, weighted value: 0.0300, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9380
Epoch 8/10, Train Loss: 0.1770 (value: 0.0006, weighted value: 0.0306, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9383
Epoch 9/10, Train Loss: 0.1743 (value: 0.0006, weighted value: 0.0283, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1742 (value: 0.0006, weighted value: 0.0280, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9384
..training done in 65.72 seconds
..evaluation done in 19.38 seconds
Old network+MCTS average reward: 0.7427, min: 0.1296, max: 1.4259, stdev: 0.2358
New network+MCTS average reward: 0.7408, min: 0.0926, max: 1.4259, stdev: 0.2393
Old bare network average reward: 0.7122, min: 0.0926, max: 1.4259, stdev: 0.2403
New bare network average reward: 0.7121, min: 0.0833, max: 1.4259, stdev: 0.2390
External policy "random" average reward: 0.2551, min: -0.2963, max: 1.0463, stdev: 0.2221
External policy "individual greedy" average reward: 0.5152, min: -0.0093, max: 1.3056, stdev: 0.2189
External policy "total greedy" average reward: 0.6348, min: 0.0926, max: 1.4167, stdev: 0.2171
New network won 50 and tied 195 out of 300 games (49.17% wins where ties are half wins)
Reverting to the old network

Training iteration 685 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.61 seconds
Training examples lengths: [64538, 64483, 65020, 65082, 64756, 64888, 64780, 64974, 64814, 64880]
Total value: 501020.39
Training on 648215 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2169 (value: 0.0011, weighted value: 0.0532, policy: 0.1636, weighted policy: 0.1636), Train Mean Max: 0.9349
Epoch 2/10, Train Loss: 0.2014 (value: 0.0009, weighted value: 0.0455, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9362
Epoch 3/10, Train Loss: 0.1941 (value: 0.0008, weighted value: 0.0413, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9364
Epoch 4/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0397, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9368
Epoch 5/10, Train Loss: 0.1863 (value: 0.0007, weighted value: 0.0373, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9371
Epoch 6/10, Train Loss: 0.1842 (value: 0.0007, weighted value: 0.0353, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1824 (value: 0.0007, weighted value: 0.0347, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1814 (value: 0.0007, weighted value: 0.0333, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1790 (value: 0.0006, weighted value: 0.0318, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1792 (value: 0.0007, weighted value: 0.0326, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9379
..training done in 65.92 seconds
..evaluation done in 18.25 seconds
Old network+MCTS average reward: 0.7527, min: 0.2222, max: 1.4815, stdev: 0.2231
New network+MCTS average reward: 0.7537, min: 0.2037, max: 1.4537, stdev: 0.2251
Old bare network average reward: 0.7249, min: 0.2222, max: 1.4907, stdev: 0.2249
New bare network average reward: 0.7232, min: 0.2037, max: 1.4907, stdev: 0.2256
External policy "random" average reward: 0.2670, min: -0.3241, max: 0.9722, stdev: 0.2165
External policy "individual greedy" average reward: 0.5329, min: -0.0370, max: 1.4259, stdev: 0.2155
External policy "total greedy" average reward: 0.6487, min: 0.0741, max: 1.5556, stdev: 0.2186
New network won 64 and tied 179 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 686 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.53 seconds
Training examples lengths: [64483, 65020, 65082, 64756, 64888, 64780, 64974, 64814, 64880, 64931]
Total value: 501580.06
Training on 648608 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1965 (value: 0.0008, weighted value: 0.0406, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9364
Epoch 2/10, Train Loss: 0.1898 (value: 0.0008, weighted value: 0.0377, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9372
Epoch 3/10, Train Loss: 0.1845 (value: 0.0007, weighted value: 0.0353, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9374
Epoch 4/10, Train Loss: 0.1811 (value: 0.0007, weighted value: 0.0334, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9378
Epoch 5/10, Train Loss: 0.1797 (value: 0.0006, weighted value: 0.0321, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9378
Epoch 6/10, Train Loss: 0.1788 (value: 0.0006, weighted value: 0.0318, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9379
Epoch 7/10, Train Loss: 0.1771 (value: 0.0006, weighted value: 0.0300, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9381
Epoch 8/10, Train Loss: 0.1768 (value: 0.0006, weighted value: 0.0299, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9383
Epoch 9/10, Train Loss: 0.1751 (value: 0.0006, weighted value: 0.0288, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1740 (value: 0.0006, weighted value: 0.0280, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9386
..training done in 66.27 seconds
..evaluation done in 18.69 seconds
Old network+MCTS average reward: 0.7496, min: -0.0926, max: 1.4167, stdev: 0.2415
New network+MCTS average reward: 0.7469, min: -0.0926, max: 1.4167, stdev: 0.2419
Old bare network average reward: 0.7216, min: -0.0926, max: 1.3426, stdev: 0.2419
New bare network average reward: 0.7180, min: -0.0926, max: 1.3426, stdev: 0.2420
External policy "random" average reward: 0.2505, min: -0.3426, max: 0.8704, stdev: 0.2318
External policy "individual greedy" average reward: 0.5291, min: -0.0278, max: 1.2593, stdev: 0.2225
External policy "total greedy" average reward: 0.6341, min: -0.0093, max: 1.2778, stdev: 0.2195
New network won 51 and tied 180 out of 300 games (47.00% wins where ties are half wins)
Reverting to the old network

Training iteration 687 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.99 seconds
Training examples lengths: [65020, 65082, 64756, 64888, 64780, 64974, 64814, 64880, 64931, 64608]
Total value: 502037.35
Training on 648733 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2147 (value: 0.0010, weighted value: 0.0508, policy: 0.1639, weighted policy: 0.1639), Train Mean Max: 0.9351
Epoch 2/10, Train Loss: 0.1998 (value: 0.0009, weighted value: 0.0439, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9363
Epoch 3/10, Train Loss: 0.1933 (value: 0.0008, weighted value: 0.0415, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9367
Epoch 4/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0393, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9369
Epoch 5/10, Train Loss: 0.1855 (value: 0.0007, weighted value: 0.0366, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9370
Epoch 6/10, Train Loss: 0.1838 (value: 0.0007, weighted value: 0.0354, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9372
Epoch 7/10, Train Loss: 0.1818 (value: 0.0007, weighted value: 0.0332, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9374
Epoch 8/10, Train Loss: 0.1802 (value: 0.0007, weighted value: 0.0331, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9376
Epoch 9/10, Train Loss: 0.1794 (value: 0.0006, weighted value: 0.0322, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1784 (value: 0.0006, weighted value: 0.0315, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9380
..training done in 64.80 seconds
..evaluation done in 19.08 seconds
Old network+MCTS average reward: 0.7311, min: 0.0648, max: 1.7315, stdev: 0.2398
New network+MCTS average reward: 0.7323, min: 0.1111, max: 1.7315, stdev: 0.2406
Old bare network average reward: 0.7045, min: 0.0370, max: 1.7130, stdev: 0.2456
New bare network average reward: 0.7041, min: 0.0370, max: 1.7315, stdev: 0.2442
External policy "random" average reward: 0.2300, min: -0.4815, max: 1.0000, stdev: 0.2295
External policy "individual greedy" average reward: 0.5028, min: -0.1204, max: 1.5185, stdev: 0.2350
External policy "total greedy" average reward: 0.6257, min: 0.0093, max: 1.5741, stdev: 0.2254
New network won 63 and tied 171 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 688 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.33 seconds
Training examples lengths: [65082, 64756, 64888, 64780, 64974, 64814, 64880, 64931, 64608, 64642]
Total value: 501605.83
Training on 648355 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2298 (value: 0.0012, weighted value: 0.0615, policy: 0.1684, weighted policy: 0.1684), Train Mean Max: 0.9342
Epoch 2/10, Train Loss: 0.2125 (value: 0.0010, weighted value: 0.0518, policy: 0.1607, weighted policy: 0.1607), Train Mean Max: 0.9352
Epoch 3/10, Train Loss: 0.2027 (value: 0.0010, weighted value: 0.0480, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9358
Epoch 4/10, Train Loss: 0.1954 (value: 0.0009, weighted value: 0.0437, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9362
Epoch 5/10, Train Loss: 0.1912 (value: 0.0008, weighted value: 0.0409, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9365
Epoch 6/10, Train Loss: 0.1876 (value: 0.0008, weighted value: 0.0386, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9366
Epoch 7/10, Train Loss: 0.1871 (value: 0.0008, weighted value: 0.0379, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9368
Epoch 8/10, Train Loss: 0.1844 (value: 0.0007, weighted value: 0.0359, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9370
Epoch 9/10, Train Loss: 0.1826 (value: 0.0007, weighted value: 0.0340, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9373
Epoch 10/10, Train Loss: 0.1814 (value: 0.0007, weighted value: 0.0337, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9374
..training done in 63.68 seconds
..evaluation done in 19.23 seconds
Old network+MCTS average reward: 0.7677, min: 0.2500, max: 1.5000, stdev: 0.2208
New network+MCTS average reward: 0.7681, min: 0.2500, max: 1.5000, stdev: 0.2255
Old bare network average reward: 0.7359, min: 0.2222, max: 1.4630, stdev: 0.2284
New bare network average reward: 0.7415, min: 0.2037, max: 1.4722, stdev: 0.2294
External policy "random" average reward: 0.2683, min: -0.3889, max: 1.0093, stdev: 0.2235
External policy "individual greedy" average reward: 0.5553, min: -0.0370, max: 1.2963, stdev: 0.2301
External policy "total greedy" average reward: 0.6631, min: 0.1019, max: 1.4167, stdev: 0.2185
New network won 62 and tied 174 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 689 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.08 seconds
Training examples lengths: [64756, 64888, 64780, 64974, 64814, 64880, 64931, 64608, 64642, 65016]
Total value: 501611.17
Training on 648289 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2476 (value: 0.0014, weighted value: 0.0716, policy: 0.1759, weighted policy: 0.1759), Train Mean Max: 0.9327
Epoch 2/10, Train Loss: 0.2221 (value: 0.0012, weighted value: 0.0583, policy: 0.1638, weighted policy: 0.1638), Train Mean Max: 0.9343
Epoch 3/10, Train Loss: 0.2108 (value: 0.0011, weighted value: 0.0530, policy: 0.1577, weighted policy: 0.1577), Train Mean Max: 0.9349
Epoch 4/10, Train Loss: 0.2035 (value: 0.0010, weighted value: 0.0493, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9352
Epoch 5/10, Train Loss: 0.1972 (value: 0.0009, weighted value: 0.0453, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9356
Epoch 6/10, Train Loss: 0.1928 (value: 0.0008, weighted value: 0.0419, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9358
Epoch 7/10, Train Loss: 0.1907 (value: 0.0008, weighted value: 0.0405, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9361
Epoch 8/10, Train Loss: 0.1883 (value: 0.0008, weighted value: 0.0385, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9363
Epoch 9/10, Train Loss: 0.1868 (value: 0.0007, weighted value: 0.0374, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9365
Epoch 10/10, Train Loss: 0.1845 (value: 0.0007, weighted value: 0.0354, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9368
..training done in 65.08 seconds
..evaluation done in 18.04 seconds
Old network+MCTS average reward: 0.7423, min: -0.0185, max: 1.5741, stdev: 0.2488
New network+MCTS average reward: 0.7442, min: -0.0185, max: 1.6389, stdev: 0.2501
Old bare network average reward: 0.7184, min: -0.0185, max: 1.5556, stdev: 0.2502
New bare network average reward: 0.7186, min: 0.0463, max: 1.6389, stdev: 0.2513
External policy "random" average reward: 0.2596, min: -0.2778, max: 1.1481, stdev: 0.2292
External policy "individual greedy" average reward: 0.5414, min: 0.0741, max: 1.6019, stdev: 0.2317
External policy "total greedy" average reward: 0.6441, min: 0.1574, max: 1.6389, stdev: 0.2338
New network won 62 and tied 175 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 690 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.92 seconds
Training examples lengths: [64888, 64780, 64974, 64814, 64880, 64931, 64608, 64642, 65016, 64774]
Total value: 502434.30
Training on 648307 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2620 (value: 0.0016, weighted value: 0.0818, policy: 0.1801, weighted policy: 0.1801), Train Mean Max: 0.9318
Epoch 2/10, Train Loss: 0.2325 (value: 0.0013, weighted value: 0.0660, policy: 0.1665, weighted policy: 0.1665), Train Mean Max: 0.9335
Epoch 3/10, Train Loss: 0.2167 (value: 0.0011, weighted value: 0.0575, policy: 0.1592, weighted policy: 0.1592), Train Mean Max: 0.9344
Epoch 4/10, Train Loss: 0.2113 (value: 0.0011, weighted value: 0.0550, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9345
Epoch 5/10, Train Loss: 0.2024 (value: 0.0010, weighted value: 0.0487, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9350
Epoch 6/10, Train Loss: 0.1986 (value: 0.0009, weighted value: 0.0459, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9352
Epoch 7/10, Train Loss: 0.1967 (value: 0.0009, weighted value: 0.0452, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9354
Epoch 8/10, Train Loss: 0.1914 (value: 0.0008, weighted value: 0.0407, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9357
Epoch 9/10, Train Loss: 0.1907 (value: 0.0008, weighted value: 0.0408, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9361
Epoch 10/10, Train Loss: 0.1880 (value: 0.0008, weighted value: 0.0387, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9362
..training done in 62.08 seconds
..evaluation done in 18.78 seconds
Old network+MCTS average reward: 0.7415, min: 0.0463, max: 1.5278, stdev: 0.2236
New network+MCTS average reward: 0.7435, min: 0.0463, max: 1.5278, stdev: 0.2274
Old bare network average reward: 0.7117, min: 0.0463, max: 1.5278, stdev: 0.2285
New bare network average reward: 0.7131, min: 0.0463, max: 1.5278, stdev: 0.2307
External policy "random" average reward: 0.2553, min: -0.3611, max: 0.9907, stdev: 0.2198
External policy "individual greedy" average reward: 0.5390, min: -0.0463, max: 1.1204, stdev: 0.2213
External policy "total greedy" average reward: 0.6465, min: 0.0463, max: 1.2593, stdev: 0.2147
New network won 72 and tied 161 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 691 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.40 seconds
Training examples lengths: [64780, 64974, 64814, 64880, 64931, 64608, 64642, 65016, 64774, 64826]
Total value: 502586.40
Training on 648245 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2034 (value: 0.0009, weighted value: 0.0469, policy: 0.1566, weighted policy: 0.1566), Train Mean Max: 0.9351
Epoch 2/10, Train Loss: 0.1963 (value: 0.0009, weighted value: 0.0431, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9358
Epoch 3/10, Train Loss: 0.1917 (value: 0.0008, weighted value: 0.0404, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9363
Epoch 4/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0390, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9365
Epoch 5/10, Train Loss: 0.1864 (value: 0.0007, weighted value: 0.0373, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9367
Epoch 6/10, Train Loss: 0.1849 (value: 0.0007, weighted value: 0.0362, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9368
Epoch 7/10, Train Loss: 0.1823 (value: 0.0007, weighted value: 0.0337, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9371
Epoch 8/10, Train Loss: 0.1815 (value: 0.0007, weighted value: 0.0334, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9373
Epoch 9/10, Train Loss: 0.1806 (value: 0.0007, weighted value: 0.0326, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9375
Epoch 10/10, Train Loss: 0.1787 (value: 0.0006, weighted value: 0.0314, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9376
..training done in 66.09 seconds
..evaluation done in 18.03 seconds
Old network+MCTS average reward: 0.7773, min: 0.0556, max: 1.6481, stdev: 0.2508
New network+MCTS average reward: 0.7772, min: 0.0556, max: 1.6481, stdev: 0.2529
Old bare network average reward: 0.7484, min: 0.0556, max: 1.6481, stdev: 0.2548
New bare network average reward: 0.7453, min: 0.0463, max: 1.6481, stdev: 0.2555
External policy "random" average reward: 0.2767, min: -0.3611, max: 1.0000, stdev: 0.2375
External policy "individual greedy" average reward: 0.5626, min: 0.0185, max: 1.3333, stdev: 0.2351
External policy "total greedy" average reward: 0.6713, min: 0.1481, max: 1.4259, stdev: 0.2246
New network won 64 and tied 178 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 692 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 74.41 seconds
Training examples lengths: [64974, 64814, 64880, 64931, 64608, 64642, 65016, 64774, 64826, 64837]
Total value: 502214.50
Training on 648302 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1975 (value: 0.0009, weighted value: 0.0427, policy: 0.1548, weighted policy: 0.1548), Train Mean Max: 0.9364
Epoch 2/10, Train Loss: 0.1898 (value: 0.0008, weighted value: 0.0379, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9370
Epoch 3/10, Train Loss: 0.1838 (value: 0.0007, weighted value: 0.0349, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9374
Epoch 4/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0337, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9375
Epoch 5/10, Train Loss: 0.1812 (value: 0.0007, weighted value: 0.0340, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1782 (value: 0.0006, weighted value: 0.0310, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9379
Epoch 7/10, Train Loss: 0.1781 (value: 0.0006, weighted value: 0.0312, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1762 (value: 0.0006, weighted value: 0.0302, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9382
Epoch 9/10, Train Loss: 0.1760 (value: 0.0006, weighted value: 0.0297, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9383
Epoch 10/10, Train Loss: 0.1749 (value: 0.0006, weighted value: 0.0288, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9385
..training done in 65.80 seconds
..evaluation done in 17.98 seconds
Old network+MCTS average reward: 0.7769, min: 0.2685, max: 1.5370, stdev: 0.2340
New network+MCTS average reward: 0.7780, min: 0.2685, max: 1.5185, stdev: 0.2302
Old bare network average reward: 0.7494, min: 0.1852, max: 1.4630, stdev: 0.2300
New bare network average reward: 0.7527, min: 0.2222, max: 1.5185, stdev: 0.2304
External policy "random" average reward: 0.2904, min: -0.1944, max: 1.0648, stdev: 0.2254
External policy "individual greedy" average reward: 0.5581, min: -0.0556, max: 1.3241, stdev: 0.2142
External policy "total greedy" average reward: 0.6746, min: 0.1667, max: 1.4352, stdev: 0.2049
New network won 56 and tied 185 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 693 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.34 seconds
Training examples lengths: [64814, 64880, 64931, 64608, 64642, 65016, 64774, 64826, 64837, 64695]
Total value: 501510.78
Training on 648023 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2139 (value: 0.0011, weighted value: 0.0527, policy: 0.1612, weighted policy: 0.1612), Train Mean Max: 0.9351
Epoch 2/10, Train Loss: 0.2005 (value: 0.0009, weighted value: 0.0450, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9359
Epoch 3/10, Train Loss: 0.1932 (value: 0.0008, weighted value: 0.0412, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9366
Epoch 4/10, Train Loss: 0.1913 (value: 0.0008, weighted value: 0.0404, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9368
Epoch 5/10, Train Loss: 0.1848 (value: 0.0007, weighted value: 0.0363, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9372
Epoch 6/10, Train Loss: 0.1841 (value: 0.0007, weighted value: 0.0362, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9373
Epoch 7/10, Train Loss: 0.1814 (value: 0.0007, weighted value: 0.0345, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9374
Epoch 8/10, Train Loss: 0.1805 (value: 0.0007, weighted value: 0.0335, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1793 (value: 0.0007, weighted value: 0.0326, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1788 (value: 0.0006, weighted value: 0.0309, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9377
..training done in 63.96 seconds
..evaluation done in 18.68 seconds
Old network+MCTS average reward: 0.7634, min: 0.0556, max: 1.8056, stdev: 0.2468
New network+MCTS average reward: 0.7623, min: 0.0463, max: 1.8056, stdev: 0.2479
Old bare network average reward: 0.7351, min: 0.0833, max: 1.6389, stdev: 0.2480
New bare network average reward: 0.7389, min: 0.0185, max: 1.6574, stdev: 0.2484
External policy "random" average reward: 0.2721, min: -0.2963, max: 0.8704, stdev: 0.2267
External policy "individual greedy" average reward: 0.5389, min: -0.1019, max: 1.2593, stdev: 0.2423
External policy "total greedy" average reward: 0.6653, min: 0.0185, max: 1.5000, stdev: 0.2259
New network won 47 and tied 191 out of 300 games (47.50% wins where ties are half wins)
Reverting to the old network

Training iteration 694 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.85 seconds
Training examples lengths: [64880, 64931, 64608, 64642, 65016, 64774, 64826, 64837, 64695, 64621]
Total value: 501317.36
Training on 647830 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2307 (value: 0.0012, weighted value: 0.0623, policy: 0.1685, weighted policy: 0.1685), Train Mean Max: 0.9337
Epoch 2/10, Train Loss: 0.2127 (value: 0.0011, weighted value: 0.0527, policy: 0.1600, weighted policy: 0.1600), Train Mean Max: 0.9350
Epoch 3/10, Train Loss: 0.2023 (value: 0.0010, weighted value: 0.0476, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9356
Epoch 4/10, Train Loss: 0.1958 (value: 0.0009, weighted value: 0.0439, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9360
Epoch 5/10, Train Loss: 0.1923 (value: 0.0008, weighted value: 0.0422, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9362
Epoch 6/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0400, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9363
Epoch 7/10, Train Loss: 0.1863 (value: 0.0008, weighted value: 0.0375, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9367
Epoch 8/10, Train Loss: 0.1845 (value: 0.0007, weighted value: 0.0365, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9369
Epoch 9/10, Train Loss: 0.1831 (value: 0.0007, weighted value: 0.0351, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9371
Epoch 10/10, Train Loss: 0.1817 (value: 0.0007, weighted value: 0.0337, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9372
..training done in 59.52 seconds
..evaluation done in 18.39 seconds
Old network+MCTS average reward: 0.7748, min: 0.0093, max: 1.4907, stdev: 0.2285
New network+MCTS average reward: 0.7780, min: 0.0185, max: 1.4907, stdev: 0.2291
Old bare network average reward: 0.7496, min: -0.0278, max: 1.4907, stdev: 0.2285
New bare network average reward: 0.7490, min: 0.0370, max: 1.4907, stdev: 0.2302
External policy "random" average reward: 0.2676, min: -0.3426, max: 0.8056, stdev: 0.2107
External policy "individual greedy" average reward: 0.5500, min: -0.1204, max: 1.1389, stdev: 0.2180
External policy "total greedy" average reward: 0.6671, min: -0.1296, max: 1.2963, stdev: 0.2140
New network won 65 and tied 185 out of 300 games (52.50% wins where ties are half wins)
Keeping the new network

Training iteration 695 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 74.64 seconds
Training examples lengths: [64931, 64608, 64642, 65016, 64774, 64826, 64837, 64695, 64621, 64827]
Total value: 501414.58
Training on 647777 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1987 (value: 0.0009, weighted value: 0.0430, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9361
Epoch 2/10, Train Loss: 0.1913 (value: 0.0008, weighted value: 0.0391, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9367
Epoch 3/10, Train Loss: 0.1867 (value: 0.0007, weighted value: 0.0369, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9372
Epoch 4/10, Train Loss: 0.1846 (value: 0.0007, weighted value: 0.0357, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9372
Epoch 5/10, Train Loss: 0.1823 (value: 0.0007, weighted value: 0.0342, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1807 (value: 0.0007, weighted value: 0.0338, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9378
Epoch 7/10, Train Loss: 0.1780 (value: 0.0006, weighted value: 0.0310, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9378
Epoch 8/10, Train Loss: 0.1787 (value: 0.0006, weighted value: 0.0321, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9380
Epoch 9/10, Train Loss: 0.1771 (value: 0.0006, weighted value: 0.0300, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9382
Epoch 10/10, Train Loss: 0.1752 (value: 0.0006, weighted value: 0.0289, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9384
..training done in 63.66 seconds
..evaluation done in 17.59 seconds
Old network+MCTS average reward: 0.7402, min: 0.1204, max: 1.3704, stdev: 0.2385
New network+MCTS average reward: 0.7388, min: 0.1204, max: 1.3704, stdev: 0.2368
Old bare network average reward: 0.7109, min: -0.0093, max: 1.3611, stdev: 0.2400
New bare network average reward: 0.7086, min: -0.0093, max: 1.3611, stdev: 0.2422
External policy "random" average reward: 0.2630, min: -0.3704, max: 1.0556, stdev: 0.2264
External policy "individual greedy" average reward: 0.5247, min: -0.0926, max: 1.2963, stdev: 0.2243
External policy "total greedy" average reward: 0.6393, min: -0.0185, max: 1.3148, stdev: 0.2287
New network won 60 and tied 173 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 696 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.35 seconds
Training examples lengths: [64608, 64642, 65016, 64774, 64826, 64837, 64695, 64621, 64827, 64964]
Total value: 501384.83
Training on 647810 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2184 (value: 0.0011, weighted value: 0.0543, policy: 0.1642, weighted policy: 0.1642), Train Mean Max: 0.9347
Epoch 2/10, Train Loss: 0.2029 (value: 0.0009, weighted value: 0.0466, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9360
Epoch 3/10, Train Loss: 0.1947 (value: 0.0009, weighted value: 0.0428, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9364
Epoch 4/10, Train Loss: 0.1898 (value: 0.0008, weighted value: 0.0398, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9367
Epoch 5/10, Train Loss: 0.1870 (value: 0.0008, weighted value: 0.0380, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9368
Epoch 6/10, Train Loss: 0.1857 (value: 0.0008, weighted value: 0.0375, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1821 (value: 0.0007, weighted value: 0.0346, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9374
Epoch 8/10, Train Loss: 0.1820 (value: 0.0007, weighted value: 0.0351, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1801 (value: 0.0007, weighted value: 0.0328, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9376
Epoch 10/10, Train Loss: 0.1793 (value: 0.0006, weighted value: 0.0324, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9379
..training done in 67.56 seconds
..evaluation done in 18.96 seconds
Old network+MCTS average reward: 0.7514, min: 0.1667, max: 1.4907, stdev: 0.2199
New network+MCTS average reward: 0.7528, min: 0.2130, max: 1.4907, stdev: 0.2229
Old bare network average reward: 0.7206, min: 0.1759, max: 1.4907, stdev: 0.2227
New bare network average reward: 0.7212, min: 0.1204, max: 1.4907, stdev: 0.2252
External policy "random" average reward: 0.2496, min: -0.3519, max: 0.8981, stdev: 0.2195
External policy "individual greedy" average reward: 0.5231, min: -0.0370, max: 1.1667, stdev: 0.2052
External policy "total greedy" average reward: 0.6419, min: 0.1204, max: 1.2870, stdev: 0.2034
New network won 67 and tied 175 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 697 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.26 seconds
Training examples lengths: [64642, 65016, 64774, 64826, 64837, 64695, 64621, 64827, 64964, 64767]
Total value: 501498.47
Training on 647969 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1968 (value: 0.0008, weighted value: 0.0416, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9366
Epoch 2/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0381, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9375
Epoch 3/10, Train Loss: 0.1837 (value: 0.0007, weighted value: 0.0356, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9378
Epoch 4/10, Train Loss: 0.1823 (value: 0.0007, weighted value: 0.0345, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9379
Epoch 5/10, Train Loss: 0.1787 (value: 0.0006, weighted value: 0.0322, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9381
Epoch 6/10, Train Loss: 0.1791 (value: 0.0007, weighted value: 0.0325, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9382
Epoch 7/10, Train Loss: 0.1768 (value: 0.0006, weighted value: 0.0307, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9385
Epoch 8/10, Train Loss: 0.1750 (value: 0.0006, weighted value: 0.0297, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9385
Epoch 9/10, Train Loss: 0.1746 (value: 0.0006, weighted value: 0.0294, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9389
Epoch 10/10, Train Loss: 0.1741 (value: 0.0006, weighted value: 0.0290, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9388
..training done in 59.64 seconds
..evaluation done in 18.29 seconds
Old network+MCTS average reward: 0.7590, min: 0.1944, max: 1.5463, stdev: 0.2230
New network+MCTS average reward: 0.7572, min: 0.2037, max: 1.3981, stdev: 0.2220
Old bare network average reward: 0.7310, min: 0.0370, max: 1.3981, stdev: 0.2283
New bare network average reward: 0.7311, min: 0.1667, max: 1.3981, stdev: 0.2245
External policy "random" average reward: 0.2755, min: -0.3426, max: 0.9444, stdev: 0.2324
External policy "individual greedy" average reward: 0.5364, min: -0.0648, max: 1.2315, stdev: 0.2086
External policy "total greedy" average reward: 0.6556, min: -0.0278, max: 1.3704, stdev: 0.2023
New network won 64 and tied 164 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 698 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.47 seconds
Training examples lengths: [65016, 64774, 64826, 64837, 64695, 64621, 64827, 64964, 64767, 64626]
Total value: 502002.06
Training on 647953 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2153 (value: 0.0010, weighted value: 0.0523, policy: 0.1630, weighted policy: 0.1630), Train Mean Max: 0.9355
Epoch 2/10, Train Loss: 0.2001 (value: 0.0009, weighted value: 0.0445, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9366
Epoch 3/10, Train Loss: 0.1932 (value: 0.0008, weighted value: 0.0418, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9369
Epoch 4/10, Train Loss: 0.1882 (value: 0.0008, weighted value: 0.0391, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9371
Epoch 5/10, Train Loss: 0.1859 (value: 0.0008, weighted value: 0.0380, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9375
Epoch 6/10, Train Loss: 0.1832 (value: 0.0007, weighted value: 0.0356, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9377
Epoch 7/10, Train Loss: 0.1815 (value: 0.0007, weighted value: 0.0339, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9378
Epoch 8/10, Train Loss: 0.1797 (value: 0.0007, weighted value: 0.0331, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1781 (value: 0.0006, weighted value: 0.0323, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9383
Epoch 10/10, Train Loss: 0.1785 (value: 0.0006, weighted value: 0.0325, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9384
..training done in 59.59 seconds
..evaluation done in 18.31 seconds
Old network+MCTS average reward: 0.7537, min: 0.1944, max: 1.3889, stdev: 0.2302
New network+MCTS average reward: 0.7517, min: 0.1944, max: 1.4444, stdev: 0.2297
Old bare network average reward: 0.7244, min: 0.1481, max: 1.3889, stdev: 0.2352
New bare network average reward: 0.7238, min: 0.1759, max: 1.4444, stdev: 0.2372
External policy "random" average reward: 0.2428, min: -0.4167, max: 0.7778, stdev: 0.2328
External policy "individual greedy" average reward: 0.5264, min: -0.0926, max: 1.1574, stdev: 0.2311
External policy "total greedy" average reward: 0.6386, min: 0.0556, max: 1.1852, stdev: 0.2235
New network won 50 and tied 194 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 699 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.51 seconds
Training examples lengths: [64774, 64826, 64837, 64695, 64621, 64827, 64964, 64767, 64626, 64725]
Total value: 502100.11
Training on 647662 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2318 (value: 0.0012, weighted value: 0.0624, policy: 0.1694, weighted policy: 0.1694), Train Mean Max: 0.9340
Epoch 2/10, Train Loss: 0.2115 (value: 0.0010, weighted value: 0.0522, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9357
Epoch 3/10, Train Loss: 0.2019 (value: 0.0010, weighted value: 0.0484, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9361
Epoch 4/10, Train Loss: 0.1942 (value: 0.0009, weighted value: 0.0433, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9365
Epoch 5/10, Train Loss: 0.1920 (value: 0.0009, weighted value: 0.0426, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9367
Epoch 6/10, Train Loss: 0.1879 (value: 0.0008, weighted value: 0.0393, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9370
Epoch 7/10, Train Loss: 0.1860 (value: 0.0008, weighted value: 0.0381, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0359, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1827 (value: 0.0007, weighted value: 0.0360, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1807 (value: 0.0007, weighted value: 0.0334, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9378
..training done in 67.85 seconds
..evaluation done in 18.31 seconds
Old network+MCTS average reward: 0.7809, min: 0.1574, max: 1.5370, stdev: 0.2415
New network+MCTS average reward: 0.7801, min: 0.1574, max: 1.5741, stdev: 0.2438
Old bare network average reward: 0.7511, min: 0.0926, max: 1.5370, stdev: 0.2487
New bare network average reward: 0.7481, min: 0.0926, max: 1.5370, stdev: 0.2470
External policy "random" average reward: 0.2624, min: -0.3611, max: 0.9444, stdev: 0.2247
External policy "individual greedy" average reward: 0.5484, min: -0.0185, max: 1.2130, stdev: 0.2317
External policy "total greedy" average reward: 0.6681, min: 0.0926, max: 1.3333, stdev: 0.2318
New network won 62 and tied 174 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 700 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.02 seconds
Training examples lengths: [64826, 64837, 64695, 64621, 64827, 64964, 64767, 64626, 64725, 64645]
Total value: 501193.34
Training on 647533 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2456 (value: 0.0014, weighted value: 0.0706, policy: 0.1750, weighted policy: 0.1750), Train Mean Max: 0.9331
Epoch 2/10, Train Loss: 0.2213 (value: 0.0012, weighted value: 0.0586, policy: 0.1627, weighted policy: 0.1627), Train Mean Max: 0.9348
Epoch 3/10, Train Loss: 0.2086 (value: 0.0011, weighted value: 0.0525, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9355
Epoch 4/10, Train Loss: 0.2014 (value: 0.0010, weighted value: 0.0490, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9359
Epoch 5/10, Train Loss: 0.1970 (value: 0.0009, weighted value: 0.0464, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9363
Epoch 6/10, Train Loss: 0.1926 (value: 0.0009, weighted value: 0.0426, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9364
Epoch 7/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0407, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9368
Epoch 8/10, Train Loss: 0.1868 (value: 0.0008, weighted value: 0.0389, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9371
Epoch 9/10, Train Loss: 0.1847 (value: 0.0008, weighted value: 0.0376, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9373
Epoch 10/10, Train Loss: 0.1831 (value: 0.0007, weighted value: 0.0361, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9376
..training done in 59.16 seconds
..evaluation done in 17.73 seconds
Old network+MCTS average reward: 0.7790, min: 0.1667, max: 1.4815, stdev: 0.2442
New network+MCTS average reward: 0.7810, min: 0.1667, max: 1.4815, stdev: 0.2429
Old bare network average reward: 0.7496, min: 0.1019, max: 1.4352, stdev: 0.2488
New bare network average reward: 0.7490, min: 0.1019, max: 1.3889, stdev: 0.2462
External policy "random" average reward: 0.2849, min: -0.2407, max: 1.0278, stdev: 0.2345
External policy "individual greedy" average reward: 0.5599, min: 0.0278, max: 1.2963, stdev: 0.2376
External policy "total greedy" average reward: 0.6674, min: -0.0556, max: 1.3704, stdev: 0.2331
New network won 64 and tied 176 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_700

Training iteration 701 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.73 seconds
Training examples lengths: [64837, 64695, 64621, 64827, 64964, 64767, 64626, 64725, 64645, 64837]
Total value: 500957.31
Training on 647544 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1999 (value: 0.0009, weighted value: 0.0453, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9363
Epoch 2/10, Train Loss: 0.1920 (value: 0.0008, weighted value: 0.0407, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9371
Epoch 3/10, Train Loss: 0.1870 (value: 0.0008, weighted value: 0.0381, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9374
Epoch 4/10, Train Loss: 0.1849 (value: 0.0007, weighted value: 0.0371, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9377
Epoch 5/10, Train Loss: 0.1825 (value: 0.0007, weighted value: 0.0360, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9380
Epoch 6/10, Train Loss: 0.1796 (value: 0.0007, weighted value: 0.0335, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9382
Epoch 7/10, Train Loss: 0.1783 (value: 0.0007, weighted value: 0.0325, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9383
Epoch 8/10, Train Loss: 0.1780 (value: 0.0006, weighted value: 0.0321, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1768 (value: 0.0006, weighted value: 0.0318, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9388
Epoch 10/10, Train Loss: 0.1751 (value: 0.0006, weighted value: 0.0294, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9387
..training done in 59.18 seconds
..evaluation done in 18.16 seconds
Old network+MCTS average reward: 0.7497, min: 0.1852, max: 1.5926, stdev: 0.2400
New network+MCTS average reward: 0.7514, min: 0.2222, max: 1.5926, stdev: 0.2401
Old bare network average reward: 0.7195, min: 0.1574, max: 1.4907, stdev: 0.2435
New bare network average reward: 0.7221, min: 0.1389, max: 1.4907, stdev: 0.2434
External policy "random" average reward: 0.2440, min: -0.3333, max: 0.9815, stdev: 0.2262
External policy "individual greedy" average reward: 0.5282, min: 0.0185, max: 1.3611, stdev: 0.2263
External policy "total greedy" average reward: 0.6500, min: 0.1296, max: 1.4444, stdev: 0.2220
New network won 58 and tied 192 out of 300 games (51.33% wins where ties are half wins)
Keeping the new network

Training iteration 702 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.42 seconds
Training examples lengths: [64695, 64621, 64827, 64964, 64767, 64626, 64725, 64645, 64837, 64833]
Total value: 501700.59
Training on 647540 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1935 (value: 0.0008, weighted value: 0.0402, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9375
Epoch 2/10, Train Loss: 0.1868 (value: 0.0007, weighted value: 0.0367, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9382
Epoch 3/10, Train Loss: 0.1808 (value: 0.0007, weighted value: 0.0337, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9385
Epoch 4/10, Train Loss: 0.1784 (value: 0.0007, weighted value: 0.0330, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9387
Epoch 5/10, Train Loss: 0.1763 (value: 0.0006, weighted value: 0.0311, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9388
Epoch 6/10, Train Loss: 0.1757 (value: 0.0006, weighted value: 0.0312, policy: 0.1446, weighted policy: 0.1446), Train Mean Max: 0.9390
Epoch 7/10, Train Loss: 0.1741 (value: 0.0006, weighted value: 0.0300, policy: 0.1441, weighted policy: 0.1441), Train Mean Max: 0.9392
Epoch 8/10, Train Loss: 0.1735 (value: 0.0006, weighted value: 0.0290, policy: 0.1445, weighted policy: 0.1445), Train Mean Max: 0.9393
Epoch 9/10, Train Loss: 0.1722 (value: 0.0006, weighted value: 0.0281, policy: 0.1441, weighted policy: 0.1441), Train Mean Max: 0.9394
Epoch 10/10, Train Loss: 0.1718 (value: 0.0006, weighted value: 0.0277, policy: 0.1440, weighted policy: 0.1440), Train Mean Max: 0.9397
..training done in 65.78 seconds
..evaluation done in 18.36 seconds
Old network+MCTS average reward: 0.7754, min: 0.2778, max: 1.7037, stdev: 0.2488
New network+MCTS average reward: 0.7769, min: 0.2593, max: 1.6389, stdev: 0.2495
Old bare network average reward: 0.7487, min: 0.1852, max: 1.7037, stdev: 0.2566
New bare network average reward: 0.7478, min: 0.1944, max: 1.7037, stdev: 0.2543
External policy "random" average reward: 0.2704, min: -0.1852, max: 1.1944, stdev: 0.2363
External policy "individual greedy" average reward: 0.5451, min: 0.0278, max: 1.5463, stdev: 0.2345
External policy "total greedy" average reward: 0.6583, min: 0.1574, max: 1.5741, stdev: 0.2143
New network won 65 and tied 178 out of 300 games (51.33% wins where ties are half wins)
Keeping the new network

Training iteration 703 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.22 seconds
Training examples lengths: [64621, 64827, 64964, 64767, 64626, 64725, 64645, 64837, 64833, 64707]
Total value: 501534.63
Training on 647552 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1913 (value: 0.0008, weighted value: 0.0382, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9380
Epoch 2/10, Train Loss: 0.1838 (value: 0.0007, weighted value: 0.0347, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9387
Epoch 3/10, Train Loss: 0.1796 (value: 0.0007, weighted value: 0.0332, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9390
Epoch 4/10, Train Loss: 0.1761 (value: 0.0006, weighted value: 0.0307, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9392
Epoch 5/10, Train Loss: 0.1749 (value: 0.0006, weighted value: 0.0308, policy: 0.1441, weighted policy: 0.1441), Train Mean Max: 0.9393
Epoch 6/10, Train Loss: 0.1729 (value: 0.0006, weighted value: 0.0285, policy: 0.1444, weighted policy: 0.1444), Train Mean Max: 0.9395
Epoch 7/10, Train Loss: 0.1729 (value: 0.0006, weighted value: 0.0287, policy: 0.1442, weighted policy: 0.1442), Train Mean Max: 0.9396
Epoch 8/10, Train Loss: 0.1726 (value: 0.0006, weighted value: 0.0285, policy: 0.1441, weighted policy: 0.1441), Train Mean Max: 0.9398
Epoch 9/10, Train Loss: 0.1699 (value: 0.0005, weighted value: 0.0264, policy: 0.1435, weighted policy: 0.1435), Train Mean Max: 0.9399
Epoch 10/10, Train Loss: 0.1705 (value: 0.0005, weighted value: 0.0269, policy: 0.1436, weighted policy: 0.1436), Train Mean Max: 0.9398
..training done in 64.76 seconds
..evaluation done in 18.68 seconds
Old network+MCTS average reward: 0.7555, min: 0.0926, max: 1.6019, stdev: 0.2577
New network+MCTS average reward: 0.7582, min: 0.0926, max: 1.5278, stdev: 0.2549
Old bare network average reward: 0.7298, min: 0.0926, max: 1.6019, stdev: 0.2583
New bare network average reward: 0.7308, min: 0.0926, max: 1.5278, stdev: 0.2579
External policy "random" average reward: 0.2488, min: -0.3704, max: 0.9259, stdev: 0.2402
External policy "individual greedy" average reward: 0.5286, min: -0.0833, max: 1.2500, stdev: 0.2420
External policy "total greedy" average reward: 0.6443, min: 0.0370, max: 1.3148, stdev: 0.2296
New network won 60 and tied 184 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 704 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.19 seconds
Training examples lengths: [64827, 64964, 64767, 64626, 64725, 64645, 64837, 64833, 64707, 65040]
Total value: 502128.06
Training on 647971 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1890 (value: 0.0007, weighted value: 0.0364, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9383
Epoch 2/10, Train Loss: 0.1833 (value: 0.0007, weighted value: 0.0341, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9389
Epoch 3/10, Train Loss: 0.1768 (value: 0.0006, weighted value: 0.0310, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9394
Epoch 4/10, Train Loss: 0.1750 (value: 0.0006, weighted value: 0.0303, policy: 0.1447, weighted policy: 0.1447), Train Mean Max: 0.9394
Epoch 5/10, Train Loss: 0.1727 (value: 0.0006, weighted value: 0.0290, policy: 0.1437, weighted policy: 0.1437), Train Mean Max: 0.9396
Epoch 6/10, Train Loss: 0.1719 (value: 0.0006, weighted value: 0.0285, policy: 0.1435, weighted policy: 0.1435), Train Mean Max: 0.9397
Epoch 7/10, Train Loss: 0.1711 (value: 0.0006, weighted value: 0.0280, policy: 0.1431, weighted policy: 0.1431), Train Mean Max: 0.9398
Epoch 8/10, Train Loss: 0.1707 (value: 0.0005, weighted value: 0.0271, policy: 0.1436, weighted policy: 0.1436), Train Mean Max: 0.9398
Epoch 9/10, Train Loss: 0.1687 (value: 0.0005, weighted value: 0.0256, policy: 0.1431, weighted policy: 0.1431), Train Mean Max: 0.9400
Epoch 10/10, Train Loss: 0.1688 (value: 0.0005, weighted value: 0.0261, policy: 0.1427, weighted policy: 0.1427), Train Mean Max: 0.9401
..training done in 64.86 seconds
..evaluation done in 19.12 seconds
Old network+MCTS average reward: 0.7705, min: 0.0833, max: 1.4537, stdev: 0.2238
New network+MCTS average reward: 0.7684, min: 0.1574, max: 1.4537, stdev: 0.2259
Old bare network average reward: 0.7418, min: 0.0833, max: 1.4537, stdev: 0.2271
New bare network average reward: 0.7412, min: 0.1574, max: 1.4537, stdev: 0.2258
External policy "random" average reward: 0.2721, min: -0.3796, max: 0.8796, stdev: 0.2197
External policy "individual greedy" average reward: 0.5566, min: -0.0093, max: 1.2315, stdev: 0.2159
External policy "total greedy" average reward: 0.6612, min: 0.1204, max: 1.3426, stdev: 0.2055
New network won 58 and tied 168 out of 300 games (47.33% wins where ties are half wins)
Reverting to the old network

Training iteration 705 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.66 seconds
Training examples lengths: [64964, 64767, 64626, 64725, 64645, 64837, 64833, 64707, 65040, 64856]
Total value: 502144.40
Training on 648000 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2070 (value: 0.0009, weighted value: 0.0467, policy: 0.1602, weighted policy: 0.1602), Train Mean Max: 0.9368
Epoch 2/10, Train Loss: 0.1949 (value: 0.0008, weighted value: 0.0417, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9379
Epoch 3/10, Train Loss: 0.1868 (value: 0.0008, weighted value: 0.0377, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9384
Epoch 4/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0362, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9385
Epoch 5/10, Train Loss: 0.1794 (value: 0.0007, weighted value: 0.0343, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9387
Epoch 6/10, Train Loss: 0.1771 (value: 0.0006, weighted value: 0.0321, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9388
Epoch 7/10, Train Loss: 0.1776 (value: 0.0007, weighted value: 0.0331, policy: 0.1445, weighted policy: 0.1445), Train Mean Max: 0.9389
Epoch 8/10, Train Loss: 0.1740 (value: 0.0006, weighted value: 0.0296, policy: 0.1444, weighted policy: 0.1444), Train Mean Max: 0.9391
Epoch 9/10, Train Loss: 0.1741 (value: 0.0006, weighted value: 0.0298, policy: 0.1443, weighted policy: 0.1443), Train Mean Max: 0.9392
Epoch 10/10, Train Loss: 0.1730 (value: 0.0006, weighted value: 0.0286, policy: 0.1444, weighted policy: 0.1444), Train Mean Max: 0.9395
..training done in 60.63 seconds
..evaluation done in 18.45 seconds
Old network+MCTS average reward: 0.7665, min: 0.2315, max: 1.5833, stdev: 0.2326
New network+MCTS average reward: 0.7663, min: 0.2037, max: 1.5833, stdev: 0.2306
Old bare network average reward: 0.7347, min: 0.2037, max: 1.5463, stdev: 0.2309
New bare network average reward: 0.7335, min: 0.1852, max: 1.5463, stdev: 0.2373
External policy "random" average reward: 0.2669, min: -0.3611, max: 0.8241, stdev: 0.2124
External policy "individual greedy" average reward: 0.5415, min: 0.0278, max: 1.2037, stdev: 0.2131
External policy "total greedy" average reward: 0.6557, min: 0.1204, max: 1.4907, stdev: 0.2046
New network won 61 and tied 190 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network

Training iteration 706 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.99 seconds
Training examples lengths: [64767, 64626, 64725, 64645, 64837, 64833, 64707, 65040, 64856, 64965]
Total value: 502351.20
Training on 648001 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1922 (value: 0.0008, weighted value: 0.0389, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9378
Epoch 2/10, Train Loss: 0.1852 (value: 0.0007, weighted value: 0.0354, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9385
Epoch 3/10, Train Loss: 0.1803 (value: 0.0007, weighted value: 0.0334, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9389
Epoch 4/10, Train Loss: 0.1775 (value: 0.0006, weighted value: 0.0318, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9389
Epoch 5/10, Train Loss: 0.1750 (value: 0.0006, weighted value: 0.0302, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9393
Epoch 6/10, Train Loss: 0.1750 (value: 0.0006, weighted value: 0.0304, policy: 0.1446, weighted policy: 0.1446), Train Mean Max: 0.9393
Epoch 7/10, Train Loss: 0.1735 (value: 0.0006, weighted value: 0.0291, policy: 0.1444, weighted policy: 0.1444), Train Mean Max: 0.9394
Epoch 8/10, Train Loss: 0.1721 (value: 0.0006, weighted value: 0.0279, policy: 0.1442, weighted policy: 0.1442), Train Mean Max: 0.9395
Epoch 9/10, Train Loss: 0.1715 (value: 0.0005, weighted value: 0.0272, policy: 0.1443, weighted policy: 0.1443), Train Mean Max: 0.9396
Epoch 10/10, Train Loss: 0.1713 (value: 0.0006, weighted value: 0.0277, policy: 0.1436, weighted policy: 0.1436), Train Mean Max: 0.9397
..training done in 59.43 seconds
..evaluation done in 19.05 seconds
Old network+MCTS average reward: 0.7461, min: 0.0278, max: 1.8889, stdev: 0.2237
New network+MCTS average reward: 0.7444, min: 0.0370, max: 1.8889, stdev: 0.2283
Old bare network average reward: 0.7136, min: 0.0278, max: 1.8056, stdev: 0.2267
New bare network average reward: 0.7178, min: 0.0278, max: 1.8889, stdev: 0.2295
External policy "random" average reward: 0.2405, min: -0.2130, max: 1.0370, stdev: 0.2077
External policy "individual greedy" average reward: 0.5229, min: -0.0741, max: 1.6481, stdev: 0.2197
External policy "total greedy" average reward: 0.6432, min: 0.1296, max: 1.7870, stdev: 0.2116
New network won 65 and tied 165 out of 300 games (49.17% wins where ties are half wins)
Reverting to the old network

Training iteration 707 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.21 seconds
Training examples lengths: [64626, 64725, 64645, 64837, 64833, 64707, 65040, 64856, 64965, 64582]
Total value: 501873.81
Training on 647816 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2107 (value: 0.0010, weighted value: 0.0493, policy: 0.1614, weighted policy: 0.1614), Train Mean Max: 0.9364
Epoch 2/10, Train Loss: 0.1984 (value: 0.0009, weighted value: 0.0438, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9374
Epoch 3/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0396, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9378
Epoch 4/10, Train Loss: 0.1846 (value: 0.0007, weighted value: 0.0368, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9381
Epoch 5/10, Train Loss: 0.1820 (value: 0.0007, weighted value: 0.0354, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9383
Epoch 6/10, Train Loss: 0.1811 (value: 0.0007, weighted value: 0.0350, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9384
Epoch 7/10, Train Loss: 0.1778 (value: 0.0006, weighted value: 0.0322, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9385
Epoch 8/10, Train Loss: 0.1777 (value: 0.0006, weighted value: 0.0321, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9387
Epoch 9/10, Train Loss: 0.1749 (value: 0.0006, weighted value: 0.0299, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9389
Epoch 10/10, Train Loss: 0.1756 (value: 0.0006, weighted value: 0.0302, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9390
..training done in 59.30 seconds
..evaluation done in 18.49 seconds
Old network+MCTS average reward: 0.7473, min: 0.1944, max: 1.5370, stdev: 0.2376
New network+MCTS average reward: 0.7495, min: 0.1944, max: 1.5370, stdev: 0.2360
Old bare network average reward: 0.7179, min: 0.1389, max: 1.5370, stdev: 0.2407
New bare network average reward: 0.7194, min: 0.1389, max: 1.5370, stdev: 0.2430
External policy "random" average reward: 0.2527, min: -0.3704, max: 0.9352, stdev: 0.2289
External policy "individual greedy" average reward: 0.5382, min: -0.0648, max: 1.2500, stdev: 0.2288
External policy "total greedy" average reward: 0.6432, min: 0.1019, max: 1.3241, stdev: 0.2205
New network won 75 and tied 153 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 708 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.50 seconds
Training examples lengths: [64725, 64645, 64837, 64833, 64707, 65040, 64856, 64965, 64582, 64788]
Total value: 501920.70
Training on 647978 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1941 (value: 0.0008, weighted value: 0.0398, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9374
Epoch 2/10, Train Loss: 0.1855 (value: 0.0007, weighted value: 0.0359, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9381
Epoch 3/10, Train Loss: 0.1812 (value: 0.0007, weighted value: 0.0340, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9386
Epoch 4/10, Train Loss: 0.1787 (value: 0.0007, weighted value: 0.0328, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9387
Epoch 5/10, Train Loss: 0.1769 (value: 0.0006, weighted value: 0.0316, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9389
Epoch 6/10, Train Loss: 0.1760 (value: 0.0006, weighted value: 0.0306, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9389
Epoch 7/10, Train Loss: 0.1741 (value: 0.0006, weighted value: 0.0291, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9391
Epoch 8/10, Train Loss: 0.1731 (value: 0.0006, weighted value: 0.0287, policy: 0.1445, weighted policy: 0.1445), Train Mean Max: 0.9391
Epoch 9/10, Train Loss: 0.1717 (value: 0.0005, weighted value: 0.0270, policy: 0.1447, weighted policy: 0.1447), Train Mean Max: 0.9394
Epoch 10/10, Train Loss: 0.1723 (value: 0.0006, weighted value: 0.0281, policy: 0.1442, weighted policy: 0.1442), Train Mean Max: 0.9395
..training done in 67.09 seconds
..evaluation done in 18.86 seconds
Old network+MCTS average reward: 0.7680, min: 0.0278, max: 1.6204, stdev: 0.2456
New network+MCTS average reward: 0.7635, min: 0.0093, max: 1.6019, stdev: 0.2468
Old bare network average reward: 0.7360, min: 0.0185, max: 1.4907, stdev: 0.2496
New bare network average reward: 0.7349, min: 0.0000, max: 1.4907, stdev: 0.2464
External policy "random" average reward: 0.2670, min: -0.4259, max: 1.1667, stdev: 0.2359
External policy "individual greedy" average reward: 0.5281, min: -0.1667, max: 1.1944, stdev: 0.2359
External policy "total greedy" average reward: 0.6553, min: 0.1389, max: 1.4630, stdev: 0.2382
New network won 50 and tied 180 out of 300 games (46.67% wins where ties are half wins)
Reverting to the old network

Training iteration 709 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.35 seconds
Training examples lengths: [64645, 64837, 64833, 64707, 65040, 64856, 64965, 64582, 64788, 64819]
Total value: 501605.49
Training on 648072 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2120 (value: 0.0010, weighted value: 0.0502, policy: 0.1618, weighted policy: 0.1618), Train Mean Max: 0.9357
Epoch 2/10, Train Loss: 0.1986 (value: 0.0009, weighted value: 0.0439, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9371
Epoch 3/10, Train Loss: 0.1907 (value: 0.0008, weighted value: 0.0397, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9376
Epoch 4/10, Train Loss: 0.1864 (value: 0.0008, weighted value: 0.0376, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9378
Epoch 5/10, Train Loss: 0.1828 (value: 0.0007, weighted value: 0.0360, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9381
Epoch 6/10, Train Loss: 0.1805 (value: 0.0007, weighted value: 0.0338, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9380
Epoch 7/10, Train Loss: 0.1790 (value: 0.0007, weighted value: 0.0328, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9383
Epoch 8/10, Train Loss: 0.1786 (value: 0.0007, weighted value: 0.0326, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1771 (value: 0.0006, weighted value: 0.0318, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9386
Epoch 10/10, Train Loss: 0.1748 (value: 0.0006, weighted value: 0.0297, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9388
..training done in 59.65 seconds
..evaluation done in 18.10 seconds
Old network+MCTS average reward: 0.7706, min: 0.2130, max: 1.5278, stdev: 0.2207
New network+MCTS average reward: 0.7691, min: 0.2500, max: 1.5278, stdev: 0.2212
Old bare network average reward: 0.7419, min: 0.2130, max: 1.5278, stdev: 0.2252
New bare network average reward: 0.7407, min: 0.1481, max: 1.5278, stdev: 0.2216
External policy "random" average reward: 0.2730, min: -0.2685, max: 0.8333, stdev: 0.2102
External policy "individual greedy" average reward: 0.5456, min: 0.0185, max: 1.4537, stdev: 0.2224
External policy "total greedy" average reward: 0.6529, min: 0.2037, max: 1.3889, stdev: 0.2099
New network won 64 and tied 175 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 710 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.82 seconds
Training examples lengths: [64837, 64833, 64707, 65040, 64856, 64965, 64582, 64788, 64819, 64769]
Total value: 501724.60
Training on 648196 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1949 (value: 0.0008, weighted value: 0.0408, policy: 0.1541, weighted policy: 0.1541), Train Mean Max: 0.9371
Epoch 2/10, Train Loss: 0.1875 (value: 0.0007, weighted value: 0.0364, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9378
Epoch 3/10, Train Loss: 0.1824 (value: 0.0007, weighted value: 0.0343, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9384
Epoch 4/10, Train Loss: 0.1797 (value: 0.0007, weighted value: 0.0330, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9384
Epoch 5/10, Train Loss: 0.1787 (value: 0.0007, weighted value: 0.0326, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9385
Epoch 6/10, Train Loss: 0.1762 (value: 0.0006, weighted value: 0.0302, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9385
Epoch 7/10, Train Loss: 0.1763 (value: 0.0006, weighted value: 0.0307, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9388
Epoch 8/10, Train Loss: 0.1737 (value: 0.0006, weighted value: 0.0288, policy: 0.1449, weighted policy: 0.1449), Train Mean Max: 0.9388
Epoch 9/10, Train Loss: 0.1724 (value: 0.0006, weighted value: 0.0281, policy: 0.1443, weighted policy: 0.1443), Train Mean Max: 0.9391
Epoch 10/10, Train Loss: 0.1722 (value: 0.0005, weighted value: 0.0273, policy: 0.1449, weighted policy: 0.1449), Train Mean Max: 0.9389
..training done in 59.29 seconds
..evaluation done in 23.84 seconds
Old network+MCTS average reward: 0.7847, min: -0.1019, max: 1.5833, stdev: 0.2541
New network+MCTS average reward: 0.7826, min: -0.0556, max: 1.6204, stdev: 0.2531
Old bare network average reward: 0.7571, min: -0.0926, max: 1.5648, stdev: 0.2556
New bare network average reward: 0.7548, min: -0.1111, max: 1.5648, stdev: 0.2563
External policy "random" average reward: 0.2776, min: -0.5000, max: 1.2593, stdev: 0.2420
External policy "individual greedy" average reward: 0.5592, min: -0.1204, max: 1.3056, stdev: 0.2424
External policy "total greedy" average reward: 0.6728, min: -0.0185, max: 1.4352, stdev: 0.2411
New network won 63 and tied 166 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 711 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.63 seconds
Training examples lengths: [64833, 64707, 65040, 64856, 64965, 64582, 64788, 64819, 64769, 64875]
Total value: 502092.06
Training on 648234 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2116 (value: 0.0010, weighted value: 0.0504, policy: 0.1612, weighted policy: 0.1612), Train Mean Max: 0.9357
Epoch 2/10, Train Loss: 0.1997 (value: 0.0009, weighted value: 0.0444, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9369
Epoch 3/10, Train Loss: 0.1908 (value: 0.0008, weighted value: 0.0403, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9372
Epoch 4/10, Train Loss: 0.1863 (value: 0.0008, weighted value: 0.0377, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9375
Epoch 5/10, Train Loss: 0.1842 (value: 0.0007, weighted value: 0.0365, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9377
Epoch 6/10, Train Loss: 0.1809 (value: 0.0007, weighted value: 0.0343, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9380
Epoch 7/10, Train Loss: 0.1804 (value: 0.0007, weighted value: 0.0338, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9380
Epoch 8/10, Train Loss: 0.1786 (value: 0.0006, weighted value: 0.0323, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9383
Epoch 9/10, Train Loss: 0.1770 (value: 0.0006, weighted value: 0.0310, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1764 (value: 0.0006, weighted value: 0.0303, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9386
..training done in 67.29 seconds
..evaluation done in 18.20 seconds
Old network+MCTS average reward: 0.7627, min: 0.0926, max: 1.5926, stdev: 0.2586
New network+MCTS average reward: 0.7649, min: 0.0648, max: 1.5926, stdev: 0.2560
Old bare network average reward: 0.7377, min: 0.0185, max: 1.5926, stdev: 0.2585
New bare network average reward: 0.7393, min: 0.0185, max: 1.5926, stdev: 0.2611
External policy "random" average reward: 0.2523, min: -0.3981, max: 0.9815, stdev: 0.2464
External policy "individual greedy" average reward: 0.5363, min: -0.0463, max: 1.3704, stdev: 0.2436
External policy "total greedy" average reward: 0.6529, min: 0.0000, max: 1.4907, stdev: 0.2538
New network won 59 and tied 186 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 712 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.44 seconds
Training examples lengths: [64707, 65040, 64856, 64965, 64582, 64788, 64819, 64769, 64875, 65023]
Total value: 502251.54
Training on 648424 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1941 (value: 0.0008, weighted value: 0.0395, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9369
Epoch 2/10, Train Loss: 0.1884 (value: 0.0007, weighted value: 0.0369, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9375
Epoch 3/10, Train Loss: 0.1835 (value: 0.0007, weighted value: 0.0349, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9378
Epoch 4/10, Train Loss: 0.1791 (value: 0.0007, weighted value: 0.0327, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9381
Epoch 5/10, Train Loss: 0.1788 (value: 0.0006, weighted value: 0.0322, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9382
Epoch 6/10, Train Loss: 0.1764 (value: 0.0006, weighted value: 0.0303, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9383
Epoch 7/10, Train Loss: 0.1751 (value: 0.0006, weighted value: 0.0289, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9385
Epoch 8/10, Train Loss: 0.1756 (value: 0.0006, weighted value: 0.0299, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9387
Epoch 9/10, Train Loss: 0.1737 (value: 0.0006, weighted value: 0.0280, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9387
Epoch 10/10, Train Loss: 0.1725 (value: 0.0006, weighted value: 0.0277, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9388
..training done in 60.24 seconds
..evaluation done in 18.57 seconds
Old network+MCTS average reward: 0.7598, min: 0.1667, max: 1.5833, stdev: 0.2369
New network+MCTS average reward: 0.7607, min: 0.2130, max: 1.5370, stdev: 0.2377
Old bare network average reward: 0.7318, min: 0.1667, max: 1.5370, stdev: 0.2383
New bare network average reward: 0.7315, min: 0.1667, max: 1.5370, stdev: 0.2389
External policy "random" average reward: 0.2452, min: -0.4352, max: 1.0648, stdev: 0.2305
External policy "individual greedy" average reward: 0.5325, min: -0.0833, max: 1.2037, stdev: 0.2327
External policy "total greedy" average reward: 0.6454, min: 0.0926, max: 1.2593, stdev: 0.2220
New network won 59 and tied 184 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 713 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 60.04 seconds
Training examples lengths: [65040, 64856, 64965, 64582, 64788, 64819, 64769, 64875, 65023, 64833]
Total value: 502615.55
Training on 648550 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1926 (value: 0.0008, weighted value: 0.0380, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9373
Epoch 2/10, Train Loss: 0.1851 (value: 0.0007, weighted value: 0.0345, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9379
Epoch 3/10, Train Loss: 0.1805 (value: 0.0006, weighted value: 0.0323, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9382
Epoch 4/10, Train Loss: 0.1779 (value: 0.0006, weighted value: 0.0317, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9385
Epoch 5/10, Train Loss: 0.1754 (value: 0.0006, weighted value: 0.0292, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9385
Epoch 6/10, Train Loss: 0.1757 (value: 0.0006, weighted value: 0.0301, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9387
Epoch 7/10, Train Loss: 0.1730 (value: 0.0006, weighted value: 0.0279, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9388
Epoch 8/10, Train Loss: 0.1736 (value: 0.0006, weighted value: 0.0282, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9388
Epoch 9/10, Train Loss: 0.1715 (value: 0.0005, weighted value: 0.0263, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9390
Epoch 10/10, Train Loss: 0.1722 (value: 0.0005, weighted value: 0.0270, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9391
..training done in 75.50 seconds
..evaluation done in 20.77 seconds
Old network+MCTS average reward: 0.7667, min: 0.2500, max: 1.3889, stdev: 0.2215
New network+MCTS average reward: 0.7686, min: 0.2500, max: 1.3889, stdev: 0.2242
Old bare network average reward: 0.7426, min: 0.0833, max: 1.3889, stdev: 0.2289
New bare network average reward: 0.7409, min: 0.0833, max: 1.3426, stdev: 0.2300
External policy "random" average reward: 0.2635, min: -0.2685, max: 0.8519, stdev: 0.2159
External policy "individual greedy" average reward: 0.5404, min: -0.0093, max: 1.2407, stdev: 0.2179
External policy "total greedy" average reward: 0.6521, min: 0.1574, max: 1.2778, stdev: 0.2118
New network won 60 and tied 184 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 714 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.27 seconds
Training examples lengths: [64856, 64965, 64582, 64788, 64819, 64769, 64875, 65023, 64833, 64331]
Total value: 502017.70
Training on 647841 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1904 (value: 0.0007, weighted value: 0.0365, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9376
Epoch 2/10, Train Loss: 0.1835 (value: 0.0007, weighted value: 0.0334, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9383
Epoch 3/10, Train Loss: 0.1779 (value: 0.0006, weighted value: 0.0305, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9385
Epoch 4/10, Train Loss: 0.1769 (value: 0.0006, weighted value: 0.0305, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9386
Epoch 5/10, Train Loss: 0.1754 (value: 0.0006, weighted value: 0.0295, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9387
Epoch 6/10, Train Loss: 0.1733 (value: 0.0006, weighted value: 0.0282, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9390
Epoch 7/10, Train Loss: 0.1719 (value: 0.0005, weighted value: 0.0268, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9390
Epoch 8/10, Train Loss: 0.1723 (value: 0.0005, weighted value: 0.0270, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9391
Epoch 9/10, Train Loss: 0.1714 (value: 0.0005, weighted value: 0.0269, policy: 0.1445, weighted policy: 0.1445), Train Mean Max: 0.9392
Epoch 10/10, Train Loss: 0.1696 (value: 0.0005, weighted value: 0.0251, policy: 0.1445, weighted policy: 0.1445), Train Mean Max: 0.9394
..training done in 66.08 seconds
..evaluation done in 18.83 seconds
Old network+MCTS average reward: 0.7572, min: 0.1019, max: 1.4352, stdev: 0.2442
New network+MCTS average reward: 0.7591, min: 0.0278, max: 1.4352, stdev: 0.2451
Old bare network average reward: 0.7299, min: -0.0463, max: 1.3981, stdev: 0.2444
New bare network average reward: 0.7283, min: 0.0278, max: 1.4352, stdev: 0.2455
External policy "random" average reward: 0.2518, min: -0.4630, max: 1.0000, stdev: 0.2248
External policy "individual greedy" average reward: 0.5236, min: -0.1389, max: 1.2593, stdev: 0.2223
External policy "total greedy" average reward: 0.6350, min: 0.0278, max: 1.3426, stdev: 0.2252
New network won 69 and tied 169 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 715 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.17 seconds
Training examples lengths: [64965, 64582, 64788, 64819, 64769, 64875, 65023, 64833, 64331, 65108]
Total value: 502481.78
Training on 648093 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1885 (value: 0.0007, weighted value: 0.0353, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9379
Epoch 2/10, Train Loss: 0.1822 (value: 0.0006, weighted value: 0.0325, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9387
Epoch 3/10, Train Loss: 0.1772 (value: 0.0006, weighted value: 0.0307, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9391
Epoch 4/10, Train Loss: 0.1743 (value: 0.0006, weighted value: 0.0291, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9390
Epoch 5/10, Train Loss: 0.1728 (value: 0.0006, weighted value: 0.0284, policy: 0.1444, weighted policy: 0.1444), Train Mean Max: 0.9393
Epoch 6/10, Train Loss: 0.1711 (value: 0.0005, weighted value: 0.0273, policy: 0.1438, weighted policy: 0.1438), Train Mean Max: 0.9393
Epoch 7/10, Train Loss: 0.1717 (value: 0.0005, weighted value: 0.0270, policy: 0.1447, weighted policy: 0.1447), Train Mean Max: 0.9393
Epoch 8/10, Train Loss: 0.1702 (value: 0.0005, weighted value: 0.0262, policy: 0.1441, weighted policy: 0.1441), Train Mean Max: 0.9395
Epoch 9/10, Train Loss: 0.1702 (value: 0.0005, weighted value: 0.0261, policy: 0.1441, weighted policy: 0.1441), Train Mean Max: 0.9395
Epoch 10/10, Train Loss: 0.1681 (value: 0.0005, weighted value: 0.0245, policy: 0.1436, weighted policy: 0.1436), Train Mean Max: 0.9396
..training done in 67.65 seconds
..evaluation done in 18.48 seconds
Old network+MCTS average reward: 0.7427, min: 0.2407, max: 1.5000, stdev: 0.2407
New network+MCTS average reward: 0.7447, min: 0.2407, max: 1.5000, stdev: 0.2393
Old bare network average reward: 0.7187, min: 0.1667, max: 1.5000, stdev: 0.2412
New bare network average reward: 0.7192, min: 0.2407, max: 1.5000, stdev: 0.2408
External policy "random" average reward: 0.2469, min: -0.3981, max: 1.0278, stdev: 0.2374
External policy "individual greedy" average reward: 0.5336, min: -0.0648, max: 1.4167, stdev: 0.2260
External policy "total greedy" average reward: 0.6405, min: 0.1944, max: 1.4537, stdev: 0.2170
New network won 70 and tied 178 out of 300 games (53.00% wins where ties are half wins)
Keeping the new network

Training iteration 716 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.18 seconds
Training examples lengths: [64582, 64788, 64819, 64769, 64875, 65023, 64833, 64331, 65108, 64977]
Total value: 502370.44
Training on 648105 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1883 (value: 0.0007, weighted value: 0.0364, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9383
Epoch 2/10, Train Loss: 0.1819 (value: 0.0007, weighted value: 0.0332, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9388
Epoch 3/10, Train Loss: 0.1763 (value: 0.0006, weighted value: 0.0303, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9392
Epoch 4/10, Train Loss: 0.1747 (value: 0.0006, weighted value: 0.0298, policy: 0.1449, weighted policy: 0.1449), Train Mean Max: 0.9392
Epoch 5/10, Train Loss: 0.1729 (value: 0.0006, weighted value: 0.0283, policy: 0.1446, weighted policy: 0.1446), Train Mean Max: 0.9393
Epoch 6/10, Train Loss: 0.1715 (value: 0.0005, weighted value: 0.0274, policy: 0.1441, weighted policy: 0.1441), Train Mean Max: 0.9395
Epoch 7/10, Train Loss: 0.1704 (value: 0.0005, weighted value: 0.0264, policy: 0.1440, weighted policy: 0.1440), Train Mean Max: 0.9395
Epoch 8/10, Train Loss: 0.1691 (value: 0.0005, weighted value: 0.0257, policy: 0.1433, weighted policy: 0.1433), Train Mean Max: 0.9398
Epoch 9/10, Train Loss: 0.1696 (value: 0.0005, weighted value: 0.0257, policy: 0.1439, weighted policy: 0.1439), Train Mean Max: 0.9397
Epoch 10/10, Train Loss: 0.1683 (value: 0.0005, weighted value: 0.0252, policy: 0.1431, weighted policy: 0.1431), Train Mean Max: 0.9399
..training done in 60.60 seconds
..evaluation done in 18.58 seconds
Old network+MCTS average reward: 0.7695, min: 0.0093, max: 1.4167, stdev: 0.2384
New network+MCTS average reward: 0.7690, min: -0.0093, max: 1.4167, stdev: 0.2386
Old bare network average reward: 0.7431, min: -0.0093, max: 1.4167, stdev: 0.2391
New bare network average reward: 0.7426, min: -0.0093, max: 1.4167, stdev: 0.2410
External policy "random" average reward: 0.2536, min: -0.2963, max: 0.8241, stdev: 0.2120
External policy "individual greedy" average reward: 0.5418, min: -0.0833, max: 1.3241, stdev: 0.2213
External policy "total greedy" average reward: 0.6630, min: 0.0926, max: 1.2593, stdev: 0.2165
New network won 55 and tied 188 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 717 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 62.63 seconds
Training examples lengths: [64788, 64819, 64769, 64875, 65023, 64833, 64331, 65108, 64977, 64627]
Total value: 502904.82
Training on 648150 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2065 (value: 0.0009, weighted value: 0.0471, policy: 0.1594, weighted policy: 0.1594), Train Mean Max: 0.9369
Epoch 2/10, Train Loss: 0.1937 (value: 0.0008, weighted value: 0.0399, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9377
Epoch 3/10, Train Loss: 0.1866 (value: 0.0008, weighted value: 0.0377, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9382
Epoch 4/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0358, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9383
Epoch 5/10, Train Loss: 0.1779 (value: 0.0007, weighted value: 0.0325, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9387
Epoch 6/10, Train Loss: 0.1781 (value: 0.0007, weighted value: 0.0331, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9387
Epoch 7/10, Train Loss: 0.1751 (value: 0.0006, weighted value: 0.0302, policy: 0.1449, weighted policy: 0.1449), Train Mean Max: 0.9389
Epoch 8/10, Train Loss: 0.1745 (value: 0.0006, weighted value: 0.0295, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9390
Epoch 9/10, Train Loss: 0.1740 (value: 0.0006, weighted value: 0.0292, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9391
Epoch 10/10, Train Loss: 0.1724 (value: 0.0006, weighted value: 0.0284, policy: 0.1440, weighted policy: 0.1440), Train Mean Max: 0.9392
..training done in 67.20 seconds
..evaluation done in 18.31 seconds
Old network+MCTS average reward: 0.7440, min: 0.0370, max: 1.5556, stdev: 0.2329
New network+MCTS average reward: 0.7446, min: 0.0370, max: 1.5278, stdev: 0.2329
Old bare network average reward: 0.7150, min: -0.1204, max: 1.5278, stdev: 0.2417
New bare network average reward: 0.7153, min: -0.0370, max: 1.5278, stdev: 0.2410
External policy "random" average reward: 0.2425, min: -0.3426, max: 0.8796, stdev: 0.2140
External policy "individual greedy" average reward: 0.5206, min: -0.1111, max: 1.3704, stdev: 0.2181
External policy "total greedy" average reward: 0.6374, min: 0.0463, max: 1.4815, stdev: 0.2169
New network won 58 and tied 180 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 718 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.84 seconds
Training examples lengths: [64819, 64769, 64875, 65023, 64833, 64331, 65108, 64977, 64627, 64686]
Total value: 502443.98
Training on 648048 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2248 (value: 0.0012, weighted value: 0.0581, policy: 0.1667, weighted policy: 0.1667), Train Mean Max: 0.9351
Epoch 2/10, Train Loss: 0.2055 (value: 0.0009, weighted value: 0.0474, policy: 0.1580, weighted policy: 0.1580), Train Mean Max: 0.9368
Epoch 3/10, Train Loss: 0.1944 (value: 0.0008, weighted value: 0.0423, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9371
Epoch 4/10, Train Loss: 0.1896 (value: 0.0008, weighted value: 0.0406, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9375
Epoch 5/10, Train Loss: 0.1858 (value: 0.0008, weighted value: 0.0383, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9377
Epoch 6/10, Train Loss: 0.1824 (value: 0.0007, weighted value: 0.0355, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9378
Epoch 7/10, Train Loss: 0.1810 (value: 0.0007, weighted value: 0.0349, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9380
Epoch 8/10, Train Loss: 0.1793 (value: 0.0007, weighted value: 0.0330, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9382
Epoch 9/10, Train Loss: 0.1768 (value: 0.0006, weighted value: 0.0312, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1763 (value: 0.0006, weighted value: 0.0310, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9385
..training done in 61.38 seconds
..evaluation done in 17.77 seconds
Old network+MCTS average reward: 0.7429, min: 0.1296, max: 1.5833, stdev: 0.2474
New network+MCTS average reward: 0.7456, min: 0.1296, max: 1.6111, stdev: 0.2515
Old bare network average reward: 0.7141, min: 0.0741, max: 1.6111, stdev: 0.2522
New bare network average reward: 0.7148, min: 0.0463, max: 1.6111, stdev: 0.2514
External policy "random" average reward: 0.2449, min: -0.2407, max: 1.1111, stdev: 0.2358
External policy "individual greedy" average reward: 0.5137, min: -0.0278, max: 1.3148, stdev: 0.2450
External policy "total greedy" average reward: 0.6286, min: 0.0093, max: 1.5000, stdev: 0.2323
New network won 72 and tied 171 out of 300 games (52.50% wins where ties are half wins)
Keeping the new network

Training iteration 719 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.21 seconds
Training examples lengths: [64769, 64875, 65023, 64833, 64331, 65108, 64977, 64627, 64686, 65005]
Total value: 503515.63
Training on 648234 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1948 (value: 0.0008, weighted value: 0.0409, policy: 0.1539, weighted policy: 0.1539), Train Mean Max: 0.9371
Epoch 2/10, Train Loss: 0.1864 (value: 0.0007, weighted value: 0.0366, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9379
Epoch 3/10, Train Loss: 0.1823 (value: 0.0007, weighted value: 0.0349, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9383
Epoch 4/10, Train Loss: 0.1787 (value: 0.0007, weighted value: 0.0326, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9384
Epoch 5/10, Train Loss: 0.1778 (value: 0.0006, weighted value: 0.0320, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9385
Epoch 6/10, Train Loss: 0.1758 (value: 0.0006, weighted value: 0.0309, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9387
Epoch 7/10, Train Loss: 0.1743 (value: 0.0006, weighted value: 0.0291, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9389
Epoch 8/10, Train Loss: 0.1743 (value: 0.0006, weighted value: 0.0295, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9389
Epoch 9/10, Train Loss: 0.1717 (value: 0.0006, weighted value: 0.0280, policy: 0.1438, weighted policy: 0.1438), Train Mean Max: 0.9392
Epoch 10/10, Train Loss: 0.1713 (value: 0.0005, weighted value: 0.0270, policy: 0.1443, weighted policy: 0.1443), Train Mean Max: 0.9394
..training done in 60.54 seconds
..evaluation done in 18.19 seconds
Old network+MCTS average reward: 0.7750, min: 0.1574, max: 1.4907, stdev: 0.2387
New network+MCTS average reward: 0.7770, min: 0.1574, max: 1.4907, stdev: 0.2387
Old bare network average reward: 0.7483, min: 0.1574, max: 1.4907, stdev: 0.2438
New bare network average reward: 0.7487, min: 0.1574, max: 1.4630, stdev: 0.2447
External policy "random" average reward: 0.2687, min: -0.2222, max: 1.2407, stdev: 0.2337
External policy "individual greedy" average reward: 0.5448, min: 0.0000, max: 1.2963, stdev: 0.2224
External policy "total greedy" average reward: 0.6598, min: 0.0000, max: 1.3981, stdev: 0.2242
New network won 68 and tied 181 out of 300 games (52.83% wins where ties are half wins)
Keeping the new network

Training iteration 720 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.46 seconds
Training examples lengths: [64875, 65023, 64833, 64331, 65108, 64977, 64627, 64686, 65005, 64801]
Total value: 503753.29
Training on 648266 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1915 (value: 0.0008, weighted value: 0.0382, policy: 0.1532, weighted policy: 0.1532), Train Mean Max: 0.9377
Epoch 2/10, Train Loss: 0.1837 (value: 0.0007, weighted value: 0.0347, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9386
Epoch 3/10, Train Loss: 0.1792 (value: 0.0006, weighted value: 0.0324, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9388
Epoch 4/10, Train Loss: 0.1762 (value: 0.0006, weighted value: 0.0311, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9391
Epoch 5/10, Train Loss: 0.1736 (value: 0.0006, weighted value: 0.0291, policy: 0.1445, weighted policy: 0.1445), Train Mean Max: 0.9392
Epoch 6/10, Train Loss: 0.1727 (value: 0.0006, weighted value: 0.0285, policy: 0.1442, weighted policy: 0.1442), Train Mean Max: 0.9393
Epoch 7/10, Train Loss: 0.1726 (value: 0.0006, weighted value: 0.0289, policy: 0.1437, weighted policy: 0.1437), Train Mean Max: 0.9394
Epoch 8/10, Train Loss: 0.1713 (value: 0.0005, weighted value: 0.0274, policy: 0.1439, weighted policy: 0.1439), Train Mean Max: 0.9395
Epoch 9/10, Train Loss: 0.1708 (value: 0.0005, weighted value: 0.0273, policy: 0.1435, weighted policy: 0.1435), Train Mean Max: 0.9396
Epoch 10/10, Train Loss: 0.1690 (value: 0.0005, weighted value: 0.0256, policy: 0.1434, weighted policy: 0.1434), Train Mean Max: 0.9397
..training done in 60.06 seconds
..evaluation done in 18.36 seconds
Old network+MCTS average reward: 0.7641, min: 0.2130, max: 1.6296, stdev: 0.2344
New network+MCTS average reward: 0.7644, min: 0.2315, max: 1.6296, stdev: 0.2376
Old bare network average reward: 0.7348, min: 0.1944, max: 1.5556, stdev: 0.2358
New bare network average reward: 0.7333, min: 0.2130, max: 1.5648, stdev: 0.2394
External policy "random" average reward: 0.2537, min: -0.3704, max: 0.8796, stdev: 0.2356
External policy "individual greedy" average reward: 0.5336, min: -0.0926, max: 1.3704, stdev: 0.2377
External policy "total greedy" average reward: 0.6463, min: 0.1296, max: 1.4352, stdev: 0.2338
New network won 65 and tied 176 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_720

Training iteration 721 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.70 seconds
Training examples lengths: [65023, 64833, 64331, 65108, 64977, 64627, 64686, 65005, 64801, 64881]
Total value: 503701.69
Training on 648272 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1901 (value: 0.0008, weighted value: 0.0376, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9383
Epoch 2/10, Train Loss: 0.1822 (value: 0.0007, weighted value: 0.0331, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9390
Epoch 3/10, Train Loss: 0.1773 (value: 0.0006, weighted value: 0.0314, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9393
Epoch 4/10, Train Loss: 0.1739 (value: 0.0006, weighted value: 0.0298, policy: 0.1440, weighted policy: 0.1440), Train Mean Max: 0.9394
Epoch 5/10, Train Loss: 0.1734 (value: 0.0006, weighted value: 0.0292, policy: 0.1442, weighted policy: 0.1442), Train Mean Max: 0.9394
Epoch 6/10, Train Loss: 0.1727 (value: 0.0006, weighted value: 0.0285, policy: 0.1442, weighted policy: 0.1442), Train Mean Max: 0.9394
Epoch 7/10, Train Loss: 0.1711 (value: 0.0006, weighted value: 0.0278, policy: 0.1433, weighted policy: 0.1433), Train Mean Max: 0.9397
Epoch 8/10, Train Loss: 0.1701 (value: 0.0005, weighted value: 0.0263, policy: 0.1438, weighted policy: 0.1438), Train Mean Max: 0.9397
Epoch 9/10, Train Loss: 0.1700 (value: 0.0005, weighted value: 0.0260, policy: 0.1440, weighted policy: 0.1440), Train Mean Max: 0.9398
Epoch 10/10, Train Loss: 0.1691 (value: 0.0005, weighted value: 0.0262, policy: 0.1430, weighted policy: 0.1430), Train Mean Max: 0.9401
..training done in 60.20 seconds
..evaluation done in 18.75 seconds
Old network+MCTS average reward: 0.7660, min: 0.1019, max: 1.4630, stdev: 0.2432
New network+MCTS average reward: 0.7647, min: 0.1019, max: 1.4630, stdev: 0.2450
Old bare network average reward: 0.7360, min: 0.0000, max: 1.4537, stdev: 0.2479
New bare network average reward: 0.7323, min: 0.0833, max: 1.4630, stdev: 0.2499
External policy "random" average reward: 0.2762, min: -0.4167, max: 0.8611, stdev: 0.2305
External policy "individual greedy" average reward: 0.5416, min: -0.1111, max: 1.2593, stdev: 0.2308
External policy "total greedy" average reward: 0.6479, min: -0.1019, max: 1.5463, stdev: 0.2356
New network won 46 and tied 192 out of 300 games (47.33% wins where ties are half wins)
Reverting to the old network

Training iteration 722 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.20 seconds
Training examples lengths: [64833, 64331, 65108, 64977, 64627, 64686, 65005, 64801, 64881, 64682]
Total value: 503566.65
Training on 647931 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2072 (value: 0.0010, weighted value: 0.0478, policy: 0.1595, weighted policy: 0.1595), Train Mean Max: 0.9366
Epoch 2/10, Train Loss: 0.1939 (value: 0.0008, weighted value: 0.0402, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9377
Epoch 3/10, Train Loss: 0.1863 (value: 0.0008, weighted value: 0.0378, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9384
Epoch 4/10, Train Loss: 0.1822 (value: 0.0007, weighted value: 0.0356, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9385
Epoch 5/10, Train Loss: 0.1793 (value: 0.0007, weighted value: 0.0341, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9385
Epoch 6/10, Train Loss: 0.1766 (value: 0.0006, weighted value: 0.0317, policy: 0.1449, weighted policy: 0.1449), Train Mean Max: 0.9388
Epoch 7/10, Train Loss: 0.1755 (value: 0.0006, weighted value: 0.0312, policy: 0.1442, weighted policy: 0.1442), Train Mean Max: 0.9390
Epoch 8/10, Train Loss: 0.1752 (value: 0.0006, weighted value: 0.0305, policy: 0.1447, weighted policy: 0.1447), Train Mean Max: 0.9389
Epoch 9/10, Train Loss: 0.1743 (value: 0.0006, weighted value: 0.0298, policy: 0.1445, weighted policy: 0.1445), Train Mean Max: 0.9392
Epoch 10/10, Train Loss: 0.1726 (value: 0.0006, weighted value: 0.0281, policy: 0.1445, weighted policy: 0.1445), Train Mean Max: 0.9394
..training done in 62.59 seconds
..evaluation done in 19.04 seconds
Old network+MCTS average reward: 0.7264, min: 0.1667, max: 1.5370, stdev: 0.2267
New network+MCTS average reward: 0.7291, min: 0.1667, max: 1.5370, stdev: 0.2275
Old bare network average reward: 0.6987, min: 0.1667, max: 1.4722, stdev: 0.2273
New bare network average reward: 0.6994, min: 0.1019, max: 1.4722, stdev: 0.2290
External policy "random" average reward: 0.2365, min: -0.2500, max: 0.8704, stdev: 0.2085
External policy "individual greedy" average reward: 0.5047, min: -0.0556, max: 1.3611, stdev: 0.2178
External policy "total greedy" average reward: 0.6245, min: 0.0463, max: 1.3241, stdev: 0.2122
New network won 62 and tied 192 out of 300 games (52.67% wins where ties are half wins)
Keeping the new network

Training iteration 723 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.32 seconds
Training examples lengths: [64331, 65108, 64977, 64627, 64686, 65005, 64801, 64881, 64682, 64818]
Total value: 503766.24
Training on 647916 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1927 (value: 0.0008, weighted value: 0.0380, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9377
Epoch 2/10, Train Loss: 0.1842 (value: 0.0007, weighted value: 0.0340, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9385
Epoch 3/10, Train Loss: 0.1809 (value: 0.0007, weighted value: 0.0337, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9387
Epoch 4/10, Train Loss: 0.1758 (value: 0.0006, weighted value: 0.0305, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9391
Epoch 5/10, Train Loss: 0.1750 (value: 0.0006, weighted value: 0.0303, policy: 0.1447, weighted policy: 0.1447), Train Mean Max: 0.9390
Epoch 6/10, Train Loss: 0.1740 (value: 0.0006, weighted value: 0.0298, policy: 0.1442, weighted policy: 0.1442), Train Mean Max: 0.9392
Epoch 7/10, Train Loss: 0.1722 (value: 0.0006, weighted value: 0.0278, policy: 0.1444, weighted policy: 0.1444), Train Mean Max: 0.9392
Epoch 8/10, Train Loss: 0.1715 (value: 0.0006, weighted value: 0.0277, policy: 0.1437, weighted policy: 0.1437), Train Mean Max: 0.9394
Epoch 9/10, Train Loss: 0.1714 (value: 0.0005, weighted value: 0.0271, policy: 0.1444, weighted policy: 0.1444), Train Mean Max: 0.9395
Epoch 10/10, Train Loss: 0.1696 (value: 0.0005, weighted value: 0.0258, policy: 0.1438, weighted policy: 0.1438), Train Mean Max: 0.9396
..training done in 63.15 seconds
..evaluation done in 19.78 seconds
Old network+MCTS average reward: 0.7600, min: 0.2315, max: 1.4815, stdev: 0.2389
New network+MCTS average reward: 0.7609, min: 0.2500, max: 1.4722, stdev: 0.2420
Old bare network average reward: 0.7315, min: 0.1667, max: 1.4815, stdev: 0.2391
New bare network average reward: 0.7295, min: 0.1574, max: 1.4815, stdev: 0.2432
External policy "random" average reward: 0.2579, min: -0.3333, max: 0.8981, stdev: 0.2346
External policy "individual greedy" average reward: 0.5431, min: -0.0833, max: 1.3426, stdev: 0.2321
External policy "total greedy" average reward: 0.6527, min: 0.0833, max: 1.3796, stdev: 0.2291
New network won 64 and tied 173 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 724 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.99 seconds
Training examples lengths: [65108, 64977, 64627, 64686, 65005, 64801, 64881, 64682, 64818, 65100]
Total value: 504912.04
Training on 648685 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1900 (value: 0.0007, weighted value: 0.0374, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9382
Epoch 2/10, Train Loss: 0.1816 (value: 0.0007, weighted value: 0.0328, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9388
Epoch 3/10, Train Loss: 0.1771 (value: 0.0006, weighted value: 0.0312, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9390
Epoch 4/10, Train Loss: 0.1752 (value: 0.0006, weighted value: 0.0303, policy: 0.1449, weighted policy: 0.1449), Train Mean Max: 0.9392
Epoch 5/10, Train Loss: 0.1731 (value: 0.0006, weighted value: 0.0286, policy: 0.1445, weighted policy: 0.1445), Train Mean Max: 0.9393
Epoch 6/10, Train Loss: 0.1733 (value: 0.0006, weighted value: 0.0293, policy: 0.1439, weighted policy: 0.1439), Train Mean Max: 0.9395
Epoch 7/10, Train Loss: 0.1706 (value: 0.0005, weighted value: 0.0268, policy: 0.1437, weighted policy: 0.1437), Train Mean Max: 0.9396
Epoch 8/10, Train Loss: 0.1705 (value: 0.0005, weighted value: 0.0265, policy: 0.1440, weighted policy: 0.1440), Train Mean Max: 0.9397
Epoch 9/10, Train Loss: 0.1701 (value: 0.0005, weighted value: 0.0264, policy: 0.1438, weighted policy: 0.1438), Train Mean Max: 0.9398
Epoch 10/10, Train Loss: 0.1680 (value: 0.0005, weighted value: 0.0252, policy: 0.1429, weighted policy: 0.1429), Train Mean Max: 0.9400
..training done in 61.02 seconds
..evaluation done in 18.35 seconds
Old network+MCTS average reward: 0.7630, min: 0.1389, max: 1.9167, stdev: 0.2457
New network+MCTS average reward: 0.7637, min: 0.1389, max: 1.9167, stdev: 0.2454
Old bare network average reward: 0.7377, min: 0.1111, max: 1.8333, stdev: 0.2458
New bare network average reward: 0.7389, min: 0.1111, max: 1.8148, stdev: 0.2498
External policy "random" average reward: 0.2618, min: -0.4259, max: 1.1019, stdev: 0.2267
External policy "individual greedy" average reward: 0.5478, min: 0.0000, max: 1.4722, stdev: 0.2375
External policy "total greedy" average reward: 0.6525, min: 0.1296, max: 1.4537, stdev: 0.2249
New network won 73 and tied 167 out of 300 games (52.17% wins where ties are half wins)
Keeping the new network

Training iteration 725 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.44 seconds
Training examples lengths: [64977, 64627, 64686, 65005, 64801, 64881, 64682, 64818, 65100, 64718]
Total value: 504834.19
Training on 648295 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1880 (value: 0.0007, weighted value: 0.0359, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9382
Epoch 2/10, Train Loss: 0.1813 (value: 0.0006, weighted value: 0.0323, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9388
Epoch 3/10, Train Loss: 0.1763 (value: 0.0006, weighted value: 0.0301, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9392
Epoch 4/10, Train Loss: 0.1744 (value: 0.0006, weighted value: 0.0295, policy: 0.1449, weighted policy: 0.1449), Train Mean Max: 0.9392
Epoch 5/10, Train Loss: 0.1736 (value: 0.0006, weighted value: 0.0288, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9393
Epoch 6/10, Train Loss: 0.1716 (value: 0.0006, weighted value: 0.0276, policy: 0.1439, weighted policy: 0.1439), Train Mean Max: 0.9395
Epoch 7/10, Train Loss: 0.1701 (value: 0.0005, weighted value: 0.0267, policy: 0.1434, weighted policy: 0.1434), Train Mean Max: 0.9396
Epoch 8/10, Train Loss: 0.1696 (value: 0.0005, weighted value: 0.0261, policy: 0.1436, weighted policy: 0.1436), Train Mean Max: 0.9397
Epoch 9/10, Train Loss: 0.1684 (value: 0.0005, weighted value: 0.0248, policy: 0.1436, weighted policy: 0.1436), Train Mean Max: 0.9397
Epoch 10/10, Train Loss: 0.1691 (value: 0.0005, weighted value: 0.0258, policy: 0.1433, weighted policy: 0.1433), Train Mean Max: 0.9399
..training done in 60.67 seconds
..evaluation done in 18.33 seconds
Old network+MCTS average reward: 0.7419, min: 0.1667, max: 1.4259, stdev: 0.2243
New network+MCTS average reward: 0.7421, min: 0.1481, max: 1.4259, stdev: 0.2248
Old bare network average reward: 0.7106, min: 0.0741, max: 1.4259, stdev: 0.2201
New bare network average reward: 0.7113, min: 0.0833, max: 1.4259, stdev: 0.2237
External policy "random" average reward: 0.2366, min: -0.3056, max: 0.9537, stdev: 0.2103
External policy "individual greedy" average reward: 0.5277, min: 0.1019, max: 1.1204, stdev: 0.2066
External policy "total greedy" average reward: 0.6314, min: 0.0741, max: 1.2593, stdev: 0.2123
New network won 64 and tied 183 out of 300 games (51.83% wins where ties are half wins)
Keeping the new network

Training iteration 726 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.73 seconds
Training examples lengths: [64627, 64686, 65005, 64801, 64881, 64682, 64818, 65100, 64718, 64904]
Total value: 504470.51
Training on 648222 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1884 (value: 0.0007, weighted value: 0.0349, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9383
Epoch 2/10, Train Loss: 0.1806 (value: 0.0006, weighted value: 0.0321, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9389
Epoch 3/10, Train Loss: 0.1758 (value: 0.0006, weighted value: 0.0302, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9393
Epoch 4/10, Train Loss: 0.1735 (value: 0.0006, weighted value: 0.0291, policy: 0.1444, weighted policy: 0.1444), Train Mean Max: 0.9394
Epoch 5/10, Train Loss: 0.1710 (value: 0.0005, weighted value: 0.0274, policy: 0.1437, weighted policy: 0.1437), Train Mean Max: 0.9396
Epoch 6/10, Train Loss: 0.1709 (value: 0.0005, weighted value: 0.0269, policy: 0.1440, weighted policy: 0.1440), Train Mean Max: 0.9396
Epoch 7/10, Train Loss: 0.1704 (value: 0.0005, weighted value: 0.0267, policy: 0.1437, weighted policy: 0.1437), Train Mean Max: 0.9397
Epoch 8/10, Train Loss: 0.1691 (value: 0.0005, weighted value: 0.0253, policy: 0.1438, weighted policy: 0.1438), Train Mean Max: 0.9398
Epoch 9/10, Train Loss: 0.1688 (value: 0.0005, weighted value: 0.0254, policy: 0.1434, weighted policy: 0.1434), Train Mean Max: 0.9400
Epoch 10/10, Train Loss: 0.1670 (value: 0.0005, weighted value: 0.0240, policy: 0.1430, weighted policy: 0.1430), Train Mean Max: 0.9401
..training done in 59.72 seconds
..evaluation done in 17.97 seconds
Old network+MCTS average reward: 0.7851, min: 0.0278, max: 1.4259, stdev: 0.2230
New network+MCTS average reward: 0.7839, min: 0.1019, max: 1.4259, stdev: 0.2202
Old bare network average reward: 0.7524, min: 0.0370, max: 1.4259, stdev: 0.2247
New bare network average reward: 0.7540, min: 0.0370, max: 1.4259, stdev: 0.2236
External policy "random" average reward: 0.2802, min: -0.2222, max: 0.8519, stdev: 0.2182
External policy "individual greedy" average reward: 0.5585, min: -0.1111, max: 1.1852, stdev: 0.2144
External policy "total greedy" average reward: 0.6721, min: 0.0926, max: 1.2963, stdev: 0.2139
New network won 60 and tied 185 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 727 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.45 seconds
Training examples lengths: [64686, 65005, 64801, 64881, 64682, 64818, 65100, 64718, 64904, 65127]
Total value: 504685.26
Training on 648722 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1884 (value: 0.0007, weighted value: 0.0359, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9384
Epoch 2/10, Train Loss: 0.1800 (value: 0.0006, weighted value: 0.0318, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9391
Epoch 3/10, Train Loss: 0.1761 (value: 0.0006, weighted value: 0.0305, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9393
Epoch 4/10, Train Loss: 0.1724 (value: 0.0006, weighted value: 0.0282, policy: 0.1442, weighted policy: 0.1442), Train Mean Max: 0.9397
Epoch 5/10, Train Loss: 0.1715 (value: 0.0006, weighted value: 0.0280, policy: 0.1436, weighted policy: 0.1436), Train Mean Max: 0.9396
Epoch 6/10, Train Loss: 0.1704 (value: 0.0005, weighted value: 0.0269, policy: 0.1435, weighted policy: 0.1435), Train Mean Max: 0.9398
Epoch 7/10, Train Loss: 0.1694 (value: 0.0005, weighted value: 0.0264, policy: 0.1430, weighted policy: 0.1430), Train Mean Max: 0.9398
Epoch 8/10, Train Loss: 0.1685 (value: 0.0005, weighted value: 0.0251, policy: 0.1434, weighted policy: 0.1434), Train Mean Max: 0.9399
Epoch 9/10, Train Loss: 0.1687 (value: 0.0005, weighted value: 0.0255, policy: 0.1432, weighted policy: 0.1432), Train Mean Max: 0.9400
Epoch 10/10, Train Loss: 0.1673 (value: 0.0005, weighted value: 0.0242, policy: 0.1431, weighted policy: 0.1431), Train Mean Max: 0.9401
..training done in 65.41 seconds
..evaluation done in 18.34 seconds
Old network+MCTS average reward: 0.7748, min: 0.1574, max: 1.5000, stdev: 0.2329
New network+MCTS average reward: 0.7747, min: 0.1667, max: 1.5000, stdev: 0.2340
Old bare network average reward: 0.7465, min: 0.1204, max: 1.5185, stdev: 0.2346
New bare network average reward: 0.7473, min: 0.1204, max: 1.5185, stdev: 0.2368
External policy "random" average reward: 0.2596, min: -0.3426, max: 1.0278, stdev: 0.2126
External policy "individual greedy" average reward: 0.5437, min: 0.0278, max: 1.1111, stdev: 0.2172
External policy "total greedy" average reward: 0.6579, min: 0.0556, max: 1.2407, stdev: 0.2159
New network won 57 and tied 188 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 728 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.50 seconds
Training examples lengths: [65005, 64801, 64881, 64682, 64818, 65100, 64718, 64904, 65127, 64959]
Total value: 505211.46
Training on 648995 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1861 (value: 0.0007, weighted value: 0.0343, policy: 0.1518, weighted policy: 0.1518), Train Mean Max: 0.9387
Epoch 2/10, Train Loss: 0.1799 (value: 0.0006, weighted value: 0.0316, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9392
Epoch 3/10, Train Loss: 0.1750 (value: 0.0006, weighted value: 0.0299, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9396
Epoch 4/10, Train Loss: 0.1736 (value: 0.0006, weighted value: 0.0289, policy: 0.1447, weighted policy: 0.1447), Train Mean Max: 0.9396
Epoch 5/10, Train Loss: 0.1699 (value: 0.0005, weighted value: 0.0267, policy: 0.1433, weighted policy: 0.1433), Train Mean Max: 0.9399
Epoch 6/10, Train Loss: 0.1709 (value: 0.0006, weighted value: 0.0275, policy: 0.1434, weighted policy: 0.1434), Train Mean Max: 0.9399
Epoch 7/10, Train Loss: 0.1693 (value: 0.0005, weighted value: 0.0266, policy: 0.1427, weighted policy: 0.1427), Train Mean Max: 0.9400
Epoch 8/10, Train Loss: 0.1680 (value: 0.0005, weighted value: 0.0252, policy: 0.1428, weighted policy: 0.1428), Train Mean Max: 0.9400
Epoch 9/10, Train Loss: 0.1677 (value: 0.0005, weighted value: 0.0246, policy: 0.1431, weighted policy: 0.1431), Train Mean Max: 0.9402
Epoch 10/10, Train Loss: 0.1670 (value: 0.0005, weighted value: 0.0241, policy: 0.1428, weighted policy: 0.1428), Train Mean Max: 0.9403
..training done in 59.98 seconds
..evaluation done in 17.93 seconds
Old network+MCTS average reward: 0.7578, min: 0.1852, max: 1.3704, stdev: 0.2344
New network+MCTS average reward: 0.7581, min: 0.1389, max: 1.3981, stdev: 0.2344
Old bare network average reward: 0.7303, min: 0.1389, max: 1.3333, stdev: 0.2380
New bare network average reward: 0.7281, min: 0.1389, max: 1.3981, stdev: 0.2361
External policy "random" average reward: 0.2527, min: -0.2500, max: 0.8148, stdev: 0.2089
External policy "individual greedy" average reward: 0.5315, min: 0.0556, max: 1.0926, stdev: 0.2228
External policy "total greedy" average reward: 0.6432, min: 0.0926, max: 1.2222, stdev: 0.2174
New network won 55 and tied 194 out of 300 games (50.67% wins where ties are half wins)
Keeping the new network

Training iteration 729 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 65.59 seconds
Training examples lengths: [64801, 64881, 64682, 64818, 65100, 64718, 64904, 65127, 64959, 64679]
Total value: 503926.50
Training on 648669 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1880 (value: 0.0007, weighted value: 0.0355, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9382
Epoch 2/10, Train Loss: 0.1811 (value: 0.0006, weighted value: 0.0320, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9390
Epoch 3/10, Train Loss: 0.1762 (value: 0.0006, weighted value: 0.0296, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9392
Epoch 4/10, Train Loss: 0.1724 (value: 0.0006, weighted value: 0.0277, policy: 0.1446, weighted policy: 0.1446), Train Mean Max: 0.9396
Epoch 5/10, Train Loss: 0.1715 (value: 0.0006, weighted value: 0.0278, policy: 0.1437, weighted policy: 0.1437), Train Mean Max: 0.9396
Epoch 6/10, Train Loss: 0.1706 (value: 0.0005, weighted value: 0.0268, policy: 0.1438, weighted policy: 0.1438), Train Mean Max: 0.9395
Epoch 7/10, Train Loss: 0.1707 (value: 0.0005, weighted value: 0.0267, policy: 0.1440, weighted policy: 0.1440), Train Mean Max: 0.9395
Epoch 8/10, Train Loss: 0.1679 (value: 0.0005, weighted value: 0.0244, policy: 0.1435, weighted policy: 0.1435), Train Mean Max: 0.9399
Epoch 9/10, Train Loss: 0.1688 (value: 0.0005, weighted value: 0.0251, policy: 0.1438, weighted policy: 0.1438), Train Mean Max: 0.9398
Epoch 10/10, Train Loss: 0.1677 (value: 0.0005, weighted value: 0.0246, policy: 0.1432, weighted policy: 0.1432), Train Mean Max: 0.9400
..training done in 70.54 seconds
..evaluation done in 17.81 seconds
Old network+MCTS average reward: 0.7840, min: 0.1204, max: 1.3889, stdev: 0.2279
New network+MCTS average reward: 0.7827, min: 0.1204, max: 1.4815, stdev: 0.2290
Old bare network average reward: 0.7510, min: 0.1204, max: 1.3889, stdev: 0.2322
New bare network average reward: 0.7561, min: 0.1204, max: 1.3889, stdev: 0.2295
External policy "random" average reward: 0.2874, min: -0.3148, max: 0.9907, stdev: 0.2263
External policy "individual greedy" average reward: 0.5527, min: -0.0556, max: 1.2685, stdev: 0.2255
External policy "total greedy" average reward: 0.6680, min: 0.0833, max: 1.3333, stdev: 0.2184
New network won 48 and tied 183 out of 300 games (46.50% wins where ties are half wins)
Reverting to the old network

Training iteration 730 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.56 seconds
Training examples lengths: [64881, 64682, 64818, 65100, 64718, 64904, 65127, 64959, 64679, 64776]
Total value: 504379.38
Training on 648644 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2061 (value: 0.0009, weighted value: 0.0463, policy: 0.1598, weighted policy: 0.1598), Train Mean Max: 0.9367
Epoch 2/10, Train Loss: 0.1928 (value: 0.0008, weighted value: 0.0393, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9379
Epoch 3/10, Train Loss: 0.1859 (value: 0.0007, weighted value: 0.0365, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9383
Epoch 4/10, Train Loss: 0.1812 (value: 0.0007, weighted value: 0.0343, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9384
Epoch 5/10, Train Loss: 0.1771 (value: 0.0006, weighted value: 0.0316, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9388
Epoch 6/10, Train Loss: 0.1765 (value: 0.0006, weighted value: 0.0313, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9387
Epoch 7/10, Train Loss: 0.1758 (value: 0.0006, weighted value: 0.0307, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9389
Epoch 8/10, Train Loss: 0.1737 (value: 0.0006, weighted value: 0.0290, policy: 0.1447, weighted policy: 0.1447), Train Mean Max: 0.9390
Epoch 9/10, Train Loss: 0.1717 (value: 0.0006, weighted value: 0.0276, policy: 0.1440, weighted policy: 0.1440), Train Mean Max: 0.9393
Epoch 10/10, Train Loss: 0.1712 (value: 0.0005, weighted value: 0.0273, policy: 0.1439, weighted policy: 0.1439), Train Mean Max: 0.9394
..training done in 68.18 seconds
..evaluation done in 19.00 seconds
Old network+MCTS average reward: 0.7805, min: 0.1111, max: 1.5093, stdev: 0.2366
New network+MCTS average reward: 0.7787, min: 0.1204, max: 1.4815, stdev: 0.2375
Old bare network average reward: 0.7506, min: 0.0833, max: 1.5093, stdev: 0.2396
New bare network average reward: 0.7501, min: 0.0833, max: 1.4815, stdev: 0.2406
External policy "random" average reward: 0.2744, min: -0.2500, max: 0.9722, stdev: 0.2101
External policy "individual greedy" average reward: 0.5555, min: -0.0185, max: 1.2222, stdev: 0.2089
External policy "total greedy" average reward: 0.6636, min: 0.0556, max: 1.3426, stdev: 0.2076
New network won 52 and tied 181 out of 300 games (47.50% wins where ties are half wins)
Reverting to the old network

Training iteration 731 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.10 seconds
Training examples lengths: [64682, 64818, 65100, 64718, 64904, 65127, 64959, 64679, 64776, 64943]
Total value: 504495.43
Training on 648706 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2237 (value: 0.0011, weighted value: 0.0568, policy: 0.1669, weighted policy: 0.1669), Train Mean Max: 0.9352
Epoch 2/10, Train Loss: 0.2048 (value: 0.0009, weighted value: 0.0469, policy: 0.1579, weighted policy: 0.1579), Train Mean Max: 0.9367
Epoch 3/10, Train Loss: 0.1941 (value: 0.0008, weighted value: 0.0422, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9373
Epoch 4/10, Train Loss: 0.1883 (value: 0.0008, weighted value: 0.0396, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1844 (value: 0.0007, weighted value: 0.0363, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1825 (value: 0.0007, weighted value: 0.0350, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9379
Epoch 7/10, Train Loss: 0.1801 (value: 0.0007, weighted value: 0.0342, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9381
Epoch 8/10, Train Loss: 0.1778 (value: 0.0006, weighted value: 0.0320, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1763 (value: 0.0006, weighted value: 0.0311, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1754 (value: 0.0006, weighted value: 0.0301, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9387
..training done in 59.87 seconds
..evaluation done in 18.72 seconds
Old network+MCTS average reward: 0.7687, min: 0.2222, max: 1.6481, stdev: 0.2481
New network+MCTS average reward: 0.7685, min: 0.2407, max: 1.5093, stdev: 0.2444
Old bare network average reward: 0.7412, min: 0.1852, max: 1.5185, stdev: 0.2511
New bare network average reward: 0.7404, min: 0.1852, max: 1.5093, stdev: 0.2518
External policy "random" average reward: 0.2557, min: -0.2963, max: 1.1481, stdev: 0.2316
External policy "individual greedy" average reward: 0.5318, min: 0.0463, max: 1.3519, stdev: 0.2380
External policy "total greedy" average reward: 0.6519, min: 0.1111, max: 1.5185, stdev: 0.2331
New network won 56 and tied 179 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 732 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.89 seconds
Training examples lengths: [64818, 65100, 64718, 64904, 65127, 64959, 64679, 64776, 64943, 64878]
Total value: 504532.38
Training on 648902 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2407 (value: 0.0014, weighted value: 0.0675, policy: 0.1732, weighted policy: 0.1732), Train Mean Max: 0.9338
Epoch 2/10, Train Loss: 0.2148 (value: 0.0011, weighted value: 0.0536, policy: 0.1612, weighted policy: 0.1612), Train Mean Max: 0.9355
Epoch 3/10, Train Loss: 0.2032 (value: 0.0010, weighted value: 0.0480, policy: 0.1552, weighted policy: 0.1552), Train Mean Max: 0.9361
Epoch 4/10, Train Loss: 0.1960 (value: 0.0009, weighted value: 0.0441, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9366
Epoch 5/10, Train Loss: 0.1906 (value: 0.0008, weighted value: 0.0413, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1873 (value: 0.0008, weighted value: 0.0388, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1858 (value: 0.0008, weighted value: 0.0380, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1822 (value: 0.0007, weighted value: 0.0355, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1803 (value: 0.0007, weighted value: 0.0335, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1789 (value: 0.0007, weighted value: 0.0326, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9378
..training done in 64.00 seconds
..evaluation done in 18.71 seconds
Old network+MCTS average reward: 0.7784, min: 0.1667, max: 1.4815, stdev: 0.2353
New network+MCTS average reward: 0.7798, min: 0.1944, max: 1.4815, stdev: 0.2388
Old bare network average reward: 0.7525, min: 0.1481, max: 1.4815, stdev: 0.2387
New bare network average reward: 0.7494, min: 0.1389, max: 1.4815, stdev: 0.2428
External policy "random" average reward: 0.2715, min: -0.3611, max: 0.8426, stdev: 0.2171
External policy "individual greedy" average reward: 0.5450, min: -0.0556, max: 1.1019, stdev: 0.2161
External policy "total greedy" average reward: 0.6682, min: 0.0463, max: 1.4167, stdev: 0.2223
New network won 64 and tied 179 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network

Training iteration 733 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 60.95 seconds
Training examples lengths: [65100, 64718, 64904, 65127, 64959, 64679, 64776, 64943, 64878, 64911]
Total value: 504889.81
Training on 648995 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1979 (value: 0.0009, weighted value: 0.0434, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9366
Epoch 2/10, Train Loss: 0.1886 (value: 0.0008, weighted value: 0.0377, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9374
Epoch 3/10, Train Loss: 0.1850 (value: 0.0007, weighted value: 0.0362, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9376
Epoch 4/10, Train Loss: 0.1802 (value: 0.0007, weighted value: 0.0338, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9380
Epoch 5/10, Train Loss: 0.1795 (value: 0.0007, weighted value: 0.0328, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9381
Epoch 6/10, Train Loss: 0.1789 (value: 0.0007, weighted value: 0.0327, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9382
Epoch 7/10, Train Loss: 0.1763 (value: 0.0006, weighted value: 0.0307, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9384
Epoch 8/10, Train Loss: 0.1750 (value: 0.0006, weighted value: 0.0298, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9386
Epoch 9/10, Train Loss: 0.1730 (value: 0.0006, weighted value: 0.0283, policy: 0.1447, weighted policy: 0.1447), Train Mean Max: 0.9388
Epoch 10/10, Train Loss: 0.1738 (value: 0.0006, weighted value: 0.0290, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9389
..training done in 67.73 seconds
..evaluation done in 18.96 seconds
Old network+MCTS average reward: 0.7591, min: 0.2593, max: 1.3981, stdev: 0.2342
New network+MCTS average reward: 0.7571, min: 0.1852, max: 1.4074, stdev: 0.2347
Old bare network average reward: 0.7356, min: 0.1481, max: 1.3889, stdev: 0.2416
New bare network average reward: 0.7331, min: 0.1667, max: 1.4352, stdev: 0.2389
External policy "random" average reward: 0.2597, min: -0.4352, max: 0.9259, stdev: 0.2287
External policy "individual greedy" average reward: 0.5350, min: -0.0093, max: 1.1296, stdev: 0.2259
External policy "total greedy" average reward: 0.6444, min: -0.0093, max: 1.2778, stdev: 0.2229
New network won 54 and tied 178 out of 300 games (47.67% wins where ties are half wins)
Reverting to the old network

Training iteration 734 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.83 seconds
Training examples lengths: [64718, 64904, 65127, 64959, 64679, 64776, 64943, 64878, 64911, 64515]
Total value: 503789.32
Training on 648410 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2150 (value: 0.0011, weighted value: 0.0532, policy: 0.1618, weighted policy: 0.1618), Train Mean Max: 0.9351
Epoch 2/10, Train Loss: 0.2008 (value: 0.0009, weighted value: 0.0452, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9361
Epoch 3/10, Train Loss: 0.1931 (value: 0.0008, weighted value: 0.0420, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9366
Epoch 4/10, Train Loss: 0.1883 (value: 0.0008, weighted value: 0.0392, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9369
Epoch 5/10, Train Loss: 0.1863 (value: 0.0008, weighted value: 0.0380, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9371
Epoch 6/10, Train Loss: 0.1823 (value: 0.0007, weighted value: 0.0356, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9375
Epoch 7/10, Train Loss: 0.1801 (value: 0.0007, weighted value: 0.0334, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9376
Epoch 8/10, Train Loss: 0.1805 (value: 0.0007, weighted value: 0.0338, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9377
Epoch 9/10, Train Loss: 0.1784 (value: 0.0007, weighted value: 0.0326, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1767 (value: 0.0006, weighted value: 0.0306, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9382
..training done in 66.96 seconds
..evaluation done in 19.56 seconds
Old network+MCTS average reward: 0.7718, min: 0.0093, max: 1.6111, stdev: 0.2501
New network+MCTS average reward: 0.7739, min: 0.0278, max: 1.6111, stdev: 0.2517
Old bare network average reward: 0.7453, min: -0.1019, max: 1.6111, stdev: 0.2533
New bare network average reward: 0.7461, min: -0.1019, max: 1.5463, stdev: 0.2513
External policy "random" average reward: 0.2693, min: -0.3426, max: 1.2315, stdev: 0.2271
External policy "individual greedy" average reward: 0.5408, min: -0.0463, max: 1.3889, stdev: 0.2323
External policy "total greedy" average reward: 0.6545, min: -0.0833, max: 1.4074, stdev: 0.2235
New network won 68 and tied 174 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 735 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.88 seconds
Training examples lengths: [64904, 65127, 64959, 64679, 64776, 64943, 64878, 64911, 64515, 64996]
Total value: 503392.29
Training on 648688 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1954 (value: 0.0008, weighted value: 0.0407, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9366
Epoch 2/10, Train Loss: 0.1867 (value: 0.0007, weighted value: 0.0360, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9374
Epoch 3/10, Train Loss: 0.1839 (value: 0.0007, weighted value: 0.0353, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9376
Epoch 4/10, Train Loss: 0.1799 (value: 0.0007, weighted value: 0.0328, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9378
Epoch 5/10, Train Loss: 0.1792 (value: 0.0006, weighted value: 0.0324, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9380
Epoch 6/10, Train Loss: 0.1759 (value: 0.0006, weighted value: 0.0300, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9383
Epoch 7/10, Train Loss: 0.1753 (value: 0.0006, weighted value: 0.0300, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9384
Epoch 8/10, Train Loss: 0.1744 (value: 0.0006, weighted value: 0.0290, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9385
Epoch 9/10, Train Loss: 0.1735 (value: 0.0006, weighted value: 0.0281, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9386
Epoch 10/10, Train Loss: 0.1732 (value: 0.0006, weighted value: 0.0284, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9388
..training done in 64.16 seconds
..evaluation done in 19.49 seconds
Old network+MCTS average reward: 0.7846, min: 0.0648, max: 1.5463, stdev: 0.2419
New network+MCTS average reward: 0.7859, min: 0.0648, max: 1.5926, stdev: 0.2445
Old bare network average reward: 0.7561, min: 0.0185, max: 1.4815, stdev: 0.2474
New bare network average reward: 0.7543, min: 0.0185, max: 1.4815, stdev: 0.2474
External policy "random" average reward: 0.2785, min: -0.2685, max: 0.8889, stdev: 0.2373
External policy "individual greedy" average reward: 0.5628, min: -0.1019, max: 1.4167, stdev: 0.2422
External policy "total greedy" average reward: 0.6747, min: 0.0741, max: 1.4259, stdev: 0.2303
New network won 61 and tied 183 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 736 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.51 seconds
Training examples lengths: [65127, 64959, 64679, 64776, 64943, 64878, 64911, 64515, 64996, 64721]
Total value: 503737.84
Training on 648505 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1917 (value: 0.0008, weighted value: 0.0384, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9372
Epoch 2/10, Train Loss: 0.1846 (value: 0.0007, weighted value: 0.0342, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9379
Epoch 3/10, Train Loss: 0.1801 (value: 0.0006, weighted value: 0.0324, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9382
Epoch 4/10, Train Loss: 0.1773 (value: 0.0006, weighted value: 0.0315, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9386
Epoch 5/10, Train Loss: 0.1754 (value: 0.0006, weighted value: 0.0295, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9385
Epoch 6/10, Train Loss: 0.1736 (value: 0.0006, weighted value: 0.0288, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9388
Epoch 7/10, Train Loss: 0.1737 (value: 0.0006, weighted value: 0.0286, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9388
Epoch 8/10, Train Loss: 0.1725 (value: 0.0005, weighted value: 0.0274, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9390
Epoch 9/10, Train Loss: 0.1717 (value: 0.0005, weighted value: 0.0270, policy: 0.1447, weighted policy: 0.1447), Train Mean Max: 0.9392
Epoch 10/10, Train Loss: 0.1713 (value: 0.0005, weighted value: 0.0264, policy: 0.1449, weighted policy: 0.1449), Train Mean Max: 0.9392
..training done in 66.37 seconds
..evaluation done in 24.43 seconds
Old network+MCTS average reward: 0.7610, min: 0.1019, max: 1.3796, stdev: 0.2266
New network+MCTS average reward: 0.7613, min: 0.1389, max: 1.3796, stdev: 0.2283
Old bare network average reward: 0.7348, min: 0.1019, max: 1.3796, stdev: 0.2345
New bare network average reward: 0.7319, min: 0.0926, max: 1.2963, stdev: 0.2366
External policy "random" average reward: 0.2366, min: -0.3611, max: 0.9444, stdev: 0.2257
External policy "individual greedy" average reward: 0.5311, min: -0.0370, max: 1.2500, stdev: 0.2318
External policy "total greedy" average reward: 0.6439, min: 0.0556, max: 1.2500, stdev: 0.2238
New network won 56 and tied 187 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 737 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 60.25 seconds
Training examples lengths: [64959, 64679, 64776, 64943, 64878, 64911, 64515, 64996, 64721, 64683]
Total value: 503019.64
Training on 648061 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2110 (value: 0.0010, weighted value: 0.0490, policy: 0.1619, weighted policy: 0.1619), Train Mean Max: 0.9356
Epoch 2/10, Train Loss: 0.1968 (value: 0.0008, weighted value: 0.0419, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9367
Epoch 3/10, Train Loss: 0.1900 (value: 0.0008, weighted value: 0.0388, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9370
Epoch 4/10, Train Loss: 0.1843 (value: 0.0007, weighted value: 0.0358, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9374
Epoch 5/10, Train Loss: 0.1818 (value: 0.0007, weighted value: 0.0343, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9375
Epoch 6/10, Train Loss: 0.1801 (value: 0.0007, weighted value: 0.0332, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9377
Epoch 7/10, Train Loss: 0.1786 (value: 0.0006, weighted value: 0.0316, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9378
Epoch 8/10, Train Loss: 0.1781 (value: 0.0006, weighted value: 0.0314, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9379
Epoch 9/10, Train Loss: 0.1757 (value: 0.0006, weighted value: 0.0295, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9381
Epoch 10/10, Train Loss: 0.1758 (value: 0.0006, weighted value: 0.0289, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9383
..training done in 66.85 seconds
..evaluation done in 18.38 seconds
Old network+MCTS average reward: 0.7636, min: 0.1852, max: 1.4259, stdev: 0.2280
New network+MCTS average reward: 0.7616, min: 0.1852, max: 1.4352, stdev: 0.2316
Old bare network average reward: 0.7394, min: 0.1481, max: 1.4259, stdev: 0.2311
New bare network average reward: 0.7372, min: 0.1481, max: 1.4352, stdev: 0.2334
External policy "random" average reward: 0.2471, min: -0.4444, max: 0.8426, stdev: 0.2221
External policy "individual greedy" average reward: 0.5247, min: -0.1019, max: 1.1944, stdev: 0.2211
External policy "total greedy" average reward: 0.6469, min: 0.0926, max: 1.2870, stdev: 0.2155
New network won 55 and tied 178 out of 300 games (48.00% wins where ties are half wins)
Reverting to the old network

Training iteration 738 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.49 seconds
Training examples lengths: [64679, 64776, 64943, 64878, 64911, 64515, 64996, 64721, 64683, 64926]
Total value: 502946.28
Training on 648028 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2266 (value: 0.0012, weighted value: 0.0586, policy: 0.1681, weighted policy: 0.1681), Train Mean Max: 0.9342
Epoch 2/10, Train Loss: 0.2080 (value: 0.0010, weighted value: 0.0489, policy: 0.1591, weighted policy: 0.1591), Train Mean Max: 0.9358
Epoch 3/10, Train Loss: 0.1981 (value: 0.0009, weighted value: 0.0448, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9363
Epoch 4/10, Train Loss: 0.1917 (value: 0.0008, weighted value: 0.0405, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9365
Epoch 5/10, Train Loss: 0.1890 (value: 0.0008, weighted value: 0.0390, policy: 0.1500, weighted policy: 0.1500), Train Mean Max: 0.9368
Epoch 6/10, Train Loss: 0.1862 (value: 0.0007, weighted value: 0.0373, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9369
Epoch 7/10, Train Loss: 0.1840 (value: 0.0007, weighted value: 0.0355, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9371
Epoch 8/10, Train Loss: 0.1811 (value: 0.0007, weighted value: 0.0335, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1801 (value: 0.0007, weighted value: 0.0330, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9375
Epoch 10/10, Train Loss: 0.1779 (value: 0.0006, weighted value: 0.0314, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9378
..training done in 59.54 seconds
..evaluation done in 18.88 seconds
Old network+MCTS average reward: 0.7502, min: 0.1019, max: 1.6111, stdev: 0.2361
New network+MCTS average reward: 0.7495, min: 0.1019, max: 1.6111, stdev: 0.2349
Old bare network average reward: 0.7266, min: 0.1019, max: 1.6111, stdev: 0.2417
New bare network average reward: 0.7262, min: 0.1019, max: 1.6111, stdev: 0.2434
External policy "random" average reward: 0.2409, min: -0.3148, max: 0.9259, stdev: 0.2301
External policy "individual greedy" average reward: 0.5213, min: -0.0741, max: 1.2130, stdev: 0.2244
External policy "total greedy" average reward: 0.6438, min: 0.0093, max: 1.2870, stdev: 0.2247
New network won 53 and tied 193 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 739 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.54 seconds
Training examples lengths: [64776, 64943, 64878, 64911, 64515, 64996, 64721, 64683, 64926, 64628]
Total value: 503360.04
Training on 647977 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2426 (value: 0.0014, weighted value: 0.0677, policy: 0.1749, weighted policy: 0.1749), Train Mean Max: 0.9334
Epoch 2/10, Train Loss: 0.2189 (value: 0.0011, weighted value: 0.0563, policy: 0.1625, weighted policy: 0.1625), Train Mean Max: 0.9351
Epoch 3/10, Train Loss: 0.2059 (value: 0.0010, weighted value: 0.0500, policy: 0.1559, weighted policy: 0.1559), Train Mean Max: 0.9356
Epoch 4/10, Train Loss: 0.1990 (value: 0.0009, weighted value: 0.0466, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9359
Epoch 5/10, Train Loss: 0.1937 (value: 0.0009, weighted value: 0.0432, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9362
Epoch 6/10, Train Loss: 0.1891 (value: 0.0008, weighted value: 0.0398, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9365
Epoch 7/10, Train Loss: 0.1869 (value: 0.0008, weighted value: 0.0383, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9367
Epoch 8/10, Train Loss: 0.1862 (value: 0.0008, weighted value: 0.0377, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9369
Epoch 9/10, Train Loss: 0.1838 (value: 0.0007, weighted value: 0.0356, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9371
Epoch 10/10, Train Loss: 0.1812 (value: 0.0007, weighted value: 0.0338, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9374
..training done in 59.95 seconds
..evaluation done in 19.17 seconds
Old network+MCTS average reward: 0.7689, min: 0.0833, max: 1.4074, stdev: 0.2365
New network+MCTS average reward: 0.7715, min: 0.0648, max: 1.4352, stdev: 0.2379
Old bare network average reward: 0.7415, min: -0.0278, max: 1.4537, stdev: 0.2394
New bare network average reward: 0.7398, min: 0.0463, max: 1.4537, stdev: 0.2426
External policy "random" average reward: 0.2409, min: -0.3333, max: 0.8889, stdev: 0.2201
External policy "individual greedy" average reward: 0.5282, min: -0.0926, max: 1.1852, stdev: 0.2370
External policy "total greedy" average reward: 0.6451, min: 0.0278, max: 1.2037, stdev: 0.2239
New network won 58 and tied 187 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 740 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.66 seconds
Training examples lengths: [64943, 64878, 64911, 64515, 64996, 64721, 64683, 64926, 64628, 64708]
Total value: 502547.33
Training on 647909 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1977 (value: 0.0009, weighted value: 0.0426, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9360
Epoch 2/10, Train Loss: 0.1922 (value: 0.0008, weighted value: 0.0399, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9367
Epoch 3/10, Train Loss: 0.1857 (value: 0.0007, weighted value: 0.0364, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9372
Epoch 4/10, Train Loss: 0.1826 (value: 0.0007, weighted value: 0.0345, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9373
Epoch 5/10, Train Loss: 0.1817 (value: 0.0007, weighted value: 0.0347, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9376
Epoch 6/10, Train Loss: 0.1795 (value: 0.0006, weighted value: 0.0324, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9376
Epoch 7/10, Train Loss: 0.1788 (value: 0.0006, weighted value: 0.0322, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9380
Epoch 8/10, Train Loss: 0.1772 (value: 0.0006, weighted value: 0.0304, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9380
Epoch 9/10, Train Loss: 0.1756 (value: 0.0006, weighted value: 0.0293, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9383
Epoch 10/10, Train Loss: 0.1754 (value: 0.0006, weighted value: 0.0286, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9383
..training done in 59.49 seconds
..evaluation done in 20.13 seconds
Old network+MCTS average reward: 0.7686, min: 0.2407, max: 1.5463, stdev: 0.2404
New network+MCTS average reward: 0.7694, min: 0.2407, max: 1.5463, stdev: 0.2409
Old bare network average reward: 0.7419, min: 0.2037, max: 1.5463, stdev: 0.2427
New bare network average reward: 0.7412, min: 0.2222, max: 1.5000, stdev: 0.2456
External policy "random" average reward: 0.2776, min: -0.2500, max: 0.9259, stdev: 0.2236
External policy "individual greedy" average reward: 0.5579, min: -0.0833, max: 1.1204, stdev: 0.2179
External policy "total greedy" average reward: 0.6656, min: -0.0093, max: 1.2130, stdev: 0.2194
New network won 70 and tied 163 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_740

Training iteration 741 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.00 seconds
Training examples lengths: [64878, 64911, 64515, 64996, 64721, 64683, 64926, 64628, 64708, 64683]
Total value: 501678.73
Training on 647649 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1926 (value: 0.0008, weighted value: 0.0383, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9371
Epoch 2/10, Train Loss: 0.1874 (value: 0.0007, weighted value: 0.0364, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9377
Epoch 3/10, Train Loss: 0.1818 (value: 0.0007, weighted value: 0.0335, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9380
Epoch 4/10, Train Loss: 0.1792 (value: 0.0006, weighted value: 0.0323, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9382
Epoch 5/10, Train Loss: 0.1764 (value: 0.0006, weighted value: 0.0303, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9383
Epoch 6/10, Train Loss: 0.1773 (value: 0.0006, weighted value: 0.0309, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9384
Epoch 7/10, Train Loss: 0.1739 (value: 0.0006, weighted value: 0.0285, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9388
Epoch 8/10, Train Loss: 0.1752 (value: 0.0006, weighted value: 0.0297, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9387
Epoch 9/10, Train Loss: 0.1717 (value: 0.0005, weighted value: 0.0269, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9390
Epoch 10/10, Train Loss: 0.1715 (value: 0.0005, weighted value: 0.0263, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9391
..training done in 59.76 seconds
..evaluation done in 18.26 seconds
Old network+MCTS average reward: 0.7770, min: 0.2315, max: 1.5278, stdev: 0.2346
New network+MCTS average reward: 0.7785, min: 0.2315, max: 1.5278, stdev: 0.2338
Old bare network average reward: 0.7497, min: 0.2222, max: 1.5278, stdev: 0.2415
New bare network average reward: 0.7497, min: 0.1667, max: 1.5278, stdev: 0.2375
External policy "random" average reward: 0.2666, min: -0.4630, max: 1.0370, stdev: 0.2285
External policy "individual greedy" average reward: 0.5478, min: 0.0926, max: 1.2500, stdev: 0.2154
External policy "total greedy" average reward: 0.6619, min: 0.1852, max: 1.4074, stdev: 0.2153
New network won 67 and tied 171 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 742 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.33 seconds
Training examples lengths: [64911, 64515, 64996, 64721, 64683, 64926, 64628, 64708, 64683, 64804]
Total value: 501347.69
Training on 647575 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1917 (value: 0.0007, weighted value: 0.0373, policy: 0.1544, weighted policy: 0.1544), Train Mean Max: 0.9374
Epoch 2/10, Train Loss: 0.1841 (value: 0.0007, weighted value: 0.0333, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9384
Epoch 3/10, Train Loss: 0.1792 (value: 0.0006, weighted value: 0.0317, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9386
Epoch 4/10, Train Loss: 0.1758 (value: 0.0006, weighted value: 0.0301, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9386
Epoch 5/10, Train Loss: 0.1754 (value: 0.0006, weighted value: 0.0297, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9388
Epoch 6/10, Train Loss: 0.1737 (value: 0.0006, weighted value: 0.0286, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9390
Epoch 7/10, Train Loss: 0.1728 (value: 0.0005, weighted value: 0.0274, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9391
Epoch 8/10, Train Loss: 0.1718 (value: 0.0005, weighted value: 0.0269, policy: 0.1449, weighted policy: 0.1449), Train Mean Max: 0.9392
Epoch 9/10, Train Loss: 0.1721 (value: 0.0005, weighted value: 0.0272, policy: 0.1449, weighted policy: 0.1449), Train Mean Max: 0.9392
Epoch 10/10, Train Loss: 0.1698 (value: 0.0005, weighted value: 0.0257, policy: 0.1441, weighted policy: 0.1441), Train Mean Max: 0.9394
..training done in 59.97 seconds
..evaluation done in 18.96 seconds
Old network+MCTS average reward: 0.7592, min: 0.0926, max: 1.7685, stdev: 0.2294
New network+MCTS average reward: 0.7554, min: 0.0926, max: 1.7685, stdev: 0.2308
Old bare network average reward: 0.7247, min: 0.0926, max: 1.7685, stdev: 0.2340
New bare network average reward: 0.7273, min: 0.0926, max: 1.7685, stdev: 0.2332
External policy "random" average reward: 0.2520, min: -0.2685, max: 0.9537, stdev: 0.2157
External policy "individual greedy" average reward: 0.5304, min: -0.0093, max: 1.2222, stdev: 0.2143
External policy "total greedy" average reward: 0.6431, min: 0.0648, max: 1.4537, stdev: 0.2157
New network won 53 and tied 182 out of 300 games (48.00% wins where ties are half wins)
Reverting to the old network

Training iteration 743 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.89 seconds
Training examples lengths: [64515, 64996, 64721, 64683, 64926, 64628, 64708, 64683, 64804, 64788]
Total value: 500746.06
Training on 647452 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2102 (value: 0.0010, weighted value: 0.0492, policy: 0.1610, weighted policy: 0.1610), Train Mean Max: 0.9359
Epoch 2/10, Train Loss: 0.1944 (value: 0.0008, weighted value: 0.0394, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9372
Epoch 3/10, Train Loss: 0.1879 (value: 0.0008, weighted value: 0.0378, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9376
Epoch 4/10, Train Loss: 0.1843 (value: 0.0007, weighted value: 0.0359, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9378
Epoch 5/10, Train Loss: 0.1807 (value: 0.0007, weighted value: 0.0337, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9380
Epoch 6/10, Train Loss: 0.1784 (value: 0.0006, weighted value: 0.0325, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9383
Epoch 7/10, Train Loss: 0.1777 (value: 0.0006, weighted value: 0.0312, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9383
Epoch 8/10, Train Loss: 0.1760 (value: 0.0006, weighted value: 0.0302, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9385
Epoch 9/10, Train Loss: 0.1751 (value: 0.0006, weighted value: 0.0296, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9388
Epoch 10/10, Train Loss: 0.1745 (value: 0.0006, weighted value: 0.0290, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9387
..training done in 66.23 seconds
..evaluation done in 22.80 seconds
Old network+MCTS average reward: 0.7538, min: 0.1296, max: 1.4630, stdev: 0.2336
New network+MCTS average reward: 0.7537, min: 0.1296, max: 1.4630, stdev: 0.2306
Old bare network average reward: 0.7245, min: 0.0926, max: 1.4630, stdev: 0.2325
New bare network average reward: 0.7282, min: 0.0926, max: 1.4352, stdev: 0.2335
External policy "random" average reward: 0.2562, min: -0.3704, max: 0.7685, stdev: 0.2198
External policy "individual greedy" average reward: 0.5267, min: 0.0000, max: 1.3611, stdev: 0.2218
External policy "total greedy" average reward: 0.6379, min: 0.0648, max: 1.3426, stdev: 0.2131
New network won 68 and tied 165 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 744 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.95 seconds
Training examples lengths: [64996, 64721, 64683, 64926, 64628, 64708, 64683, 64804, 64788, 64809]
Total value: 501346.84
Training on 647746 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1919 (value: 0.0008, weighted value: 0.0386, policy: 0.1533, weighted policy: 0.1533), Train Mean Max: 0.9376
Epoch 2/10, Train Loss: 0.1848 (value: 0.0007, weighted value: 0.0345, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9382
Epoch 3/10, Train Loss: 0.1797 (value: 0.0007, weighted value: 0.0326, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9385
Epoch 4/10, Train Loss: 0.1770 (value: 0.0006, weighted value: 0.0314, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9386
Epoch 5/10, Train Loss: 0.1757 (value: 0.0006, weighted value: 0.0302, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9388
Epoch 6/10, Train Loss: 0.1734 (value: 0.0006, weighted value: 0.0288, policy: 0.1446, weighted policy: 0.1446), Train Mean Max: 0.9390
Epoch 7/10, Train Loss: 0.1738 (value: 0.0006, weighted value: 0.0297, policy: 0.1442, weighted policy: 0.1442), Train Mean Max: 0.9390
Epoch 8/10, Train Loss: 0.1710 (value: 0.0005, weighted value: 0.0266, policy: 0.1444, weighted policy: 0.1444), Train Mean Max: 0.9392
Epoch 9/10, Train Loss: 0.1711 (value: 0.0005, weighted value: 0.0269, policy: 0.1442, weighted policy: 0.1442), Train Mean Max: 0.9394
Epoch 10/10, Train Loss: 0.1708 (value: 0.0005, weighted value: 0.0263, policy: 0.1445, weighted policy: 0.1445), Train Mean Max: 0.9394
..training done in 72.81 seconds
..evaluation done in 19.70 seconds
Old network+MCTS average reward: 0.7573, min: -0.0370, max: 1.5093, stdev: 0.2496
New network+MCTS average reward: 0.7567, min: -0.0370, max: 1.5278, stdev: 0.2491
Old bare network average reward: 0.7280, min: -0.1389, max: 1.4815, stdev: 0.2498
New bare network average reward: 0.7294, min: -0.1389, max: 1.5278, stdev: 0.2542
External policy "random" average reward: 0.2580, min: -0.2778, max: 1.0278, stdev: 0.2419
External policy "individual greedy" average reward: 0.5283, min: -0.0463, max: 1.1574, stdev: 0.2279
External policy "total greedy" average reward: 0.6447, min: -0.0185, max: 1.3333, stdev: 0.2354
New network won 60 and tied 173 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 745 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.41 seconds
Training examples lengths: [64721, 64683, 64926, 64628, 64708, 64683, 64804, 64788, 64809, 64746]
Total value: 501125.47
Training on 647496 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2096 (value: 0.0010, weighted value: 0.0477, policy: 0.1619, weighted policy: 0.1619), Train Mean Max: 0.9359
Epoch 2/10, Train Loss: 0.1968 (value: 0.0008, weighted value: 0.0421, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9371
Epoch 3/10, Train Loss: 0.1888 (value: 0.0008, weighted value: 0.0383, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9376
Epoch 4/10, Train Loss: 0.1848 (value: 0.0007, weighted value: 0.0359, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9377
Epoch 5/10, Train Loss: 0.1828 (value: 0.0007, weighted value: 0.0351, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9380
Epoch 6/10, Train Loss: 0.1805 (value: 0.0007, weighted value: 0.0346, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9382
Epoch 7/10, Train Loss: 0.1766 (value: 0.0006, weighted value: 0.0310, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9383
Epoch 8/10, Train Loss: 0.1763 (value: 0.0006, weighted value: 0.0303, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1758 (value: 0.0006, weighted value: 0.0300, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9386
Epoch 10/10, Train Loss: 0.1749 (value: 0.0006, weighted value: 0.0292, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9387
..training done in 67.13 seconds
..evaluation done in 18.94 seconds
Old network+MCTS average reward: 0.7545, min: 0.1296, max: 1.5185, stdev: 0.2506
New network+MCTS average reward: 0.7535, min: 0.1296, max: 1.5185, stdev: 0.2495
Old bare network average reward: 0.7196, min: 0.1019, max: 1.4630, stdev: 0.2509
New bare network average reward: 0.7232, min: 0.1019, max: 1.4722, stdev: 0.2500
External policy "random" average reward: 0.2549, min: -0.2870, max: 0.8796, stdev: 0.2158
External policy "individual greedy" average reward: 0.5249, min: -0.1019, max: 1.3241, stdev: 0.2250
External policy "total greedy" average reward: 0.6370, min: 0.0648, max: 1.3889, stdev: 0.2168
New network won 69 and tied 168 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 746 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.73 seconds
Training examples lengths: [64683, 64926, 64628, 64708, 64683, 64804, 64788, 64809, 64746, 64634]
Total value: 500624.82
Training on 647409 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1924 (value: 0.0008, weighted value: 0.0384, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9372
Epoch 2/10, Train Loss: 0.1853 (value: 0.0007, weighted value: 0.0345, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9381
Epoch 3/10, Train Loss: 0.1821 (value: 0.0007, weighted value: 0.0338, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9382
Epoch 4/10, Train Loss: 0.1779 (value: 0.0006, weighted value: 0.0313, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9386
Epoch 5/10, Train Loss: 0.1767 (value: 0.0006, weighted value: 0.0304, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9386
Epoch 6/10, Train Loss: 0.1747 (value: 0.0006, weighted value: 0.0294, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9388
Epoch 7/10, Train Loss: 0.1741 (value: 0.0006, weighted value: 0.0283, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9389
Epoch 8/10, Train Loss: 0.1731 (value: 0.0006, weighted value: 0.0282, policy: 0.1449, weighted policy: 0.1449), Train Mean Max: 0.9392
Epoch 9/10, Train Loss: 0.1709 (value: 0.0005, weighted value: 0.0263, policy: 0.1446, weighted policy: 0.1446), Train Mean Max: 0.9392
Epoch 10/10, Train Loss: 0.1717 (value: 0.0005, weighted value: 0.0269, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9394
..training done in 65.21 seconds
..evaluation done in 18.61 seconds
Old network+MCTS average reward: 0.7711, min: 0.2130, max: 1.4815, stdev: 0.2308
New network+MCTS average reward: 0.7756, min: 0.2130, max: 1.4815, stdev: 0.2281
Old bare network average reward: 0.7441, min: 0.2500, max: 1.4815, stdev: 0.2330
New bare network average reward: 0.7488, min: 0.2130, max: 1.4815, stdev: 0.2341
External policy "random" average reward: 0.2728, min: -0.3519, max: 0.9352, stdev: 0.2239
External policy "individual greedy" average reward: 0.5559, min: 0.0833, max: 1.3056, stdev: 0.2326
External policy "total greedy" average reward: 0.6644, min: 0.1759, max: 1.3426, stdev: 0.2263
New network won 80 and tied 162 out of 300 games (53.67% wins where ties are half wins)
Keeping the new network

Training iteration 747 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.64 seconds
Training examples lengths: [64926, 64628, 64708, 64683, 64804, 64788, 64809, 64746, 64634, 64854]
Total value: 501255.06
Training on 647580 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1922 (value: 0.0007, weighted value: 0.0371, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9378
Epoch 2/10, Train Loss: 0.1840 (value: 0.0007, weighted value: 0.0330, policy: 0.1510, weighted policy: 0.1510), Train Mean Max: 0.9385
Epoch 3/10, Train Loss: 0.1790 (value: 0.0006, weighted value: 0.0320, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9388
Epoch 4/10, Train Loss: 0.1754 (value: 0.0006, weighted value: 0.0299, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9390
Epoch 5/10, Train Loss: 0.1738 (value: 0.0006, weighted value: 0.0290, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9390
Epoch 6/10, Train Loss: 0.1728 (value: 0.0006, weighted value: 0.0279, policy: 0.1449, weighted policy: 0.1449), Train Mean Max: 0.9391
Epoch 7/10, Train Loss: 0.1713 (value: 0.0005, weighted value: 0.0269, policy: 0.1444, weighted policy: 0.1444), Train Mean Max: 0.9393
Epoch 8/10, Train Loss: 0.1712 (value: 0.0005, weighted value: 0.0265, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9393
Epoch 9/10, Train Loss: 0.1713 (value: 0.0005, weighted value: 0.0267, policy: 0.1446, weighted policy: 0.1446), Train Mean Max: 0.9395
Epoch 10/10, Train Loss: 0.1694 (value: 0.0005, weighted value: 0.0252, policy: 0.1442, weighted policy: 0.1442), Train Mean Max: 0.9396
..training done in 66.08 seconds
..evaluation done in 18.60 seconds
Old network+MCTS average reward: 0.7399, min: 0.1111, max: 1.4167, stdev: 0.2152
New network+MCTS average reward: 0.7390, min: 0.1111, max: 1.4074, stdev: 0.2161
Old bare network average reward: 0.7132, min: 0.1111, max: 1.4630, stdev: 0.2177
New bare network average reward: 0.7128, min: 0.1111, max: 1.4259, stdev: 0.2165
External policy "random" average reward: 0.2465, min: -0.3981, max: 0.8426, stdev: 0.2217
External policy "individual greedy" average reward: 0.5270, min: -0.1019, max: 1.4630, stdev: 0.2191
External policy "total greedy" average reward: 0.6315, min: -0.0185, max: 1.5278, stdev: 0.2171
New network won 64 and tied 162 out of 300 games (48.33% wins where ties are half wins)
Reverting to the old network

Training iteration 748 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.10 seconds
Training examples lengths: [64628, 64708, 64683, 64804, 64788, 64809, 64746, 64634, 64854, 64673]
Total value: 501510.17
Training on 647327 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2097 (value: 0.0009, weighted value: 0.0468, policy: 0.1629, weighted policy: 0.1629), Train Mean Max: 0.9362
Epoch 2/10, Train Loss: 0.1974 (value: 0.0008, weighted value: 0.0412, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9374
Epoch 3/10, Train Loss: 0.1871 (value: 0.0007, weighted value: 0.0369, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9379
Epoch 4/10, Train Loss: 0.1847 (value: 0.0007, weighted value: 0.0355, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9380
Epoch 5/10, Train Loss: 0.1810 (value: 0.0007, weighted value: 0.0342, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9384
Epoch 6/10, Train Loss: 0.1797 (value: 0.0006, weighted value: 0.0320, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9381
Epoch 7/10, Train Loss: 0.1766 (value: 0.0006, weighted value: 0.0302, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9384
Epoch 8/10, Train Loss: 0.1774 (value: 0.0006, weighted value: 0.0306, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9385
Epoch 9/10, Train Loss: 0.1745 (value: 0.0006, weighted value: 0.0287, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9388
Epoch 10/10, Train Loss: 0.1737 (value: 0.0006, weighted value: 0.0279, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9389
..training done in 67.31 seconds
..evaluation done in 18.35 seconds
Old network+MCTS average reward: 0.7706, min: 0.1574, max: 1.4352, stdev: 0.2212
New network+MCTS average reward: 0.7714, min: 0.1574, max: 1.4444, stdev: 0.2157
Old bare network average reward: 0.7473, min: 0.1296, max: 1.4722, stdev: 0.2210
New bare network average reward: 0.7446, min: 0.0833, max: 1.4722, stdev: 0.2210
External policy "random" average reward: 0.2585, min: -0.2778, max: 0.8519, stdev: 0.2218
External policy "individual greedy" average reward: 0.5355, min: -0.0463, max: 1.1667, stdev: 0.2093
External policy "total greedy" average reward: 0.6590, min: 0.0556, max: 1.2963, stdev: 0.2123
New network won 62 and tied 179 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 749 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.74 seconds
Training examples lengths: [64708, 64683, 64804, 64788, 64809, 64746, 64634, 64854, 64673, 64861]
Total value: 501262.52
Training on 647560 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1923 (value: 0.0008, weighted value: 0.0386, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9374
Epoch 2/10, Train Loss: 0.1852 (value: 0.0007, weighted value: 0.0340, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9381
Epoch 3/10, Train Loss: 0.1814 (value: 0.0007, weighted value: 0.0330, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9383
Epoch 4/10, Train Loss: 0.1768 (value: 0.0006, weighted value: 0.0310, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9385
Epoch 5/10, Train Loss: 0.1755 (value: 0.0006, weighted value: 0.0297, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9387
Epoch 6/10, Train Loss: 0.1751 (value: 0.0006, weighted value: 0.0296, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9387
Epoch 7/10, Train Loss: 0.1723 (value: 0.0005, weighted value: 0.0273, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9389
Epoch 8/10, Train Loss: 0.1732 (value: 0.0006, weighted value: 0.0275, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9390
Epoch 9/10, Train Loss: 0.1717 (value: 0.0005, weighted value: 0.0264, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9392
Epoch 10/10, Train Loss: 0.1716 (value: 0.0005, weighted value: 0.0268, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9392
..training done in 66.50 seconds
..evaluation done in 18.69 seconds
Old network+MCTS average reward: 0.7714, min: 0.1667, max: 1.4722, stdev: 0.2255
New network+MCTS average reward: 0.7726, min: 0.2130, max: 1.4722, stdev: 0.2279
Old bare network average reward: 0.7422, min: 0.1667, max: 1.4167, stdev: 0.2312
New bare network average reward: 0.7440, min: 0.1667, max: 1.4074, stdev: 0.2306
External policy "random" average reward: 0.2495, min: -0.4815, max: 1.1852, stdev: 0.2186
External policy "individual greedy" average reward: 0.5461, min: -0.0185, max: 1.2593, stdev: 0.2182
External policy "total greedy" average reward: 0.6614, min: 0.1019, max: 1.3889, stdev: 0.2133
New network won 68 and tied 172 out of 300 games (51.33% wins where ties are half wins)
Keeping the new network

Training iteration 750 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.92 seconds
Training examples lengths: [64683, 64804, 64788, 64809, 64746, 64634, 64854, 64673, 64861, 64655]
Total value: 502093.51
Training on 647507 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1880 (value: 0.0007, weighted value: 0.0352, policy: 0.1528, weighted policy: 0.1528), Train Mean Max: 0.9379
Epoch 2/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0331, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9385
Epoch 3/10, Train Loss: 0.1781 (value: 0.0006, weighted value: 0.0306, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9389
Epoch 4/10, Train Loss: 0.1746 (value: 0.0006, weighted value: 0.0288, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9391
Epoch 5/10, Train Loss: 0.1730 (value: 0.0006, weighted value: 0.0285, policy: 0.1446, weighted policy: 0.1446), Train Mean Max: 0.9391
Epoch 6/10, Train Loss: 0.1728 (value: 0.0006, weighted value: 0.0278, policy: 0.1449, weighted policy: 0.1449), Train Mean Max: 0.9392
Epoch 7/10, Train Loss: 0.1712 (value: 0.0005, weighted value: 0.0265, policy: 0.1447, weighted policy: 0.1447), Train Mean Max: 0.9393
Epoch 8/10, Train Loss: 0.1715 (value: 0.0005, weighted value: 0.0264, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9395
Epoch 9/10, Train Loss: 0.1700 (value: 0.0005, weighted value: 0.0254, policy: 0.1446, weighted policy: 0.1446), Train Mean Max: 0.9397
Epoch 10/10, Train Loss: 0.1690 (value: 0.0005, weighted value: 0.0247, policy: 0.1442, weighted policy: 0.1442), Train Mean Max: 0.9397
..training done in 71.36 seconds
..evaluation done in 18.12 seconds
Old network+MCTS average reward: 0.7760, min: 0.0648, max: 1.4722, stdev: 0.2281
New network+MCTS average reward: 0.7744, min: 0.0556, max: 1.4630, stdev: 0.2282
Old bare network average reward: 0.7518, min: 0.0926, max: 1.4537, stdev: 0.2281
New bare network average reward: 0.7523, min: 0.0093, max: 1.4537, stdev: 0.2316
External policy "random" average reward: 0.2594, min: -0.3148, max: 1.0926, stdev: 0.2264
External policy "individual greedy" average reward: 0.5519, min: -0.0648, max: 1.2037, stdev: 0.2218
External policy "total greedy" average reward: 0.6633, min: 0.1944, max: 1.3148, stdev: 0.2172
New network won 55 and tied 182 out of 300 games (48.67% wins where ties are half wins)
Reverting to the old network

Training iteration 751 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 62.52 seconds
Training examples lengths: [64804, 64788, 64809, 64746, 64634, 64854, 64673, 64861, 64655, 64927]
Total value: 503021.00
Training on 647751 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2061 (value: 0.0009, weighted value: 0.0457, policy: 0.1604, weighted policy: 0.1604), Train Mean Max: 0.9363
Epoch 2/10, Train Loss: 0.1949 (value: 0.0008, weighted value: 0.0403, policy: 0.1546, weighted policy: 0.1546), Train Mean Max: 0.9373
Epoch 3/10, Train Loss: 0.1861 (value: 0.0007, weighted value: 0.0364, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9377
Epoch 4/10, Train Loss: 0.1831 (value: 0.0007, weighted value: 0.0352, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9380
Epoch 5/10, Train Loss: 0.1794 (value: 0.0007, weighted value: 0.0326, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9382
Epoch 6/10, Train Loss: 0.1782 (value: 0.0006, weighted value: 0.0318, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9383
Epoch 7/10, Train Loss: 0.1762 (value: 0.0006, weighted value: 0.0302, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9386
Epoch 8/10, Train Loss: 0.1743 (value: 0.0006, weighted value: 0.0294, policy: 0.1449, weighted policy: 0.1449), Train Mean Max: 0.9387
Epoch 9/10, Train Loss: 0.1746 (value: 0.0006, weighted value: 0.0286, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9387
Epoch 10/10, Train Loss: 0.1733 (value: 0.0006, weighted value: 0.0277, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9390
..training done in 70.70 seconds
..evaluation done in 18.66 seconds
Old network+MCTS average reward: 0.7712, min: 0.2222, max: 1.4907, stdev: 0.2511
New network+MCTS average reward: 0.7709, min: 0.2315, max: 1.4907, stdev: 0.2505
Old bare network average reward: 0.7384, min: 0.1852, max: 1.4630, stdev: 0.2471
New bare network average reward: 0.7416, min: 0.1759, max: 1.4630, stdev: 0.2527
External policy "random" average reward: 0.2542, min: -0.5741, max: 0.9722, stdev: 0.2304
External policy "individual greedy" average reward: 0.5413, min: -0.0185, max: 1.4444, stdev: 0.2386
External policy "total greedy" average reward: 0.6516, min: 0.0926, max: 1.4907, stdev: 0.2322
New network won 56 and tied 179 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 752 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.28 seconds
Training examples lengths: [64788, 64809, 64746, 64634, 64854, 64673, 64861, 64655, 64927, 65052]
Total value: 503324.46
Training on 647999 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2242 (value: 0.0011, weighted value: 0.0565, policy: 0.1677, weighted policy: 0.1677), Train Mean Max: 0.9349
Epoch 2/10, Train Loss: 0.2048 (value: 0.0009, weighted value: 0.0472, policy: 0.1576, weighted policy: 0.1576), Train Mean Max: 0.9362
Epoch 3/10, Train Loss: 0.1961 (value: 0.0009, weighted value: 0.0430, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9366
Epoch 4/10, Train Loss: 0.1898 (value: 0.0008, weighted value: 0.0396, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9370
Epoch 5/10, Train Loss: 0.1857 (value: 0.0007, weighted value: 0.0370, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9373
Epoch 6/10, Train Loss: 0.1845 (value: 0.0007, weighted value: 0.0363, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9373
Epoch 7/10, Train Loss: 0.1816 (value: 0.0007, weighted value: 0.0339, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9376
Epoch 8/10, Train Loss: 0.1804 (value: 0.0007, weighted value: 0.0332, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1773 (value: 0.0006, weighted value: 0.0309, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9380
Epoch 10/10, Train Loss: 0.1781 (value: 0.0006, weighted value: 0.0318, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9381
..training done in 64.63 seconds
..evaluation done in 18.16 seconds
Old network+MCTS average reward: 0.7842, min: 0.1759, max: 1.5833, stdev: 0.2257
New network+MCTS average reward: 0.7861, min: 0.1759, max: 1.5833, stdev: 0.2257
Old bare network average reward: 0.7560, min: 0.1296, max: 1.5370, stdev: 0.2312
New bare network average reward: 0.7584, min: 0.1759, max: 1.5370, stdev: 0.2276
External policy "random" average reward: 0.2775, min: -0.3056, max: 0.9907, stdev: 0.2160
External policy "individual greedy" average reward: 0.5595, min: 0.0463, max: 1.2963, stdev: 0.2207
External policy "total greedy" average reward: 0.6687, min: 0.1944, max: 1.4537, stdev: 0.2156
New network won 66 and tied 169 out of 300 games (50.17% wins where ties are half wins)
Keeping the new network

Training iteration 753 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.72 seconds
Training examples lengths: [64809, 64746, 64634, 64854, 64673, 64861, 64655, 64927, 65052, 64781]
Total value: 503551.94
Training on 647992 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1947 (value: 0.0008, weighted value: 0.0396, policy: 0.1551, weighted policy: 0.1551), Train Mean Max: 0.9365
Epoch 2/10, Train Loss: 0.1882 (value: 0.0007, weighted value: 0.0366, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9373
Epoch 3/10, Train Loss: 0.1831 (value: 0.0007, weighted value: 0.0339, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9375
Epoch 4/10, Train Loss: 0.1807 (value: 0.0007, weighted value: 0.0327, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9376
Epoch 5/10, Train Loss: 0.1789 (value: 0.0006, weighted value: 0.0315, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9378
Epoch 6/10, Train Loss: 0.1774 (value: 0.0006, weighted value: 0.0300, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9380
Epoch 7/10, Train Loss: 0.1763 (value: 0.0006, weighted value: 0.0295, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9381
Epoch 8/10, Train Loss: 0.1749 (value: 0.0006, weighted value: 0.0286, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9383
Epoch 9/10, Train Loss: 0.1741 (value: 0.0006, weighted value: 0.0278, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1735 (value: 0.0005, weighted value: 0.0273, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9386
..training done in 67.43 seconds
..evaluation done in 18.27 seconds
Old network+MCTS average reward: 0.7740, min: 0.1204, max: 1.6296, stdev: 0.2575
New network+MCTS average reward: 0.7740, min: 0.1204, max: 1.6481, stdev: 0.2568
Old bare network average reward: 0.7461, min: 0.1204, max: 1.6296, stdev: 0.2619
New bare network average reward: 0.7454, min: 0.1204, max: 1.6481, stdev: 0.2608
External policy "random" average reward: 0.2433, min: -0.3704, max: 0.9722, stdev: 0.2277
External policy "individual greedy" average reward: 0.5430, min: -0.1667, max: 1.3333, stdev: 0.2429
External policy "total greedy" average reward: 0.6500, min: -0.0463, max: 1.3889, stdev: 0.2392
New network won 58 and tied 172 out of 300 games (48.00% wins where ties are half wins)
Reverting to the old network

Training iteration 754 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 66.17 seconds
Training examples lengths: [64746, 64634, 64854, 64673, 64861, 64655, 64927, 65052, 64781, 64808]
Total value: 503795.36
Training on 647991 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2131 (value: 0.0010, weighted value: 0.0501, policy: 0.1631, weighted policy: 0.1631), Train Mean Max: 0.9349
Epoch 2/10, Train Loss: 0.1999 (value: 0.0009, weighted value: 0.0432, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9361
Epoch 3/10, Train Loss: 0.1927 (value: 0.0008, weighted value: 0.0400, policy: 0.1527, weighted policy: 0.1527), Train Mean Max: 0.9366
Epoch 4/10, Train Loss: 0.1870 (value: 0.0007, weighted value: 0.0371, policy: 0.1499, weighted policy: 0.1499), Train Mean Max: 0.9368
Epoch 5/10, Train Loss: 0.1843 (value: 0.0007, weighted value: 0.0352, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1826 (value: 0.0007, weighted value: 0.0344, policy: 0.1481, weighted policy: 0.1481), Train Mean Max: 0.9371
Epoch 7/10, Train Loss: 0.1806 (value: 0.0007, weighted value: 0.0326, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9374
Epoch 8/10, Train Loss: 0.1802 (value: 0.0006, weighted value: 0.0318, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1775 (value: 0.0006, weighted value: 0.0304, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1766 (value: 0.0006, weighted value: 0.0297, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9379
..training done in 67.02 seconds
..evaluation done in 18.22 seconds
Old network+MCTS average reward: 0.7632, min: 0.0556, max: 1.4815, stdev: 0.2381
New network+MCTS average reward: 0.7650, min: 0.0556, max: 1.4815, stdev: 0.2383
Old bare network average reward: 0.7377, min: 0.0556, max: 1.4815, stdev: 0.2419
New bare network average reward: 0.7385, min: 0.0556, max: 1.4815, stdev: 0.2425
External policy "random" average reward: 0.2636, min: -0.3333, max: 0.9815, stdev: 0.2309
External policy "individual greedy" average reward: 0.5388, min: 0.0185, max: 1.2407, stdev: 0.2293
External policy "total greedy" average reward: 0.6595, min: 0.1574, max: 1.3148, stdev: 0.2252
New network won 64 and tied 174 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 755 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.29 seconds
Training examples lengths: [64634, 64854, 64673, 64861, 64655, 64927, 65052, 64781, 64808, 64655]
Total value: 503809.43
Training on 647900 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1945 (value: 0.0008, weighted value: 0.0389, policy: 0.1556, weighted policy: 0.1556), Train Mean Max: 0.9364
Epoch 2/10, Train Loss: 0.1868 (value: 0.0007, weighted value: 0.0351, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9371
Epoch 3/10, Train Loss: 0.1818 (value: 0.0007, weighted value: 0.0329, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9376
Epoch 4/10, Train Loss: 0.1807 (value: 0.0007, weighted value: 0.0330, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9378
Epoch 5/10, Train Loss: 0.1770 (value: 0.0006, weighted value: 0.0296, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9378
Epoch 6/10, Train Loss: 0.1762 (value: 0.0006, weighted value: 0.0295, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9381
Epoch 7/10, Train Loss: 0.1757 (value: 0.0006, weighted value: 0.0290, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9381
Epoch 8/10, Train Loss: 0.1742 (value: 0.0006, weighted value: 0.0278, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1742 (value: 0.0006, weighted value: 0.0282, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9385
Epoch 10/10, Train Loss: 0.1723 (value: 0.0005, weighted value: 0.0267, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9385
..training done in 70.12 seconds
..evaluation done in 19.33 seconds
Old network+MCTS average reward: 0.7901, min: 0.1296, max: 1.4167, stdev: 0.2363
New network+MCTS average reward: 0.7903, min: 0.1667, max: 1.4074, stdev: 0.2343
Old bare network average reward: 0.7652, min: 0.0185, max: 1.3796, stdev: 0.2385
New bare network average reward: 0.7627, min: -0.0093, max: 1.3981, stdev: 0.2377
External policy "random" average reward: 0.2757, min: -0.3241, max: 0.8796, stdev: 0.2135
External policy "individual greedy" average reward: 0.5581, min: 0.0000, max: 1.1481, stdev: 0.2199
External policy "total greedy" average reward: 0.6778, min: 0.0833, max: 1.2870, stdev: 0.2277
New network won 48 and tied 200 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 756 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.97 seconds
Training examples lengths: [64854, 64673, 64861, 64655, 64927, 65052, 64781, 64808, 64655, 64647]
Total value: 504347.00
Training on 647913 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2109 (value: 0.0010, weighted value: 0.0494, policy: 0.1615, weighted policy: 0.1615), Train Mean Max: 0.9352
Epoch 2/10, Train Loss: 0.1979 (value: 0.0008, weighted value: 0.0420, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9362
Epoch 3/10, Train Loss: 0.1910 (value: 0.0008, weighted value: 0.0394, policy: 0.1517, weighted policy: 0.1517), Train Mean Max: 0.9368
Epoch 4/10, Train Loss: 0.1863 (value: 0.0007, weighted value: 0.0371, policy: 0.1492, weighted policy: 0.1492), Train Mean Max: 0.9371
Epoch 5/10, Train Loss: 0.1822 (value: 0.0007, weighted value: 0.0345, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9373
Epoch 6/10, Train Loss: 0.1812 (value: 0.0007, weighted value: 0.0339, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9375
Epoch 7/10, Train Loss: 0.1793 (value: 0.0006, weighted value: 0.0321, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9377
Epoch 8/10, Train Loss: 0.1781 (value: 0.0006, weighted value: 0.0311, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1760 (value: 0.0006, weighted value: 0.0298, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9381
Epoch 10/10, Train Loss: 0.1747 (value: 0.0006, weighted value: 0.0288, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9383
..training done in 67.91 seconds
..evaluation done in 18.25 seconds
Old network+MCTS average reward: 0.7844, min: 0.1759, max: 1.5926, stdev: 0.2381
New network+MCTS average reward: 0.7880, min: 0.2222, max: 1.5926, stdev: 0.2382
Old bare network average reward: 0.7577, min: 0.1389, max: 1.5926, stdev: 0.2426
New bare network average reward: 0.7570, min: 0.1389, max: 1.5926, stdev: 0.2432
External policy "random" average reward: 0.2794, min: -0.2963, max: 1.0278, stdev: 0.2299
External policy "individual greedy" average reward: 0.5528, min: -0.0463, max: 1.2685, stdev: 0.2276
External policy "total greedy" average reward: 0.6742, min: 0.1204, max: 1.4537, stdev: 0.2261
New network won 76 and tied 169 out of 300 games (53.50% wins where ties are half wins)
Keeping the new network

Training iteration 757 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.09 seconds
Training examples lengths: [64673, 64861, 64655, 64927, 65052, 64781, 64808, 64655, 64647, 64630]
Total value: 503497.36
Training on 647689 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1936 (value: 0.0008, weighted value: 0.0389, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9369
Epoch 2/10, Train Loss: 0.1866 (value: 0.0007, weighted value: 0.0355, policy: 0.1511, weighted policy: 0.1511), Train Mean Max: 0.9376
Epoch 3/10, Train Loss: 0.1817 (value: 0.0007, weighted value: 0.0328, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9379
Epoch 4/10, Train Loss: 0.1794 (value: 0.0006, weighted value: 0.0320, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9382
Epoch 5/10, Train Loss: 0.1767 (value: 0.0006, weighted value: 0.0301, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9383
Epoch 6/10, Train Loss: 0.1750 (value: 0.0006, weighted value: 0.0295, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9384
Epoch 7/10, Train Loss: 0.1738 (value: 0.0006, weighted value: 0.0285, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9386
Epoch 8/10, Train Loss: 0.1742 (value: 0.0006, weighted value: 0.0280, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9386
Epoch 9/10, Train Loss: 0.1738 (value: 0.0006, weighted value: 0.0281, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9387
Epoch 10/10, Train Loss: 0.1712 (value: 0.0005, weighted value: 0.0260, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9389
..training done in 69.68 seconds
..evaluation done in 18.88 seconds
Old network+MCTS average reward: 0.7869, min: 0.2407, max: 1.6019, stdev: 0.2352
New network+MCTS average reward: 0.7856, min: 0.2130, max: 1.5093, stdev: 0.2369
Old bare network average reward: 0.7602, min: 0.1667, max: 1.4815, stdev: 0.2397
New bare network average reward: 0.7593, min: 0.1852, max: 1.5093, stdev: 0.2397
External policy "random" average reward: 0.2746, min: -0.2778, max: 1.0093, stdev: 0.2284
External policy "individual greedy" average reward: 0.5564, min: 0.0926, max: 1.3148, stdev: 0.2320
External policy "total greedy" average reward: 0.6767, min: -0.0185, max: 1.3981, stdev: 0.2335
New network won 56 and tied 186 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 758 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.81 seconds
Training examples lengths: [64861, 64655, 64927, 65052, 64781, 64808, 64655, 64647, 64630, 64766]
Total value: 503315.08
Training on 647782 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2096 (value: 0.0010, weighted value: 0.0480, policy: 0.1616, weighted policy: 0.1616), Train Mean Max: 0.9354
Epoch 2/10, Train Loss: 0.1988 (value: 0.0009, weighted value: 0.0425, policy: 0.1562, weighted policy: 0.1562), Train Mean Max: 0.9363
Epoch 3/10, Train Loss: 0.1889 (value: 0.0008, weighted value: 0.0380, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9370
Epoch 4/10, Train Loss: 0.1868 (value: 0.0007, weighted value: 0.0371, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9371
Epoch 5/10, Train Loss: 0.1834 (value: 0.0007, weighted value: 0.0355, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9375
Epoch 6/10, Train Loss: 0.1802 (value: 0.0007, weighted value: 0.0326, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9376
Epoch 7/10, Train Loss: 0.1785 (value: 0.0006, weighted value: 0.0315, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9377
Epoch 8/10, Train Loss: 0.1766 (value: 0.0006, weighted value: 0.0302, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9379
Epoch 9/10, Train Loss: 0.1766 (value: 0.0006, weighted value: 0.0300, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9381
Epoch 10/10, Train Loss: 0.1754 (value: 0.0006, weighted value: 0.0290, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9382
..training done in 70.98 seconds
..evaluation done in 19.19 seconds
Old network+MCTS average reward: 0.7670, min: 0.1574, max: 1.4815, stdev: 0.2375
New network+MCTS average reward: 0.7642, min: 0.1389, max: 1.4722, stdev: 0.2399
Old bare network average reward: 0.7418, min: 0.1296, max: 1.4722, stdev: 0.2408
New bare network average reward: 0.7394, min: 0.0926, max: 1.4722, stdev: 0.2398
External policy "random" average reward: 0.2703, min: -0.3611, max: 0.9259, stdev: 0.2252
External policy "individual greedy" average reward: 0.5392, min: -0.1019, max: 1.3333, stdev: 0.2279
External policy "total greedy" average reward: 0.6516, min: 0.0000, max: 1.4074, stdev: 0.2237
New network won 67 and tied 166 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network

Training iteration 759 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.58 seconds
Training examples lengths: [64655, 64927, 65052, 64781, 64808, 64655, 64647, 64630, 64766, 64636]
Total value: 503449.35
Training on 647557 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1944 (value: 0.0008, weighted value: 0.0391, policy: 0.1553, weighted policy: 0.1553), Train Mean Max: 0.9367
Epoch 2/10, Train Loss: 0.1869 (value: 0.0007, weighted value: 0.0348, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9375
Epoch 3/10, Train Loss: 0.1819 (value: 0.0007, weighted value: 0.0332, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9380
Epoch 4/10, Train Loss: 0.1782 (value: 0.0006, weighted value: 0.0310, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9382
Epoch 5/10, Train Loss: 0.1775 (value: 0.0006, weighted value: 0.0309, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9382
Epoch 6/10, Train Loss: 0.1764 (value: 0.0006, weighted value: 0.0292, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9382
Epoch 7/10, Train Loss: 0.1745 (value: 0.0006, weighted value: 0.0286, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9385
Epoch 8/10, Train Loss: 0.1729 (value: 0.0006, weighted value: 0.0277, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9387
Epoch 9/10, Train Loss: 0.1728 (value: 0.0006, weighted value: 0.0275, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9387
Epoch 10/10, Train Loss: 0.1714 (value: 0.0005, weighted value: 0.0263, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9388
..training done in 65.46 seconds
..evaluation done in 18.66 seconds
Old network+MCTS average reward: 0.7923, min: 0.2315, max: 1.5000, stdev: 0.2205
New network+MCTS average reward: 0.7935, min: 0.2130, max: 1.4815, stdev: 0.2210
Old bare network average reward: 0.7617, min: 0.1759, max: 1.4352, stdev: 0.2230
New bare network average reward: 0.7656, min: 0.2315, max: 1.4815, stdev: 0.2242
External policy "random" average reward: 0.2818, min: -0.1759, max: 1.0556, stdev: 0.2184
External policy "individual greedy" average reward: 0.5747, min: -0.0185, max: 1.1944, stdev: 0.2235
External policy "total greedy" average reward: 0.6762, min: 0.1019, max: 1.4537, stdev: 0.2158
New network won 62 and tied 175 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 760 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 64.01 seconds
Training examples lengths: [64927, 65052, 64781, 64808, 64655, 64647, 64630, 64766, 64636, 64663]
Total value: 503191.79
Training on 647565 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2119 (value: 0.0010, weighted value: 0.0502, policy: 0.1616, weighted policy: 0.1616), Train Mean Max: 0.9353
Epoch 2/10, Train Loss: 0.1981 (value: 0.0008, weighted value: 0.0418, policy: 0.1563, weighted policy: 0.1563), Train Mean Max: 0.9366
Epoch 3/10, Train Loss: 0.1907 (value: 0.0008, weighted value: 0.0386, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9370
Epoch 4/10, Train Loss: 0.1872 (value: 0.0008, weighted value: 0.0375, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9372
Epoch 5/10, Train Loss: 0.1835 (value: 0.0007, weighted value: 0.0350, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9374
Epoch 6/10, Train Loss: 0.1811 (value: 0.0007, weighted value: 0.0331, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9376
Epoch 7/10, Train Loss: 0.1792 (value: 0.0006, weighted value: 0.0319, policy: 0.1474, weighted policy: 0.1474), Train Mean Max: 0.9378
Epoch 8/10, Train Loss: 0.1793 (value: 0.0006, weighted value: 0.0314, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9378
Epoch 9/10, Train Loss: 0.1772 (value: 0.0006, weighted value: 0.0303, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9381
Epoch 10/10, Train Loss: 0.1757 (value: 0.0006, weighted value: 0.0299, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9383
..training done in 70.58 seconds
..evaluation done in 17.97 seconds
Old network+MCTS average reward: 0.7629, min: 0.1759, max: 1.3333, stdev: 0.2319
New network+MCTS average reward: 0.7637, min: 0.1667, max: 1.3333, stdev: 0.2322
Old bare network average reward: 0.7354, min: 0.1296, max: 1.2778, stdev: 0.2332
New bare network average reward: 0.7344, min: 0.1296, max: 1.3333, stdev: 0.2340
External policy "random" average reward: 0.2493, min: -0.2870, max: 0.8519, stdev: 0.2222
External policy "individual greedy" average reward: 0.5362, min: -0.0278, max: 1.0648, stdev: 0.2175
External policy "total greedy" average reward: 0.6499, min: 0.0556, max: 1.3056, stdev: 0.2183
New network won 70 and tied 167 out of 300 games (51.17% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_760

Training iteration 761 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.45 seconds
Training examples lengths: [65052, 64781, 64808, 64655, 64647, 64630, 64766, 64636, 64663, 64981]
Total value: 502997.99
Training on 647619 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1928 (value: 0.0008, weighted value: 0.0381, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9369
Epoch 2/10, Train Loss: 0.1869 (value: 0.0007, weighted value: 0.0350, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9374
Epoch 3/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0341, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9377
Epoch 4/10, Train Loss: 0.1791 (value: 0.0006, weighted value: 0.0310, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9379
Epoch 5/10, Train Loss: 0.1768 (value: 0.0006, weighted value: 0.0304, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9382
Epoch 6/10, Train Loss: 0.1762 (value: 0.0006, weighted value: 0.0297, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9381
Epoch 7/10, Train Loss: 0.1749 (value: 0.0006, weighted value: 0.0287, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9383
Epoch 8/10, Train Loss: 0.1737 (value: 0.0006, weighted value: 0.0276, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9386
Epoch 9/10, Train Loss: 0.1734 (value: 0.0006, weighted value: 0.0276, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9386
Epoch 10/10, Train Loss: 0.1722 (value: 0.0005, weighted value: 0.0264, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9386
..training done in 68.39 seconds
..evaluation done in 18.67 seconds
Old network+MCTS average reward: 0.7897, min: 0.2130, max: 1.3796, stdev: 0.2255
New network+MCTS average reward: 0.7914, min: 0.1944, max: 1.4259, stdev: 0.2274
Old bare network average reward: 0.7618, min: 0.1204, max: 1.3796, stdev: 0.2315
New bare network average reward: 0.7630, min: 0.1481, max: 1.3796, stdev: 0.2314
External policy "random" average reward: 0.2789, min: -0.2500, max: 0.9907, stdev: 0.2146
External policy "individual greedy" average reward: 0.5600, min: 0.0185, max: 1.2963, stdev: 0.2207
External policy "total greedy" average reward: 0.6807, min: 0.2222, max: 1.3148, stdev: 0.2168
New network won 69 and tied 181 out of 300 games (53.17% wins where ties are half wins)
Keeping the new network

Training iteration 762 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.50 seconds
Training examples lengths: [64781, 64808, 64655, 64647, 64630, 64766, 64636, 64663, 64981, 64872]
Total value: 503375.58
Training on 647439 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1905 (value: 0.0007, weighted value: 0.0366, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9375
Epoch 2/10, Train Loss: 0.1840 (value: 0.0007, weighted value: 0.0328, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9380
Epoch 3/10, Train Loss: 0.1804 (value: 0.0006, weighted value: 0.0317, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9381
Epoch 4/10, Train Loss: 0.1767 (value: 0.0006, weighted value: 0.0297, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9385
Epoch 5/10, Train Loss: 0.1745 (value: 0.0006, weighted value: 0.0284, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9386
Epoch 6/10, Train Loss: 0.1739 (value: 0.0006, weighted value: 0.0280, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9387
Epoch 7/10, Train Loss: 0.1734 (value: 0.0006, weighted value: 0.0282, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9389
Epoch 8/10, Train Loss: 0.1710 (value: 0.0005, weighted value: 0.0258, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9388
Epoch 9/10, Train Loss: 0.1716 (value: 0.0005, weighted value: 0.0267, policy: 0.1449, weighted policy: 0.1449), Train Mean Max: 0.9390
Epoch 10/10, Train Loss: 0.1698 (value: 0.0005, weighted value: 0.0245, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9392
..training done in 59.90 seconds
..evaluation done in 19.72 seconds
Old network+MCTS average reward: 0.7712, min: 0.1296, max: 1.6389, stdev: 0.2503
New network+MCTS average reward: 0.7684, min: 0.1111, max: 1.6667, stdev: 0.2489
Old bare network average reward: 0.7459, min: 0.1111, max: 1.5926, stdev: 0.2521
New bare network average reward: 0.7416, min: 0.1111, max: 1.5926, stdev: 0.2553
External policy "random" average reward: 0.2736, min: -0.3241, max: 1.2315, stdev: 0.2374
External policy "individual greedy" average reward: 0.5489, min: -0.1296, max: 1.3426, stdev: 0.2407
External policy "total greedy" average reward: 0.6648, min: 0.0648, max: 1.4259, stdev: 0.2376
New network won 67 and tied 163 out of 300 games (49.50% wins where ties are half wins)
Reverting to the old network

Training iteration 763 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.07 seconds
Training examples lengths: [64808, 64655, 64647, 64630, 64766, 64636, 64663, 64981, 64872, 64802]
Total value: 503332.45
Training on 647460 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2092 (value: 0.0009, weighted value: 0.0468, policy: 0.1624, weighted policy: 0.1624), Train Mean Max: 0.9359
Epoch 2/10, Train Loss: 0.1976 (value: 0.0008, weighted value: 0.0408, policy: 0.1569, weighted policy: 0.1569), Train Mean Max: 0.9368
Epoch 3/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0375, policy: 0.1509, weighted policy: 0.1509), Train Mean Max: 0.9375
Epoch 4/10, Train Loss: 0.1830 (value: 0.0007, weighted value: 0.0345, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9377
Epoch 5/10, Train Loss: 0.1809 (value: 0.0007, weighted value: 0.0337, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9380
Epoch 6/10, Train Loss: 0.1775 (value: 0.0006, weighted value: 0.0309, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9380
Epoch 7/10, Train Loss: 0.1779 (value: 0.0006, weighted value: 0.0315, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9382
Epoch 8/10, Train Loss: 0.1751 (value: 0.0006, weighted value: 0.0290, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9383
Epoch 9/10, Train Loss: 0.1756 (value: 0.0006, weighted value: 0.0298, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9385
Epoch 10/10, Train Loss: 0.1730 (value: 0.0005, weighted value: 0.0274, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9386
..training done in 70.32 seconds
..evaluation done in 20.19 seconds
Old network+MCTS average reward: 0.7834, min: -0.2130, max: 1.5926, stdev: 0.2498
New network+MCTS average reward: 0.7818, min: -0.1389, max: 1.5926, stdev: 0.2483
Old bare network average reward: 0.7580, min: -0.1852, max: 1.5926, stdev: 0.2556
New bare network average reward: 0.7552, min: -0.1852, max: 1.5926, stdev: 0.2546
External policy "random" average reward: 0.2619, min: -0.3148, max: 1.1574, stdev: 0.2380
External policy "individual greedy" average reward: 0.5551, min: -0.1296, max: 1.2685, stdev: 0.2425
External policy "total greedy" average reward: 0.6676, min: -0.0833, max: 1.4167, stdev: 0.2267
New network won 57 and tied 168 out of 300 games (47.00% wins where ties are half wins)
Reverting to the old network

Training iteration 764 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 65.53 seconds
Training examples lengths: [64655, 64647, 64630, 64766, 64636, 64663, 64981, 64872, 64802, 65073]
Total value: 503799.74
Training on 647725 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2245 (value: 0.0011, weighted value: 0.0566, policy: 0.1678, weighted policy: 0.1678), Train Mean Max: 0.9347
Epoch 2/10, Train Loss: 0.2060 (value: 0.0009, weighted value: 0.0467, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9361
Epoch 3/10, Train Loss: 0.1957 (value: 0.0009, weighted value: 0.0426, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9367
Epoch 4/10, Train Loss: 0.1906 (value: 0.0008, weighted value: 0.0401, policy: 0.1505, weighted policy: 0.1505), Train Mean Max: 0.9369
Epoch 5/10, Train Loss: 0.1860 (value: 0.0007, weighted value: 0.0371, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9373
Epoch 6/10, Train Loss: 0.1840 (value: 0.0007, weighted value: 0.0361, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9373
Epoch 7/10, Train Loss: 0.1814 (value: 0.0007, weighted value: 0.0336, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9375
Epoch 8/10, Train Loss: 0.1796 (value: 0.0007, weighted value: 0.0327, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9379
Epoch 9/10, Train Loss: 0.1785 (value: 0.0006, weighted value: 0.0319, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9379
Epoch 10/10, Train Loss: 0.1760 (value: 0.0006, weighted value: 0.0298, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9382
..training done in 70.40 seconds
..evaluation done in 19.42 seconds
Old network+MCTS average reward: 0.7551, min: 0.2130, max: 1.8241, stdev: 0.2358
New network+MCTS average reward: 0.7540, min: 0.1667, max: 1.8241, stdev: 0.2355
Old bare network average reward: 0.7256, min: 0.1667, max: 1.7315, stdev: 0.2372
New bare network average reward: 0.7301, min: 0.1667, max: 1.7315, stdev: 0.2395
External policy "random" average reward: 0.2622, min: -0.4444, max: 1.1667, stdev: 0.2139
External policy "individual greedy" average reward: 0.5390, min: 0.0000, max: 1.5278, stdev: 0.2238
External policy "total greedy" average reward: 0.6464, min: 0.1111, max: 1.5926, stdev: 0.2255
New network won 60 and tied 169 out of 300 games (48.17% wins where ties are half wins)
Reverting to the old network

Training iteration 765 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.78 seconds
Training examples lengths: [64647, 64630, 64766, 64636, 64663, 64981, 64872, 64802, 65073, 64746]
Total value: 504189.33
Training on 647816 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2409 (value: 0.0013, weighted value: 0.0666, policy: 0.1743, weighted policy: 0.1743), Train Mean Max: 0.9334
Epoch 2/10, Train Loss: 0.2157 (value: 0.0011, weighted value: 0.0535, policy: 0.1622, weighted policy: 0.1622), Train Mean Max: 0.9352
Epoch 3/10, Train Loss: 0.2039 (value: 0.0010, weighted value: 0.0484, policy: 0.1555, weighted policy: 0.1555), Train Mean Max: 0.9357
Epoch 4/10, Train Loss: 0.1964 (value: 0.0009, weighted value: 0.0442, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9361
Epoch 5/10, Train Loss: 0.1920 (value: 0.0008, weighted value: 0.0417, policy: 0.1503, weighted policy: 0.1503), Train Mean Max: 0.9364
Epoch 6/10, Train Loss: 0.1887 (value: 0.0008, weighted value: 0.0389, policy: 0.1498, weighted policy: 0.1498), Train Mean Max: 0.9364
Epoch 7/10, Train Loss: 0.1854 (value: 0.0007, weighted value: 0.0372, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9368
Epoch 8/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0349, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9370
Epoch 9/10, Train Loss: 0.1818 (value: 0.0007, weighted value: 0.0338, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9372
Epoch 10/10, Train Loss: 0.1808 (value: 0.0007, weighted value: 0.0330, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9375
..training done in 62.30 seconds
..evaluation done in 18.64 seconds
Old network+MCTS average reward: 0.7846, min: 0.0926, max: 1.7037, stdev: 0.2402
New network+MCTS average reward: 0.7852, min: 0.0926, max: 1.7778, stdev: 0.2372
Old bare network average reward: 0.7581, min: 0.0926, max: 1.7037, stdev: 0.2421
New bare network average reward: 0.7598, min: 0.0926, max: 1.7037, stdev: 0.2393
External policy "random" average reward: 0.2840, min: -0.2407, max: 1.2778, stdev: 0.2247
External policy "individual greedy" average reward: 0.5637, min: -0.0185, max: 1.4907, stdev: 0.2359
External policy "total greedy" average reward: 0.6766, min: 0.0278, max: 1.6389, stdev: 0.2271
New network won 63 and tied 170 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 766 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.47 seconds
Training examples lengths: [64630, 64766, 64636, 64663, 64981, 64872, 64802, 65073, 64746, 64942]
Total value: 504276.14
Training on 648111 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2563 (value: 0.0015, weighted value: 0.0766, policy: 0.1797, weighted policy: 0.1797), Train Mean Max: 0.9322
Epoch 2/10, Train Loss: 0.2267 (value: 0.0012, weighted value: 0.0609, policy: 0.1658, weighted policy: 0.1658), Train Mean Max: 0.9340
Epoch 3/10, Train Loss: 0.2133 (value: 0.0011, weighted value: 0.0546, policy: 0.1587, weighted policy: 0.1587), Train Mean Max: 0.9346
Epoch 4/10, Train Loss: 0.2049 (value: 0.0010, weighted value: 0.0500, policy: 0.1549, weighted policy: 0.1549), Train Mean Max: 0.9349
Epoch 5/10, Train Loss: 0.1970 (value: 0.0009, weighted value: 0.0448, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9353
Epoch 6/10, Train Loss: 0.1945 (value: 0.0008, weighted value: 0.0424, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9354
Epoch 7/10, Train Loss: 0.1910 (value: 0.0008, weighted value: 0.0406, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9357
Epoch 8/10, Train Loss: 0.1890 (value: 0.0008, weighted value: 0.0388, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9360
Epoch 9/10, Train Loss: 0.1868 (value: 0.0007, weighted value: 0.0373, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9363
Epoch 10/10, Train Loss: 0.1842 (value: 0.0007, weighted value: 0.0348, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9365
..training done in 64.47 seconds
..evaluation done in 19.30 seconds
Old network+MCTS average reward: 0.7747, min: 0.1296, max: 1.4907, stdev: 0.2376
New network+MCTS average reward: 0.7718, min: 0.1296, max: 1.4444, stdev: 0.2366
Old bare network average reward: 0.7430, min: -0.0278, max: 1.4444, stdev: 0.2438
New bare network average reward: 0.7414, min: 0.0185, max: 1.4444, stdev: 0.2377
External policy "random" average reward: 0.2694, min: -0.3611, max: 1.1296, stdev: 0.2348
External policy "individual greedy" average reward: 0.5504, min: -0.0833, max: 1.3704, stdev: 0.2380
External policy "total greedy" average reward: 0.6514, min: -0.0093, max: 1.3611, stdev: 0.2302
New network won 55 and tied 183 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 767 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.95 seconds
Training examples lengths: [64766, 64636, 64663, 64981, 64872, 64802, 65073, 64746, 64942, 64838]
Total value: 504837.44
Training on 648319 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2698 (value: 0.0017, weighted value: 0.0845, policy: 0.1854, weighted policy: 0.1854), Train Mean Max: 0.9312
Epoch 2/10, Train Loss: 0.2364 (value: 0.0014, weighted value: 0.0676, policy: 0.1688, weighted policy: 0.1688), Train Mean Max: 0.9333
Epoch 3/10, Train Loss: 0.2190 (value: 0.0012, weighted value: 0.0586, policy: 0.1603, weighted policy: 0.1603), Train Mean Max: 0.9337
Epoch 4/10, Train Loss: 0.2095 (value: 0.0011, weighted value: 0.0532, policy: 0.1564, weighted policy: 0.1564), Train Mean Max: 0.9344
Epoch 5/10, Train Loss: 0.2029 (value: 0.0010, weighted value: 0.0487, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9345
Epoch 6/10, Train Loss: 0.1998 (value: 0.0009, weighted value: 0.0472, policy: 0.1526, weighted policy: 0.1526), Train Mean Max: 0.9349
Epoch 7/10, Train Loss: 0.1945 (value: 0.0009, weighted value: 0.0431, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9352
Epoch 8/10, Train Loss: 0.1910 (value: 0.0008, weighted value: 0.0406, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9355
Epoch 9/10, Train Loss: 0.1890 (value: 0.0008, weighted value: 0.0388, policy: 0.1502, weighted policy: 0.1502), Train Mean Max: 0.9357
Epoch 10/10, Train Loss: 0.1858 (value: 0.0007, weighted value: 0.0367, policy: 0.1491, weighted policy: 0.1491), Train Mean Max: 0.9360
..training done in 64.88 seconds
..evaluation done in 18.99 seconds
Old network+MCTS average reward: 0.7601, min: 0.1204, max: 1.6111, stdev: 0.2347
New network+MCTS average reward: 0.7669, min: 0.1574, max: 1.7037, stdev: 0.2374
Old bare network average reward: 0.7327, min: 0.1019, max: 1.6389, stdev: 0.2409
New bare network average reward: 0.7319, min: 0.1204, max: 1.6667, stdev: 0.2438
External policy "random" average reward: 0.2594, min: -0.3704, max: 1.2315, stdev: 0.2389
External policy "individual greedy" average reward: 0.5303, min: -0.0370, max: 1.4259, stdev: 0.2346
External policy "total greedy" average reward: 0.6469, min: 0.0556, max: 1.3426, stdev: 0.2290
New network won 91 and tied 161 out of 300 games (57.17% wins where ties are half wins)
Keeping the new network

Training iteration 768 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 77.83 seconds
Training examples lengths: [64636, 64663, 64981, 64872, 64802, 65073, 64746, 64942, 64838, 64765]
Total value: 504492.31
Training on 648318 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2021 (value: 0.0009, weighted value: 0.0452, policy: 0.1568, weighted policy: 0.1568), Train Mean Max: 0.9350
Epoch 2/10, Train Loss: 0.1947 (value: 0.0008, weighted value: 0.0411, policy: 0.1536, weighted policy: 0.1536), Train Mean Max: 0.9356
Epoch 3/10, Train Loss: 0.1891 (value: 0.0008, weighted value: 0.0384, policy: 0.1507, weighted policy: 0.1507), Train Mean Max: 0.9360
Epoch 4/10, Train Loss: 0.1864 (value: 0.0007, weighted value: 0.0370, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9362
Epoch 5/10, Train Loss: 0.1844 (value: 0.0007, weighted value: 0.0351, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9364
Epoch 6/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0341, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9368
Epoch 7/10, Train Loss: 0.1806 (value: 0.0007, weighted value: 0.0326, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9370
Epoch 8/10, Train Loss: 0.1796 (value: 0.0006, weighted value: 0.0324, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9371
Epoch 9/10, Train Loss: 0.1771 (value: 0.0006, weighted value: 0.0299, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9374
Epoch 10/10, Train Loss: 0.1777 (value: 0.0006, weighted value: 0.0300, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9375
..training done in 64.53 seconds
..evaluation done in 18.41 seconds
Old network+MCTS average reward: 0.7596, min: 0.1296, max: 1.3611, stdev: 0.2234
New network+MCTS average reward: 0.7591, min: 0.1296, max: 1.3519, stdev: 0.2241
Old bare network average reward: 0.7352, min: 0.1296, max: 1.4537, stdev: 0.2323
New bare network average reward: 0.7318, min: 0.1296, max: 1.3519, stdev: 0.2316
External policy "random" average reward: 0.2527, min: -0.3426, max: 1.0185, stdev: 0.2269
External policy "individual greedy" average reward: 0.5256, min: -0.0556, max: 1.2685, stdev: 0.2275
External policy "total greedy" average reward: 0.6480, min: 0.0556, max: 1.2500, stdev: 0.2203
New network won 66 and tied 170 out of 300 games (50.33% wins where ties are half wins)
Keeping the new network

Training iteration 769 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.98 seconds
Training examples lengths: [64663, 64981, 64872, 64802, 65073, 64746, 64942, 64838, 64765, 64757]
Total value: 504297.40
Training on 648439 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1937 (value: 0.0008, weighted value: 0.0390, policy: 0.1547, weighted policy: 0.1547), Train Mean Max: 0.9363
Epoch 2/10, Train Loss: 0.1877 (value: 0.0007, weighted value: 0.0361, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9369
Epoch 3/10, Train Loss: 0.1826 (value: 0.0007, weighted value: 0.0336, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9372
Epoch 4/10, Train Loss: 0.1803 (value: 0.0006, weighted value: 0.0320, policy: 0.1482, weighted policy: 0.1482), Train Mean Max: 0.9374
Epoch 5/10, Train Loss: 0.1781 (value: 0.0006, weighted value: 0.0311, policy: 0.1469, weighted policy: 0.1469), Train Mean Max: 0.9377
Epoch 6/10, Train Loss: 0.1764 (value: 0.0006, weighted value: 0.0300, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9378
Epoch 7/10, Train Loss: 0.1759 (value: 0.0006, weighted value: 0.0294, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1750 (value: 0.0006, weighted value: 0.0290, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9380
Epoch 9/10, Train Loss: 0.1731 (value: 0.0005, weighted value: 0.0274, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9383
Epoch 10/10, Train Loss: 0.1736 (value: 0.0005, weighted value: 0.0273, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9382
..training done in 64.87 seconds
..evaluation done in 18.89 seconds
Old network+MCTS average reward: 0.7917, min: 0.1852, max: 1.4259, stdev: 0.2468
New network+MCTS average reward: 0.7903, min: 0.1944, max: 1.4259, stdev: 0.2493
Old bare network average reward: 0.7649, min: 0.1111, max: 1.4167, stdev: 0.2496
New bare network average reward: 0.7625, min: 0.1296, max: 1.4167, stdev: 0.2514
External policy "random" average reward: 0.2653, min: -0.1759, max: 1.0093, stdev: 0.2232
External policy "individual greedy" average reward: 0.5421, min: -0.0556, max: 1.2500, stdev: 0.2223
External policy "total greedy" average reward: 0.6613, min: 0.0926, max: 1.2870, stdev: 0.2288
New network won 53 and tied 182 out of 300 games (48.00% wins where ties are half wins)
Reverting to the old network

Training iteration 770 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.40 seconds
Training examples lengths: [64981, 64872, 64802, 65073, 64746, 64942, 64838, 64765, 64757, 64902]
Total value: 504871.87
Training on 648678 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2112 (value: 0.0010, weighted value: 0.0494, policy: 0.1618, weighted policy: 0.1618), Train Mean Max: 0.9349
Epoch 2/10, Train Loss: 0.1985 (value: 0.0009, weighted value: 0.0428, policy: 0.1557, weighted policy: 0.1557), Train Mean Max: 0.9360
Epoch 3/10, Train Loss: 0.1911 (value: 0.0008, weighted value: 0.0392, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9363
Epoch 4/10, Train Loss: 0.1859 (value: 0.0007, weighted value: 0.0364, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9368
Epoch 5/10, Train Loss: 0.1849 (value: 0.0007, weighted value: 0.0365, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1808 (value: 0.0007, weighted value: 0.0332, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9372
Epoch 7/10, Train Loss: 0.1796 (value: 0.0006, weighted value: 0.0325, policy: 0.1472, weighted policy: 0.1472), Train Mean Max: 0.9373
Epoch 8/10, Train Loss: 0.1792 (value: 0.0006, weighted value: 0.0325, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9375
Epoch 9/10, Train Loss: 0.1769 (value: 0.0006, weighted value: 0.0296, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1765 (value: 0.0006, weighted value: 0.0301, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9378
..training done in 68.14 seconds
..evaluation done in 18.06 seconds
Old network+MCTS average reward: 0.7727, min: 0.2685, max: 1.4259, stdev: 0.2277
New network+MCTS average reward: 0.7743, min: 0.2500, max: 1.4259, stdev: 0.2269
Old bare network average reward: 0.7458, min: 0.2037, max: 1.4259, stdev: 0.2312
New bare network average reward: 0.7452, min: 0.2500, max: 1.4259, stdev: 0.2306
External policy "random" average reward: 0.2886, min: -0.2963, max: 0.9167, stdev: 0.2309
External policy "individual greedy" average reward: 0.5520, min: 0.0833, max: 1.2870, stdev: 0.2248
External policy "total greedy" average reward: 0.6753, min: 0.1852, max: 1.4259, stdev: 0.2186
New network won 62 and tied 181 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 771 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.91 seconds
Training examples lengths: [64872, 64802, 65073, 64746, 64942, 64838, 64765, 64757, 64902, 64994]
Total value: 504967.98
Training on 648691 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1920 (value: 0.0008, weighted value: 0.0389, policy: 0.1531, weighted policy: 0.1531), Train Mean Max: 0.9369
Epoch 2/10, Train Loss: 0.1872 (value: 0.0007, weighted value: 0.0359, policy: 0.1513, weighted policy: 0.1513), Train Mean Max: 0.9374
Epoch 3/10, Train Loss: 0.1812 (value: 0.0007, weighted value: 0.0333, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9378
Epoch 4/10, Train Loss: 0.1781 (value: 0.0006, weighted value: 0.0316, policy: 0.1465, weighted policy: 0.1465), Train Mean Max: 0.9380
Epoch 5/10, Train Loss: 0.1776 (value: 0.0006, weighted value: 0.0316, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9382
Epoch 6/10, Train Loss: 0.1761 (value: 0.0006, weighted value: 0.0303, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9381
Epoch 7/10, Train Loss: 0.1729 (value: 0.0006, weighted value: 0.0279, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9385
Epoch 8/10, Train Loss: 0.1738 (value: 0.0006, weighted value: 0.0282, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9387
Epoch 9/10, Train Loss: 0.1733 (value: 0.0006, weighted value: 0.0281, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9388
Epoch 10/10, Train Loss: 0.1707 (value: 0.0005, weighted value: 0.0261, policy: 0.1445, weighted policy: 0.1445), Train Mean Max: 0.9390
..training done in 64.74 seconds
..evaluation done in 18.38 seconds
Old network+MCTS average reward: 0.7955, min: 0.1852, max: 1.7130, stdev: 0.2519
New network+MCTS average reward: 0.7973, min: 0.2037, max: 1.6667, stdev: 0.2515
Old bare network average reward: 0.7685, min: 0.1111, max: 1.6667, stdev: 0.2588
New bare network average reward: 0.7685, min: 0.1204, max: 1.6667, stdev: 0.2564
External policy "random" average reward: 0.2825, min: -0.3611, max: 0.9815, stdev: 0.2327
External policy "individual greedy" average reward: 0.5728, min: 0.0370, max: 1.2870, stdev: 0.2370
External policy "total greedy" average reward: 0.6709, min: 0.1111, max: 1.3611, stdev: 0.2273
New network won 61 and tied 188 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 772 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.06 seconds
Training examples lengths: [64802, 65073, 64746, 64942, 64838, 64765, 64757, 64902, 64994, 64785]
Total value: 504800.38
Training on 648604 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1905 (value: 0.0007, weighted value: 0.0365, policy: 0.1540, weighted policy: 0.1540), Train Mean Max: 0.9376
Epoch 2/10, Train Loss: 0.1823 (value: 0.0007, weighted value: 0.0328, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9383
Epoch 3/10, Train Loss: 0.1797 (value: 0.0006, weighted value: 0.0317, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9385
Epoch 4/10, Train Loss: 0.1761 (value: 0.0006, weighted value: 0.0302, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9387
Epoch 5/10, Train Loss: 0.1732 (value: 0.0006, weighted value: 0.0287, policy: 0.1445, weighted policy: 0.1445), Train Mean Max: 0.9389
Epoch 6/10, Train Loss: 0.1733 (value: 0.0006, weighted value: 0.0281, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9389
Epoch 7/10, Train Loss: 0.1714 (value: 0.0005, weighted value: 0.0269, policy: 0.1444, weighted policy: 0.1444), Train Mean Max: 0.9391
Epoch 8/10, Train Loss: 0.1708 (value: 0.0005, weighted value: 0.0268, policy: 0.1440, weighted policy: 0.1440), Train Mean Max: 0.9392
Epoch 9/10, Train Loss: 0.1701 (value: 0.0005, weighted value: 0.0257, policy: 0.1444, weighted policy: 0.1444), Train Mean Max: 0.9392
Epoch 10/10, Train Loss: 0.1701 (value: 0.0005, weighted value: 0.0254, policy: 0.1447, weighted policy: 0.1447), Train Mean Max: 0.9393
..training done in 63.89 seconds
..evaluation done in 19.16 seconds
Old network+MCTS average reward: 0.7381, min: 0.1204, max: 1.3704, stdev: 0.2407
New network+MCTS average reward: 0.7398, min: 0.0833, max: 1.4074, stdev: 0.2417
Old bare network average reward: 0.7144, min: 0.0833, max: 1.3704, stdev: 0.2437
New bare network average reward: 0.7151, min: 0.0833, max: 1.4630, stdev: 0.2435
External policy "random" average reward: 0.2515, min: -0.3704, max: 0.8148, stdev: 0.2221
External policy "individual greedy" average reward: 0.5137, min: -0.0093, max: 1.1667, stdev: 0.2232
External policy "total greedy" average reward: 0.6209, min: 0.0556, max: 1.2407, stdev: 0.2150
New network won 55 and tied 195 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 773 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.96 seconds
Training examples lengths: [65073, 64746, 64942, 64838, 64765, 64757, 64902, 64994, 64785, 64839]
Total value: 505211.93
Training on 648641 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1888 (value: 0.0007, weighted value: 0.0350, policy: 0.1538, weighted policy: 0.1538), Train Mean Max: 0.9381
Epoch 2/10, Train Loss: 0.1812 (value: 0.0006, weighted value: 0.0318, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9388
Epoch 3/10, Train Loss: 0.1775 (value: 0.0006, weighted value: 0.0309, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9392
Epoch 4/10, Train Loss: 0.1736 (value: 0.0006, weighted value: 0.0286, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9392
Epoch 5/10, Train Loss: 0.1727 (value: 0.0006, weighted value: 0.0284, policy: 0.1443, weighted policy: 0.1443), Train Mean Max: 0.9393
Epoch 6/10, Train Loss: 0.1717 (value: 0.0005, weighted value: 0.0274, policy: 0.1442, weighted policy: 0.1442), Train Mean Max: 0.9393
Epoch 7/10, Train Loss: 0.1706 (value: 0.0005, weighted value: 0.0265, policy: 0.1440, weighted policy: 0.1440), Train Mean Max: 0.9394
Epoch 8/10, Train Loss: 0.1693 (value: 0.0005, weighted value: 0.0254, policy: 0.1439, weighted policy: 0.1439), Train Mean Max: 0.9396
Epoch 9/10, Train Loss: 0.1687 (value: 0.0005, weighted value: 0.0251, policy: 0.1436, weighted policy: 0.1436), Train Mean Max: 0.9397
Epoch 10/10, Train Loss: 0.1676 (value: 0.0005, weighted value: 0.0245, policy: 0.1431, weighted policy: 0.1431), Train Mean Max: 0.9398
..training done in 63.46 seconds
..evaluation done in 18.66 seconds
Old network+MCTS average reward: 0.7775, min: 0.1389, max: 1.3704, stdev: 0.2401
New network+MCTS average reward: 0.7763, min: 0.1852, max: 1.3704, stdev: 0.2380
Old bare network average reward: 0.7532, min: 0.0648, max: 1.3704, stdev: 0.2410
New bare network average reward: 0.7511, min: 0.0648, max: 1.3704, stdev: 0.2422
External policy "random" average reward: 0.2798, min: -0.3519, max: 1.2037, stdev: 0.2377
External policy "individual greedy" average reward: 0.5487, min: -0.0185, max: 1.2870, stdev: 0.2341
External policy "total greedy" average reward: 0.6674, min: 0.0833, max: 1.3704, stdev: 0.2330
New network won 60 and tied 186 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 774 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.87 seconds
Training examples lengths: [64746, 64942, 64838, 64765, 64757, 64902, 64994, 64785, 64839, 64761]
Total value: 504462.68
Training on 648329 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1886 (value: 0.0007, weighted value: 0.0356, policy: 0.1530, weighted policy: 0.1530), Train Mean Max: 0.9382
Epoch 2/10, Train Loss: 0.1792 (value: 0.0006, weighted value: 0.0305, policy: 0.1488, weighted policy: 0.1488), Train Mean Max: 0.9390
Epoch 3/10, Train Loss: 0.1763 (value: 0.0006, weighted value: 0.0305, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9392
Epoch 4/10, Train Loss: 0.1724 (value: 0.0006, weighted value: 0.0278, policy: 0.1446, weighted policy: 0.1446), Train Mean Max: 0.9393
Epoch 5/10, Train Loss: 0.1729 (value: 0.0006, weighted value: 0.0281, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9394
Epoch 6/10, Train Loss: 0.1703 (value: 0.0005, weighted value: 0.0262, policy: 0.1441, weighted policy: 0.1441), Train Mean Max: 0.9395
Epoch 7/10, Train Loss: 0.1692 (value: 0.0005, weighted value: 0.0258, policy: 0.1435, weighted policy: 0.1435), Train Mean Max: 0.9397
Epoch 8/10, Train Loss: 0.1687 (value: 0.0005, weighted value: 0.0254, policy: 0.1433, weighted policy: 0.1433), Train Mean Max: 0.9398
Epoch 9/10, Train Loss: 0.1679 (value: 0.0005, weighted value: 0.0245, policy: 0.1434, weighted policy: 0.1434), Train Mean Max: 0.9397
Epoch 10/10, Train Loss: 0.1686 (value: 0.0005, weighted value: 0.0245, policy: 0.1441, weighted policy: 0.1441), Train Mean Max: 0.9399
..training done in 63.27 seconds
..evaluation done in 18.37 seconds
Old network+MCTS average reward: 0.7531, min: 0.1852, max: 1.4815, stdev: 0.2416
New network+MCTS average reward: 0.7559, min: 0.1852, max: 1.4907, stdev: 0.2421
Old bare network average reward: 0.7273, min: 0.1852, max: 1.4444, stdev: 0.2450
New bare network average reward: 0.7269, min: 0.1574, max: 1.4444, stdev: 0.2469
External policy "random" average reward: 0.2606, min: -0.2963, max: 0.9259, stdev: 0.2120
External policy "individual greedy" average reward: 0.5280, min: -0.1667, max: 1.2407, stdev: 0.2306
External policy "total greedy" average reward: 0.6416, min: 0.0556, max: 1.3611, stdev: 0.2173
New network won 76 and tied 169 out of 300 games (53.50% wins where ties are half wins)
Keeping the new network

Training iteration 775 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.69 seconds
Training examples lengths: [64942, 64838, 64765, 64757, 64902, 64994, 64785, 64839, 64761, 64978]
Total value: 504734.19
Training on 648561 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1872 (value: 0.0007, weighted value: 0.0338, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9384
Epoch 2/10, Train Loss: 0.1802 (value: 0.0006, weighted value: 0.0319, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9392
Epoch 3/10, Train Loss: 0.1739 (value: 0.0006, weighted value: 0.0283, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9393
Epoch 4/10, Train Loss: 0.1733 (value: 0.0006, weighted value: 0.0289, policy: 0.1444, weighted policy: 0.1444), Train Mean Max: 0.9395
Epoch 5/10, Train Loss: 0.1699 (value: 0.0005, weighted value: 0.0264, policy: 0.1435, weighted policy: 0.1435), Train Mean Max: 0.9396
Epoch 6/10, Train Loss: 0.1695 (value: 0.0005, weighted value: 0.0263, policy: 0.1432, weighted policy: 0.1432), Train Mean Max: 0.9397
Epoch 7/10, Train Loss: 0.1692 (value: 0.0005, weighted value: 0.0259, policy: 0.1433, weighted policy: 0.1433), Train Mean Max: 0.9397
Epoch 8/10, Train Loss: 0.1684 (value: 0.0005, weighted value: 0.0251, policy: 0.1434, weighted policy: 0.1434), Train Mean Max: 0.9400
Epoch 9/10, Train Loss: 0.1673 (value: 0.0005, weighted value: 0.0237, policy: 0.1436, weighted policy: 0.1436), Train Mean Max: 0.9400
Epoch 10/10, Train Loss: 0.1683 (value: 0.0005, weighted value: 0.0243, policy: 0.1440, weighted policy: 0.1440), Train Mean Max: 0.9400
..training done in 64.12 seconds
..evaluation done in 19.00 seconds
Old network+MCTS average reward: 0.7781, min: 0.2037, max: 1.5463, stdev: 0.2300
New network+MCTS average reward: 0.7767, min: 0.2037, max: 1.5278, stdev: 0.2309
Old bare network average reward: 0.7505, min: 0.1759, max: 1.5278, stdev: 0.2343
New bare network average reward: 0.7497, min: 0.1759, max: 1.5370, stdev: 0.2351
External policy "random" average reward: 0.2770, min: -0.3241, max: 0.9815, stdev: 0.2152
External policy "individual greedy" average reward: 0.5524, min: -0.0278, max: 1.3148, stdev: 0.2203
External policy "total greedy" average reward: 0.6624, min: 0.0278, max: 1.4537, stdev: 0.2181
New network won 59 and tied 178 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 776 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.51 seconds
Training examples lengths: [64838, 64765, 64757, 64902, 64994, 64785, 64839, 64761, 64978, 64996]
Total value: 504945.69
Training on 648615 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2041 (value: 0.0009, weighted value: 0.0449, policy: 0.1593, weighted policy: 0.1593), Train Mean Max: 0.9371
Epoch 2/10, Train Loss: 0.1922 (value: 0.0008, weighted value: 0.0388, policy: 0.1534, weighted policy: 0.1534), Train Mean Max: 0.9381
Epoch 3/10, Train Loss: 0.1837 (value: 0.0007, weighted value: 0.0354, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9387
Epoch 4/10, Train Loss: 0.1797 (value: 0.0007, weighted value: 0.0335, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9387
Epoch 5/10, Train Loss: 0.1773 (value: 0.0006, weighted value: 0.0320, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9388
Epoch 6/10, Train Loss: 0.1750 (value: 0.0006, weighted value: 0.0304, policy: 0.1447, weighted policy: 0.1447), Train Mean Max: 0.9389
Epoch 7/10, Train Loss: 0.1724 (value: 0.0006, weighted value: 0.0286, policy: 0.1438, weighted policy: 0.1438), Train Mean Max: 0.9392
Epoch 8/10, Train Loss: 0.1729 (value: 0.0006, weighted value: 0.0291, policy: 0.1438, weighted policy: 0.1438), Train Mean Max: 0.9393
Epoch 9/10, Train Loss: 0.1710 (value: 0.0005, weighted value: 0.0271, policy: 0.1439, weighted policy: 0.1439), Train Mean Max: 0.9393
Epoch 10/10, Train Loss: 0.1706 (value: 0.0005, weighted value: 0.0268, policy: 0.1439, weighted policy: 0.1439), Train Mean Max: 0.9396
..training done in 59.57 seconds
..evaluation done in 19.17 seconds
Old network+MCTS average reward: 0.7505, min: 0.1111, max: 1.3889, stdev: 0.2228
New network+MCTS average reward: 0.7531, min: 0.1111, max: 1.4167, stdev: 0.2257
Old bare network average reward: 0.7246, min: 0.1111, max: 1.3889, stdev: 0.2272
New bare network average reward: 0.7248, min: 0.1111, max: 1.3889, stdev: 0.2263
External policy "random" average reward: 0.2486, min: -0.5278, max: 0.8519, stdev: 0.2234
External policy "individual greedy" average reward: 0.5225, min: -0.1204, max: 1.2130, stdev: 0.2198
External policy "total greedy" average reward: 0.6382, min: 0.0556, max: 1.2778, stdev: 0.2164
New network won 63 and tied 177 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 777 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.62 seconds
Training examples lengths: [64765, 64757, 64902, 64994, 64785, 64839, 64761, 64978, 64996, 65005]
Total value: 504879.16
Training on 648782 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1885 (value: 0.0007, weighted value: 0.0364, policy: 0.1521, weighted policy: 0.1521), Train Mean Max: 0.9381
Epoch 2/10, Train Loss: 0.1824 (value: 0.0007, weighted value: 0.0329, policy: 0.1496, weighted policy: 0.1496), Train Mean Max: 0.9388
Epoch 3/10, Train Loss: 0.1768 (value: 0.0006, weighted value: 0.0310, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9392
Epoch 4/10, Train Loss: 0.1744 (value: 0.0006, weighted value: 0.0296, policy: 0.1447, weighted policy: 0.1447), Train Mean Max: 0.9393
Epoch 5/10, Train Loss: 0.1728 (value: 0.0006, weighted value: 0.0288, policy: 0.1440, weighted policy: 0.1440), Train Mean Max: 0.9394
Epoch 6/10, Train Loss: 0.1717 (value: 0.0006, weighted value: 0.0280, policy: 0.1436, weighted policy: 0.1436), Train Mean Max: 0.9394
Epoch 7/10, Train Loss: 0.1695 (value: 0.0005, weighted value: 0.0262, policy: 0.1433, weighted policy: 0.1433), Train Mean Max: 0.9397
Epoch 8/10, Train Loss: 0.1702 (value: 0.0005, weighted value: 0.0268, policy: 0.1434, weighted policy: 0.1434), Train Mean Max: 0.9397
Epoch 9/10, Train Loss: 0.1681 (value: 0.0005, weighted value: 0.0250, policy: 0.1431, weighted policy: 0.1431), Train Mean Max: 0.9399
Epoch 10/10, Train Loss: 0.1685 (value: 0.0005, weighted value: 0.0255, policy: 0.1431, weighted policy: 0.1431), Train Mean Max: 0.9399
..training done in 65.03 seconds
..evaluation done in 18.91 seconds
Old network+MCTS average reward: 0.7687, min: 0.1481, max: 1.6019, stdev: 0.2415
New network+MCTS average reward: 0.7710, min: 0.1481, max: 1.6019, stdev: 0.2433
Old bare network average reward: 0.7391, min: 0.0833, max: 1.6019, stdev: 0.2476
New bare network average reward: 0.7432, min: 0.1481, max: 1.5556, stdev: 0.2483
External policy "random" average reward: 0.2612, min: -0.4352, max: 1.0000, stdev: 0.2414
External policy "individual greedy" average reward: 0.5560, min: -0.1481, max: 1.3611, stdev: 0.2394
External policy "total greedy" average reward: 0.6597, min: 0.1111, max: 1.2963, stdev: 0.2170
New network won 63 and tied 186 out of 300 games (52.00% wins where ties are half wins)
Keeping the new network

Training iteration 778 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.71 seconds
Training examples lengths: [64757, 64902, 64994, 64785, 64839, 64761, 64978, 64996, 65005, 64847]
Total value: 505340.25
Training on 648864 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1868 (value: 0.0007, weighted value: 0.0348, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9384
Epoch 2/10, Train Loss: 0.1797 (value: 0.0006, weighted value: 0.0314, policy: 0.1484, weighted policy: 0.1484), Train Mean Max: 0.9391
Epoch 3/10, Train Loss: 0.1752 (value: 0.0006, weighted value: 0.0296, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9394
Epoch 4/10, Train Loss: 0.1733 (value: 0.0006, weighted value: 0.0293, policy: 0.1441, weighted policy: 0.1441), Train Mean Max: 0.9395
Epoch 5/10, Train Loss: 0.1698 (value: 0.0005, weighted value: 0.0267, policy: 0.1430, weighted policy: 0.1430), Train Mean Max: 0.9395
Epoch 6/10, Train Loss: 0.1701 (value: 0.0005, weighted value: 0.0267, policy: 0.1434, weighted policy: 0.1434), Train Mean Max: 0.9397
Epoch 7/10, Train Loss: 0.1689 (value: 0.0005, weighted value: 0.0252, policy: 0.1437, weighted policy: 0.1437), Train Mean Max: 0.9397
Epoch 8/10, Train Loss: 0.1687 (value: 0.0005, weighted value: 0.0256, policy: 0.1432, weighted policy: 0.1432), Train Mean Max: 0.9399
Epoch 9/10, Train Loss: 0.1672 (value: 0.0005, weighted value: 0.0243, policy: 0.1429, weighted policy: 0.1429), Train Mean Max: 0.9401
Epoch 10/10, Train Loss: 0.1685 (value: 0.0005, weighted value: 0.0255, policy: 0.1431, weighted policy: 0.1431), Train Mean Max: 0.9401
..training done in 63.58 seconds
..evaluation done in 18.20 seconds
Old network+MCTS average reward: 0.7684, min: 0.2315, max: 1.3889, stdev: 0.2307
New network+MCTS average reward: 0.7686, min: 0.2222, max: 1.3981, stdev: 0.2287
Old bare network average reward: 0.7413, min: 0.2315, max: 1.3796, stdev: 0.2304
New bare network average reward: 0.7402, min: 0.2222, max: 1.3981, stdev: 0.2303
External policy "random" average reward: 0.2740, min: -0.3241, max: 1.0278, stdev: 0.2188
External policy "individual greedy" average reward: 0.5402, min: 0.0093, max: 1.0926, stdev: 0.2239
External policy "total greedy" average reward: 0.6508, min: 0.1574, max: 1.3241, stdev: 0.2141
New network won 49 and tied 193 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 779 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.19 seconds
Training examples lengths: [64902, 64994, 64785, 64839, 64761, 64978, 64996, 65005, 64847, 65033]
Total value: 505940.73
Training on 649140 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2055 (value: 0.0009, weighted value: 0.0464, policy: 0.1591, weighted policy: 0.1591), Train Mean Max: 0.9371
Epoch 2/10, Train Loss: 0.1914 (value: 0.0008, weighted value: 0.0389, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9381
Epoch 3/10, Train Loss: 0.1838 (value: 0.0007, weighted value: 0.0353, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9385
Epoch 4/10, Train Loss: 0.1798 (value: 0.0007, weighted value: 0.0336, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9388
Epoch 5/10, Train Loss: 0.1779 (value: 0.0007, weighted value: 0.0326, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9388
Epoch 6/10, Train Loss: 0.1756 (value: 0.0006, weighted value: 0.0313, policy: 0.1443, weighted policy: 0.1443), Train Mean Max: 0.9390
Epoch 7/10, Train Loss: 0.1734 (value: 0.0006, weighted value: 0.0292, policy: 0.1442, weighted policy: 0.1442), Train Mean Max: 0.9391
Epoch 8/10, Train Loss: 0.1728 (value: 0.0006, weighted value: 0.0290, policy: 0.1438, weighted policy: 0.1438), Train Mean Max: 0.9391
Epoch 9/10, Train Loss: 0.1707 (value: 0.0005, weighted value: 0.0274, policy: 0.1433, weighted policy: 0.1433), Train Mean Max: 0.9394
Epoch 10/10, Train Loss: 0.1708 (value: 0.0005, weighted value: 0.0273, policy: 0.1435, weighted policy: 0.1435), Train Mean Max: 0.9396
..training done in 63.73 seconds
..evaluation done in 18.34 seconds
Old network+MCTS average reward: 0.7654, min: 0.1759, max: 1.3981, stdev: 0.2236
New network+MCTS average reward: 0.7676, min: 0.1852, max: 1.3981, stdev: 0.2270
Old bare network average reward: 0.7402, min: 0.1296, max: 1.3981, stdev: 0.2278
New bare network average reward: 0.7362, min: 0.1296, max: 1.3981, stdev: 0.2284
External policy "random" average reward: 0.2659, min: -0.3426, max: 0.9352, stdev: 0.2132
External policy "individual greedy" average reward: 0.5300, min: 0.0000, max: 1.0370, stdev: 0.2139
External policy "total greedy" average reward: 0.6508, min: 0.1852, max: 1.2222, stdev: 0.2000
New network won 75 and tied 174 out of 300 games (54.00% wins where ties are half wins)
Keeping the new network

Training iteration 780 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.15 seconds
Training examples lengths: [64994, 64785, 64839, 64761, 64978, 64996, 65005, 64847, 65033, 64827]
Total value: 505422.46
Training on 649065 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1886 (value: 0.0007, weighted value: 0.0366, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9380
Epoch 2/10, Train Loss: 0.1825 (value: 0.0007, weighted value: 0.0339, policy: 0.1486, weighted policy: 0.1486), Train Mean Max: 0.9388
Epoch 3/10, Train Loss: 0.1775 (value: 0.0006, weighted value: 0.0316, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9393
Epoch 4/10, Train Loss: 0.1734 (value: 0.0006, weighted value: 0.0294, policy: 0.1440, weighted policy: 0.1440), Train Mean Max: 0.9394
Epoch 5/10, Train Loss: 0.1728 (value: 0.0006, weighted value: 0.0293, policy: 0.1435, weighted policy: 0.1435), Train Mean Max: 0.9394
Epoch 6/10, Train Loss: 0.1711 (value: 0.0006, weighted value: 0.0278, policy: 0.1433, weighted policy: 0.1433), Train Mean Max: 0.9397
Epoch 7/10, Train Loss: 0.1701 (value: 0.0005, weighted value: 0.0272, policy: 0.1429, weighted policy: 0.1429), Train Mean Max: 0.9397
Epoch 8/10, Train Loss: 0.1693 (value: 0.0005, weighted value: 0.0264, policy: 0.1429, weighted policy: 0.1429), Train Mean Max: 0.9398
Epoch 9/10, Train Loss: 0.1684 (value: 0.0005, weighted value: 0.0256, policy: 0.1428, weighted policy: 0.1428), Train Mean Max: 0.9399
Epoch 10/10, Train Loss: 0.1676 (value: 0.0005, weighted value: 0.0250, policy: 0.1426, weighted policy: 0.1426), Train Mean Max: 0.9401
..training done in 64.08 seconds
..evaluation done in 17.64 seconds
Old network+MCTS average reward: 0.7565, min: 0.1759, max: 1.4074, stdev: 0.2261
New network+MCTS average reward: 0.7572, min: 0.2037, max: 1.4074, stdev: 0.2251
Old bare network average reward: 0.7242, min: 0.1759, max: 1.3796, stdev: 0.2289
New bare network average reward: 0.7270, min: 0.1111, max: 1.3796, stdev: 0.2248
External policy "random" average reward: 0.2524, min: -0.3241, max: 0.8426, stdev: 0.2240
External policy "individual greedy" average reward: 0.5251, min: -0.2037, max: 1.1389, stdev: 0.2139
External policy "total greedy" average reward: 0.6444, min: 0.0833, max: 1.1852, stdev: 0.2174
New network won 57 and tied 186 out of 300 games (50.00% wins where ties are half wins)
Keeping the new network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_780

Training iteration 781 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.34 seconds
Training examples lengths: [64785, 64839, 64761, 64978, 64996, 65005, 64847, 65033, 64827, 64981]
Total value: 505306.27
Training on 649052 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1879 (value: 0.0007, weighted value: 0.0360, policy: 0.1519, weighted policy: 0.1519), Train Mean Max: 0.9384
Epoch 2/10, Train Loss: 0.1807 (value: 0.0006, weighted value: 0.0321, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9391
Epoch 3/10, Train Loss: 0.1755 (value: 0.0006, weighted value: 0.0298, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9394
Epoch 4/10, Train Loss: 0.1732 (value: 0.0006, weighted value: 0.0289, policy: 0.1443, weighted policy: 0.1443), Train Mean Max: 0.9395
Epoch 5/10, Train Loss: 0.1707 (value: 0.0005, weighted value: 0.0273, policy: 0.1434, weighted policy: 0.1434), Train Mean Max: 0.9396
Epoch 6/10, Train Loss: 0.1693 (value: 0.0005, weighted value: 0.0267, policy: 0.1426, weighted policy: 0.1426), Train Mean Max: 0.9398
Epoch 7/10, Train Loss: 0.1708 (value: 0.0006, weighted value: 0.0275, policy: 0.1432, weighted policy: 0.1432), Train Mean Max: 0.9397
Epoch 8/10, Train Loss: 0.1676 (value: 0.0005, weighted value: 0.0244, policy: 0.1432, weighted policy: 0.1432), Train Mean Max: 0.9398
Epoch 9/10, Train Loss: 0.1685 (value: 0.0005, weighted value: 0.0256, policy: 0.1429, weighted policy: 0.1429), Train Mean Max: 0.9401
Epoch 10/10, Train Loss: 0.1682 (value: 0.0005, weighted value: 0.0253, policy: 0.1429, weighted policy: 0.1429), Train Mean Max: 0.9401
..training done in 64.09 seconds
..evaluation done in 18.22 seconds
Old network+MCTS average reward: 0.7722, min: 0.1111, max: 1.6296, stdev: 0.2512
New network+MCTS average reward: 0.7722, min: 0.0648, max: 1.5926, stdev: 0.2499
Old bare network average reward: 0.7382, min: 0.1111, max: 1.5833, stdev: 0.2545
New bare network average reward: 0.7430, min: 0.0648, max: 1.5093, stdev: 0.2538
External policy "random" average reward: 0.2657, min: -0.3981, max: 0.9722, stdev: 0.2411
External policy "individual greedy" average reward: 0.5475, min: 0.0000, max: 1.3426, stdev: 0.2433
External policy "total greedy" average reward: 0.6492, min: 0.0648, max: 1.4537, stdev: 0.2351
New network won 55 and tied 176 out of 300 games (47.67% wins where ties are half wins)
Reverting to the old network

Training iteration 782 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.46 seconds
Training examples lengths: [64839, 64761, 64978, 64996, 65005, 64847, 65033, 64827, 64981, 64836]
Total value: 504838.27
Training on 649103 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2069 (value: 0.0009, weighted value: 0.0459, policy: 0.1611, weighted policy: 0.1611), Train Mean Max: 0.9368
Epoch 2/10, Train Loss: 0.1916 (value: 0.0008, weighted value: 0.0392, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9381
Epoch 3/10, Train Loss: 0.1838 (value: 0.0007, weighted value: 0.0358, policy: 0.1480, weighted policy: 0.1480), Train Mean Max: 0.9385
Epoch 4/10, Train Loss: 0.1808 (value: 0.0007, weighted value: 0.0346, policy: 0.1462, weighted policy: 0.1462), Train Mean Max: 0.9386
Epoch 5/10, Train Loss: 0.1765 (value: 0.0006, weighted value: 0.0317, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9389
Epoch 6/10, Train Loss: 0.1758 (value: 0.0006, weighted value: 0.0311, policy: 0.1446, weighted policy: 0.1446), Train Mean Max: 0.9389
Epoch 7/10, Train Loss: 0.1740 (value: 0.0006, weighted value: 0.0296, policy: 0.1444, weighted policy: 0.1444), Train Mean Max: 0.9391
Epoch 8/10, Train Loss: 0.1731 (value: 0.0006, weighted value: 0.0292, policy: 0.1438, weighted policy: 0.1438), Train Mean Max: 0.9391
Epoch 9/10, Train Loss: 0.1716 (value: 0.0006, weighted value: 0.0279, policy: 0.1437, weighted policy: 0.1437), Train Mean Max: 0.9393
Epoch 10/10, Train Loss: 0.1705 (value: 0.0005, weighted value: 0.0270, policy: 0.1435, weighted policy: 0.1435), Train Mean Max: 0.9396
..training done in 59.34 seconds
..evaluation done in 18.03 seconds
Old network+MCTS average reward: 0.7573, min: 0.1111, max: 1.4167, stdev: 0.2424
New network+MCTS average reward: 0.7586, min: 0.1111, max: 1.4352, stdev: 0.2422
Old bare network average reward: 0.7308, min: 0.1389, max: 1.3796, stdev: 0.2379
New bare network average reward: 0.7287, min: 0.1389, max: 1.3889, stdev: 0.2360
External policy "random" average reward: 0.2631, min: -0.3704, max: 0.9074, stdev: 0.2222
External policy "individual greedy" average reward: 0.5361, min: -0.0556, max: 1.1759, stdev: 0.2297
External policy "total greedy" average reward: 0.6552, min: 0.1019, max: 1.2315, stdev: 0.2228
New network won 61 and tied 176 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 783 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.48 seconds
Training examples lengths: [64761, 64978, 64996, 65005, 64847, 65033, 64827, 64981, 64836, 64833]
Total value: 504564.78
Training on 649097 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2229 (value: 0.0011, weighted value: 0.0557, policy: 0.1672, weighted policy: 0.1672), Train Mean Max: 0.9354
Epoch 2/10, Train Loss: 0.2034 (value: 0.0009, weighted value: 0.0467, policy: 0.1567, weighted policy: 0.1567), Train Mean Max: 0.9369
Epoch 3/10, Train Loss: 0.1941 (value: 0.0009, weighted value: 0.0426, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9373
Epoch 4/10, Train Loss: 0.1872 (value: 0.0008, weighted value: 0.0387, policy: 0.1485, weighted policy: 0.1485), Train Mean Max: 0.9377
Epoch 5/10, Train Loss: 0.1837 (value: 0.0007, weighted value: 0.0367, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9378
Epoch 6/10, Train Loss: 0.1810 (value: 0.0007, weighted value: 0.0351, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9382
Epoch 7/10, Train Loss: 0.1782 (value: 0.0007, weighted value: 0.0328, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9383
Epoch 8/10, Train Loss: 0.1775 (value: 0.0006, weighted value: 0.0322, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9384
Epoch 9/10, Train Loss: 0.1764 (value: 0.0006, weighted value: 0.0312, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9387
Epoch 10/10, Train Loss: 0.1746 (value: 0.0006, weighted value: 0.0302, policy: 0.1445, weighted policy: 0.1445), Train Mean Max: 0.9389
..training done in 59.38 seconds
..evaluation done in 18.14 seconds
Old network+MCTS average reward: 0.7453, min: 0.1481, max: 1.4537, stdev: 0.2272
New network+MCTS average reward: 0.7463, min: 0.1481, max: 1.5000, stdev: 0.2262
Old bare network average reward: 0.7175, min: 0.1481, max: 1.4537, stdev: 0.2287
New bare network average reward: 0.7195, min: 0.1481, max: 1.4537, stdev: 0.2269
External policy "random" average reward: 0.2583, min: -0.3519, max: 0.7963, stdev: 0.2065
External policy "individual greedy" average reward: 0.5249, min: 0.0556, max: 1.3889, stdev: 0.2212
External policy "total greedy" average reward: 0.6431, min: 0.1389, max: 1.3981, stdev: 0.2123
New network won 53 and tied 192 out of 300 games (49.67% wins where ties are half wins)
Reverting to the old network

Training iteration 784 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.28 seconds
Training examples lengths: [64978, 64996, 65005, 64847, 65033, 64827, 64981, 64836, 64833, 64832]
Total value: 504914.12
Training on 649168 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2400 (value: 0.0013, weighted value: 0.0671, policy: 0.1729, weighted policy: 0.1729), Train Mean Max: 0.9340
Epoch 2/10, Train Loss: 0.2130 (value: 0.0011, weighted value: 0.0527, policy: 0.1603, weighted policy: 0.1603), Train Mean Max: 0.9359
Epoch 3/10, Train Loss: 0.2014 (value: 0.0010, weighted value: 0.0479, policy: 0.1535, weighted policy: 0.1535), Train Mean Max: 0.9366
Epoch 4/10, Train Loss: 0.1944 (value: 0.0009, weighted value: 0.0438, policy: 0.1506, weighted policy: 0.1506), Train Mean Max: 0.9368
Epoch 5/10, Train Loss: 0.1890 (value: 0.0008, weighted value: 0.0403, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9369
Epoch 6/10, Train Loss: 0.1869 (value: 0.0008, weighted value: 0.0390, policy: 0.1479, weighted policy: 0.1479), Train Mean Max: 0.9373
Epoch 7/10, Train Loss: 0.1825 (value: 0.0007, weighted value: 0.0358, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9376
Epoch 8/10, Train Loss: 0.1806 (value: 0.0007, weighted value: 0.0349, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9379
Epoch 9/10, Train Loss: 0.1794 (value: 0.0007, weighted value: 0.0336, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9380
Epoch 10/10, Train Loss: 0.1785 (value: 0.0006, weighted value: 0.0325, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9382
..training done in 59.28 seconds
..evaluation done in 18.39 seconds
Old network+MCTS average reward: 0.7736, min: 0.1944, max: 1.3981, stdev: 0.2317
New network+MCTS average reward: 0.7749, min: 0.2130, max: 1.3981, stdev: 0.2347
Old bare network average reward: 0.7457, min: 0.1852, max: 1.3981, stdev: 0.2362
New bare network average reward: 0.7441, min: 0.1852, max: 1.3981, stdev: 0.2346
External policy "random" average reward: 0.2660, min: -0.2963, max: 0.8796, stdev: 0.2036
External policy "individual greedy" average reward: 0.5501, min: 0.0000, max: 1.2130, stdev: 0.2112
External policy "total greedy" average reward: 0.6564, min: 0.1296, max: 1.3426, stdev: 0.2099
New network won 61 and tied 191 out of 300 games (52.17% wins where ties are half wins)
Keeping the new network

Training iteration 785 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.96 seconds
Training examples lengths: [64996, 65005, 64847, 65033, 64827, 64981, 64836, 64833, 64832, 64716]
Total value: 503938.64
Training on 648906 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1950 (value: 0.0008, weighted value: 0.0408, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9367
Epoch 2/10, Train Loss: 0.1884 (value: 0.0008, weighted value: 0.0383, policy: 0.1501, weighted policy: 0.1501), Train Mean Max: 0.9375
Epoch 3/10, Train Loss: 0.1818 (value: 0.0007, weighted value: 0.0344, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9379
Epoch 4/10, Train Loss: 0.1798 (value: 0.0007, weighted value: 0.0339, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9380
Epoch 5/10, Train Loss: 0.1777 (value: 0.0006, weighted value: 0.0321, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9382
Epoch 6/10, Train Loss: 0.1763 (value: 0.0006, weighted value: 0.0304, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9383
Epoch 7/10, Train Loss: 0.1765 (value: 0.0006, weighted value: 0.0304, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9385
Epoch 8/10, Train Loss: 0.1738 (value: 0.0006, weighted value: 0.0290, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9388
Epoch 9/10, Train Loss: 0.1731 (value: 0.0006, weighted value: 0.0284, policy: 0.1447, weighted policy: 0.1447), Train Mean Max: 0.9389
Epoch 10/10, Train Loss: 0.1717 (value: 0.0005, weighted value: 0.0274, policy: 0.1443, weighted policy: 0.1443), Train Mean Max: 0.9391
..training done in 59.45 seconds
..evaluation done in 18.09 seconds
Old network+MCTS average reward: 0.7711, min: 0.1759, max: 1.4537, stdev: 0.2214
New network+MCTS average reward: 0.7717, min: 0.1759, max: 1.4537, stdev: 0.2233
Old bare network average reward: 0.7447, min: 0.1574, max: 1.4537, stdev: 0.2273
New bare network average reward: 0.7413, min: 0.1667, max: 1.4537, stdev: 0.2286
External policy "random" average reward: 0.2623, min: -0.2870, max: 0.9259, stdev: 0.2145
External policy "individual greedy" average reward: 0.5428, min: 0.0093, max: 1.2593, stdev: 0.2256
External policy "total greedy" average reward: 0.6592, min: 0.1481, max: 1.2778, stdev: 0.2135
New network won 61 and tied 187 out of 300 games (51.50% wins where ties are half wins)
Keeping the new network

Training iteration 786 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 56.87 seconds
Training examples lengths: [65005, 64847, 65033, 64827, 64981, 64836, 64833, 64832, 64716, 64734]
Total value: 503474.17
Training on 648644 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1895 (value: 0.0007, weighted value: 0.0366, policy: 0.1529, weighted policy: 0.1529), Train Mean Max: 0.9374
Epoch 2/10, Train Loss: 0.1836 (value: 0.0007, weighted value: 0.0339, policy: 0.1497, weighted policy: 0.1497), Train Mean Max: 0.9380
Epoch 3/10, Train Loss: 0.1782 (value: 0.0006, weighted value: 0.0315, policy: 0.1467, weighted policy: 0.1467), Train Mean Max: 0.9385
Epoch 4/10, Train Loss: 0.1759 (value: 0.0006, weighted value: 0.0303, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9386
Epoch 5/10, Train Loss: 0.1739 (value: 0.0006, weighted value: 0.0289, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9389
Epoch 6/10, Train Loss: 0.1729 (value: 0.0006, weighted value: 0.0283, policy: 0.1446, weighted policy: 0.1446), Train Mean Max: 0.9389
Epoch 7/10, Train Loss: 0.1727 (value: 0.0006, weighted value: 0.0279, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9391
Epoch 8/10, Train Loss: 0.1708 (value: 0.0005, weighted value: 0.0268, policy: 0.1440, weighted policy: 0.1440), Train Mean Max: 0.9393
Epoch 9/10, Train Loss: 0.1699 (value: 0.0005, weighted value: 0.0260, policy: 0.1439, weighted policy: 0.1439), Train Mean Max: 0.9393
Epoch 10/10, Train Loss: 0.1700 (value: 0.0005, weighted value: 0.0257, policy: 0.1443, weighted policy: 0.1443), Train Mean Max: 0.9395
..training done in 58.02 seconds
..evaluation done in 18.28 seconds
Old network+MCTS average reward: 0.7529, min: 0.0741, max: 1.6296, stdev: 0.2393
New network+MCTS average reward: 0.7518, min: 0.0741, max: 1.6296, stdev: 0.2366
Old bare network average reward: 0.7229, min: 0.0741, max: 1.6204, stdev: 0.2402
New bare network average reward: 0.7228, min: 0.0370, max: 1.6296, stdev: 0.2435
External policy "random" average reward: 0.2655, min: -0.4444, max: 1.0741, stdev: 0.2202
External policy "individual greedy" average reward: 0.5351, min: -0.0093, max: 1.4907, stdev: 0.2322
External policy "total greedy" average reward: 0.6519, min: 0.0741, max: 1.4537, stdev: 0.2172
New network won 59 and tied 175 out of 300 games (48.83% wins where ties are half wins)
Reverting to the old network

Training iteration 787 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.95 seconds
Training examples lengths: [64847, 65033, 64827, 64981, 64836, 64833, 64832, 64716, 64734, 64519]
Total value: 503400.58
Training on 648158 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2072 (value: 0.0009, weighted value: 0.0471, policy: 0.1601, weighted policy: 0.1601), Train Mean Max: 0.9359
Epoch 2/10, Train Loss: 0.1951 (value: 0.0008, weighted value: 0.0409, policy: 0.1542, weighted policy: 0.1542), Train Mean Max: 0.9373
Epoch 3/10, Train Loss: 0.1880 (value: 0.0008, weighted value: 0.0385, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9376
Epoch 4/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0351, policy: 0.1478, weighted policy: 0.1478), Train Mean Max: 0.9379
Epoch 5/10, Train Loss: 0.1794 (value: 0.0007, weighted value: 0.0336, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9381
Epoch 6/10, Train Loss: 0.1781 (value: 0.0006, weighted value: 0.0321, policy: 0.1460, weighted policy: 0.1460), Train Mean Max: 0.9382
Epoch 7/10, Train Loss: 0.1772 (value: 0.0006, weighted value: 0.0318, policy: 0.1455, weighted policy: 0.1455), Train Mean Max: 0.9384
Epoch 8/10, Train Loss: 0.1747 (value: 0.0006, weighted value: 0.0297, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9385
Epoch 9/10, Train Loss: 0.1739 (value: 0.0006, weighted value: 0.0290, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9387
Epoch 10/10, Train Loss: 0.1732 (value: 0.0006, weighted value: 0.0289, policy: 0.1443, weighted policy: 0.1443), Train Mean Max: 0.9388
..training done in 59.24 seconds
..evaluation done in 19.07 seconds
Old network+MCTS average reward: 0.7847, min: 0.2593, max: 1.3796, stdev: 0.2227
New network+MCTS average reward: 0.7791, min: 0.2593, max: 1.3796, stdev: 0.2230
Old bare network average reward: 0.7535, min: 0.2593, max: 1.3796, stdev: 0.2244
New bare network average reward: 0.7525, min: 0.2315, max: 1.3796, stdev: 0.2275
External policy "random" average reward: 0.2730, min: -0.2222, max: 0.8704, stdev: 0.2092
External policy "individual greedy" average reward: 0.5594, min: -0.0278, max: 1.1944, stdev: 0.2154
External policy "total greedy" average reward: 0.6784, min: 0.1667, max: 1.2963, stdev: 0.2170
New network won 50 and tied 172 out of 300 games (45.33% wins where ties are half wins)
Reverting to the old network

Training iteration 788 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.49 seconds
Training examples lengths: [65033, 64827, 64981, 64836, 64833, 64832, 64716, 64734, 64519, 64988]
Total value: 504051.55
Training on 648299 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2233 (value: 0.0011, weighted value: 0.0568, policy: 0.1665, weighted policy: 0.1665), Train Mean Max: 0.9348
Epoch 2/10, Train Loss: 0.2061 (value: 0.0010, weighted value: 0.0487, policy: 0.1574, weighted policy: 0.1574), Train Mean Max: 0.9362
Epoch 3/10, Train Loss: 0.1940 (value: 0.0008, weighted value: 0.0424, policy: 0.1516, weighted policy: 0.1516), Train Mean Max: 0.9368
Epoch 4/10, Train Loss: 0.1891 (value: 0.0008, weighted value: 0.0402, policy: 0.1489, weighted policy: 0.1489), Train Mean Max: 0.9372
Epoch 5/10, Train Loss: 0.1865 (value: 0.0008, weighted value: 0.0388, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9373
Epoch 6/10, Train Loss: 0.1819 (value: 0.0007, weighted value: 0.0355, policy: 0.1464, weighted policy: 0.1464), Train Mean Max: 0.9378
Epoch 7/10, Train Loss: 0.1810 (value: 0.0007, weighted value: 0.0348, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1784 (value: 0.0007, weighted value: 0.0326, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9381
Epoch 9/10, Train Loss: 0.1778 (value: 0.0006, weighted value: 0.0321, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9383
Epoch 10/10, Train Loss: 0.1755 (value: 0.0006, weighted value: 0.0303, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9385
..training done in 59.12 seconds
..evaluation done in 18.19 seconds
Old network+MCTS average reward: 0.7772, min: -0.0648, max: 1.4259, stdev: 0.2470
New network+MCTS average reward: 0.7751, min: -0.0556, max: 1.4259, stdev: 0.2463
Old bare network average reward: 0.7490, min: -0.0556, max: 1.4259, stdev: 0.2476
New bare network average reward: 0.7484, min: -0.0833, max: 1.4259, stdev: 0.2510
External policy "random" average reward: 0.2618, min: -0.2685, max: 0.9537, stdev: 0.2249
External policy "individual greedy" average reward: 0.5436, min: -0.1481, max: 1.2963, stdev: 0.2413
External policy "total greedy" average reward: 0.6594, min: -0.0741, max: 1.2685, stdev: 0.2290
New network won 65 and tied 175 out of 300 games (50.83% wins where ties are half wins)
Keeping the new network

Training iteration 789 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.86 seconds
Training examples lengths: [64827, 64981, 64836, 64833, 64832, 64716, 64734, 64519, 64988, 64496]
Total value: 503220.75
Training on 647762 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1923 (value: 0.0008, weighted value: 0.0398, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9373
Epoch 2/10, Train Loss: 0.1853 (value: 0.0007, weighted value: 0.0359, policy: 0.1494, weighted policy: 0.1494), Train Mean Max: 0.9379
Epoch 3/10, Train Loss: 0.1812 (value: 0.0007, weighted value: 0.0341, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9383
Epoch 4/10, Train Loss: 0.1785 (value: 0.0007, weighted value: 0.0327, policy: 0.1458, weighted policy: 0.1458), Train Mean Max: 0.9385
Epoch 5/10, Train Loss: 0.1764 (value: 0.0006, weighted value: 0.0316, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9387
Epoch 6/10, Train Loss: 0.1748 (value: 0.0006, weighted value: 0.0303, policy: 0.1445, weighted policy: 0.1445), Train Mean Max: 0.9388
Epoch 7/10, Train Loss: 0.1731 (value: 0.0006, weighted value: 0.0293, policy: 0.1438, weighted policy: 0.1438), Train Mean Max: 0.9391
Epoch 8/10, Train Loss: 0.1724 (value: 0.0006, weighted value: 0.0281, policy: 0.1443, weighted policy: 0.1443), Train Mean Max: 0.9391
Epoch 9/10, Train Loss: 0.1712 (value: 0.0005, weighted value: 0.0272, policy: 0.1441, weighted policy: 0.1441), Train Mean Max: 0.9392
Epoch 10/10, Train Loss: 0.1707 (value: 0.0005, weighted value: 0.0271, policy: 0.1437, weighted policy: 0.1437), Train Mean Max: 0.9395
..training done in 65.30 seconds
..evaluation done in 18.44 seconds
Old network+MCTS average reward: 0.7820, min: 0.1019, max: 1.4259, stdev: 0.2395
New network+MCTS average reward: 0.7819, min: 0.1019, max: 1.3981, stdev: 0.2394
Old bare network average reward: 0.7555, min: 0.0370, max: 1.4074, stdev: 0.2394
New bare network average reward: 0.7527, min: 0.0370, max: 1.3981, stdev: 0.2385
External policy "random" average reward: 0.2691, min: -0.3426, max: 0.9722, stdev: 0.2307
External policy "individual greedy" average reward: 0.5454, min: -0.0463, max: 1.1574, stdev: 0.2162
External policy "total greedy" average reward: 0.6549, min: -0.0278, max: 1.3889, stdev: 0.2213
New network won 60 and tied 183 out of 300 games (50.50% wins where ties are half wins)
Keeping the new network

Training iteration 790 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.20 seconds
Training examples lengths: [64981, 64836, 64833, 64832, 64716, 64734, 64519, 64988, 64496, 64539]
Total value: 503641.78
Training on 647474 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1887 (value: 0.0007, weighted value: 0.0366, policy: 0.1522, weighted policy: 0.1522), Train Mean Max: 0.9379
Epoch 2/10, Train Loss: 0.1829 (value: 0.0007, weighted value: 0.0336, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9386
Epoch 3/10, Train Loss: 0.1775 (value: 0.0006, weighted value: 0.0314, policy: 0.1461, weighted policy: 0.1461), Train Mean Max: 0.9391
Epoch 4/10, Train Loss: 0.1738 (value: 0.0006, weighted value: 0.0298, policy: 0.1440, weighted policy: 0.1440), Train Mean Max: 0.9394
Epoch 5/10, Train Loss: 0.1734 (value: 0.0006, weighted value: 0.0294, policy: 0.1440, weighted policy: 0.1440), Train Mean Max: 0.9393
Epoch 6/10, Train Loss: 0.1711 (value: 0.0006, weighted value: 0.0278, policy: 0.1433, weighted policy: 0.1433), Train Mean Max: 0.9395
Epoch 7/10, Train Loss: 0.1706 (value: 0.0005, weighted value: 0.0273, policy: 0.1433, weighted policy: 0.1433), Train Mean Max: 0.9396
Epoch 8/10, Train Loss: 0.1695 (value: 0.0005, weighted value: 0.0259, policy: 0.1436, weighted policy: 0.1436), Train Mean Max: 0.9398
Epoch 9/10, Train Loss: 0.1699 (value: 0.0005, weighted value: 0.0266, policy: 0.1433, weighted policy: 0.1433), Train Mean Max: 0.9398
Epoch 10/10, Train Loss: 0.1680 (value: 0.0005, weighted value: 0.0250, policy: 0.1430, weighted policy: 0.1430), Train Mean Max: 0.9400
..training done in 63.50 seconds
..evaluation done in 19.25 seconds
Old network+MCTS average reward: 0.7675, min: 0.1481, max: 1.5278, stdev: 0.2180
New network+MCTS average reward: 0.7661, min: 0.1481, max: 1.5278, stdev: 0.2185
Old bare network average reward: 0.7413, min: 0.0648, max: 1.5278, stdev: 0.2198
New bare network average reward: 0.7380, min: 0.0648, max: 1.5278, stdev: 0.2237
External policy "random" average reward: 0.2717, min: -0.3056, max: 0.8889, stdev: 0.2021
External policy "individual greedy" average reward: 0.5333, min: 0.1019, max: 1.0926, stdev: 0.2035
External policy "total greedy" average reward: 0.6467, min: 0.1389, max: 1.2407, stdev: 0.1993
New network won 53 and tied 190 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 791 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.11 seconds
Training examples lengths: [64836, 64833, 64832, 64716, 64734, 64519, 64988, 64496, 64539, 64856]
Total value: 504138.95
Training on 647349 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2060 (value: 0.0009, weighted value: 0.0469, policy: 0.1591, weighted policy: 0.1591), Train Mean Max: 0.9367
Epoch 2/10, Train Loss: 0.1942 (value: 0.0008, weighted value: 0.0405, policy: 0.1537, weighted policy: 0.1537), Train Mean Max: 0.9377
Epoch 3/10, Train Loss: 0.1863 (value: 0.0008, weighted value: 0.0376, policy: 0.1487, weighted policy: 0.1487), Train Mean Max: 0.9384
Epoch 4/10, Train Loss: 0.1795 (value: 0.0007, weighted value: 0.0337, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9388
Epoch 5/10, Train Loss: 0.1788 (value: 0.0007, weighted value: 0.0340, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9389
Epoch 6/10, Train Loss: 0.1771 (value: 0.0006, weighted value: 0.0323, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9390
Epoch 7/10, Train Loss: 0.1749 (value: 0.0006, weighted value: 0.0299, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9392
Epoch 8/10, Train Loss: 0.1735 (value: 0.0006, weighted value: 0.0296, policy: 0.1439, weighted policy: 0.1439), Train Mean Max: 0.9393
Epoch 9/10, Train Loss: 0.1727 (value: 0.0006, weighted value: 0.0289, policy: 0.1438, weighted policy: 0.1438), Train Mean Max: 0.9395
Epoch 10/10, Train Loss: 0.1714 (value: 0.0006, weighted value: 0.0279, policy: 0.1435, weighted policy: 0.1435), Train Mean Max: 0.9398
..training done in 59.19 seconds
..evaluation done in 17.99 seconds
Old network+MCTS average reward: 0.7381, min: 0.1574, max: 1.5093, stdev: 0.2376
New network+MCTS average reward: 0.7374, min: 0.1759, max: 1.5000, stdev: 0.2393
Old bare network average reward: 0.7113, min: 0.1574, max: 1.5093, stdev: 0.2425
New bare network average reward: 0.7077, min: 0.1111, max: 1.5093, stdev: 0.2380
External policy "random" average reward: 0.2395, min: -0.3426, max: 0.9907, stdev: 0.2282
External policy "individual greedy" average reward: 0.5181, min: -0.0370, max: 1.2778, stdev: 0.2235
External policy "total greedy" average reward: 0.6316, min: 0.0833, max: 1.4259, stdev: 0.2209
New network won 58 and tied 178 out of 300 games (49.00% wins where ties are half wins)
Reverting to the old network

Training iteration 792 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.54 seconds
Training examples lengths: [64833, 64832, 64716, 64734, 64519, 64988, 64496, 64539, 64856, 64518]
Total value: 503940.67
Training on 647031 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2204 (value: 0.0011, weighted value: 0.0564, policy: 0.1640, weighted policy: 0.1640), Train Mean Max: 0.9355
Epoch 2/10, Train Loss: 0.2036 (value: 0.0009, weighted value: 0.0475, policy: 0.1561, weighted policy: 0.1561), Train Mean Max: 0.9369
Epoch 3/10, Train Loss: 0.1930 (value: 0.0008, weighted value: 0.0419, policy: 0.1512, weighted policy: 0.1512), Train Mean Max: 0.9376
Epoch 4/10, Train Loss: 0.1875 (value: 0.0008, weighted value: 0.0398, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9380
Epoch 5/10, Train Loss: 0.1831 (value: 0.0007, weighted value: 0.0374, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9383
Epoch 6/10, Train Loss: 0.1809 (value: 0.0007, weighted value: 0.0357, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9383
Epoch 7/10, Train Loss: 0.1784 (value: 0.0007, weighted value: 0.0336, policy: 0.1448, weighted policy: 0.1448), Train Mean Max: 0.9386
Epoch 8/10, Train Loss: 0.1766 (value: 0.0006, weighted value: 0.0321, policy: 0.1444, weighted policy: 0.1444), Train Mean Max: 0.9389
Epoch 9/10, Train Loss: 0.1766 (value: 0.0007, weighted value: 0.0327, policy: 0.1439, weighted policy: 0.1439), Train Mean Max: 0.9391
Epoch 10/10, Train Loss: 0.1734 (value: 0.0006, weighted value: 0.0293, policy: 0.1440, weighted policy: 0.1440), Train Mean Max: 0.9392
..training done in 58.79 seconds
..evaluation done in 18.60 seconds
Old network+MCTS average reward: 0.7844, min: 0.0741, max: 1.5185, stdev: 0.2555
New network+MCTS average reward: 0.7840, min: 0.1019, max: 1.5185, stdev: 0.2538
Old bare network average reward: 0.7531, min: 0.0463, max: 1.5185, stdev: 0.2571
New bare network average reward: 0.7536, min: -0.0093, max: 1.5093, stdev: 0.2569
External policy "random" average reward: 0.2733, min: -0.3981, max: 1.0833, stdev: 0.2370
External policy "individual greedy" average reward: 0.5572, min: 0.0000, max: 1.3889, stdev: 0.2312
External policy "total greedy" average reward: 0.6778, min: 0.0370, max: 1.5833, stdev: 0.2399
New network won 69 and tied 161 out of 300 games (49.83% wins where ties are half wins)
Reverting to the old network

Training iteration 793 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.73 seconds
Training examples lengths: [64832, 64716, 64734, 64519, 64988, 64496, 64539, 64856, 64518, 64710]
Total value: 504051.86
Training on 646908 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2360 (value: 0.0013, weighted value: 0.0655, policy: 0.1705, weighted policy: 0.1705), Train Mean Max: 0.9343
Epoch 2/10, Train Loss: 0.2116 (value: 0.0011, weighted value: 0.0530, policy: 0.1586, weighted policy: 0.1586), Train Mean Max: 0.9361
Epoch 3/10, Train Loss: 0.2000 (value: 0.0010, weighted value: 0.0475, policy: 0.1525, weighted policy: 0.1525), Train Mean Max: 0.9368
Epoch 4/10, Train Loss: 0.1934 (value: 0.0009, weighted value: 0.0441, policy: 0.1493, weighted policy: 0.1493), Train Mean Max: 0.9371
Epoch 5/10, Train Loss: 0.1888 (value: 0.0008, weighted value: 0.0411, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9374
Epoch 6/10, Train Loss: 0.1852 (value: 0.0008, weighted value: 0.0384, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9376
Epoch 7/10, Train Loss: 0.1828 (value: 0.0007, weighted value: 0.0369, policy: 0.1459, weighted policy: 0.1459), Train Mean Max: 0.9379
Epoch 8/10, Train Loss: 0.1816 (value: 0.0007, weighted value: 0.0359, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9382
Epoch 9/10, Train Loss: 0.1793 (value: 0.0007, weighted value: 0.0340, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1769 (value: 0.0007, weighted value: 0.0325, policy: 0.1444, weighted policy: 0.1444), Train Mean Max: 0.9387
..training done in 59.74 seconds
..evaluation done in 18.43 seconds
Old network+MCTS average reward: 0.7599, min: 0.2037, max: 1.5463, stdev: 0.2489
New network+MCTS average reward: 0.7612, min: 0.1759, max: 1.5463, stdev: 0.2474
Old bare network average reward: 0.7335, min: 0.1019, max: 1.5463, stdev: 0.2531
New bare network average reward: 0.7315, min: 0.1759, max: 1.5463, stdev: 0.2512
External policy "random" average reward: 0.2414, min: -0.3889, max: 1.0833, stdev: 0.2284
External policy "individual greedy" average reward: 0.5356, min: 0.0463, max: 1.4630, stdev: 0.2264
External policy "total greedy" average reward: 0.6484, min: 0.1111, max: 1.5556, stdev: 0.2279
New network won 63 and tied 184 out of 300 games (51.67% wins where ties are half wins)
Keeping the new network

Training iteration 794 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.90 seconds
Training examples lengths: [64716, 64734, 64519, 64988, 64496, 64539, 64856, 64518, 64710, 64969]
Total value: 503936.31
Training on 647045 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.1931 (value: 0.0008, weighted value: 0.0416, policy: 0.1515, weighted policy: 0.1515), Train Mean Max: 0.9374
Epoch 2/10, Train Loss: 0.1863 (value: 0.0007, weighted value: 0.0373, policy: 0.1490, weighted policy: 0.1490), Train Mean Max: 0.9380
Epoch 3/10, Train Loss: 0.1817 (value: 0.0007, weighted value: 0.0351, policy: 0.1466, weighted policy: 0.1466), Train Mean Max: 0.9384
Epoch 4/10, Train Loss: 0.1783 (value: 0.0007, weighted value: 0.0333, policy: 0.1449, weighted policy: 0.1449), Train Mean Max: 0.9386
Epoch 5/10, Train Loss: 0.1773 (value: 0.0007, weighted value: 0.0327, policy: 0.1445, weighted policy: 0.1445), Train Mean Max: 0.9387
Epoch 6/10, Train Loss: 0.1765 (value: 0.0006, weighted value: 0.0319, policy: 0.1445, weighted policy: 0.1445), Train Mean Max: 0.9389
Epoch 7/10, Train Loss: 0.1733 (value: 0.0006, weighted value: 0.0298, policy: 0.1435, weighted policy: 0.1435), Train Mean Max: 0.9392
Epoch 8/10, Train Loss: 0.1733 (value: 0.0006, weighted value: 0.0299, policy: 0.1434, weighted policy: 0.1434), Train Mean Max: 0.9393
Epoch 9/10, Train Loss: 0.1719 (value: 0.0006, weighted value: 0.0286, policy: 0.1433, weighted policy: 0.1433), Train Mean Max: 0.9394
Epoch 10/10, Train Loss: 0.1703 (value: 0.0006, weighted value: 0.0276, policy: 0.1428, weighted policy: 0.1428), Train Mean Max: 0.9397
..training done in 58.64 seconds
..evaluation done in 18.60 seconds
Old network+MCTS average reward: 0.7722, min: 0.0741, max: 1.5833, stdev: 0.2318
New network+MCTS average reward: 0.7734, min: 0.0741, max: 1.6296, stdev: 0.2318
Old bare network average reward: 0.7491, min: 0.0278, max: 1.4907, stdev: 0.2328
New bare network average reward: 0.7489, min: 0.0741, max: 1.5185, stdev: 0.2336
External policy "random" average reward: 0.2735, min: -0.3333, max: 0.9167, stdev: 0.2306
External policy "individual greedy" average reward: 0.5457, min: -0.0185, max: 1.3333, stdev: 0.2265
External policy "total greedy" average reward: 0.6533, min: 0.0926, max: 1.4259, stdev: 0.2232
New network won 62 and tied 182 out of 300 games (51.00% wins where ties are half wins)
Keeping the new network

Training iteration 795 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.57 seconds
Training examples lengths: [64734, 64519, 64988, 64496, 64539, 64856, 64518, 64710, 64969, 64588]
Total value: 504950.67
Training on 646917 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.1881 (value: 0.0007, weighted value: 0.0367, policy: 0.1514, weighted policy: 0.1514), Train Mean Max: 0.9383
Epoch 2/10, Train Loss: 0.1813 (value: 0.0007, weighted value: 0.0338, policy: 0.1475, weighted policy: 0.1475), Train Mean Max: 0.9390
Epoch 3/10, Train Loss: 0.1773 (value: 0.0006, weighted value: 0.0322, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9394
Epoch 4/10, Train Loss: 0.1737 (value: 0.0006, weighted value: 0.0304, policy: 0.1433, weighted policy: 0.1433), Train Mean Max: 0.9396
Epoch 5/10, Train Loss: 0.1718 (value: 0.0006, weighted value: 0.0295, policy: 0.1423, weighted policy: 0.1423), Train Mean Max: 0.9398
Epoch 6/10, Train Loss: 0.1709 (value: 0.0006, weighted value: 0.0284, policy: 0.1425, weighted policy: 0.1425), Train Mean Max: 0.9398
Epoch 7/10, Train Loss: 0.1704 (value: 0.0006, weighted value: 0.0278, policy: 0.1425, weighted policy: 0.1425), Train Mean Max: 0.9400
Epoch 8/10, Train Loss: 0.1695 (value: 0.0005, weighted value: 0.0272, policy: 0.1423, weighted policy: 0.1423), Train Mean Max: 0.9401
Epoch 9/10, Train Loss: 0.1680 (value: 0.0005, weighted value: 0.0261, policy: 0.1419, weighted policy: 0.1419), Train Mean Max: 0.9402
Epoch 10/10, Train Loss: 0.1682 (value: 0.0005, weighted value: 0.0257, policy: 0.1424, weighted policy: 0.1424), Train Mean Max: 0.9403
..training done in 70.96 seconds
..evaluation done in 19.26 seconds
Old network+MCTS average reward: 0.7530, min: 0.2593, max: 1.7685, stdev: 0.2371
New network+MCTS average reward: 0.7522, min: 0.2593, max: 1.7685, stdev: 0.2342
Old bare network average reward: 0.7264, min: 0.2593, max: 1.7963, stdev: 0.2341
New bare network average reward: 0.7284, min: 0.2593, max: 1.7037, stdev: 0.2357
External policy "random" average reward: 0.2483, min: -0.3519, max: 1.0833, stdev: 0.2415
External policy "individual greedy" average reward: 0.5132, min: -0.2130, max: 1.3704, stdev: 0.2375
External policy "total greedy" average reward: 0.6327, min: 0.0093, max: 1.5556, stdev: 0.2253
New network won 61 and tied 174 out of 300 games (49.33% wins where ties are half wins)
Reverting to the old network

Training iteration 796 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.66 seconds
Training examples lengths: [64519, 64988, 64496, 64539, 64856, 64518, 64710, 64969, 64588, 64950]
Total value: 505296.59
Training on 647133 examples
Training with 316 batches of size 2048
Epoch 1/10, Train Loss: 0.2056 (value: 0.0009, weighted value: 0.0466, policy: 0.1590, weighted policy: 0.1590), Train Mean Max: 0.9370
Epoch 2/10, Train Loss: 0.1934 (value: 0.0008, weighted value: 0.0410, policy: 0.1524, weighted policy: 0.1524), Train Mean Max: 0.9381
Epoch 3/10, Train Loss: 0.1851 (value: 0.0007, weighted value: 0.0374, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9387
Epoch 4/10, Train Loss: 0.1808 (value: 0.0007, weighted value: 0.0357, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9389
Epoch 5/10, Train Loss: 0.1773 (value: 0.0007, weighted value: 0.0333, policy: 0.1441, weighted policy: 0.1441), Train Mean Max: 0.9390
Epoch 6/10, Train Loss: 0.1754 (value: 0.0006, weighted value: 0.0320, policy: 0.1434, weighted policy: 0.1434), Train Mean Max: 0.9392
Epoch 7/10, Train Loss: 0.1747 (value: 0.0006, weighted value: 0.0312, policy: 0.1435, weighted policy: 0.1435), Train Mean Max: 0.9393
Epoch 8/10, Train Loss: 0.1734 (value: 0.0006, weighted value: 0.0304, policy: 0.1430, weighted policy: 0.1430), Train Mean Max: 0.9397
Epoch 9/10, Train Loss: 0.1715 (value: 0.0006, weighted value: 0.0287, policy: 0.1428, weighted policy: 0.1428), Train Mean Max: 0.9398
Epoch 10/10, Train Loss: 0.1710 (value: 0.0006, weighted value: 0.0285, policy: 0.1424, weighted policy: 0.1424), Train Mean Max: 0.9399
..training done in 63.81 seconds
..evaluation done in 18.99 seconds
Old network+MCTS average reward: 0.7803, min: 0.1019, max: 1.4074, stdev: 0.2319
New network+MCTS average reward: 0.7772, min: 0.1019, max: 1.4074, stdev: 0.2318
Old bare network average reward: 0.7525, min: 0.1019, max: 1.4074, stdev: 0.2374
New bare network average reward: 0.7523, min: 0.1019, max: 1.4074, stdev: 0.2391
External policy "random" average reward: 0.2894, min: -0.3426, max: 0.9074, stdev: 0.2271
External policy "individual greedy" average reward: 0.5531, min: -0.0185, max: 1.2778, stdev: 0.2237
External policy "total greedy" average reward: 0.6644, min: 0.1204, max: 1.3796, stdev: 0.2200
New network won 51 and tied 181 out of 300 games (47.17% wins where ties are half wins)
Reverting to the old network

Training iteration 797 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 57.57 seconds
Training examples lengths: [64988, 64496, 64539, 64856, 64518, 64710, 64969, 64588, 64950, 64851]
Total value: 505659.84
Training on 647465 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2218 (value: 0.0011, weighted value: 0.0564, policy: 0.1654, weighted policy: 0.1654), Train Mean Max: 0.9357
Epoch 2/10, Train Loss: 0.2045 (value: 0.0010, weighted value: 0.0476, policy: 0.1569, weighted policy: 0.1569), Train Mean Max: 0.9369
Epoch 3/10, Train Loss: 0.1938 (value: 0.0009, weighted value: 0.0430, policy: 0.1508, weighted policy: 0.1508), Train Mean Max: 0.9376
Epoch 4/10, Train Loss: 0.1880 (value: 0.0008, weighted value: 0.0398, policy: 0.1483, weighted policy: 0.1483), Train Mean Max: 0.9380
Epoch 5/10, Train Loss: 0.1834 (value: 0.0008, weighted value: 0.0376, policy: 0.1457, weighted policy: 0.1457), Train Mean Max: 0.9383
Epoch 6/10, Train Loss: 0.1809 (value: 0.0007, weighted value: 0.0362, policy: 0.1447, weighted policy: 0.1447), Train Mean Max: 0.9385
Epoch 7/10, Train Loss: 0.1800 (value: 0.0007, weighted value: 0.0346, policy: 0.1453, weighted policy: 0.1453), Train Mean Max: 0.9385
Epoch 8/10, Train Loss: 0.1768 (value: 0.0006, weighted value: 0.0323, policy: 0.1445, weighted policy: 0.1445), Train Mean Max: 0.9391
Epoch 9/10, Train Loss: 0.1763 (value: 0.0006, weighted value: 0.0322, policy: 0.1442, weighted policy: 0.1442), Train Mean Max: 0.9390
Epoch 10/10, Train Loss: 0.1744 (value: 0.0006, weighted value: 0.0310, policy: 0.1434, weighted policy: 0.1434), Train Mean Max: 0.9392
..training done in 69.94 seconds
..evaluation done in 19.29 seconds
Old network+MCTS average reward: 0.7672, min: 0.1296, max: 1.5000, stdev: 0.2336
New network+MCTS average reward: 0.7664, min: 0.1296, max: 1.5000, stdev: 0.2321
Old bare network average reward: 0.7355, min: 0.0556, max: 1.5000, stdev: 0.2340
New bare network average reward: 0.7373, min: 0.0556, max: 1.4815, stdev: 0.2338
External policy "random" average reward: 0.2665, min: -0.3796, max: 0.8426, stdev: 0.2250
External policy "individual greedy" average reward: 0.5480, min: 0.0556, max: 1.3148, stdev: 0.2182
External policy "total greedy" average reward: 0.6516, min: 0.0926, max: 1.3241, stdev: 0.2143
New network won 60 and tied 167 out of 300 games (47.83% wins where ties are half wins)
Reverting to the old network

Training iteration 798 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 59.12 seconds
Training examples lengths: [64496, 64539, 64856, 64518, 64710, 64969, 64588, 64950, 64851, 64694]
Total value: 505404.85
Training on 647171 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2369 (value: 0.0013, weighted value: 0.0661, policy: 0.1708, weighted policy: 0.1708), Train Mean Max: 0.9345
Epoch 2/10, Train Loss: 0.2347 (value: 0.0014, weighted value: 0.0716, policy: 0.1630, weighted policy: 0.1630), Train Mean Max: 0.9349
Epoch 3/10, Train Loss: 0.2120 (value: 0.0010, weighted value: 0.0492, policy: 0.1628, weighted policy: 0.1628), Train Mean Max: 0.9361
Epoch 4/10, Train Loss: 0.2027 (value: 0.0010, weighted value: 0.0484, policy: 0.1543, weighted policy: 0.1543), Train Mean Max: 0.9366
Epoch 5/10, Train Loss: 0.2233 (value: 0.0010, weighted value: 0.0512, policy: 0.1721, weighted policy: 0.1721), Train Mean Max: 0.9358
Epoch 6/10, Train Loss: 0.1878 (value: 0.0008, weighted value: 0.0404, policy: 0.1473, weighted policy: 0.1473), Train Mean Max: 0.9379
Epoch 7/10, Train Loss: 0.1901 (value: 0.0009, weighted value: 0.0445, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9377
Epoch 8/10, Train Loss: 0.1966 (value: 0.0008, weighted value: 0.0393, policy: 0.1573, weighted policy: 0.1573), Train Mean Max: 0.9373
Epoch 9/10, Train Loss: 0.1811 (value: 0.0007, weighted value: 0.0358, policy: 0.1452, weighted policy: 0.1452), Train Mean Max: 0.9384
Epoch 10/10, Train Loss: 0.1876 (value: 0.0009, weighted value: 0.0437, policy: 0.1439, weighted policy: 0.1439), Train Mean Max: 0.9377
..training done in 58.76 seconds
..evaluation done in 19.31 seconds
Old network+MCTS average reward: 0.7797, min: 0.2130, max: 1.5741, stdev: 0.2318
New network+MCTS average reward: 0.7768, min: 0.1667, max: 1.5741, stdev: 0.2316
Old bare network average reward: 0.7557, min: 0.1667, max: 1.5741, stdev: 0.2357
New bare network average reward: 0.7527, min: 0.1667, max: 1.5741, stdev: 0.2342
External policy "random" average reward: 0.2726, min: -0.3611, max: 0.9815, stdev: 0.2379
External policy "individual greedy" average reward: 0.5521, min: -0.0185, max: 1.4074, stdev: 0.2374
External policy "total greedy" average reward: 0.6579, min: 0.0278, max: 1.4352, stdev: 0.2289
New network won 78 and tied 135 out of 300 games (48.50% wins where ties are half wins)
Reverting to the old network

Training iteration 799 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.01 seconds
Training examples lengths: [64539, 64856, 64518, 64710, 64969, 64588, 64950, 64851, 64694, 64821]
Total value: 506071.77
Training on 647496 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.2510 (value: 0.0015, weighted value: 0.0739, policy: 0.1771, weighted policy: 0.1771), Train Mean Max: 0.9331
Epoch 2/10, Train Loss: 0.2233 (value: 0.0012, weighted value: 0.0603, policy: 0.1631, weighted policy: 0.1631), Train Mean Max: 0.9349
Epoch 3/10, Train Loss: 0.2090 (value: 0.0011, weighted value: 0.0529, policy: 0.1560, weighted policy: 0.1560), Train Mean Max: 0.9357
Epoch 4/10, Train Loss: 0.2009 (value: 0.0010, weighted value: 0.0488, policy: 0.1520, weighted policy: 0.1520), Train Mean Max: 0.9361
Epoch 5/10, Train Loss: 0.1967 (value: 0.0009, weighted value: 0.0463, policy: 0.1504, weighted policy: 0.1504), Train Mean Max: 0.9365
Epoch 6/10, Train Loss: 0.1889 (value: 0.0008, weighted value: 0.0413, policy: 0.1476, weighted policy: 0.1476), Train Mean Max: 0.9369
Epoch 7/10, Train Loss: 0.1876 (value: 0.0008, weighted value: 0.0407, policy: 0.1470, weighted policy: 0.1470), Train Mean Max: 0.9371
Epoch 8/10, Train Loss: 0.1850 (value: 0.0008, weighted value: 0.0379, policy: 0.1471, weighted policy: 0.1471), Train Mean Max: 0.9374
Epoch 9/10, Train Loss: 0.1833 (value: 0.0007, weighted value: 0.0365, policy: 0.1468, weighted policy: 0.1468), Train Mean Max: 0.9377
Epoch 10/10, Train Loss: 0.1808 (value: 0.0007, weighted value: 0.0345, policy: 0.1463, weighted policy: 0.1463), Train Mean Max: 0.9379
..training done in 59.59 seconds
..evaluation done in 18.36 seconds
Old network+MCTS average reward: 0.7693, min: 0.1574, max: 1.4907, stdev: 0.2428
New network+MCTS average reward: 0.7710, min: 0.2407, max: 1.4907, stdev: 0.2435
Old bare network average reward: 0.7416, min: 0.1574, max: 1.4907, stdev: 0.2494
New bare network average reward: 0.7416, min: 0.1944, max: 1.4907, stdev: 0.2493
External policy "random" average reward: 0.2381, min: -0.4537, max: 0.9815, stdev: 0.2255
External policy "individual greedy" average reward: 0.5339, min: 0.0463, max: 1.2685, stdev: 0.2285
External policy "total greedy" average reward: 0.6521, min: 0.0833, max: 1.3426, stdev: 0.2258
New network won 72 and tied 179 out of 300 games (53.83% wins where ties are half wins)
Keeping the new network

Training iteration 800 of 1000: will play 3000 games, train, and evaluate on 300 games
..games done in 58.80 seconds
Training examples lengths: [64856, 64518, 64710, 64969, 64588, 64950, 64851, 64694, 64821, 64802]
Total value: 505799.10
Training on 647759 examples
Training with 317 batches of size 2048
Epoch 1/10, Train Loss: 0.1978 (value: 0.0009, weighted value: 0.0439, policy: 0.1539, weighted policy: 0.1539), Train Mean Max: 0.9367
Epoch 2/10, Train Loss: 0.1879 (value: 0.0008, weighted value: 0.0384, policy: 0.1495, weighted policy: 0.1495), Train Mean Max: 0.9375
Epoch 3/10, Train Loss: 0.1845 (value: 0.0007, weighted value: 0.0368, policy: 0.1477, weighted policy: 0.1477), Train Mean Max: 0.9377
Epoch 4/10, Train Loss: 0.1811 (value: 0.0007, weighted value: 0.0355, policy: 0.1456, weighted policy: 0.1456), Train Mean Max: 0.9380
Epoch 5/10, Train Loss: 0.1787 (value: 0.0007, weighted value: 0.0336, policy: 0.1451, weighted policy: 0.1451), Train Mean Max: 0.9384
Epoch 6/10, Train Loss: 0.1768 (value: 0.0006, weighted value: 0.0317, policy: 0.1450, weighted policy: 0.1450), Train Mean Max: 0.9384
Epoch 7/10, Train Loss: 0.1763 (value: 0.0006, weighted value: 0.0308, policy: 0.1454, weighted policy: 0.1454), Train Mean Max: 0.9386
Epoch 8/10, Train Loss: 0.1755 (value: 0.0006, weighted value: 0.0312, policy: 0.1443, weighted policy: 0.1443), Train Mean Max: 0.9388
Epoch 9/10, Train Loss: 0.1739 (value: 0.0006, weighted value: 0.0293, policy: 0.1446, weighted policy: 0.1446), Train Mean Max: 0.9389
Epoch 10/10, Train Loss: 0.1728 (value: 0.0006, weighted value: 0.0288, policy: 0.1440, weighted policy: 0.1440), Train Mean Max: 0.9391
..training done in 70.28 seconds
..evaluation done in 20.62 seconds
Old network+MCTS average reward: 0.8077, min: 0.1389, max: 1.5093, stdev: 0.2332
New network+MCTS average reward: 0.8063, min: 0.1389, max: 1.5093, stdev: 0.2350
Old bare network average reward: 0.7781, min: 0.0185, max: 1.4815, stdev: 0.2383
New bare network average reward: 0.7786, min: 0.0000, max: 1.4815, stdev: 0.2388
External policy "random" average reward: 0.2859, min: -0.3889, max: 0.9630, stdev: 0.2234
External policy "individual greedy" average reward: 0.5848, min: -0.0093, max: 1.2593, stdev: 0.2276
External policy "total greedy" average reward: 0.6931, min: 0.0463, max: 1.2870, stdev: 0.2204
New network won 59 and tied 170 out of 300 games (48.00% wins where ties are half wins)
Reverting to the old network
Saving intermediate checkpoint to /scratch/gkonars/NSAI/zgaz/zgaz_checkpoints/1768522711_iter_800
