{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21170495",
   "metadata": {},
   "source": [
    "# Cartpole AlphaZero Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0efbd9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network training will occur on device 'mps'\n",
      "RNG seeds are fully specified\n",
      "Training iteration 1 of 5: will play 100 games, train, and evaluate on 20 games\n",
      "..games done in 4.49 seconds\n",
      "Training on 2266 examples\n",
      "Training with 71 batches of size 32\n",
      "Epoch 1/10, Train Loss: 1.4119 (value: 0.0334, policy: 0.6893, weighted policy: 1.3785)\n",
      "Epoch 10/10, Train Loss: 1.3859 (value: 0.0208, policy: 0.6826, weighted policy: 1.3651)\n",
      "..training done in 7.98 seconds\n",
      "..evaluation done in 3.74 seconds\n",
      "Old network average reward: 0.19, min: 0.11, max: 0.40, stdev: 0.08\n",
      "New network average reward: 0.25, min: 0.10, max: 1.00, stdev: 0.21\n",
      "New network won 8 and tied 8 out of 20 games (60.00% wins where ties are half wins)\n",
      "Keeping the new network\n",
      "Training iteration 2 of 5: will play 100 games, train, and evaluate on 20 games\n",
      "..games done in 8.11 seconds\n",
      "Training on 5903 examples\n",
      "Training with 185 batches of size 32\n",
      "Epoch 1/10, Train Loss: 1.4080 (value: 0.0490, policy: 0.6795, weighted policy: 1.3590)\n",
      "Epoch 10/10, Train Loss: 1.3884 (value: 0.0406, policy: 0.6739, weighted policy: 1.3478)\n",
      "..training done in 17.09 seconds\n",
      "..evaluation done in 3.86 seconds\n",
      "Old network average reward: 0.35, min: 0.11, max: 0.98, stdev: 0.23\n",
      "New network average reward: 0.64, min: 0.12, max: 1.00, stdev: 0.34\n",
      "New network won 15 and tied 3 out of 20 games (82.50% wins where ties are half wins)\n",
      "Keeping the new network\n",
      "Training iteration 3 of 5: will play 100 games, train, and evaluate on 20 games\n",
      "..games done in 7.83 seconds\n",
      "Training on 11846 examples\n",
      "Training with 371 batches of size 32\n",
      "Epoch 1/10, Train Loss: 1.3774 (value: 0.0577, policy: 0.6599, weighted policy: 1.3198)\n",
      "Epoch 10/10, Train Loss: 1.3730 (value: 0.0551, policy: 0.6589, weighted policy: 1.3178)\n",
      "..training done in 31.44 seconds\n",
      "..evaluation done in 4.85 seconds\n",
      "Old network average reward: 0.58, min: 0.14, max: 1.00, stdev: 0.33\n",
      "New network average reward: 0.86, min: 0.14, max: 1.00, stdev: 0.25\n",
      "New network won 15 and tied 5 out of 20 games (87.50% wins where ties are half wins)\n",
      "Keeping the new network\n",
      "Training iteration 4 of 5: will play 100 games, train, and evaluate on 20 games\n",
      "..games done in 11.58 seconds\n",
      "Training on 20576 examples\n",
      "Training with 643 batches of size 32\n",
      "Epoch 1/10, Train Loss: 1.3473 (value: 0.0584, policy: 0.6444, weighted policy: 1.2889)\n",
      "Epoch 10/10, Train Loss: 1.3454 (value: 0.0577, policy: 0.6439, weighted policy: 1.2877)\n",
      "..training done in 55.54 seconds\n",
      "..evaluation done in 5.38 seconds\n",
      "Old network average reward: 0.90, min: 0.33, max: 1.00, stdev: 0.18\n",
      "New network average reward: 0.92, min: 0.34, max: 1.00, stdev: 0.18\n",
      "New network won 6 and tied 13 out of 20 games (62.50% wins where ties are half wins)\n",
      "Keeping the new network\n",
      "Training iteration 5 of 5: will play 100 games, train, and evaluate on 20 games\n",
      "..games done in 11.73 seconds\n",
      "Training on 29792 examples\n",
      "Training with 931 batches of size 32\n",
      "Epoch 1/10, Train Loss: 1.3207 (value: 0.0512, policy: 0.6347, weighted policy: 1.2695)\n",
      "Epoch 10/10, Train Loss: 1.3204 (value: 0.0513, policy: 0.6346, weighted policy: 1.2691)\n",
      "..training done in 76.75 seconds\n",
      "..evaluation done in 5.33 seconds\n",
      "Old network average reward: 0.87, min: 0.23, max: 1.00, stdev: 0.25\n",
      "New network average reward: 1.00, min: 0.96, max: 1.00, stdev: 0.01\n",
      "New network won 5 and tied 15 out of 20 games (62.50% wins where ties are half wins)\n",
      "Keeping the new network\n"
     ]
    }
   ],
   "source": [
    "from nsai_experiments.general_az_1p.utils import disable_numpy_multithreading, use_deterministic_cuda\n",
    "disable_numpy_multithreading()\n",
    "use_deterministic_cuda()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nsai_experiments.general_az_1p.agent import Agent\n",
    "\n",
    "from nsai_experiments.general_az_1p.cartpole.cartpole_az_impl import CartPoleGame\n",
    "from nsai_experiments.general_az_1p.cartpole.cartpole_az_impl import CartPolePolicyValueNet\n",
    "\n",
    "mygame = CartPoleGame()\n",
    "# mynet = CartPolePolicyValueNet(random_seed=47, training_params={\"epochs\": 10, \"learning_rate\": 0.01, \"policy_weight\": 4.0})\n",
    "mynet = CartPolePolicyValueNet(random_seed=47, training_params={\"epochs\": 10, \"learning_rate\": 0.01, \"policy_weight\": 2.0})\n",
    "# myagent = Agent(mygame, mynet, random_seeds={\"mcts\": 48, \"train\": 49, \"eval\": 50}, threshold_to_keep=-1.0, n_games_per_eval=1, mcts_params={\"n_simulations\": 5})\n",
    "# myagent = Agent(mygame, mynet, random_seeds={\"mcts\": 48, \"train\": 49, \"eval\": 50}, threshold_to_keep=-1.0, n_procs=-1)\n",
    "# myagent = Agent(mygame, mynet, random_seeds={\"mcts\": 48, \"train\": 49, \"eval\": 50}, threshold_to_keep=-1.0)\n",
    "myagent = Agent(mygame, mynet, random_seeds={\"mcts\": 48, \"train\": 49, \"eval\": 50})\n",
    "\n",
    "\n",
    "myagent.play_train_multiple(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3689d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
