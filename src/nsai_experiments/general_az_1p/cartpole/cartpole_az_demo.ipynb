{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21170495",
   "metadata": {},
   "source": [
    "# Cartpole AlphaZero Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0efbd9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network training will occur on device 'mps'\n",
      "RNG seeds are fully specified\n",
      "Training iteration 1 of 1: will play 100 games, train, and evaluate on 20 games\n",
      "..games done in 11.39 seconds\n",
      "Training on 1701 examples\n",
      "Training with 54 batches of size 32\n",
      "Epoch 1/10, Train Loss: 2.8198 (value: 0.0652, policy: 0.6887, weighted policy: 2.7546)\n",
      "Epoch 2/10, Train Loss: 2.7709 (value: 0.0251, policy: 0.6865, weighted policy: 2.7458)\n",
      "Epoch 3/10, Train Loss: 2.7635 (value: 0.0246, policy: 0.6847, weighted policy: 2.7389)\n",
      "Epoch 4/10, Train Loss: 2.7654 (value: 0.0257, policy: 0.6849, weighted policy: 2.7396)\n",
      "Epoch 5/10, Train Loss: 2.7561 (value: 0.0215, policy: 0.6836, weighted policy: 2.7346)\n",
      "Epoch 6/10, Train Loss: 2.7530 (value: 0.0230, policy: 0.6825, weighted policy: 2.7299)\n",
      "Epoch 7/10, Train Loss: 2.7506 (value: 0.0235, policy: 0.6818, weighted policy: 2.7271)\n",
      "Epoch 8/10, Train Loss: 2.7503 (value: 0.0212, policy: 0.6823, weighted policy: 2.7291)\n",
      "Epoch 9/10, Train Loss: 2.7549 (value: 0.0235, policy: 0.6829, weighted policy: 2.7315)\n",
      "Epoch 10/10, Train Loss: 2.7480 (value: 0.0236, policy: 0.6811, weighted policy: 2.7244)\n",
      "..training done in 6.44 seconds\n",
      "pred on old [ 0.03201292  0.0423298   0.00057171 -0.0489544 ] (array([0.5288325 , 0.47116745], dtype=float32), array(-0.10375415, dtype=float32))\n",
      "pred on new [ 0.03201292  0.0423298   0.00057171 -0.0489544 ] (array([0.5428241 , 0.45717594], dtype=float32), array(0.8236929, dtype=float32))\n",
      "Reward from old: 0.55 Reward from new: 0.6\n",
      "Reward from old: 1.0 Reward from new: 1.0\n",
      "Reward from old: 0.6 Reward from new: 1.0\n",
      "Reward from old: 0.7 Reward from new: 0.75\n",
      "Reward from old: 0.9 Reward from new: 0.95\n",
      "Reward from old: 1.0 Reward from new: 1.0\n",
      "Reward from old: 0.85 Reward from new: 1.0\n",
      "Reward from old: 1.0 Reward from new: 1.0\n",
      "Reward from old: 0.55 Reward from new: 0.55\n",
      "Reward from old: 0.6 Reward from new: 0.65\n",
      "Reward from old: 0.9 Reward from new: 1.0\n",
      "Reward from old: 0.7 Reward from new: 0.9\n",
      "Reward from old: 0.85 Reward from new: 1.0\n",
      "Reward from old: 1.0 Reward from new: 0.95\n",
      "Reward from old: 0.9 Reward from new: 1.0\n",
      "Reward from old: 1.0 Reward from new: 1.0\n",
      "Reward from old: 0.6 Reward from new: 0.55\n",
      "Reward from old: 0.7 Reward from new: 0.75\n",
      "Reward from old: 0.65 Reward from new: 0.65\n",
      "Reward from old: 1.0 Reward from new: 1.0\n",
      "..evaluation done in 4.56 seconds\n",
      "Old network average reward: 0.80, min: 0.55, max: 1.00, stdev: 0.17\n",
      "New network average reward: 0.86, min: 0.55, max: 1.00, stdev: 0.17\n",
      "New network won 11 and tied 7 out of 20 games (72.50% wins where ties are half wins)\n",
      "Keeping the new network\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from nsai_experiments.general_az_1p.agent import Agent\n",
    "\n",
    "from nsai_experiments.general_az_1p.cartpole.cartpole_az_impl import CartPoleGame\n",
    "from nsai_experiments.general_az_1p.cartpole.cartpole_az_impl import CartPolePolicyValueNet\n",
    "\n",
    "mygame = CartPoleGame()\n",
    "mynet = CartPolePolicyValueNet(random_seed=47, training_params={\"epochs\": 10, \"learning_rate\": 0.01, \"policy_weight\": 4.0})\n",
    "# myagent = Agent(mygame, mynet, random_seeds={\"mcts\": 48, \"train\": 49, \"eval\": 50}, threshold_to_keep=-1.0, n_games_per_eval=1, mcts_params={\"n_simulations\": 5})\n",
    "myagent = Agent(mygame, mynet, random_seeds={\"mcts\": 48, \"train\": 49, \"eval\": 50}, threshold_to_keep=-1.0, n_procs=-1)\n",
    "# myagent = Agent(mygame, mynet, random_seeds={\"mcts\": 48, \"train\": 49, \"eval\": 50}, threshold_to_keep=-1.0)\n",
    "\n",
    "\n",
    "myagent.play_train_multiple(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3689d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
