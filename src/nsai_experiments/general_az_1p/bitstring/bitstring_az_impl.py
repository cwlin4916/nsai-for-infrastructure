import gymnasium as gym
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

from nsai_experiments.general_az_1p.game import EnvGame
from nsai_experiments.general_az_1p.policy_value_net import TorchPolicyValueNet
from nsai_experiments.general_az_1p.utils import get_accelerator
from nsai_experiments.general_az_1p.agent import Agent
from nsai_experiments.general_az_1p.mcts import MCTS, entab

import gymnasium.spaces as spaces
from typing import Hashable
import warnings
from copy import deepcopy



class CumulativeRewardWrapper(gym.Wrapper):
    """Wrapper that changes reward behavior: 0 at every step, total steps at termination."""
    
    def __init__(self, env):
        super().__init__(env)
        self.step_count = 0
        self.max_steps = env.max_steps
        self.nsites = env.nsites  # Store the number of sites for later use
    
    def reset(self, **kwargs):
        self.step_count = 0
        return self.env.reset(**kwargs)
    
    def step(self, action):
        observation, reward, terminated, truncated, info = self.env.step(action)
        self.step_count += 1
        if self.step_count >= self.max_steps:
            truncated = True
        
        # Give 0 reward during the episode, final reward at termination
        if terminated or truncated:
            print(f"AM TERMINATED {terminated} OR TRUNCATED {truncated}")
            reward = self.step_count / self.max_steps
        else:
            reward = 0

        # assert reward == 0.0 or (terminated or truncated)
        # assert reward <= 1.0
            
        return observation, reward, terminated, truncated, info

class BitStringGameGym(gym.Env):
    metadata = {'render.modes': ['human']}

    def __init__(self, nsites=10):
        """
        Every environment should be derived from gym.Env and at least contain the variables observation_space and action_space 
        specifying the type of possible observations and actions using spaces.Box or spaces.Discrete.

        Example:
        >>> EnvTest = FooEnv()
        >>> EnvTest.observation_space=spaces.Box(low=-1, high=1, shape=(3,4))
        >>> EnvTest.action_space=spaces.Discrete(2)
        """
        self.bitflipmode = True  # "setting" a 1 flips it  back to 0
        self.sparsemode = True  # score is only given at end of (fixed length?) episode
        self.nones = 2 # number of bits that are initially set to 1

        self.nsites = nsites
        self.max_steps = 2 * nsites if not self.sparsemode else nsites - self.nones
        self.observation_space = spaces.MultiBinary([self.nsites]) #, seed=42)
        self.action_space = spaces.Discrete(self.nsites)
        self.reset()

    def step(self, action):
        """
        This method is the primary interface between environment and agent.

        Paramters: 
            action: int
                    the index of the respective action (if action space is discrete)

        Returns:
            output: (array, float, bool, dict)
                    information provided by the environment about its current state:
                    (observation, reward, done, info)
        """
#        print ("prestep state <a, s, s(a)>", action, self.state, self.state[action])
        self.step_count += 1
        done = self.step_count >= self.max_steps
        r = -1 - self.bitflipmode  # why bitflipmode affect this ?
        r = -1 

        if action == -1:  # this is being used as a flag for a bad action (e.g., generated by garbage rule)
            return self.state, r, done, {}

        if self.state[action] == 0:
            r = 1
        if self.bitflipmode:
            self.state[action] = 1 - self.state[action]
        else:
            self.state[action] = 1
        done = done or sum(self.state) == self.nsites

        normalizer = self.nsites # playing around with scale of pi vs value loss
        if self.sparsemode:
            if done:
                filledness = sum(self.state) / normalizer  # 0 if all 0s, 1 if all 1s
                # turn_inefficiency = self.step_count / (normalizer-self.nones)  # 1 if did it in minimum number of turns, 2 if took 2x as long, etc.
                # print(self.step_count)
                # r = filledness/turn_inefficiency  # 1 if filledness=1 and turn_inefficiency=1, decreases from there
                r = filledness
                # print("state", self.state, "r", r, flush=True)
            else:
                r = 0
        # if done:
        #     print ("Net Episode done <s, r, t, steps>", self.state, r, done, self.step_count)
        return self.state, r, done, done, {}

    def reset(self, seed = None):
        """
        This method resets the environment to its initial values.

        Returns:
            observation:    array
                            the initial state of the environment
        """
        if seed is not None:
            np.random.seed(seed)
            torch.manual_seed(seed)
            torch.use_deterministic_algorithms(True, warn_only=True)
        ones = np.random.choice(range(self.nsites), self.nones, replace=False)
        self.state = np.zeros(self.nsites, dtype=np.float32)
        self.state[ones] = 1
        self.step_count = 0
        return self.state, {}


class BitStringGame(EnvGame):
    def __init__(self, use_cumulative_reward_rescale=True, **kwargs):
        env = BitStringGameGym(**kwargs)
        # if use_cumulative_reward_rescale:
        #     env = CumulativeRewardWrapper(env)
        super().__init__(env)
        self._action_mask = np.ones(env.nsites)  # all actions are always available, otherwise it's cheating.
    
    def get_action_mask(self):
        return self._action_mask
    
    @property
    def hashable_obs(self) -> Hashable:
        "Returns a hashable representation of the current observation `obs`."
        return "".join([str(int(x)) for x in self.obs])  + " " + str(self.env.step_count)# Convert the bitstring to a string of '0's and '1's, which is hashable

class BitStringModel(nn.Module):
    def __init__(self, nsites = 10, n_hidden_layers = 2, hidden_size = 128):
        super().__init__()
        self.input_size = nsites   # observation is a bitstring of length nsites
        self.action_size = nsites  # action is an index of a bit to flip
        self.body = nn.Sequential(
            nn.Sequential(nn.Linear(self.input_size, hidden_size), nn.ReLU()),
            *[nn.Sequential(nn.Linear(hidden_size, hidden_size), nn.ReLU()) for _ in range(n_hidden_layers)],
        )
        self.policy_head = nn.Linear(hidden_size, nsites)  # thinking of it as logits for action of filling a bit
        self.value_head = nn.Linear(hidden_size, 1)

    def forward(self, x):
        x = self.body(x)
        policy = self.policy_head(x)
        value = self.value_head(x).squeeze(-1)
        return policy, value

class BitStringPolicyValueNet(TorchPolicyValueNet):
    save_file_name = "bitstring_checkpoint.pt"
    default_training_params = {
        "epochs": 10,
        "batch_size": 32,
        "learning_rate": 0.001,
        "weight_decay": 1e-4,
        "policy_weight": 1.0,
    }

    def __init__(self, random_seed = None, nsites = 10, n_hidden_layers = 2, hidden_size = 128, training_params = {}, device = None):
        if random_seed is not None:
            torch.manual_seed(random_seed)
            torch.use_deterministic_algorithms(True, warn_only=True)

        model = BitStringModel(nsites, n_hidden_layers=n_hidden_layers, hidden_size=hidden_size)
        self.nsites = nsites
        super().__init__(model)
        self.training_params = self.default_training_params | training_params
        self.DEVICE = get_accelerator() if device is None else device
        print(f"Neural network training will occur on device '{self.DEVICE}'")
        
    def train(self, examples, needs_reshape=True, print_all_epochs=False):
        model = self.model
        model.to(self.DEVICE)
        tp = self.training_params
        policy_weight = tp["policy_weight"]

        model = model.to(self.DEVICE)
        criterion_value = nn.MSELoss()
        criterion_policy = nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=tp["learning_rate"], weight_decay=tp["weight_decay"])

        # The usual case is that `examples` comes from AlphaZero and is a list of tuples
        # that will need to be reformatted. For convenience, we also support the case where
        # `examples` is already in the proper format, perhaps because the network is being
        # tested outside the AlphaZero context; for this, pass `needs_reshape=False`.
        if needs_reshape:
            # PERF we could use a single Python loop for all three of these
            states = torch.from_numpy(np.array([state for state, (_, _) in examples], dtype=np.float32))
            policies = torch.from_numpy(np.array([policy for _, (policy, _) in examples], dtype=np.float32))
            values = torch.from_numpy(np.array([value for _, (_, value) in examples], dtype=np.float32))
            dataset = torch.utils.data.TensorDataset(states, policies, values)
        else:
            print("Skipping reshape of `examples`.")
            dataset = examples
        
        train_loader = torch.utils.data.DataLoader(dataset, batch_size=tp["batch_size"], shuffle=True)
        print(f"Training with {len(train_loader)} batches of size {tp['batch_size']}")

        train_mini_losses = []
        train_losses = []

        for epoch in range(tp["epochs"]):
            # Training phase
            model.train()
            train_loss = 0.0
            policy_loss = 0.0
            value_loss = 0.0
            for inputs, targets_policy, targets_value in train_loader:
                inputs, targets_value, targets_policy = inputs.to(self.DEVICE), targets_value.to(self.DEVICE), targets_policy.to(self.DEVICE)
                optimizer.zero_grad()
                assert len(inputs.shape) == 2, f"Expected input shape to be 2D, got {inputs.shape}"
                assert inputs.shape[1] == model.input_size, f"Expected input size {model.input_size}, got {inputs.shape[1]}"
                assert len(targets_value.shape) == 1
                assert targets_policy.shape[1] == model.action_size, f"Expected policy answers size {model.action_size}, got {targets_policy.shape[1]}"
                outputs_policy, outputs_value = model(inputs)
                assert outputs_value.shape == targets_value.shape, f"Expected predicted value shape {targets_value.shape}, got {outputs_value.shape}"
                loss_value = criterion_value(outputs_value, targets_value)
                assert outputs_policy.shape == targets_policy.shape, f"Expected predicted policy shape {targets_policy.shape}, got {outputs_policy.shape}"
                loss_policy = criterion_policy(outputs_policy, targets_policy)
                loss = loss_value + policy_weight*loss_policy

                loss.backward()
                optimizer.step()
                loss = loss.item()
                train_mini_losses.append(loss)
                train_loss += loss
                policy_loss += loss_policy
                value_loss += loss_value

            train_losses.append(train_loss / len(train_loader))
            if print_all_epochs or epoch == 0 or epoch == tp["epochs"] - 1:
            # if True:
                print(f"Epoch {epoch+1}/{tp['epochs']}, Train Loss: {train_losses[-1]:.4f} (value: {value_loss / len(train_loader):.4f}, policy: {policy_loss / len(train_loader):.4f}, weighted policy: {policy_weight * (policy_loss / len(train_loader)):.4f})")

        return model, train_mini_losses, train_losses
    
    def predict(self, state):
        self.model.cpu()
        nn_input = torch.tensor(state).reshape(1, -1)
        with torch.no_grad():
            policy, value = self.model(nn_input)
            policy_prob = F.softmax(policy, dim=-1)
        
        policy_prob = policy_prob.numpy()
        policy_prob = policy_prob.squeeze(0)
        assert policy_prob.shape == (self.nsites,)

        value = value.numpy()
        value = value.squeeze(0)
        assert value.shape == ()

        # policy_prob = np.random.random(2)
        return policy_prob, value


class BitStringAgent(Agent):
    """
    This class is a simple agent that plays the BitStringGame perfectly to generate training data.
    """

    def get_exact_move_probs(self, msg: str = "") -> np.ndarray:
        """
        Returns the exact move probabilities for the current game state.
        """
        if msg: print(msg, "Calculating exact move probabilities for", self.game.obs)
        # return the exact move probabilities
        # based on the current game state. 
        probs = np.zeros(self.game.env.nsites, dtype=np.float32)
        for i in range(self.game.env.nsites):
            if self.game.obs[i] == 0:
                probs[i] = 1.0
            else:
                probs[i] = 0.0
        probs /= np.sum(probs)  # Normalize to make it a probability distribution
        if msg: print(msg, "Exact move probabilities:", probs)
        return probs

    def play_single_game(self, max_moves: int = 10_000, random_seed: int | None = None, msg = ""):
        train_examples = []
        rewards = []
        # mcts = MCTS(self.game, self.net, **self.mcts_params)
        rng = np.random.default_rng(random_seed)
        for i in range(max_moves):
            if msg: print(msg, f"starting move {i}")
            # move_probs = mcts.perform_simulations(entab(msg, f", m{i+1}"))
            # self.game = mcts.game  # TODO HACK because MCTS modifies the game state in place
            move_probs = self.get_exact_move_probs(entab(msg, f", m{i+1}"))
            train_examples.append((deepcopy(self.game.obs), (move_probs, None)))
            selected_move = rng.choice(len(move_probs), p=move_probs)
            if msg: print(msg, "obs", self.game.obs, "hobs", self.game.hashable_obs, "move_probs", move_probs, "selmove", selected_move)
            # print(f"Taking move {selected_move} with probability {move_probs[selected_move]:.2f}")  # TODO logging
            self.game.step_wrapper(selected_move)
            rewards.append(self.game.reward)
            if self.game.terminated or self.game.truncated:
                break
        else:
            # In this case, we might not have any reward to work with
            warnings.warn(f"`play_single_game` timed out after {max_moves} moves without termination/truncation, returning no training examples")
            return []

        # Propagate rewards backwards through steps
        for i in range(len(rewards) - 1, 0, -1):
            rewards[i-1] += self.reward_discount * rewards[i]
        
        # Attach rewards to training examples
        for i in range(len(train_examples)):
            state, (policy, _) = train_examples[i]
            train_examples[i] = (state, (policy, rewards[i]))
        
        return train_examples

    

if __name__ == "__main__":
    from nsai_experiments.general_az_1p.utils import disable_numpy_multithreading, use_deterministic_cuda
    disable_numpy_multithreading()
    use_deterministic_cuda()

    import numpy as np

    from nsai_experiments.general_az_1p.agent import Agent

    from nsai_experiments.general_az_1p.bitstring.bitstring_az_impl import BitStringGame
    from nsai_experiments.general_az_1p.bitstring.bitstring_az_impl import BitStringPolicyValueNet

    nsites = 10  # Number of bits in the bitstring
    mygame = BitStringGame(nsites = nsites)
    # mynet = CartPolePolicyValueNet(random_seed=47, training_params={"epochs": 10, "learning_rate": 0.01, "policy_weight": 4.0})
    mynet = BitStringPolicyValueNet(random_seed=47, nsites=nsites, n_hidden_layers=1, training_params={"epochs": 5, "learning_rate": 1e-2, "policy_weight": 2.0})
    # myagent = Agent(mygame, mynet, random_seeds={"mcts": 48, "train": 49, "eval": 50}, threshold_to_keep=-1.0, n_games_per_eval=1, mcts_params={"n_simulations": 5})
    # myagent = Agent(mygame, mynet, random_seeds={"mcts": 48, "train": 49, "eval": 50}, threshold_to_keep=-1.0, n_procs=-1)
    # myagent = Agent(mygame, mynet, random_seeds={"mcts": 48, "train": 49, "eval": 50}, threshold_to_keep=-1.0)
#    myagent = BitStringAgent(mygame, mynet, n_procs=-1, n_games_per_train=50, n_games_per_eval=10, random_seeds={"mcts": 48, "train": 49, "eval": 50}, mcts_params={"c_exploration": 1})
    myagent = Agent(mygame, mynet, n_games_per_train=100, n_games_per_eval=10, threshold_to_keep=0.4,  n_past_iterations_to_train=5,
                    random_seeds={"mcts": 48, "train": 49, "eval": 50}, mcts_params={"n_simulations": 30, "c_exploration": 0.4})


    myagent.play_train_multiple(100)    